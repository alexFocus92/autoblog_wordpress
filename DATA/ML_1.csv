;periodico;url;fecha;titulo;cuerpo
0;machinelearningmastery.com;https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/;2019-06-23;How to use the UpSampling2D and Conv2DTranspose Layers in Keras;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are an architecture for training generative models, such as deep convolutional neural networks for generating images.

The GAN architecture is comprised of both a generator and a discriminator model. The generator is responsible for creating new outputs, such as images, that plausibly could have come from the original dataset. The generator model is typically implemented using a deep convolutional neural network and results-specialized layers that learn to fill in features in an image rather than extract features from an input image.

Two common types of layers that can be used in the generator model are a upsample layer (UpSampling2D) that simply doubles the dimensions of the input and the transpose convolutional layer (Conv2DTranspose) that performs an inverse convolution operation.

In this tutorial, you will discover how to use UpSampling2D and Conv2DTranspose Layers in Generative Adversarial Networks when generating images.

After completing this tutorial, you will know:

Generative models in the GAN architecture are required to upsample input data in order to generate an output image.

The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.

The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Need for Upsampling in GANs

How to Use the Upsampling Layer

How to Use the Transpose Convolutional Layer

Need for Upsampling in Generative Adversarial Networks

Generative Adversarial Networks are an architecture for neural networks for training a generative model.

The architecture is comprised of a generator and a discriminator model, both of which are implemented as a deep convolutional neural network. The discriminator is responsible for classifying images as either real (from the domain) or fake (generated). The generator is responsible for generating new plausible examples from the problem domain.

The generator works by taking a random point from the latent space as input and outputting a complete image, in a one-shot manner.

A traditional convolutional neural network for image classification, and related tasks, will use pooling layers to downsample input images. For example, an average pooling or max pooling layer will reduce the feature maps from a convolutional by half on each dimension, resulting in an output that is one quarter the area of the input.

Convolutional layers themselves also perform a form of downsampling by applying each filter across the input images or feature maps; the resulting activations are an output feature map that is smaller because of the border effects. Often padding is used to counter this effect.

The generator model in a GAN requires an inverse operation of a pooling layer in a traditional convolutional layer. It needs a layer to translate from coarse salient features to a more dense and detailed output.

A simple version of an unpooling or opposite pooling layer is called an upsampling layer. It works by repeating the rows and columns of the input.

A more elaborate approach is to perform a backwards convolutional operation, originally referred to as a deconvolution, which is incorrect, but is more commonly referred to as a fractional convolutional layer or a transposed convolutional layer.

Both of these layers can be used on a GAN to perform the required upsampling operation to transform a small input into a large image output.

In the following sections, we will take a closer look at each and develop an intuition for how they work so that we can use them effectively in our GAN models.

How to Use the UpSampling2D Layer

Perhaps the simplest way to upsample an input is to double each row and column.

For example, an input image with the shape 2×2 would be output as 4×4.

1, 2 Input = (3, 4) 1, 1, 2, 2 Output = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4 1 2 3 4 5 6 7 1, 2 Input = (3, 4) 1, 1, 2, 2 Output = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4

Worked Example Using the UpSampling2D Layer

The Keras deep learning library provides this capability in a layer called UpSampling2D.

It can be added to a convolutional neural network and repeats the rows and columns provided as input in the output. For example:

... # define model model = Sequential() model.add(UpSampling2D()) 1 2 3 4 . . . # define model model = Sequential ( ) model . add ( UpSampling2D ( ) )

We can demonstrate the behavior of this layer with a simple contrived example.

First, we can define a contrived input image that is 2×2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input.

... # define input data X = asarray([[1, 2], [3, 4]]) # show input data for context print(X) 1 2 3 4 5 6 . . . # define input data X = asarray ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) # show input data for context print ( X )

Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.

... # reshape input data into one sample a sample with a channel X = X.reshape((1, 2, 2, 1)) 1 2 3 . . . # reshape input data into one sample a sample with a channel X = X . reshape ( ( 1 , 2 , 2 , 1 ) )

We can now define our model.

The model has only the UpSampling2D layer which takes 2×2 grayscale images as input directly and outputs the result of the upsampling operation.

... # define model model = Sequential() model.add(UpSampling2D(input_shape=(2, 2, 1))) # summarize the model model.summary() 1 2 3 4 5 6 . . . # define model model = Sequential ( ) model . add ( UpSampling2D ( input_shape = ( 2 , 2 , 1 ) ) ) # summarize the model model . summary ( )

We can then use the model to make a prediction, that is upsample a provided input image.

... # make a prediction with the model yhat = model.predict(X) 1 2 3 . . . # make a prediction with the model yhat = model . predict ( X )

The output will have four dimensions, like the input, therefore, we can convert it back to a 2×2 array to make it easier to review the result.

... # reshape output to remove channel to make printing easier yhat = yhat.reshape((4, 4)) # summarize output print(yhat) 1 2 3 4 5 . . . # reshape output to remove channel to make printing easier yhat = yhat . reshape ( ( 4 , 4 ) ) # summarize output print ( yhat )

Tying all of this together, the complete example of using the UpSampling2D layer in Keras is provided below.

# example of using the upsampling layer from numpy import asarray from keras.models import Sequential from keras.layers import UpSampling2D # define input data X = asarray([[1, 2], [3, 4]]) # show input data for context print(X) # reshape input data into one sample a sample with a channel X = X.reshape((1, 2, 2, 1)) # define model model = Sequential() model.add(UpSampling2D(input_shape=(2, 2, 1))) # summarize the model model.summary() # make a prediction with the model yhat = model.predict(X) # reshape output to remove channel to make printing easier yhat = yhat.reshape((4, 4)) # summarize output print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # example of using the upsampling layer from numpy import asarray from keras . models import Sequential from keras . layers import UpSampling2D # define input data X = asarray ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) # show input data for context print ( X ) # reshape input data into one sample a sample with a channel X = X . reshape ( ( 1 , 2 , 2 , 1 ) ) # define model model = Sequential ( ) model . add ( UpSampling2D ( input_shape = ( 2 , 2 , 1 ) ) ) # summarize the model model . summary ( ) # make a prediction with the model yhat = model . predict ( X ) # reshape output to remove channel to make printing easier yhat = yhat . reshape ( ( 4 , 4 ) ) # summarize output print ( yhat )

Running the example first creates and summarizes our 2×2 input data.

Next, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer has no parameters or model weights. This is because it is not learning anything; it is just doubling the input.

Finally, the model is used to upsample our input, resulting in a doubling of each row and column for our input data, as we expected.

[[1 2] [3 4]] _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= up_sampling2d_1 (UpSampling2 (None, 4, 4, 1) 0 ================================================================= Total params: 0 Trainable params: 0 Non-trainable params: 0 _________________________________________________________________ [[1. 1. 2. 2.] [1. 1. 2. 2.] [3. 3. 4. 4.] [3. 3. 4. 4.]] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [[1 2] [3 4]] _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= up_sampling2d_1 (UpSampling2 (None, 4, 4, 1) 0 ================================================================= Total params: 0 Trainable params: 0 Non-trainable params: 0 _________________________________________________________________ [[1. 1. 2. 2.] [1. 1. 2. 2.] [3. 3. 4. 4.] [3. 3. 4. 4.]]

By default, the UpSampling2D will double each input dimension. This is defined by the ‘size‘ argument that is set to the tuple (2,2).

You may want to use different factors on each dimension, such as double the width and triple the height. This could be achieved by setting the ‘size‘ argument to (2, 3). The result of applying this operation to a 2×2 image would be a 4×6 output image (e.g. 2×2 and 2×3). For example:

... # example of using different scale factors for each dimension model.add(UpSampling2D(size=(2, 3))) 1 2 3 . . . # example of using different scale factors for each dimension model . add ( UpSampling2D ( size = ( 2 , 3 ) ) )

Additionally, by default, the UpSampling2D layer will use a nearest neighbor algorithm to fill in the new rows and columns. This has the effect of simply doubling rows and columns, as described and is specified by the ‘interpolation‘ argument set to ‘nearest‘.

Alternately, a bilinear interpolation method can be used which draws upon multiple surrounding points. This can be specified via setting the ‘interpolation‘ argument to ‘bilinear‘. For example:

... # example of using bilinear interpolation when upsampling model.add(UpSampling2D(interpolation='bilinear')) 1 2 3 . . . # example of using bilinear interpolation when upsampling model . add ( UpSampling2D ( interpolation = 'bilinear' ) )

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Simple Generator Model With the UpSampling2D Layer

The UpSampling2D layer is simple and effective, although does not perform any learning.

It is not able to fill in useful detail in the upsampling operation. To be useful in a GAN, each UpSampling2D layer must be followed by a Conv2D layer that will learn to interpret the doubled input and be trained to translate it into meaningful detail.

We can demonstrate this with an example.

In this case, our little GAN generator model must produce a 10×10 image and take a 100 element vector from the latent space as input.

First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image.

... # define model model = Sequential() # define input shape, output enough activations for for 128 5x5 image model.add(Dense(128 * 5 * 5, input_dim=100)) # reshape vector of activations into 128 feature maps with 5x5 model.add(Reshape((5, 5, 128))) 1 2 3 4 5 6 7 . . . # define model model = Sequential ( ) # define input shape, output enough activations for for 128 5x5 image model . add ( Dense ( 128 * 5 * 5 , input_dim = 100 ) ) # reshape vector of activations into 128 feature maps with 5x5 model . add ( Reshape ( ( 5 , 5 , 128 ) ) )

Next, the 5×5 feature maps can be upsampled to a 10×10 feature map.

... # double input from 128 5x5 to 1 10x10 feature map model.add(UpSampling2D()) 1 2 3 . . . # double input from 128 5x5 to 1 10x10 feature map model . add ( UpSampling2D ( ) )

Finally, the upsampled feature maps can be interpreted and filled in with hopefully useful detail by a Conv2D layer.

The Conv2D has a single feature map as output to create the single image we require.

... # fill in detail in the upsampled feature maps model.add(Conv2D(1, (3,3), padding='same')) 1 2 3 . . . # fill in detail in the upsampled feature maps model . add ( Conv2D ( 1 , ( 3 , 3 ) , padding = 'same' ) )

Tying this together, the complete example is listed below.

# example of using upsampling in a simple generator model from keras.models import Sequential from keras.layers import Dense from keras.layers import Reshape from keras.layers import UpSampling2D from keras.layers import Conv2D # define model model = Sequential() # define input shape, output enough activations for for 128 5x5 image model.add(Dense(128 * 5 * 5, input_dim=100)) # reshape vector of activations into 128 feature maps with 5x5 model.add(Reshape((5, 5, 128))) # double input from 128 5x5 to 1 10x10 feature map model.add(UpSampling2D()) # fill in detail in the upsampled feature maps and output a single image model.add(Conv2D(1, (3,3), padding='same')) # summarize model model.summary() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # example of using upsampling in a simple generator model from keras . models import Sequential from keras . layers import Dense from keras . layers import Reshape from keras . layers import UpSampling2D from keras . layers import Conv2D # define model model = Sequential ( ) # define input shape, output enough activations for for 128 5x5 image model . add ( Dense ( 128 * 5 * 5 , input_dim = 100 ) ) # reshape vector of activations into 128 feature maps with 5x5 model . add ( Reshape ( ( 5 , 5 , 128 ) ) ) # double input from 128 5x5 to 1 10x10 feature map model . add ( UpSampling2D ( ) ) # fill in detail in the upsampled feature maps and output a single image model . add ( Conv2D ( 1 , ( 3 , 3 ) , padding = 'same' ) ) # summarize model model . summary ( )

Running the example creates the model and summarizes the output shape of each layer.

We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5.

The widths and heights are doubled to 10×10 by the UpSampling2D layer, resulting in a feature map with quadruple the area.

Finally, the Conv2D processes these feature maps and adds in detail, outputting a single 10×10 image.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 3200) 323200 _________________________________________________________________ reshape_1 (Reshape) (None, 5, 5, 128) 0 _________________________________________________________________ up_sampling2d_1 (UpSampling2 (None, 10, 10, 128) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 10, 10, 1) 1153 ================================================================= Total params: 324,353 Trainable params: 324,353 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 3200) 323200 _________________________________________________________________ reshape_1 (Reshape) (None, 5, 5, 128) 0 _________________________________________________________________ up_sampling2d_1 (UpSampling2 (None, 10, 10, 128) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 10, 10, 1) 1153 ================================================================= Total params: 324,353 Trainable params: 324,353 Non-trainable params: 0 _________________________________________________________________

How to Use the Conv2DTranspose Layer

The Conv2DTranspose or transpose convolutional layer is more complex than a simple upsampling layer.

A simple way to think about it is that it both performs the upsample operation and interprets the coarse input data to fill in the detail while it is upsampling. It is like a layer that combines the UpSampling2D and Conv2D layers into one layer. This is a crude understanding, but a practical starting point.

The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution

— A Guide To Convolution Arithmetic For Deep Learning, 2016.

In fact, the transpose convolutional layer performs an inverse convolution operation.

Specifically, the forward and backward passes of the convolutional layer are reversed.

One way to put it is to note that the kernel defines a convolution, but whether it’s a direct convolution or a transposed convolution is determined by how the forward and backward passes are computed.

— A Guide To Convolution Arithmetic For Deep Learning, 2016.

It is sometimes called a deconvolution or deconvolutional layer and models that use these layers can be referred to as deconvolutional networks, or deconvnets.

A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite.

— Visualizing and Understanding Convolutional Networks, 2013.

Referring to this operation as a deconvolution is technically incorrect as a deconvolution is a specific mathematical operation not performed by this layer.

In fact, the traditional convolutional layer does not technically perform a convolutional operation, it performs a cross-correlation.

The deconvolution layer, to which people commonly refer, first appears in Zeiler’s paper as part of the deconvolutional network but does not have a specific name. […] It also has many names including (but not limited to) sub­pixel or fractional convolutional layer, transposed convolutional layer, inverse, up or backward convolutional layer.

— Is the deconvolution layer the same as a convolutional layer?, 2016.

It is a very flexible layer, although we will focus on its use in the generative models from upsampling an input image.

The transpose convolutional layer is much like a normal convolutional layer. It requires that you specify the number of filters and the kernel size of each filter. The key to the layer is the stride.

Typically, the stride of a convolutional layer is (1×1), that is a filter is moved along one pixel horizontally for each read from left-to-right, then down pixel for the next row of reads. A stride of 2×2 on a normal convolutional layer has the effect of downsampling the input, much like a pooling layer. In fact, a 2×2 stride can be used instead of a pooling layer in the discriminator model.

The transpose convolutional layer is like an inverse convolutional layer. As such, you would intuitively think that a 2×2 stride would upsample the input instead of downsample, which is exactly what happens.

Stride or strides refers to the manner of a filter scanning across an input in a traditional convolutional layer. Whereas, in a transpose convolutional layer, stride refers to the manner in which outputs in the feature map are laid down.

This effect can be implemented with a normal convolutional layer using a fractional input stride (f), e.g. with a stride of f=1/2. When inverted, the output stride is set to the numerator of this fraction, e.g. f=2.

In a sense, upsampling with factor f is convolution with a fractional input stride of 1/f. So long as f is integral, a natural way to upsample is therefore backwards convolution (sometimes called deconvolution) with an output stride of f.

— Fully Convolutional Networks for Semantic Segmentation, 2014.

One way that this effect can be achieved with a normal convolutional layer is by inserting new rows and columns of 0.0 values in the input data.

Finally note that it is always possible to emulate a transposed convolution with a direct convolution. The disadvantage is that it usually involves adding many columns and rows of zeros to the input …

— A Guide To Convolution Arithmetic For Deep Learning, 2016.

Let’s make this concrete with an example.

Consider an input image wit the size 2×2 as follows:

1, 2 Input = (3, 4) 1 2 1, 2 Input = (3, 4)

Assuming a single filter with a 1×1 kernel and model weights that result in no changes to the inputs when output (e.g. a model weight of 1.0 and a bias of 0.0), a transpose convolutional operation with an output stride of 1×1 will reproduce the output as-is:

1, 2 Output = (3, 4) 1 2 1, 2 Output = (3, 4)

With an output stride of (2,2), the 1×1 convolution requires the insertion of additional rows and columns into the input image so that the reads of the operation can be performed. Therefore, the input looks as follows:

1, 0, 2, 0 Input = (0, 0, 0, 0) 3, 0, 4, 0 0, 0, 0, 0 1 2 3 4 1, 0, 2, 0 Input = (0, 0, 0, 0) 3, 0, 4, 0 0, 0, 0, 0

The model can then read across this input using an output stride of (2,2) and will output a 4×4 image, in this case with no change as our model weights have no effect by design:

1, 0, 2, 0 Output = (0, 0, 0, 0) 3, 0, 4, 0 0, 0, 0, 0 1 2 3 4 1, 0, 2, 0 Output = (0, 0, 0, 0) 3, 0, 4, 0 0, 0, 0, 0

Worked Example Using the Conv2DTranspose Layer

Keras provides the transpose convolution capability via the Conv2DTranspose layer.

It can be added to your model directly; for example:

... # define model model = Sequential() model.add(Conv2DTranspose(...)) 1 2 3 4 . . . # define model model = Sequential ( ) model . add ( Conv2DTranspose ( . . . ) )

We can demonstrate the behavior of this layer with a simple contrived example.

First, we can define a contrived input image that is 2×2 pixels, as we did in the previous section. We can use specific values for each pixel so that after the transpose convolutional operation, we can see exactly what effect the operation had on the input.

... # define input data X = asarray([[1, 2], [3, 4]]) # show input data for context print(X) 1 2 3 4 5 6 . . . # define input data X = asarray ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) # show input data for context print ( X )

Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.

... # reshape input data into one sample a sample with a channel X = X.reshape((1, 2, 2, 1)) 1 2 3 . . . # reshape input data into one sample a sample with a channel X = X . reshape ( ( 1 , 2 , 2 , 1 ) )

We can now define our model.

The model has only the Conv2DTranspose layer, which takes 2×2 grayscale images as input directly and outputs the result of the operation.

The Conv2DTranspose both upsamples and performs a convolution. As such, we must specify both the number of filters and the size of the filters as we do for Conv2D layers. Additionally, we must specify a stride of (2,2) because the upsampling is achieved by the stride behavior of the convolution on the input.

Specifying a stride of (2,2) has the effect of spacing out the input. Specifically, rows and columns of 0.0 values are inserted to achieve the desired stride.

In this example, we will use one filter, with a 1×1 kernel and a stride of 2×2 so that the 2×2 input image is upsampled to 4×4.

... # define model model = Sequential() model.add(Conv2DTranspose(1, (1,1), strides=(2,2), input_shape=(2, 2, 1))) # summarize the model model.summary() 1 2 3 4 5 6 . . . # define model model = Sequential ( ) model . add ( Conv2DTranspose ( 1 , ( 1 , 1 ) , strides = ( 2 , 2 ) , input_shape = ( 2 , 2 , 1 ) ) ) # summarize the model model . summary ( )

To make it clear what the Conv2DTranspose layer is doing, we will fix the single weight in the single filter to the value of 1.0 and use a bias value of 0.0.

These weights, along with a kernel size of (1,1) will mean that values in the input will be multiplied by 1 and output as-is, and the 0 values in the new rows and columns added via the stride of 2×2 will be output as 0 (e.g. 1 * 0 in each case).

... # define weights that they do nothing weights = [asarray([[[[1]]]]), asarray([0])] # store the weights in the model model.set_weights(weights) 1 2 3 4 5 . . . # define weights that they do nothing weights = [ asarray ( [ [ [ [ 1 ] ] ] ] ) , asarray ( [ 0 ] ) ] # store the weights in the model model . set_weights ( weights )

We can then use the model to make a prediction, that is upsample a provided input image.

... # make a prediction with the model yhat = model.predict(X) 1 2 3 . . . # make a prediction with the model yhat = model . predict ( X )

The output will have four dimensions, like the input, therefore, we can convert it back to a 2×2 array to make it easier to review the result.

... # reshape output to remove channel to make printing easier yhat = yhat.reshape((4, 4)) # summarize output print(yhat) 1 2 3 4 5 . . . # reshape output to remove channel to make printing easier yhat = yhat . reshape ( ( 4 , 4 ) ) # summarize output print ( yhat )

Tying all of this together, the complete example of using the Conv2DTranspose layer in Keras is provided below.

# example of using the transpose convolutional layer from numpy import asarray from keras.models import Sequential from keras.layers import Conv2DTranspose # define input data X = asarray([[1, 2], [3, 4]]) # show input data for context print(X) # reshape input data into one sample a sample with a channel X = X.reshape((1, 2, 2, 1)) # define model model = Sequential() model.add(Conv2DTranspose(1, (1,1), strides=(2,2), input_shape=(2, 2, 1))) # summarize the model model.summary() # define weights that they do nothing weights = [asarray([[[[1]]]]), asarray([0])] # store the weights in the model model.set_weights(weights) # make a prediction with the model yhat = model.predict(X) # reshape output to remove channel to make printing easier yhat = yhat.reshape((4, 4)) # summarize output print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # example of using the transpose convolutional layer from numpy import asarray from keras . models import Sequential from keras . layers import Conv2DTranspose # define input data X = asarray ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) # show input data for context print ( X ) # reshape input data into one sample a sample with a channel X = X . reshape ( ( 1 , 2 , 2 , 1 ) ) # define model model = Sequential ( ) model . add ( Conv2DTranspose ( 1 , ( 1 , 1 ) , strides = ( 2 , 2 ) , input_shape = ( 2 , 2 , 1 ) ) ) # summarize the model model . summary ( ) # define weights that they do nothing weights = [ asarray ( [ [ [ [ 1 ] ] ] ] ) , asarray ( [ 0 ] ) ] # store the weights in the model model . set_weights ( weights ) # make a prediction with the model yhat = model . predict ( X ) # reshape output to remove channel to make printing easier yhat = yhat . reshape ( ( 4 , 4 ) ) # summarize output print ( yhat )

Running the example first creates and summarizes our 2×2 input data.

Next, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer two parameters or model weights. One for the single 1×1 filter and one for the bias. Unlike the UpSampling2D layer, the Conv2DTranspose will learn during training and will attempt to fill in detail as part of the upsampling process.

Finally, the model is used to upsample our input. We can see that the calculations of the cells that involve real values as input result in the real value as output (e.g. 1×1, 1×2, etc.). We can see that where new rows and columns have been inserted by the stride of 2×2, that their 0.0 values multiplied by the 1.0 values in the single 1×1 filter have resulted in 0 values in the output.

[[1 2] [3 4]] _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_transpose_1 (Conv2DTr (None, 4, 4, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ [[1. 0. 2. 0.] [0. 0. 0. 0.] [3. 0. 4. 0.] [0. 0. 0. 0.]] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [[1 2] [3 4]] _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_transpose_1 (Conv2DTr (None, 4, 4, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ [[1. 0. 2. 0.] [0. 0. 0. 0.] [3. 0. 4. 0.] [0. 0. 0. 0.]]

Remember: this is a contrived case where we artificially specified the model weights so that we could see the effect of the transpose convolutional operation.

In practice, we will use a large number of filters (e.g. 64 or 128), a larger kernel (e.g. 3×3, 5×5, etc.), and the layer will be initialized with random weights that will learn how to effectively upsample with detail during training.

In fact, you might imagine how different sized kernels will result in different sized outputs, more than doubling the width and height of the input. In this case, the ‘padding‘ argument of the layer can be set to ‘same‘ to force the output to have the desired (doubled) output shape; for example:

... # example of using padding to ensure that the output is only doubled model.add(Conv2DTranspose(1, (3,3), strides=(2,2), padding='same', input_shape=(2, 2, 1))) 1 2 3 . . . # example of using padding to ensure that the output is only doubled model . add ( Conv2DTranspose ( 1 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 2 , 2 , 1 ) ) )

Simple Generator Model With the Conv2DTranspose Layer

The Conv2DTranspose is more complex than the UpSampling2D layer, but it is also effective when used in GAN models, specifically the generator model.

Either approach can be used, although the Conv2DTranspose layer is preferred, perhaps because of the simpler generator models and possibly better results, although GAN performance and skill is notoriously difficult to quantify.

We can demonstrate using the Conv2DTranspose layer in a generator model with another simple example.

In this case, our little GAN generator model must produce a 10×10 image and take a 100-element vector from the latent space as input, as in the previous UpSampling2D example.

First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image.

... # define model model = Sequential() # define input shape, output enough activations for for 128 5x5 image model.add(Dense(128 * 5 * 5, input_dim=100)) # reshape vector of activations into 128 feature maps with 5x5 model.add(Reshape((5, 5, 128))) 1 2 3 4 5 6 7 . . . # define model model = Sequential ( ) # define input shape, output enough activations for for 128 5x5 image model . add ( Dense ( 128 * 5 * 5 , input_dim = 100 ) ) # reshape vector of activations into 128 feature maps with 5x5 model . add ( Reshape ( ( 5 , 5 , 128 ) ) )

Next, the 5×5 feature maps can be upsampled to a 10×10 feature map.

We will use a 3×3 kernel size for the single filter, which will result in a slightly larger than doubled width and height in the output feature map (11×11).

Therefore, we will set ‘padding‘ to ‘same’ to ensure the output dimensions are 10×10 as required.

... # double input from 128 5x5 to 1 10x10 feature map model.add(Conv2DTranspose(1, (3,3), strides=(2,2), padding='same')) 1 2 3 . . . # double input from 128 5x5 to 1 10x10 feature map model . add ( Conv2DTranspose ( 1 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

Tying this together, the complete example is listed below.

# example of using transpose conv in a simple generator model from keras.models import Sequential from keras.layers import Dense from keras.layers import Reshape from keras.layers import Conv2DTranspose from keras.layers import Conv2D # define model model = Sequential() # define input shape, output enough activations for for 128 5x5 image model.add(Dense(128 * 5 * 5, input_dim=100)) # reshape vector of activations into 128 feature maps with 5x5 model.add(Reshape((5, 5, 128))) # double input from 128 5x5 to 1 10x10 feature map model.add(Conv2DTranspose(1, (3,3), strides=(2,2), padding='same')) # summarize model model.summary() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # example of using transpose conv in a simple generator model from keras . models import Sequential from keras . layers import Dense from keras . layers import Reshape from keras . layers import Conv2DTranspose from keras . layers import Conv2D # define model model = Sequential ( ) # define input shape, output enough activations for for 128 5x5 image model . add ( Dense ( 128 * 5 * 5 , input_dim = 100 ) ) # reshape vector of activations into 128 feature maps with 5x5 model . add ( Reshape ( ( 5 , 5 , 128 ) ) ) # double input from 128 5x5 to 1 10x10 feature map model . add ( Conv2DTranspose ( 1 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) ) # summarize model model . summary ( )

Running the example creates the model and summarizes the output shape of each layer.

We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5.

The widths and heights are doubled to 10×10 by the Conv2DTranspose layer resulting in a single feature map with quadruple the area.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 3200) 323200 _________________________________________________________________ reshape_1 (Reshape) (None, 5, 5, 128) 0 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 10, 10, 1) 1153 ================================================================= Total params: 324,353 Trainable params: 324,353 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 10 11 12 13 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 3200) 323200 _________________________________________________________________ reshape_1 (Reshape) (None, 5, 5, 128) 0 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 10, 10, 1) 1153 ================================================================= Total params: 324,353 Trainable params: 324,353 Non-trainable params: 0 _________________________________________________________________

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

API

Articles

Summary

In this tutorial, you discovered how to use UpSampling2D and Conv2DTranspose Layers in Generative Adversarial Networks when generating images.

Specifically, you learned:

Generative models in the GAN architecture are required to upsample input data in order to generate an output image.

The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.

The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Generative Adversarial Networks Today! Develop Your GAN Models in Minutes ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Generative Adversarial Networks with Python It provides self-study tutorials and end-to-end projects on:

DCGAN, conditional GANs, image translation, Pix2Pix, CycleGAN

and much more... Finally Bring GAN Models to your Vision Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
1;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/;2018-09-23;How to Develop RNN Models for Human Activity Recognition Time Series Classification;"# convlstm model

from numpy import mean

from numpy import std

from numpy import dstack

from pandas import read_csv

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import Dropout

from keras . layers import LSTM

from keras . layers import TimeDistributed

from keras . layers import ConvLSTM2D

from keras . utils import to_categorical

from matplotlib import pyplot

# load a single file as a numpy array

def load_file ( filepath ) :

dataframe = read_csv ( filepath , header = None , delim_whitespace = True )

return dataframe . values

# load a list of files and return as a 3d numpy array

def load_group ( filenames , prefix = '' ) :

loaded = list ( )

for name in filenames :

data = load_file ( prefix + name )

loaded . append ( data )

# stack group so that features are the 3rd dimension

loaded = dstack ( loaded )

return loaded

# load a dataset group, such as train or test

def load_dataset_group ( group , prefix = '' ) :

filepath = prefix + group + '/Inertial Signals/'

# load all 9 files as a single array

filenames = list ( )

# total acceleration

filenames += [ 'total_acc_x_' + group + '.txt' , 'total_acc_y_' + group + '.txt' , 'total_acc_z_' + group + '.txt' ]

# body acceleration

filenames += [ 'body_acc_x_' + group + '.txt' , 'body_acc_y_' + group + '.txt' , 'body_acc_z_' + group + '.txt' ]

# body gyroscope

filenames += [ 'body_gyro_x_' + group + '.txt' , 'body_gyro_y_' + group + '.txt' , 'body_gyro_z_' + group + '.txt' ]

# load input data

X = load_group ( filenames , filepath )

# load class output

y = load_file ( prefix + group + '/y_' + group + '.txt' )

return X , y

# load the dataset, returns train and test X and y elements

def load_dataset ( prefix = '' ) :

# load all train

trainX , trainy = load_dataset_group ( 'train' , prefix + 'HARDataset/' )

print ( trainX . shape , trainy . shape )

# load all test

testX , testy = load_dataset_group ( 'test' , prefix + 'HARDataset/' )

print ( testX . shape , testy . shape )

# zero-offset class values

trainy = trainy - 1

testy = testy - 1

# one hot encode y

trainy = to_categorical ( trainy )

testy = to_categorical ( testy )

print ( trainX . shape , trainy . shape , testX . shape , testy . shape )

return trainX , trainy , testX , testy

# fit and evaluate a model

def evaluate_model ( trainX , trainy , testX , testy ) :

# define model

verbose , epochs , batch_size = 0 , 25 , 64

n_timesteps , n_features , n_outputs = trainX . shape [ 1 ] , trainX . shape [ 2 ] , trainy . shape [ 1 ]

# reshape into subsequences (samples, time steps, rows, cols, channels)

n_steps , n_length = 4 , 32

trainX = trainX . reshape ( ( trainX . shape [ 0 ] , n_steps , 1 , n_length , n_features ) )

testX = testX . reshape ( ( testX . shape [ 0 ] , n_steps , 1 , n_length , n_features ) )

# define model

model = Sequential ( )

model . add ( ConvLSTM2D ( filters = 64 , kernel_size = ( 1 , 3 ) , activation = 'relu' , input_shape = ( n_steps , 1 , n_length , n_features ) ) )

model . add ( Dropout ( 0.5 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 100 , activation = 'relu' ) )

model . add ( Dense ( n_outputs , activation = 'softmax' ) )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( trainX , trainy , epochs = epochs , batch_size = batch_size , verbose = verbose )

# evaluate model

_ , accuracy = model . evaluate ( testX , testy , batch_size = batch_size , verbose = 0 )

return accuracy

# summarize scores

def summarize_results ( scores ) :

print ( scores )

m , s = mean ( scores ) , std ( scores )

print ( 'Accuracy: %.3f%% (+/-%.3f)' % ( m , s ) )

# run an experiment

def run_experiment ( repeats = 10 ) :

# load data

trainX , trainy , testX , testy = load_dataset ( )

# repeat experiment

scores = list ( )

for r in range ( repeats ) :

score = evaluate_model ( trainX , trainy , testX , testy )

score = score * 100.0

print ( '>#%d: %.3f' % ( r + 1 , score ) )

scores . append ( score )

# summarize results

summarize_results ( scores )

# run the experiment"
2;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-and-demonstrate-competence-with-deep-learning-for-computer-vision/;2019-03-10;How to Develop Competence With Deep Learning for Computer Vision;"Tweet Share Share

Last Updated on July 5, 2019

Computer vision is perhaps one area that has been most impacted by developments in deep learning.

It can be difficult to both develop and to demonstrate competence with deep learning for problems in the field of computer vision. It is not clear how to get started, what the most important techniques are, and the types of problems and projects that can best highlight the value that deep learning can bring to the field.

On approach is to systematically develop, and at the same time demonstrate competence with, data handling, modeling techniques, and application domains and present your results in a public portfolio of completed projects. This approach allows you to compound your skills from project to project. It also provides the basis for real projects that can be presented and discussed with prospective employers in order to demonstrate your capabilities.

In this post, you will discover how to develop and demonstrate competence in deep learning applied to problems in computer vision.

After reading this post, you will know:

Developing a portfolio of completed small projects can both be leveraged on new projects in the future and demonstrate your competence with deep learning for computer vision projects.

Projects can be kept small in scope, although they can still demonstrate a systematic approach to problem-solving and the development of skillful models.

A three-level competence framework can be followed that includes data handling competence, technique competence, and application competence.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Deep Learning for Computer Vision Develop a Portfolio of Small Projects Deep Learning for Computer Vision Competence Framework

Deep Learning for Computer Vision

Perhaps one domain that has been the most impacted by developments in deep learning is computer vision.

Computer vision is a subfield of artificial intelligence concerned with understanding data in images, such as photos and videos.

Computer vision tasks such as recognizing handwritten digits and objects in photographs were some of the early case studies demonstrating the capability of modern deep learning techniques achieving state-of-the-art results.

As a practitioner, you may wish to develop and demonstrate your skills with deep learning in computer vision.

This does assume a few things, such as:

You are familiar with applied machine learning, meaning that you are able to work through a predictive modeling project end-to-end and deliver a skillful model.

You are familiar with deep learning techniques, meaning that you know the difference between the main methods and when to use them.

This does not mean that you are an expert, only that you have a working knowledge and are able to wok through problems systematically.

As a machine learning or even deep learning practitioner, how can you show competence with computer vision applications?

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Develop a Portfolio of Small Projects

Competence with deep learning for computer vision can be developed and demonstrated using a project-based approach.

Specifically, the skills can be built and demonstrated incrementally by completing and presenting small projects that use deep learning techniques on computer vision problems.

This requires you to develop a portfolio of completed projects. A portfolio helps you in two specific ways:

Skill Development : The code and findings from the projects in the portfolio can be leveraged by you on future projects, accelerating your progress and allowing you to take on larger and more challenging projects.

: The code and findings from the projects in the portfolio can be leveraged by you on future projects, accelerating your progress and allowing you to take on larger and more challenging projects. Skill Demonstration: The public presentation of the projects provides a demonstration of your capabilities, providing the basis for discussion of APIs, model selection, and design decisions with prospective employers.

Projects can be focused on standard and publicly available computer vision datasets, such as those developed and hosted by academics or those used in machine learning competitions.

Projects can be completed in a systematic manner, including aspects such as clear problem definition, review of relevant literature and models, model development and tuning, and the presentation of results and findings in a report, notebook, or even slideshow presentation format.

Projects are small, meaning that they can be completed in a workday, perhaps spread over a number of nights and weekends. This is important as it limits the scope of the project to focus on workflow and delivering a skillful result, rather than developing a state-of-the-art result.

Deep Learning for Computer Vision Competence Framework

Projects can be selected carefully in such a way to both build in terms of challenge or complexity and in terms of leverage or skill development.

Below is a three-level framework for developing and demonstrating competence with deep learning for computer vision, intended for practitioners already familiar with the basics of applied machine learning and the basics of deep learning:

Level 1: Data Handling Competence . That you know how to load and manipulate image data.

. That you know how to load and manipulate image data. Level 2: Technique Competence . That you know how to define, fit, and tune convolutional neural networks.

. That you know how to define, fit, and tune convolutional neural networks. Level 3: Application Competence. That you can develop skillful deep learning models for common computer vision problems.

Level 1: Data Handling Competence

Data handling competence refers to the ability to load and transform data.

This includes basic data I/O operations such as loading and saving image or video data.

Most importantly, it involves using standard APIs to manipulate image data in ways that may be useful when preparing data for molding with deep learning neural networks.

Examples include:

Image resizing and interpolation.

Image blurring and sharpening.

Image affine transforms.

Image whitening and thresholding.

Data handling could be demonstrated with one of many image handling APIs, such as:

It may include the basic data handing capability of machine learning and deep learning libraries, such as:

What are your favorite image handling APIs in Python?

Let me know in the comments below.

Level 2: Technique Competence

Technique competence refers to the ability to use the specific deep learning models and methods that are used for computer vision problems.

This includes from a high-level the three main classes of methods:

Multilayer Perceptrons, or MLPs.

Convolutional Neural Networks, or CNNs.

Recurrent Neural Networks, such as the Long Short-Term Memory Network, or LSTM.

More specifically, this requires a demonstration of strong skills with how to configure and get the most of the layers used in a CNN, such as:

Convolutional Layers.

Pooling Layers.

Patterns of using Layers.

This may also include skill with some general classes of effective models, such as:

ImageNet CNNs such as AlexNet, VGG, ResNet, Inception, etc.

CNN-LSTMs, LSTM-CNNs, etc.

R-CNNs, YOLO, etc.

What are your favorite deep learning techniques for computer vision?

Let me know in the comments below.

Level 3: Application Competence

Application competence refers to the ability to work through a specific computer vision problem and use deep learning methods to deliver a skillful model.

A skillful model means a model that is capable of making predictions that have better performance than a naive baseline method. It does not mean achieving state-of-the-art results and replicating a model and results in a paper, although they are fine project ideas if they are within scope of a small project.

The project should be completed systematically, including most if not all of the following steps:

Problem Description. Describe the predictive modeling problem, including the domain and relevant background. Literature Review. Describe standard or common approaches to solving the problem using deep learning methods as described in seminal and/or recent research papers. Summarize Data. Describe the available data, including statistical summaries and data visualization. Evaluate Models. Spot-check a suite of model types, configurations, data preparation schemes, and more in order to narrow down what works well on the problem. Improve Performance. Improve the performance of the model or models that work well with hyperparameter tuning and perhaps ensemble methods. Present Results. Present the findings of the project.

A step before this process, step zero, might be to choose a publicly available dataset appropriate for the project.

The backbone of deep learning for computer vision is image classification, commonly referred to as image recognition or object detection. This involves predicting a class label given an image, often a photograph.

Problems of this type should be the focus.

Two standard computer vision datasets of this type include:

Classifying handwritten digits (e.g. MNIST and SVHN).

Classifying photos of objects (e.g. CIFAR-10 and CIFAR-100).

Classifying photos of faces (e.g. VGGFace2)

A related computer vision task is identifying the location of one or more objects within photographs, also referred to as object recognition or object localization or segmentation.

Object Recognition and Localization (e.g. COCO)

There are also tasks that involve a mixture of computer vision and natural language processing, for example:

Photo Captioning (e.g. Flickr8k)

Finally, there are computer vision tasks that can be performed using manipulations of existing standard datasets or catalogs of photos, such as:

What are your favorite applications of deep learning for computer vision?

Let me know in the comments below.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

APIs

Datasets

Articles

Summary

In this post, you discovered how to develop and demonstrate competence in deep learning applied to problems in computer vision.

Specifically, you learned:

Developing a portfolio of completed small projects can both be leveraged on new projects in the future and demonstrate your competence with deep learning for computer vision projects.

Projects can be kept small in scope although they can still demonstrate a systematic approach to problem-solving and the development of skillful models.

A three-level competence framework can be followed that includes data handling competence, technique competence, and application competence.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning Models for Vision Today! Develop Your Own Vision Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Computer Vision It provides self-study tutorials on topics like:

classification, object detection (yolo and rcnn), face recognition (vggface and facenet), data preparation and much more... Finally Bring Deep Learning to your Vision Projects Skip the Academics. Just Results. See What's Inside"
3;machinelearningmastery.com;https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/;2019-06-20;How to Implement GAN Hacks in Keras to Train Stable Models;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are challenging to train.

This is because the architecture involves both a generator and a discriminator model that compete in a zero-sum game. It means that improvements to one model come at the cost of a degrading of performance in the other model. The result is a very unstable training process that can often lead to failure, e.g. a generator that generates the same image all the time or generates nonsense.

As such, there are a number of heuristics or best practices (called “GAN hacks“) that can be used when configuring and training your GAN models. These heuristics are been hard won by practitioners testing and evaluating hundreds or thousands of combinations of configuration operations on a range of problems over many years.

Some of these heuristics can be challenging to implement, especially for beginners.

Further, some or all of them may be required for a given project, although it may not be clear which subset of heuristics should be adopted, requiring experimentation. This means a practitioner must be ready to implement a given heuristic with little notice.

In this tutorial, you will discover how to implement a suite of best practices or GAN hacks that you can copy-and-paste directly into your GAN project.

After reading this tutorial, you will know:

The best sources for practical heuristics or hacks when developing generative adversarial networks.

How to implement seven best practices for the deep convolutional GAN model architecture from scratch.

How to implement four additional best practices from Soumith Chintala’s GAN Hacks presentation and list.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Heuristics for Training Stable GANs Best Practices for Deep Convolutional GANs Downsample Using Strided Convolutions Upsample Using Strided Convolutions Use LeakyReLU Use Batch Normalization Use Gaussian Weight Initialization Use Adam Stochastic Gradient Descent Scale Images to the Range [-1,1] Soumith Chintala’s GAN Hacks Use a Gaussian Latent Space Separate Batches of Real and Fake Images Use Label Smoothing Use Noisy Labels

Heuristics for Training Stable GANs

GANs are difficult to train.

At the time of writing, there is no good theoretical foundation as to how to design and train GAN models, but there is established literature of heuristics, or “hacks,” that have been empirically demonstrated to work well in practice.

As such, there are a range of best practices to consider and implement when developing a GAN model.

Perhaps the two most important sources of suggested configuration and training parameters are:

Alec Radford, et al’s 2015 paper that introduced the DCGAN architecture. Soumith Chintala’s 2016 presentation and associated “GAN Hacks” list.

In this tutorial, we will explore how to implement the most important best practices from these two sources.

Best Practices for Deep Convolutional GANs

Perhaps one of the most important steps forward in the design and training of stable GAN models was the 2015 paper by Alec Radford, et al. titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.”

In the paper, they describe the Deep Convolutional GAN, or DCGAN, approach to GAN development that has become the de facto standard.

We will look at how to implement seven best practices for the DCGAN model architecture in this section.

1. Downsample Using Strided Convolutions

The discriminator model is a standard convolutional neural network model that takes an image as input and must output a binary classification as to whether it is real or fake.

It is standard practice with deep convolutional networks to use pooling layers to downsample the input and feature maps with the depth of the network.

This is not recommended for the DCGAN, and instead, they recommend downsampling using strided convolutions.

This involves defining a convolutional layer as per normal, but instead of using the default two-dimensional stride of (1,1) to change it to (2,2). This has the effect of downsampling the input, specifically halving the width and height of the input, resulting in output feature maps with one quarter the area.

The example below demonstrates this with a single hidden convolutional layer that uses downsampling strided convolutions by setting the ‘strides‘ argument to (2,2). The effect is the model will downsample the input from 64×64 to 32×32.

# example of downsampling with strided convolutions from keras.models import Sequential from keras.layers import Conv2D # define model model = Sequential() model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3))) # summarize model model.summary() 1 2 3 4 5 6 7 8 # example of downsampling with strided convolutions from keras . models import Sequential from keras . layers import Conv2D # define model model = Sequential ( ) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 64 , 64 , 3 ) ) ) # summarize model model . summary ( )

Running the example shows the shape of the output of the convolutional layer, where the feature maps have one quarter of the area.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________

2. Upsample Using Strided Convolutions

The generator model must generate an output image given as input at a random point from the latent space.

The recommended approach for achieving this is to use a transpose convolutional layer with a strided convolution. This is a special type of layer that performs the convolution operation in reverse. Intuitively, this means that setting a stride of 2×2 will have the opposite effect, upsampling the input instead of downsampling it in the case of a normal convolutional layer.

By stacking a transpose convolutional layer with strided convolutions, the generator model is able to scale a given input to the desired output dimensions.

The example below demonstrates this with a single hidden transpose convolutional layer that uses upsampling strided convolutions by setting the ‘strides‘ argument to (2,2).

The effect is the model will upsample the input from 64×64 to 128×128.

# example of upsampling with strided convolutions from keras.models import Sequential from keras.layers import Conv2DTranspose # define model model = Sequential() model.add(Conv2DTranspose(64, kernel_size=(4,4), strides=(2,2), padding='same', input_shape=(64,64,3))) # summarize model model.summary() 1 2 3 4 5 6 7 8 # example of upsampling with strided convolutions from keras . models import Sequential from keras . layers import Conv2DTranspose # define model model = Sequential ( ) model . add ( Conv2DTranspose ( 64 , kernel_size = ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 64 , 64 , 3 ) ) ) # summarize model model . summary ( )

Running the example shows the shape of the output of the convolutional layer, where the feature maps have quadruple the area.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_transpose_1 (Conv2DTr (None, 128, 128, 64) 3136 ================================================================= Total params: 3,136 Trainable params: 3,136 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_transpose_1 (Conv2DTr (None, 128, 128, 64) 3136 ================================================================= Total params: 3,136 Trainable params: 3,136 Non-trainable params: 0 _________________________________________________________________

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

3. Use LeakyReLU

The rectified linear activation unit, or ReLU for short, is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less.

It has become a best practice when developing deep convolutional neural networks generally.

The best practice for GANs is to use a variation of the ReLU that allows some values less than zero and learns where the cut-off should be in each node. This is called the leaky rectified linear activation unit, or LeakyReLU for short.

A negative slope can be specified for the LeakyReLU and the default value of 0.2 is recommended.

Originally, ReLU was recommend for use in the generator model and LeakyReLU was recommended for use in the discriminator model, although more recently, the LeakyReLU is recommended in both models.

The example below demonstrates using the LeakyReLU with the default slope of 0.2 after a convolutional layer in a discriminator model.

# example of using leakyrelu in a discriminator model from keras.models import Sequential from keras.layers import Conv2D from keras.layers import BatchNormalization from keras.layers import LeakyReLU # define model model = Sequential() model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3))) model.add(LeakyReLU(0.2)) # summarize model model.summary() 1 2 3 4 5 6 7 8 9 10 11 # example of using leakyrelu in a discriminator model from keras . models import Sequential from keras . layers import Conv2D from keras . layers import BatchNormalization from keras . layers import LeakyReLU # define model model = Sequential ( ) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 64 , 64 , 3 ) ) ) model . add ( LeakyReLU ( 0.2 ) ) # summarize model model . summary ( )

Running the example demonstrates the structure of the model with a single convolutional layer followed by the activation layer.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 32, 32, 64) 0 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 10 11 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 32, 32, 64) 0 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________

4. Use Batch Normalization

Batch normalization standardizes the activations from a prior layer to have a zero mean and unit variance. This has the effect of stabilizing the training process.

Batch normalization is used after the activation of convolution and transpose convolutional layers in the discriminator and generator models respectively.

It is added to the model after the hidden layer, but before the activation, such as LeakyReLU.

The example below demonstrates adding a Batch Normalization layer after a Conv2D layer in a discriminator model but before the activation.

# example of using batch norm in a discriminator model from keras.models import Sequential from keras.layers import Conv2D from keras.layers import BatchNormalization from keras.layers import LeakyReLU # define model model = Sequential() model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3))) model.add(BatchNormalization()) model.add(LeakyReLU(0.2)) # summarize model model.summary() 1 2 3 4 5 6 7 8 9 10 11 12 # example of using batch norm in a discriminator model from keras . models import Sequential from keras . layers import Conv2D from keras . layers import BatchNormalization from keras . layers import LeakyReLU # define model model = Sequential ( ) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 64 , 64 , 3 ) ) ) model . add ( BatchNormalization ( ) ) model . add ( LeakyReLU ( 0.2 ) ) # summarize model model . summary ( )

Running the example shows the desired usage of batch norm between the outputs of the convolutional layer and the activation function.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 64) 256 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 32, 32, 64) 0 ================================================================= Total params: 2,048 Trainable params: 1,920 Non-trainable params: 128 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 10 11 12 13 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 _________________________________________________________________ batch_normalization_1 (Batch (None, 32, 32, 64) 256 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 32, 32, 64) 0 ================================================================= Total params: 2,048 Trainable params: 1,920 Non-trainable params: 128 _________________________________________________________________

5. Use Gaussian Weight Initialization

Before a neural network can be trained, the model weights (parameters) must be initialized to small random variables.

The best practice for DCAGAN models reported in the paper is to initialize all weights using a zero-centered Gaussian distribution (the normal or bell-shaped distribution) with a standard deviation of 0.02.

The example below demonstrates defining a random Gaussian weight initializer with a mean of 0 and a standard deviation of 0.02 for use in a transpose convolutional layer in a generator model.

The same weight initializer instance could be used for each layer in a given model.

# example of gaussian weight initialization in a generator model from keras.models import Sequential from keras.layers import Conv2DTranspose from keras.initializers import RandomNormal # define model model = Sequential() init = RandomNormal(mean=0.0, stddev=0.02) model.add(Conv2DTranspose(64, kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=init, input_shape=(64,64,3))) 1 2 3 4 5 6 7 8 # example of gaussian weight initialization in a generator model from keras . models import Sequential from keras . layers import Conv2DTranspose from keras . initializers import RandomNormal # define model model = Sequential ( ) init = RandomNormal ( mean = 0.0 , stddev = 0.02 ) model . add ( Conv2DTranspose ( 64 , kernel_size = ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init , input_shape = ( 64 , 64 , 3 ) ) )

6. Use Adam Stochastic Gradient Descent

Stochastic gradient descent, or SGD for short, is the standard algorithm used to optimize the weights of convolutional neural network models.

There are many variants of the training algorithm. The best practice for training DCGAN models is to use the Adam version of stochastic gradient descent with the learning rate of 0.0002 and the beta1 momentum value of 0.5 instead of the default of 0.9.

The Adam optimization algorithm with this configuration is recommended when both optimizing the discriminator and generator models.

The example below demonstrates configuring the Adam stochastic gradient descent optimization algorithm for training a discriminator model.

# example of using adam when training a discriminator model from keras.models import Sequential from keras.layers import Conv2D from keras.optimizers import Adam # define model model = Sequential() model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3))) # compile model opt = Adam(lr=0.0002, beta_1=0.5) model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) 1 2 3 4 5 6 7 8 9 10 # example of using adam when training a discriminator model from keras . models import Sequential from keras . layers import Conv2D from keras . optimizers import Adam # define model model = Sequential ( ) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 64 , 64 , 3 ) ) ) # compile model opt = Adam ( lr = 0.0002 , beta_1 = 0.5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

7. Scale Images to the Range [-1,1]

It is recommended to use the hyperbolic tangent activation function as the output from the generator model.

As such, it is also recommended that real images used to train the discriminator are scaled so that their pixel values are in the range [-1,1]. This is so that the discriminator will always receive images as input, real and fake, that have pixel values in the same range.

Typically, image data is loaded as a NumPy array such that pixel values are 8-bit unsigned integer (uint8) values in the range [0, 255].

First, the array must be converted to floating point values, then rescaled to the required range.

The example below provides a function that will appropriately scale a NumPy array of loaded image data to the required range of [-1,1].

# example of a function for scaling images # scale image data from [0,255] to [-1,1] def scale_images(images): # convert from unit8 to float32 images = images.astype('float32') # scale from [0,255] to [-1,1] images = (images - 127.5) / 127.5 return images 1 2 3 4 5 6 7 8 9 # example of a function for scaling images # scale image data from [0,255] to [-1,1] def scale_images ( images ) : # convert from unit8 to float32 images = images . astype ( 'float32' ) # scale from [0,255] to [-1,1] images = ( images - 127.5 ) / 127.5 return images

Soumith Chintala’s GAN Hacks

Soumith Chintala, one of the co-authors of the DCGAN paper, made a presentation at NIPS 2016 titled “How to Train a GAN?” summarizing many tips and tricks.

The video is available on YouTube and is highly recommended. A summary of the tips is also available as a GitHub repository titled “How to Train a GAN? Tips and tricks to make GANs work.”

The tips draw upon the suggestions from the DCGAN paper as well as elsewhere.

In this section, we will review how to implement four additional GAN best practices not covered in the previous section.

1. Use a Gaussian Latent Space

The latent space defines the shape and distribution of the input to the generator model used to generate new images.

The DCGAN recommends sampling from a uniform distribution, meaning that the shape of the latent space is a hypercube.

The more recent best practice is to sample from a standard Gaussian distribution, meaning that the shape of the latent space is a hypersphere, with a mean of zero and a standard deviation of one.

The example below demonstrates how to generate 500 random Gaussian points from a 100-dimensional latent space that can be used as input to a generator model; each point could be used to generate an image.

# example of sampling from a gaussian latent space from numpy.random import randn # generate points in latent space as input for the generator def generate_latent_points(latent_dim, n_samples): # generate points in the latent space x_input = randn(latent_dim * n_samples) # reshape into a batch of inputs for the network x_input = x_input.reshape((n_samples, latent_dim)) return x_input # size of latent space n_dim = 100 # number of samples to generate n_samples = 500 # generate samples samples = generate_latent_points(n_dim, n_samples) # summarize print(samples.shape, samples.mean(), samples.std()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # example of sampling from a gaussian latent space from numpy . random import randn # generate points in latent space as input for the generator def generate_latent_points ( latent_dim , n_samples ) : # generate points in the latent space x_input = randn ( latent_dim * n_samples ) # reshape into a batch of inputs for the network x_input = x_input . reshape ( ( n_samples , latent_dim ) ) return x_input # size of latent space n_dim = 100 # number of samples to generate n_samples = 500 # generate samples samples = generate_latent_points ( n_dim , n_samples ) # summarize print ( samples . shape , samples . mean ( ) , samples . std ( ) )

Running the example summarizes the generation of 500 points, each comprised of 100 random Gaussian values with a mean close to zero and a standard deviation close to 1, e.g. a standard Gaussian distribution.

(500, 100) -0.004791256735601787 0.9976912528950904 1 (500, 100) -0.004791256735601787 0.9976912528950904

2. Separate Batches of Real and Fake Images

The discriminator model is trained using stochastic gradient descent with mini-batches.

The best practice is to update the discriminator with separate batches of real and fake images rather than combining real and fake images into a single batch.

This can be achieved by updating the model weights for the discriminator model with two separate calls to the train_on_batch() function.

The code snippet below demonstrates how you can do this within the inner loop of code when training your discriminator model.

... # get randomly selected 'real' samples X_real, y_real = ... # update discriminator model weights discriminator.train_on_batch(X_real, y_real) # generate 'fake' examples X_fake, y_fake = ... # update discriminator model weights discriminator.train_on_batch(X_fake, y_fake) 1 2 3 4 5 6 7 8 9 . . . # get randomly selected 'real' samples X_real , y_real = . . . # update discriminator model weights discriminator . train_on_batch ( X_real , y_real ) # generate 'fake' examples X_fake , y_fake = . . . # update discriminator model weights discriminator . train_on_batch ( X_fake , y_fake )

3. Use Label Smoothing

It is common to use the class label 1 to represent real images and class label 0 to represent fake images when training the discriminator model.

These are called hard labels, as the label values are precise or crisp.

It is a good practice to use soft labels, such as values slightly more or less than 1.0 or slightly more than 0.0 for real and fake images respectively, where the variation for each image is random.

This is often referred to as label smoothing and can have a regularizing effect when training the model.

The example below demonstrates defining 1,000 labels for the positive class (class=1) and smoothing the label values uniformly into the range [0.7,1.2] as recommended.

# example of positive label smoothing from numpy import ones from numpy.random import random # example of smoothing class=1 to [0.7, 1.2] def smooth_positive_labels(y): return y - 0.3 + (random(y.shape) * 0.5) # generate 'real' class labels (1) n_samples = 1000 y = ones((n_samples, 1)) # smooth labels y = smooth_positive_labels(y) # summarize smooth labels print(y.shape, y.min(), y.max()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # example of positive label smoothing from numpy import ones from numpy . random import random # example of smoothing class=1 to [0.7, 1.2] def smooth_positive_labels ( y ) : return y - 0.3 + ( random ( y . shape ) * 0.5 ) # generate 'real' class labels (1) n_samples = 1000 y = ones ( ( n_samples , 1 ) ) # smooth labels y = smooth_positive_labels ( y ) # summarize smooth labels print ( y . shape , y . min ( ) , y . max ( ) )

Running the example summarizes the min and max values for the smooth values, showing they are close to the expected values.

(1000, 1) 0.7003103006957805 1.1997858934066357 1 (1000, 1) 0.7003103006957805 1.1997858934066357

There have been some suggestions that only positive-class label smoothing is required and to values less than 1.0. Nevertheless, you can also smooth negative class labels.

The example below demonstrates generating 1,000 labels for the negative class (class=0) and smoothing the label values uniformly into the range [0.0, 0.3] as recommended.

# example of negative label smoothing from numpy import zeros from numpy.random import random # example of smoothing class=0 to [0.0, 0.3] def smooth_negative_labels(y): return y + random(y.shape) * 0.3 # generate 'fake' class labels (0) n_samples = 1000 y = zeros((n_samples, 1)) # smooth labels y = smooth_negative_labels(y) # summarize smooth labels print(y.shape, y.min(), y.max()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # example of negative label smoothing from numpy import zeros from numpy . random import random # example of smoothing class=0 to [0.0, 0.3] def smooth_negative_labels ( y ) : return y + random ( y . shape ) * 0.3 # generate 'fake' class labels (0) n_samples = 1000 y = zeros ( ( n_samples , 1 ) ) # smooth labels y = smooth_negative_labels ( y ) # summarize smooth labels print ( y . shape , y . min ( ) , y . max ( ) )

4. Use Noisy Labels

The labels used when training the discriminator model are always correct.

This means that fake images are always labeled with class 0 and real images are always labeled with class 1.

It is recommended to introduce some errors to these labels where some fake images are marked as real, and some real images are marked as fake.

If you are using separate batches to update the discriminator for real and fake images, this may mean randomly adding some fake images to the batch of real images, or randomly adding some real images to the batch of fake images.

If you are updating the discriminator with a combined batch of real and fake images, then this may involve randomly flipping the labels on some images.

The example below demonstrates this by creating 1,000 samples of real (class=1) labels and flipping them with a 5% probability, then doing the same with 1,000 samples of fake (class=0) labels.

# example of noisy labels from numpy import ones from numpy import zeros from numpy.random import choice # randomly flip some labels def noisy_labels(y, p_flip): # determine the number of labels to flip n_select = int(p_flip * y.shape[0]) # choose labels to flip flip_ix = choice([i for i in range(y.shape[0])], size=n_select) # invert the labels in place y[flip_ix] = 1 - y[flip_ix] return y # generate 'real' class labels (1) n_samples = 1000 y = ones((n_samples, 1)) # flip labels with 5% probability y = noisy_labels(y, 0.05) # summarize labels print(y.sum()) # generate 'fake' class labels (0) y = zeros((n_samples, 1)) # flip labels with 5% probability y = noisy_labels(y, 0.05) # summarize labels print(y.sum()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # example of noisy labels from numpy import ones from numpy import zeros from numpy . random import choice # randomly flip some labels def noisy_labels ( y , p_flip ) : # determine the number of labels to flip n_select = int ( p_flip * y . shape [ 0 ] ) # choose labels to flip flip_ix = choice ( [ i for i in range ( y . shape [ 0 ] ) ] , size = n_select ) # invert the labels in place y [ flip_ix ] = 1 - y [ flip_ix ] return y # generate 'real' class labels (1) n_samples = 1000 y = ones ( ( n_samples , 1 ) ) # flip labels with 5% probability y = noisy_labels ( y , 0.05 ) # summarize labels print ( y . sum ( ) ) # generate 'fake' class labels (0) y = zeros ( ( n_samples , 1 ) ) # flip labels with 5% probability y = noisy_labels ( y , 0.05 ) # summarize labels print ( y . sum ( ) )

Try running the example a few times.

The results show that approximately 50 “1”s are flipped to 1s for the positive labels (e.g. 5% of 1,0000) and approximately 50 “0”s are flopped to 1s in for the negative labels.

950.049.0 1 950.049.0

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

API

Articles

Summary

In this tutorial, you discovered how to implement a suite of best practices or GAN hacks that you can copy-and-paste directly into your GAN project.

Specifically, you learned:

The best sources for practical heuristics or hacks when developing generative adversarial networks.

How to implement seven best practices for the deep convolutional GAN model architecture from scratch.

How to implement four additional best practices from Soumith Chintala’s GAN Hacks presentation and list.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Generative Adversarial Networks Today! Develop Your GAN Models in Minutes ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Generative Adversarial Networks with Python It provides self-study tutorials and end-to-end projects on:

DCGAN, conditional GANs, image translation, Pix2Pix, CycleGAN

and much more... Finally Bring GAN Models to your Vision Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
4;machinelearningmastery.com;https://machinelearningmastery.com/how-to-evaluate-pixel-scaling-methods-for-image-classification/;2019-03-26;How to Evaluate Pixel Scaling Methods for Image Classification With CNNs;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113

# comparison of training-set based pixel scaling methods on MNIST from numpy import mean from numpy import std from matplotlib import pyplot from keras . datasets import mnist from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten # load train and test dataset def load_dataset ( ) : # load dataset ( trainX , trainY ) , ( testX , testY ) = mnist . load_data ( ) # reshape dataset to have a single channel width , height , channels = trainX . shape [ 1 ] , trainX . shape [ 2 ] , 1 trainX = trainX . reshape ( ( trainX . shape [ 0 ] , width , height , channels ) ) testX = testX . reshape ( ( testX . shape [ 0 ] , width , height , channels ) ) # one hot encode target values trainY = to_categorical ( trainY ) testY = to_categorical ( testY ) return trainX , trainY , testX , testY # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , input_shape = ( 28 , 28 , 1 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 64 , activation = 'relu' ) ) model . add ( Dense ( 10 , activation = 'softmax' ) ) # compile model model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] ) return model # normalize images def prep_normalize ( train , test ) : # convert from integers to floats train_norm = train . astype ( 'float32' ) test_norm = test . astype ( 'float32' ) # normalize to range 0-1 train_norm = train_norm / 255.0 test_norm = test_norm / 255.0 # return normalized images return train_norm , test_norm # center images def prep_center ( train , test ) : # convert from integers to floats train_cent = train . astype ( 'float32' ) test_cent = test . astype ( 'float32' ) # calculate statistics m = train_cent . mean ( ) # center datasets train_cent = train_cent - m test_cent = test_cent - m # return normalized images return train_cent , test_cent # standardize images def prep_standardize ( train , test ) : # convert from integers to floats train_stan = train . astype ( 'float32' ) test_stan = test . astype ( 'float32' ) # calculate statistics m = train_stan . mean ( ) s = train_stan . std ( ) # center datasets train_stan = ( train_stan - m ) / s test_stan = ( test_stan - m ) / s # return normalized images return train_stan , test_stan # repeated evaluation of model with data prep scheme def repeated_evaluation ( datapre_func , n_repeats = 10 ) : # prepare data trainX , trainY , testX , testY = load_dataset ( ) # repeated evaluation scores = list ( ) for i in range ( n_repeats ) : # define model model = define_model ( ) # prepare data prep_trainX , prep_testX = datapre_func ( trainX , testX ) # fit model model . fit ( prep_trainX , trainY , epochs = 5 , batch_size = 64 , verbose = 0 ) # evaluate model _ , acc = model . evaluate ( prep_testX , testY , verbose = 0 ) # store result scores . append ( acc ) print ( '> %d: %.3f' % ( i , acc * 100.0 ) ) return scores all_scores = list ( ) # normalization scores = repeated_evaluation ( prep_normalize ) print ( 'Normalization: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # center scores = repeated_evaluation ( prep_center ) print ( 'Centered: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # standardize scores = repeated_evaluation ( prep_standardize ) print ( 'Standardized: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # box and whisker plots of results pyplot . boxplot ( all_scores , labels = [ 'norm' , 'cent' , 'stan' ] ) pyplot . show ( )"
5;machinelearningmastery.com;https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/;2017-06-06;The 5 Step Life-Cycle for Long Short-Term Memory Models in Keras;"# Example of LSTM to learn a sequence

from pandas import DataFrame

from pandas import concat

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# create sequence

length = 10

sequence = [ i / float ( length ) for i in range ( length ) ]

print ( sequence )

# create X/y pairs

df = DataFrame ( sequence )

df = concat ( [ df . shift ( 1 ) , df ] , axis = 1 )

df . dropna ( inplace = True )

# convert to LSTM friendly format

values = df . values

X , y = values [ : , 0 ] , values [ : , 1 ]

X = X . reshape ( len ( X ) , 1 , 1 )

# 1. define network

model = Sequential ( )

model . add ( LSTM ( 10 , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 ) )

# 2. compile network

model . compile ( optimizer = 'adam' , loss = 'mean_squared_error' )

# 3. fit network

history = model . fit ( X , y , epochs = 1000 , batch_size = len ( X ) , verbose = 0 )

# 4. evaluate network

loss = model . evaluate ( X , y , verbose = 0 )

print ( loss )

# 5. make predictions

predictions = model . predict ( X , verbose = 0 )"
6;machinelearningmastery.com;http://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/;2016-04-21;Bagging and Random Forest Ensemble Algorithms for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.

In this post you will discover the Bagging ensemble algorithm and the Random Forest algorithm for predictive modeling. After reading this post you will know about:

The bootstrap method for estimating statistical quantities from samples.

The Bootstrap Aggregation algorithm for creating multiple different models from a single training dataset.

The Random Forest algorithm that makes a small tweak to Bagging and results in a very powerful classifier.

This post was written for developers and assumes no background in statistics or mathematics. The post focuses on how the algorithm works and how to use it for predictive modeling problems.

If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Bootstrap Method

Before we get to Bagging, let’s take a quick look at an important foundation technique called the bootstrap.

The bootstrap is a powerful statistical method for estimating a quantity from a data sample. This is easiest to understand if the quantity is a descriptive statistic such as a mean or a standard deviation.

Let’s assume we have a sample of 100 values (x) and we’d like to get an estimate of the mean of the sample.

We can calculate the mean directly from the sample as:

mean(x) = 1/100 * sum(x)

We know that our sample is small and that our mean has error in it. We can improve the estimate of our mean using the bootstrap procedure:

Create many (e.g. 1000) random sub-samples of our dataset with replacement (meaning we can select the same value multiple times). Calculate the mean of each sub-sample. Calculate the average of all of our collected means and use that as our estimated mean for the data.

For example, let’s say we used 3 resamples and got the mean values 2.3, 4.5 and 3.3. Taking the average of these we could take the estimated mean of the data to be 3.367.

This process can be used to estimate other quantities like the standard deviation and even quantities used in machine learning algorithms, like learned coefficients.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Bootstrap Aggregation (Bagging)

Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.

An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.

Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees, like classification and regression trees (CART).

Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.

Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.

Let’s assume we have a sample dataset of 1000 instances (x) and we are using the CART algorithm. Bagging of the CART algorithm would work as follows.

Create many (e.g. 100) random sub-samples of our dataset with replacement. Train a CART model on each sample. Given a new dataset, calculate the average prediction from each model.

For example, if we had 5 bagged decision trees that made the following class predictions for a in input sample: blue, blue, red, blue and red, we would take the most frequent class and predict blue.

When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging.

The only parameters when bagging decision trees is the number of samples and hence the number of trees to include. This can be chosen by increasing the number of trees on run after run until the accuracy begins to stop showing improvement (e.g. on a cross validation test harness). Very large numbers of models may take a long time to prepare, but will not overfit the training data.

Just like the decision trees themselves, Bagging can be used for classification and regression problems.

Random Forest

Random Forests are an improvement over bagged decision trees.

A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions.

Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.

Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation.

It is a simple tweak. In CART, when selecting a split point, the learning algorithm is allowed to look through all variables and all variable values in order to select the most optimal split-point. The random forest algorithm changes this procedure so that the learning algorithm is limited to a random sample of features of which to search.

The number of features that can be searched at each split point (m) must be specified as a parameter to the algorithm. You can try different values and tune it using cross validation.

For classification a good default is: m = sqrt(p)

For regression a good default is: m = p/3

Where m is the number of randomly selected features that can be searched at a split point and p is the number of input variables. For example, if a dataset had 25 input variables for a classification problem, then:

m = sqrt(25)

m = 5

Estimated Performance

For each bootstrap sample taken from the training data, there will be samples left behind that were not included. These samples are called Out-Of-Bag samples or OOB.

The performance of each model on its left out samples when averaged can provide an estimated accuracy of the bagged models. This estimated performance is often called the OOB estimate of performance.

These performance measures are reliable test error estimate and correlate well with cross validation estimates.

Variable Importance

As the Bagged decision trees are constructed, we can calculate how much the error function drops for a variable at each split point.

In regression problems this may be the drop in sum squared error and in classification this might be the Gini score.

These drops in error can be averaged across all decision trees and output to provide an estimate of the importance of each input variable. The greater the drop when the variable was chosen, the greater the importance.

These outputs can help identify subsets of input variables that may be most or least relevant to the problem and suggest at possible feature selection experiments you could perform where some features are removed from the dataset.

Further Reading

Bagging is a simple technique that is covered in most introductory machine learning texts. Some examples are listed below.

Summary

In this post you discovered the Bagging ensemble machine learning algorithm and the popular variation called Random Forest. You learned:

How to estimate statistical quantities from a data sample.

How to combine the predictions from multiple high-variance models using bagging.

How to tweak the construction of decision trees when bagging to de-correlate their predictions, a technique called Random Forests.

Do you have any questions about this post or the Bagging or Random Forest Ensemble algorithms?

Leave a comment and ask your question and I will do my best to answer it.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
7;machinelearningmastery.com;https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/;2020-01-07;Tour of Evaluation Metrics for Imbalanced Classification;"Tweet Share Share

Last Updated on January 14, 2020

A classifier is only as good as the metric used to evaluate it.

If you choose the wrong metric to evaluate your models, you are likely to choose a poor model, or in the worst case, be misled about the expected performance of your model.

Choosing an appropriate metric is challenging generally in applied machine learning, but is particularly difficult for imbalanced classification problems. Firstly, because most of the standard metrics that are widely used assume a balanced class distribution, and because typically not all classes, and therefore, not all prediction errors, are equal for imbalanced classification.

In this tutorial, you will discover metrics that you can use for imbalanced classification.

After completing this tutorial, you will know:

About the challenge of choosing metrics for classification, and how it is particularly difficult when there is a skewed class distribution.

How there are three main types of metrics for evaluating classifier models, referred to as rank, threshold, and probability.

How to choose a metric for imbalanced classification if you don’t know where to start.

Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more in my new book, with 30 step-by-step tutorials and full Python source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Challenge of Evaluation Metrics Taxonomy of Classifier Evaluation Metrics How to Choose an Evaluation Metric

Challenge of Evaluation Metrics

An evaluation metric quantifies the performance of a predictive model.

This typically involves training a model on a dataset, using the model to make predictions on a holdout dataset not used during training, then comparing the predictions to the expected values in the holdout dataset.

For classification problems, metrics involve comparing the expected class label to the predicted class label or interpreting the predicted probabilities for the class labels for the problem.

Selecting a model, and even the data preparation methods together are a search problem that is guided by the evaluation metric. Experiments are performed with different models and the outcome of each experiment is quantified with a metric.

Evaluation measures play a crucial role in both assessing the classification performance and guiding the classifier modeling.

— Classification Of Imbalanced Data: A Review, 2009.

There are standard metrics that are widely used for evaluating classification predictive models, such as classification accuracy or classification error.

Standard metrics work well on most problems, which is why they are widely adopted. But all metrics make assumptions about the problem or about what is important in the problem. Therefore an evaluation metric must be chosen that best captures what you or your project stakeholders believe is important about the model or predictions, which makes choosing model evaluation metrics challenging.

This challenge is made even more difficult when there is a skew in the class distribution. The reason for this is that many of the standard metrics become unreliable or even misleading when classes are imbalanced, or severely imbalanced, such as 1:100 or 1:1000 ratio between a minority and majority class.

In the case of class imbalances, the problem is even more acute because the default, relatively robust procedures used for unskewed data can break down miserably when the data is skewed.

— Page 187, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

For example, reporting classification accuracy for a severely imbalanced classification problem could be dangerously misleading. This is the case if project stakeholders use the results to draw conclusions or plan new projects.

In fact, the use of common metrics in imbalanced domains can lead to sub-optimal classification models and might produce misleading conclusions since these measures are insensitive to skewed domains.

— A Survey of Predictive Modelling under Imbalanced Distributions, 2015.

Importantly, different evaluation metrics are often required when working with imbalanced classification.

Unlike standard evaluation metrics that treat all classes as equally important, imbalanced classification problems typically rate classification errors with the minority class as more important than those with the majority class. As such performance metrics may be needed that focus on the minority class, which is made challenging because it is the minority class where we lack observations required to train an effective model.

The main problem of imbalanced data sets lies on the fact that they are often associated with a user preference bias towards the performance on cases that are poorly represented in the available data sample.

— A Survey of Predictive Modelling under Imbalanced Distributions, 2015.

Now that we are familiar with the challenge of choosing a model evaluation metric, let’s look at some examples of different metrics from which we might choose.

Want to Get Started With Imbalance Classification? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Taxonomy of Classifier Evaluation Metrics

There are tens of metrics to choose from when evaluating classifier models, and perhaps hundreds, if you consider all of the pet versions of metrics proposed by academics.

In order to get a handle on the metrics that you could choose from, we will use a taxonomy proposed by Cesar Ferri, et al. in their 2008 paper titled “An Experimental Comparison Of Performance Measures For Classification.” It was also adopted in the 2013 book titled “Imbalanced Learning” and I think proves useful.

We can divide evaluation metrics into three useful groups; they are:

Threshold Metrics Ranking Metrics Probability Metrics.

This division is useful because the top metrics used by practitioners for classifiers generally, and specifically imbalanced classification, fit into the taxonomy neatly.

Several machine learning researchers have identified three families of evaluation metrics used in the context of classification. These are the threshold metrics (e.g., accuracy and F-measure), the ranking methods and metrics (e.g., receiver operating characteristics (ROC) analysis and AUC), and the probabilistic metrics (e.g., root-mean-squared error).

— Page 189, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

Let’s take a closer look at each group in turn.

Threshold Metrics for Imbalanced Classification

Threshold metrics are those that quantify the classification prediction errors.

That is, they are designed to summarize the fraction, ratio, or rate of when a predicted class does not match the expected class in a holdout dataset.

Metrics based on a threshold and a qualitative understanding of error […] These measures are used when we want a model to minimise the number of errors.

— An Experimental Comparison Of Performance Measures For Classification, 2008.

Perhaps the most widely used threshold metric is classification accuracy.

Accuracy = Correct Predictions / Total Predictions

And the complement of classification accuracy called classification error.

Error = Incorrect Predictions / Total Predictions

Although widely used, classification accuracy is almost universally inappropriate for imbalanced classification. The reason is, a high accuracy (or low error) is achievable by a no skill model that only predicts the majority class.

For more on the failure of classification accuracy, see the tutorial:

For imbalanced classification problems, the majority class is typically referred to as the negative outcome (e.g. such as “no change” or “negative test result“), and the minority class is typically referred to as the positive outcome (e.g. “change” or “positive test result“).

Majority Class : Negative outcome, class 0.

: Negative outcome, class 0. Minority Class: Positive outcome, class 1.

Most threshold metrics can be best understood by the terms used in a confusion matrix for a binary (two-class) classification problem. This does not mean that the metrics are limited for use on binary classification; it is just an easy way to quickly understand what is being measured.

The confusion matrix provides more insight into not only the performance of a predictive model but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made. In this type of confusion matrix, each cell in the table has a specific and well-understood name, summarized as follows:

| Positive Prediction | Negative Prediction Positive Class | True Positive (TP) | False Negative (FN) Negative Class | False Positive (FP) | True Negative (TN) 1 2 3 | Positive Prediction | Negative Prediction Positive Class | True Positive (TP) | False Negative (FN) Negative Class | False Positive (FP) | True Negative (TN)

There are two groups of metrics that may be useful for imbalanced classification because they focus on one class; they are sensitivity-specificity and precision-recall.

Sensitivity-Specificity Metrics

Sensitivity refers to the true positive rate and summarizes how well the positive class was predicted.

Sensitivity = TruePositive / (TruePositive + FalseNegative)

Specificity is the complement to sensitivity, or the true negative rate, and summarises how well the negative class was predicted.

Specificity = TrueNegative / (FalsePositive + TrueNegative)

For imbalanced classification, the sensitivity might be more interesting than the specificity.

Sensitivity and Specificity can be combined into a single score that balances both concerns, called the geometric mean or G-Mean.

G-Mean = sqrt(Sensitivity * Specificity)

Precision-Recall Metrics

Precision summarizes the fraction of examples assigned the positive class that belong to the positive class.

Precision = TruePositive / (TruePositive + FalsePositive)

Recall summarizes how well the positive class was predicted and is the same calculation as sensitivity.

Recall = TruePositive / (TruePositive + FalseNegative)

Precision and recall can be combined into a single score that seeks to balance both concerns, called the F-score or the F-measure.

F-Measure = (2 * Precision * Recall) / (Precision + Recall)

The F-Measure is a popular metric for imbalanced classification.

The Fbeta-measure measure is an abstraction of the F-measure where the balance of precision and recall in the calculation of the harmonic mean is controlled by a coefficient called beta.

Fbeta-Measure = ((1 + beta^2) * Precision * Recall) / (beta^2 * Precision + Recall)

For more on precision, recall and F-measure for imbalanced classification, see the tutorial:

Additional Threshold Metrics

These are probably the most popular metrics to consider, although many others do exist. To give you a taste, these include Kappa, Macro-Average Accuracy, Mean-Class-Weighted Accuracy, Optimized Precision, Adjusted Geometric Mean, Balanced Accuracy, and more.

Threshold metrics are easy to calculate and easy to understand.

One limitation of these metrics is that they assume that the class distribution observed in the training dataset will match the distribution in the test set and in real data when the model is used to make predictions. This is often the case, but when it is not the case, the performance can be quite misleading.

An important disadvantage of all the threshold metrics discussed in the previous section is that they assume full knowledge of the conditions under which the classifier will be deployed. In particular, they assume that the class imbalance present in the training set is the one that will be encountered throughout the operating life of the classifier

— Page 196, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

Ranking metrics don’t make any assumptions about class distributions.

Ranking Metrics for Imbalanced Classification

Rank metrics are more concerned with evaluating classifiers based on how effective they are at separating classes.

Metrics based on how well the model ranks the examples […] These are important for many applications […] where classifiers are used to select the best n instances of a set of data or when good class separation is crucial.

— An Experimental Comparison Of Performance Measures For Classification, 2008.

These metrics require that a classifier predicts a score or a probability of class membership.

From this score, different thresholds can be applied to test the effectiveness of classifiers. Those models that maintain a good score across a range of thresholds will have good class separation and will be ranked higher.

… consider a classifier that gives a numeric score for an instance to be classified in the positive class. Therefore, instead of a simple positive or negative prediction, the score introduces a level of granularity

– Page 53, Learning from Imbalanced Data Sets, 2018.

The most commonly used ranking metric is the ROC Curve or ROC Analysis.

ROC is an acronym that means Receiver Operating Characteristic and summarizes a field of study for analyzing binary classifiers based on their ability to discriminate classes.

A ROC curve is a diagnostic plot for summarizing the behavior of a model by calculating the false positive rate and true positive rate for a set of predictions by the model under different thresholds.

The true positive rate is the recall or sensitivity.

TruePositiveRate = TruePositive / (TruePositive + FalseNegative)

The false positive rate is calculated as:

FalsePositiveRate = FalsePositive / (FalsePositive + TrueNegative)

Each threshold is a point on the plot and the points are connected to form a curve. A classifier that has no skill (e.g. predicts the majority class under all thresholds) will be represented by a diagonal line from the bottom left to the top right.

Any points below this line have worse than no skill. A perfect model will be a point in the top right of the plot.

The ROC Curve is a helpful diagnostic for one model.

The area under the ROC curve can be calculated and provides a single score to summarize the plot that can be used to compare models.

A no skill classifier will have a score of 0.5, whereas a perfect classifier will have a score of 1.0.

ROC AUC = ROC Area Under Curve

Although generally effective, the ROC Curve and ROC AUC can be optimistic under a severe class imbalance, especially when the number of examples in the minority class is small.

An alternative to the ROC Curve is the precision-recall curve that can be used in a similar way, although focuses on the performance of the classifier on the minority class.

Again, different thresholds are used on a set of predictions by a model, and in this case, the precision and recall are calculated. The points form a curve and classifiers that perform better under a range of different thresholds will be ranked higher.

A no-skill classifier will be a horizontal line on the plot with a precision that is proportional to the number of positive examples in the dataset. For a balanced dataset this will be 0.5. A perfect classifier is represented by a point in the top right.

Like the ROC Curve, the Precision-Recall Curve is a helpful diagnostic tool for evaluating a single classifier but challenging for comparing classifiers.

And like the ROC AUC, we can calculate the area under the curve as a score and use that score to compare classifiers. In this case, the focus on the minority class makes the Precision-Recall AUC more useful for imbalanced classification problems.

PR AUC = Precision-Recall Area Under Curve

There are other ranking metrics that are less widely used, such as modification to the ROC Curve for imbalanced classification and cost curves.

For more on ROC curves and precision-recall curves for imbalanced classification, see the tutorial:

Probabilistic Metrics for Imbalanced Classification

Probabilistic metrics are designed specifically to quantify the uncertainty in a classifier’s predictions.

These are useful for problems where we are less interested in incorrect vs. correct class predictions and more interested in the uncertainty the model has in predictions and penalizing those predictions that are wrong but highly confident.

Metrics based on a probabilistic understanding of error, i.e. measuring the deviation from the true probability […] These measures are especially useful when we want an assessment of the reliability of the classifiers, not only measuring when they fail but whether they have selected the wrong class with a high or low probability.

— An Experimental Comparison Of Performance Measures For Classification, 2008.

Evaluating a model based on the predicted probabilities requires that the probabilities are calibrated.

Some classifiers are trained using a probabilistic framework, such as maximum likelihood estimation, meaning that their probabilities are already calibrated. An example would be logistic regression.

Many nonlinear classifiers are not trained under a probabilistic framework and therefore require their probabilities to be calibrated against a dataset prior to being evaluated via a probabilistic metric. Examples might include support vector machines and k-nearest neighbors.

Perhaps the most common metric for evaluating predicted probabilities is log loss for binary classification (or the negative log likelihood), or known more generally as cross-entropy.

For a binary classification dataset where the expected values are y and the predicted values are yhat, this can be calculated as follows:

LogLoss = -((1 – y) * log(1 – yhat) + y * log(yhat))

The score can be generalized to multiple classes by simply adding the terms; for example:

LogLoss = -( sum c in C y_c * log(yhat_c))

The score summarizes the average difference between two probability distributions. A perfect classifier has a log loss of 0.0, with worse values being positive up to infinity.

Another popular score for predicted probabilities is the Brier score.

The benefit of the Brier score is that it is focused on the positive class, which for imbalanced classification is the minority class. This makes it more preferable than log loss, which is focused on the entire probability distribution.

The Brier score is calculated as the mean squared error between the expected probabilities for the positive class (e.g. 1.0) and the predicted probabilities. Recall that the mean squared error is the average of the squared differences between the values.

BrierScore = 1/N * Sum i to N (yhat_i – y_i)^2

A perfect classifier has a Brier score of 0.0. Although typically described in terms of binary classification tasks, the Brier score can also be calculated for multiclass classification problems.

The differences in Brier score for different classifiers can be very small. In order to address this problem, the score can be scaled against a reference score, such as the score from a no skill classifier (e.g. predicting the probability distribution of the positive class in the training dataset).

Using the reference score, a Brier Skill Score, or BSS, can be calculated where 0.0 represents no skill, worse than no skill results are negative, and the perfect skill is represented by a value of 1.0.

BrierSkillScore = 1 – (BrierScore / BrierScore_ref)

Although popular for balanced classification problems, probability scoring methods are less widely used for classification problems with a skewed class distribution.

For more on probabilistic metrics for imbalanced classification, see the tutorial:

How to Choose an Evaluation Metric

There is an enormous number of model evaluation metrics to choose from.

Given that choosing an evaluation metric is so important and there are tens or perhaps hundreds of metrics to choose from, what are you supposed to do?

The correct evaluation of learned models is one of the most important issues in pattern recognition.

— An Experimental Comparison Of Performance Measures For Classification, 2008.

Perhaps the best approach is to talk to project stakeholders and figure out what is important about a model or set of predictions. Then select a few metrics that seem to capture what is important, then test the metric with different scenarios.

A scenario might be a mock set of predictions for a test dataset with a skewed class distribution that matches your problem domain. You can test what happens to the metric if a model predicts all the majority class, all the minority class, does well, does poorly, and so on. A few small tests can rapidly help you get a feeling for how the metric might perform.

Another approach might be to perform a literature review and discover what metrics are most commonly used by other practitioners or academics working on the same general type of problem. This can often be insightful, but be warned that some fields of study may fall into groupthink and adopt a metric that might be excellent for comparing large numbers of models at scale, but terrible for model selection in practice.

Still have no idea?

Here are some first-order suggestions:

Are you predicting probabilities? Do you need class labels? Is the positive class more important? Use Precision-Recall AUC Are both classes important? Use ROC AUC Do you need probabilities? Use Brier Score and Brier Skill Score

Are you predicting class labels? Is the positive class more important? Are False Negatives and False Positives Equally Important? Use F1-Measure Are False Negatives More Important? Use F2-Measure Are False Positives More Important? Use F0.5-Measure Are both classes important? Do you have < 80%-90% Examples for the Majority Class? Use Accuracy Do you have > 80%-90% Examples for the Majority Class? Use G-Mean



These suggestions take the important case into account where we might use models that predict probabilities, but require crisp class labels. This is an important class of problems that allow the operator or implementor to choose the threshold to trade-off misclassification errors. In this scenario, error metrics are required that consider all reasonable thresholds, hence the use of the area under curve metrics.

We can transform these suggestions into a helpful template.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

Books

Articles

Summary

In this tutorial, you discovered metrics that you can use for imbalanced classification.

Specifically, you learned:

About the challenge of choosing metrics for classification, and how it is particularly difficult when there is a skewed class distribution.

How there are three main types of metrics for evaluating classifier models, referred to as rank, threshold, and probability.

How to choose a metric for imbalanced classification if you don’t know where to start.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Imbalanced Classification! Develop Imbalanced Learning Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Imbalanced Classification with Python It provides self-study tutorials and end-to-end projects on:

Performance Metrics, Undersampling Methods, SMOTE, Threshold Moving, Probability Calibration, Cost-Sensitive Algorithms

and much more... Bring Imbalanced Classification Methods to Your Machine Learning Projects See What's Inside"
8;news.mit.edu;http://news.mit.edu/2020/how-to-stage-revolution-mit-history-class-0107;;How to stage a revolution;"Revolutions are monumental social upheavals that can remake whole nations, dismantling — often violently — old paradigms. But the stories of the epic struggles that leave their mark on the world’s history are frequently fragile, precarious, and idiosyncratic in their details, leaving some key questions only partially understood: Why and how do peoples overthrow their governments? Why do some revolutions succeed and others fail?

These are not simple questions, and, for 12 years, MIT students and faculty have set out to answer them in a survey course that spans centuries and continents.

Course 21H.001 (How to Stage a Revolution, or Revolutions for short) is an MIT history class that examines the roots, drivers, and complexities of how governments fall. Co-taught this past fall by three historians — History Section head Professor Jeffrey Ravel, Associate Professor Tanalís Padilla, and Lecturer Pouya Alimagham — the semester is divided into three parts, with each instructor covering, respectively, the French Revolution, the Mexican Revolution, and the Iranian Revolution.

During a mixture of lectures and breakout discussion sessions, students explore the causes, tactics, goals, and significant factors of each revolution, drawing insights from music, film, art, constitutions, declarations, and the writings of revolutionaries themselves.

A wide-angle approach

The topics covered this year span centuries, from the near-mythic French Revolution (1789–99) to the Mexican Revolution (1910-20) to events that have emerged in the students’ own lifetimes, such as the Arab Spring (2010-12). Alimagham brought the semester to a close with a focus on the Iranian Revolution; having students begin their exploration with the roots of American intervention in Iran the latter half of the 20th century, and tracking developments through to today’s western media narrative of the Sunni/Shia conflict.

“Revolutions are a surprisingly good way to learn about a culture,” says Quinn Bowers, a first-year student who took the opportunity to deepen his understanding of history as a parallel to his intended double major in mechanical engineering and aerospace engineering. “Revolutions draw attention to the values the culture holds. This class did a lot to dispel assumptions I didn’t even know I had.”

For another first-year student, Somaia Saba, the offering leapt out at her as she browsed the course catalog to plan her first semester at MIT. With an intended major in computation and cognition (Course 6.9), she was drawn to the class by a fascination with major political transformations, “especially because of the tense political climate in which we are currently living.”

The freedom and exploration in essay-writing was a transformative experience for Saba; essay prompts and writing assignments had never been her favorite aspect of the classroom. But, snagged by a brief mention in class about women’s roles during the Mexican Revolution, she found herself writing extensively on the subject, drawing on her personal attentiveness to women’s issues and roles in history.

“I did not realize the extent to which these issues mattered to me until [seeing the professor’s] comments on my essay.” She also notes that the class has given her ways of thinking and analyzing that allow her to be more engaged with current political events.

Ever-evolving

How to Stage a Revolution is also a chameleon course in that its subject matter fluxes from year to year depending on the expertise of the faculty instructors — a plan that allows a venerable course to cover any number of revolutionary histories. Two years ago, for instance, when Alimagham first taught the course, working alongside MIT historians Caley Horan and Malick Ghachem, the class consisted of modules on the Haitian Revolution, the American Civil War (as America’s second revolution), and the Iranian Revolution.

Not only is the course constantly transforming, Alimagham notes, but its three co-instructors are always adapting as well. “When you’re involved in a team-taught course that includes material in which you are not the primary expert, you evolve as an instructor. It keeps you on your toes.”

Ravel agrees: “One benefit of co-teaching is that we learn from each other. It’s a great conversation among the three of us.”

Ravel currently serves as the head of the MIT History Section, as president of the American Society for Eighteenth-Century Studies, and as a co-director for the Comédie-Française Registers Project, which is producing a collaborative, extensive history of one of France’s iconic theater groups. “Co-teaching reminds me of what it’s like to be a student again,” reflects Padilla. “It makes me more sensitive to how students are taking in information that, for me, is now second nature.”

Padilla is a historian of Latin America and a contributor to numerous publications and volumes surrounding the Mexican Revolution. Her current book project centers on how rural schoolteachers “went from being agents of state consolidation to activists against a government that increasingly abandoned its commitment to social justice.”

The technological contexts of revolutions

Like a number of other humanistic courses at MIT, How to Stage a Revolution is also a hands-on “maker class.” In addition to classroom lectures and discussion sessions, students produce posters on MIT’s Beaver Press, a student-built replica of the wooden, handset printing presses on which the great documents of the Renaissance, the Reformation, and the Scientific Revolution were printed.

Carving linoleum printing plates and inking them by hand, students use their academic understanding of various revolutions to design and produce colorful pro- and counter-revolutionary posters. In one print, the evocative image of a Mexican worker raises the Olympic rings between his hands like chains. In another, the guillotine stands ready with its victims nearby, indicating a mounting death toll, each head labeled respectively with Liberté, Egalité, and Fraternité.

Historic revolutionary narratives have a particular urgency in an MIT classroom: From the dissemination of revolutionary messages via an 18th century printing press to changing fuel technologies to the global social media that shaped the Arab Spring, the technological contexts of revolutions are intrinsic to understanding them.

“Whatever we end up doing in our post-MIT lives and careers will be in the context of complex, real-world problems,” says Bowers. “This class sheds light on some of the world’s most volatile problems.”

Story by MIT SHASS Communications

Editorial and design director: Emily Hiestand

Writer/reporter: Alison Lanier"
9;machinelearningmastery.com;http://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/;2016-09-01;Avoid Overfitting By Early Stopping With XGBoost In Python;"# plot learning curve

from numpy import loadtxt

from xgboost import XGBClassifier

from sklearn . model_selection import train_test_split

from sklearn . metrics import accuracy_score

from matplotlib import pyplot

# load data

dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" )

# split data into X and y

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# split data into train and test sets

X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.33 , random_state = 7 )

# fit model no training data

model = XGBClassifier ( )

eval_set = [ ( X_train , y_train ) , ( X_test , y_test ) ]

model . fit ( X_train , y_train , eval_metric = [ ""error"" , ""logloss"" ] , eval_set = eval_set , verbose = True )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )

print ( ""Accuracy: %.2f%%"" % ( accuracy * 100.0 ) )

# retrieve performance metrics

results = model . evals_result ( )

epochs = len ( results [ 'validation_0' ] [ 'error' ] )

x_axis = range ( 0 , epochs )

# plot log loss

fig , ax = pyplot . subplots ( )

ax . plot ( x_axis , results [ 'validation_0' ] [ 'logloss' ] , label = 'Train' )

ax . plot ( x_axis , results [ 'validation_1' ] [ 'logloss' ] , label = 'Test' )

ax . legend ( )

pyplot . ylabel ( 'Log Loss' )

pyplot . title ( 'XGBoost Log Loss' )

pyplot . show ( )

# plot classification error

fig , ax = pyplot . subplots ( )

ax . plot ( x_axis , results [ 'validation_0' ] [ 'error' ] , label = 'Train' )

ax . plot ( x_axis , results [ 'validation_1' ] [ 'error' ] , label = 'Test' )

ax . legend ( )

pyplot . ylabel ( 'Classification Error' )

pyplot . title ( 'XGBoost Classification Error' )"
10;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/;2019-06-27;How to Develop a GAN for Generating MNIST Handwritten Digits;"# example of training a gan on mnist

from numpy import expand_dims

from numpy import zeros

from numpy import ones

from numpy import vstack

from numpy . random import randn

from numpy . random import randint

from keras . datasets . mnist import load_data

from keras . optimizers import Adam

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Reshape

from keras . layers import Flatten

from keras . layers import Conv2D

from keras . layers import Conv2DTranspose

from keras . layers import LeakyReLU

from keras . layers import Dropout

from matplotlib import pyplot

# define the standalone discriminator model

def define_discriminator ( in_shape = ( 28 , 28 , 1 ) ) :

model = Sequential ( )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = in_shape ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

return model

# define the standalone generator model

def define_generator ( latent_dim ) :

model = Sequential ( )

# foundation for 7x7 image

n_nodes = 128 * 7 * 7

model . add ( Dense ( n_nodes , input_dim = latent_dim ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Reshape ( ( 7 , 7 , 128 ) ) )

# upsample to 14x14

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# upsample to 28x28

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Conv2D ( 1 , ( 7 , 7 ) , activation = 'sigmoid' , padding = 'same' ) )

return model

# define the combined generator and discriminator model, for updating the generator

def define_gan ( g_model , d_model ) :

# make weights in the discriminator not trainable

d_model . trainable = False

# connect them

model = Sequential ( )

# add generator

model . add ( g_model )

# add the discriminator

model . add ( d_model )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt )

return model

# load and prepare mnist training images

def load_real_samples ( ) :

# load mnist dataset

( trainX , _ ) , ( _ , _ ) = load_data ( )

# expand to 3d, e.g. add channels dimension

X = expand_dims ( trainX , axis = - 1 )

# convert from unsigned ints to floats

X = X . astype ( 'float32' )

# scale from [0,255] to [0,1]

X = X / 255.0

return X

# select real samples

def generate_real_samples ( dataset , n_samples ) :

# choose random instances

ix = randint ( 0 , dataset . shape [ 0 ] , n_samples )

# retrieve selected images

X = dataset [ ix ]

# generate 'real' class labels (1)

y = ones ( ( n_samples , 1 ) )

return X , y

# generate points in latent space as input for the generator

def generate_latent_points ( latent_dim , n_samples ) :

# generate points in the latent space

x_input = randn ( latent_dim * n_samples )

# reshape into a batch of inputs for the network

x_input = x_input . reshape ( n_samples , latent_dim )

return x_input

# use the generator to generate n fake examples, with class labels

def generate_fake_samples ( g_model , latent_dim , n_samples ) :

# generate points in latent space

x_input = generate_latent_points ( latent_dim , n_samples )

# predict outputs

X = g_model . predict ( x_input )

# create 'fake' class labels (0)

y = zeros ( ( n_samples , 1 ) )

return X , y

# create and save a plot of generated images (reversed grayscale)

def save_plot ( examples , epoch , n = 10 ) :

# plot images

for i in range ( n * n ) :

# define subplot

pyplot . subplot ( n , n , 1 + i )

# turn off axis

pyplot . axis ( 'off' )

# plot raw pixel data

pyplot . imshow ( examples [ i , : , : , 0 ] , cmap = 'gray_r' )

# save plot to file

filename = 'generated_plot_e%03d.png' % ( epoch + 1 )

pyplot . savefig ( filename )

pyplot . close ( )

# evaluate the discriminator, plot generated images, save generator model

def summarize_performance ( epoch , g_model , d_model , dataset , latent_dim , n_samples = 100 ) :

# prepare real samples

X_real , y_real = generate_real_samples ( dataset , n_samples )

# evaluate discriminator on real examples

_ , acc_real = d_model . evaluate ( X_real , y_real , verbose = 0 )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( g_model , latent_dim , n_samples )

# evaluate discriminator on fake examples

_ , acc_fake = d_model . evaluate ( x_fake , y_fake , verbose = 0 )

# summarize discriminator performance

print ( '>Accuracy real: %.0f%%, fake: %.0f%%' % ( acc_real* 100 , acc_fake* 100 ) )

# save plot

save_plot ( x_fake , epoch )

# save the generator model tile file

filename = 'generator_model_%03d.h5' % ( epoch + 1 )

g_model . save ( filename )

# train the generator and discriminator

def train ( g_model , d_model , gan_model , dataset , latent_dim , n_epochs = 100 , n_batch = 256 ) :

bat_per_epo = int ( dataset . shape [ 0 ] / n_batch )

half_batch = int ( n_batch / 2 )

# manually enumerate epochs

for i in range ( n_epochs ) :

# enumerate batches over the training set

for j in range ( bat_per_epo ) :

# get randomly selected 'real' samples

X_real , y_real = generate_real_samples ( dataset , half_batch )

# generate 'fake' examples

X_fake , y_fake = generate_fake_samples ( g_model , latent_dim , half_batch )

# create training set for the discriminator

X , y = vstack ( ( X_real , X_fake ) ) , vstack ( ( y_real , y_fake ) )

# update discriminator model weights

d_loss , _ = d_model . train_on_batch ( X , y )

# prepare points in latent space as input for the generator

X_gan = generate_latent_points ( latent_dim , n_batch )

# create inverted labels for the fake samples

y_gan = ones ( ( n_batch , 1 ) )

# update the generator via the discriminator's error

g_loss = gan_model . train_on_batch ( X_gan , y_gan )

# summarize loss on this batch

print ( '>%d, %d/%d, d=%.3f, g=%.3f' % ( i + 1 , j + 1 , bat_per_epo , d_loss , g_loss ) )

# evaluate the model performance, sometimes

if ( i + 1 ) % 10 == 0 :

summarize_performance ( i , g_model , d_model , dataset , latent_dim )

# size of the latent space

latent_dim = 100

# create the discriminator

d_model = define_discriminator ( )

# create the generator

g_model = define_generator ( latent_dim )

# create the gan

gan_model = define_gan ( g_model , d_model )

# load image data

dataset = load_real_samples ( )

# train model"
11;machinelearningmastery.com;https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/;2017-05-23;A Gentle Introduction to Long Short-Term Memory Networks by the Experts;"Tweet Share Share

Last Updated on February 20, 2020

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.

This is a behavior required in complex problem domains like machine translation, speech recognition, and more.

LSTMs are a complex area of deep learning. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional and sequence-to-sequence relate to the field.

In this post, you will get insight into LSTMs using the words of research scientists that developed the methods and applied them to new and important problems.

There are few that are better at clearly and precisely articulating both the promise of LSTMs and how they work than the experts that developed them.

We will explore key questions in the field of LSTMs using quotes from the experts, and if you’re interested, you will be able to dive into the original papers from which the quotes were taken.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

The Promise of Recurrent Neural Networks

Recurrent neural networks are different from traditional feed-forward neural networks.

This difference in the addition of complexity comes with the promise of new behaviors that the traditional methods cannot achieve.

Recurrent networks … have an internal state that can represent context information. … [they] keep information about past inputs for an amount of time that is not fixed a priori, but rather depends on its weights and on the input data. … A recurrent network whose inputs are not fixed but rather constitute an input sequence can be used to transform an input sequence into an output sequence while taking into account contextual information in a flexible way.

— Yoshua Bengio, et al., Learning Long-Term Dependencies with Gradient Descent is Difficult, 1994.

The paper defines 3 basic requirements of a recurrent neural network:

That the system be able to store information for an arbitrary duration.

That the system be resistant to noise (i.e. fluctuations of the inputs that are random or irrelevant to predicting a correct output).

That the system parameters be trainable (in reasonable time).

The paper also describes the “minimal task” for demonstrating recurrent neural networks.

Context is key.

Recurrent neural networks must use context when making predictions, but to this extent, the context required must also be learned.

… recurrent neural networks contain cycles that feed the network activations from a previous time step as inputs to the network to influence predictions at the current time step. These activations are stored in the internal states of the network which can in principle hold long-term temporal contextual information. This mechanism allows RNNs to exploit a dynamically changing contextual window over the input sequence history

— Hassim Sak, et al., Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling, 2014

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

LSTMs Deliver on the Promise

The success of LSTMs is in their claim to be one of the first implements to overcome the technical problems and deliver on the promise of recurrent neural networks.

Hence standard RNNs fail to learn in the presence of time lags greater than 5 – 10 discrete time steps between relevant input events and target signals. The vanishing error problem casts doubt on whether standard RNNs can indeed exhibit significant practical advantages over time window-based feedforward networks. A recent model, “Long Short-Term Memory” (LSTM), is not affected by this problem. LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through “constant error carrousels” (CECs) within special units, called cells

— Felix A. Gers, et al., Learning to Forget: Continual Prediction with LSTM, 2000

The two technical problems overcome by LSTMs are vanishing gradients and exploding gradients, both related to how the network is trained.

Unfortunately, the range of contextual information that standard RNNs can access is in practice quite limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network’s recurrent connections. This shortcoming … referred to in the literature as the vanishing gradient problem … Long Short-Term Memory (LSTM) is an RNN architecture specifically designed to address the vanishing gradient problem.

— Alex Graves, et al., A Novel Connectionist System for Unconstrained Handwriting Recognition, 2009

The key to the LSTM solution to the technical problems was the specific internal structure of the units used in the model.

… governed by its ability to deal with vanishing and exploding gradients, the most common challenge in designing and training RNNs. To address this challenge, a particular form of recurrent nets, called LSTM, was introduced and applied with great success to translation and sequence generation.

— Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.

How do LSTMs Work?

Rather than go into the equations that govern how LSTMs are fit, analogy is a useful tool to quickly get a handle on how they work.

We use networks with one input layer, one hidden layer, and one output layer… The (fully) self-connected hidden layer contains memory cells and corresponding gate units… … Each memory cell’s internal architecture guarantees constant error ow within its constant error carrousel CEC… This represents the basis for bridging very long time lags. Two gate units learn to open and close access to error ow within each memory cell’s CEC. The multiplicative input gate affords protection of the CEC from perturbation by irrelevant inputs. Likewise, the multiplicative output gate protects other units from perturbation by currently irrelevant memory contents.

— Sepp Hochreiter and Jurgen Schmidhuber, Long Short-Term Memory, 1997.

Multiple analogies can help to give purchase on what differentiates LSTMs from traditional neural networks comprised of simple neurons.

The Long Short Term Memory architecture was motivated by an analysis of error flow in existing RNNs which found that long time lags were inaccessible to existing architectures, because backpropagated error either blows up or decays exponentially. An LSTM layer consists of a set of recurrently connected blocks, known as memory blocks. These blocks can be thought of as a differentiable version of the memory chips in a digital computer. Each one contains one or more recurrently connected memory cells and three multiplicative units – the input, output and forget gates – that provide continuous analogues of write, read and reset operations for the cells. … The net can only interact with the cells via the gates.

— Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.

It is interesting to note, that even after more than 20 years, the simple (or vanilla) LSTM may still be the best place to start when applying the technique.

The most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets… Learning rate and network size are the most crucial tunable LSTM hyperparameters … … This implies that the hyperparameters can be tuned independently. In particular, the learning rate can be calibrated first using a fairly small network, thus saving a lot of experimentation time.

— Klaus Greff, et al., LSTM: A Search Space Odyssey, 2015

What are LSTM Applications?

It is important to get a handle on exactly what type of sequence learning problems that LSTMs are suitable to address.

Long Short-Term Memory (LSTM) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). … … LSTM holds promise for any sequential processing task in which we suspect that a hierarchical decomposition may exist, but do not know in advance what this decomposition is.

— Felix A. Gers, et al., Learning to Forget: Continual Prediction with LSTM, 2000

The Recurrent Neural Network (RNN) is neural sequence model that achieves state of the art performance on important tasks that include language modeling, speech recognition, and machine translation.

— Wojciech Zaremba, Recurrent Neural Network Regularization, 2014.

Since LSTMs are effective at capturing long-term temporal dependencies without suffering from the optimization hurdles that plague simple recurrent networks (SRNs), they have been used to advance the state of the art for many difficult problems. This includes handwriting recognition and generation, language modeling and translation, acoustic modeling of speech, speech synthesis, protein secondary structure prediction, analysis of audio, and video data among others.

— Klaus Greff, et al., LSTM: A Search Space Odyssey, 2015

What are Bidirectional LSTMs?

A commonly mentioned improvement upon LSTMs are bidirectional LSTMs.

The basic idea of bidirectional recurrent neural nets is to present each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. … This means that for every point in a given sequence, the BRNN has complete, sequential information about all points before and after it. Also, because the net is free to use as much or as little of this context as necessary, there is no need to find a (task-dependent) time-window or target delay size. … for temporal problems like speech recognition, relying on knowledge of the future seems at first sight to violate causality … How can we base our understanding of what we’ve heard on something that hasn’t been said yet? However, human listeners do exactly that. Sounds, words, and even whole sentences that at first mean nothing are found to make sense in the light of future context.

— Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.

One shortcoming of conventional RNNs is that they are only able to make use of previous context. … Bidirectional RNNs (BRNNs) do this by processing the data in both directions with two separate hidden layers, which are then fed forwards to the same output layer. … Combining BRNNs with LSTM gives bidirectional LSTM, which can access long-range context in both input directions

— Alex Graves, et al., Speech recognition with deep recurrent neural networks, 2013

Unlike conventional RNNs, bidirectional RNNs utilize both the previous and future context, by processing the data from two directions with two separate hidden layers. One layer processes the input sequence in the forward direction, while the other processes the input in the reverse direction. The output of current time step is then generated by combining both layers’ hidden vector…

— Di Wang and Eric Nyberg, A Long Short-Term Memory Model for Answer Sentence Selection in

Question Answering, 2015

What are seq2seq LSTMs or RNN Encoder-Decoders?

The sequence-to-sequence LSTM, also called encoder-decoder LSTMs, are an application of LSTMs that are receiving a lot of attention given their impressive capability.

… a straightforward application of the Long Short-Term Memory (LSTM) architecture can solve general sequence to sequence problems. … The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector. The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence. The LSTM’s ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs. We were able to do well on long sentences because we reversed the order of words in the source sentence but not the target sentences in the training and test set. By doing so, we introduced many short term dependencies that made the optimization problem much simpler. … The simple trick of reversing the words in the source sentence is one of the key technical contributions of this work

— Ilya Sutskever, et al., Sequence to Sequence Learning with Neural Networks, 2014

An “encoder” RNN reads the source sentence and transforms it into a rich fixed-length vector representation, which in turn in used as the initial hidden state of a “decoder” RNN that generates the target sentence. Here, we propose to follow this elegant recipe, replacing the encoder RNN by a deep convolution neural network (CNN). … it is natural to use a CNN as an image “encoder”, by first pre-training it for an image classification task and using the last hidden layer as an input to the RNN decoder that generates sentences.

— Oriol Vinyals, et al., Show and Tell: A Neural Image Caption Generator, 2014

… an RNN Encoder–Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence.

— Kyunghyun Cho, et al., Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014

Summary

In this post, you received a gentle introduction to LSTMs in the words of the research scientists that developed and applied the techniques.

This provides you both with a clear and precise idea of what LSTMs are and how they work, as well as important articulation on the promise of LSTMs in the field of recurrent neural networks.

Did any of the quotes help your understanding or inspire you?

Let me know in the comments below.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside"
12;machinelearningmastery.com;https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/;2018-09-03;How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course);"# univariate mlp example

from numpy import array

from keras . models import Sequential

from keras . layers import Dense

# define dataset

X = array ( [ [ 10 , 20 , 30 ] , [ 20 , 30 , 40 ] , [ 30 , 40 , 50 ] , [ 40 , 50 , 60 ] ] )

y = array ( [ 40 , 50 , 60 , 70 ] )

# define model

model = Sequential ( )

model . add ( Dense ( 100 , activation = 'relu' , input_dim = 3 ) )

model . add ( Dense ( 1 ) )

model . compile ( optimizer = 'adam' , loss = 'mse' )

# fit model

model . fit ( X , y , epochs = 2000 , verbose = 0 )

# demonstrate prediction

x_input = array ( [ 50 , 60 , 70 ] )

x_input = x_input . reshape ( ( 1 , 3 ) )

yhat = model . predict ( x_input , verbose = 0 )"
13;news.mit.edu;http://news.mit.edu/2020/cnt-nanosensor-smartphone-plant-stress-0415;;Nanosensor can alert a smartphone when plants are stressed;"MIT engineers have developed a way to closely track how plants respond to stresses such as injury, infection, and light damage, using sensors made of carbon nanotubes. These sensors can be embedded in plant leaves, where they report on hydrogen peroxide signaling waves.

Plants use hydrogen peroxide to communicate within their leaves, sending out a distress signal that stimulates leaf cells to produce compounds that will help them repair damage or fend off predators such as insects. The new sensors can use these hydrogen peroxide signals to distinguish between different types of stress, as well as between different species of plants.

“Plants have a very sophisticated form of internal communication, which we can now observe for the first time. That means that in real-time, we can see a living plant’s response, communicating the specific type of stress that it’s experiencing,” says Michael Strano, the Carbon P. Dubbs Professor of Chemical Engineering at MIT.

This kind of sensor could be used to study how plants respond to different types of stress, potentially helping agricultural scientists develop new strategies to improve crop yields. The researchers demonstrated their approach in eight different plant species, including spinach, strawberry plants, and arugula, and they believe it could work in many more.

Strano is the senior author of the study, which appears today in Nature Plants. MIT graduate student Tedrick Thomas Salim Lew is the lead author of the paper.

Embedded sensors

Over the past several years, Strano’s lab has been exploring the potential for engineering “nanobionic plants” — plants that incorporate nanomaterials that give the plants new functions, such as emitting light or detecting water shortages. In the new study, he set out to incorporate sensors that would report back on the plants’ health status.

Strano had previously developed carbon nanotube sensors that can detect various molecules, including hydrogen peroxide. About three years ago, Lew began working on trying to incorporate these sensors into plant leaves. Studies in Arabidopsis thaliana, often used for molecular studies of plants, had suggested that plants might use hydrogen peroxide as a signaling molecule, but its exact role was unclear.

Lew used a method called lipid exchange envelope penetration (LEEP) to incorporate the sensors into plant leaves. LEEP, which Strano’s lab developed several years ago, allows for the design of nanoparticles that can penetrate plant cell membranes. As Lew was working on embedding the carbon nanotube sensors, he made a serendipitous discovery.

“I was training myself to get familiarized with the technique, and in the process of the training I accidentally inflicted a wound on the plant. Then I saw this evolution of the hydrogen peroxide signal,” he says.

He saw that after a leaf was injured, hydrogen peroxide was released from the wound site and generated a wave that spread along the leaf, similar to the way that neurons transmit electrical impulses in our brains. As a plant cell releases hydrogen peroxide, it triggers calcium release within adjacent cells, which stimulates those cells to release more hydrogen peroxide.

“Like dominos successively falling, this makes a wave that can propagate much further than a hydrogen peroxide puff alone would,” Strano says. “The wave itself is powered by the cells that receive and propagate it.”

This flood of hydrogen peroxide stimulates plant cells to produce molecules called secondary metabolites, such as flavonoids or carotenoids, which help them to repair the damage. Some plants also produce other secondary metabolites that can be secreted to fend off predators. These metabolites are often the source of the food flavors that we desire in our edible plants, and they are only produced under stress.

A key advantage of the new sensing technique is that it can be used in many different plant species. Traditionally, plant biologists have done much of their molecular biology research in certain plants that are amenable to genetic manipulation, including Arabidopsis thaliana and tobacco plants. However, the new MIT approach is applicable to potentially any plant.

“In this study, we were able to quickly compare eight plant species, and you would not be able to do that with the old tools,” Strano says.

The researchers tested strawberry plants, spinach, arugula, lettuce, watercress, and sorrel, and found that different species appear to produce different waveforms — the distinctive shape produced by mapping the concentration of hydrogen peroxide over time. They hypothesize that each plant’s response is related to its ability to counteract the damage. Each species also appears to respond differently to different types of stress, including mechanical injury, infection, and heat or light damage.

“This waveform holds a lot of information for each species, and even more exciting is that the type of stress on a given plant is encoded in this waveform,” Strano says. “You can look at the real time response that a plant experiences in almost any new environment.”

Stress response

The near-infrared fluorescence produced by the sensors can be imaged using a small infrared camera connected to a Raspberry Pi, a $35 credit-card-sized computer similar to the computer inside a smartphone. “Very inexpensive instrumentation can be used to capture the signal,” Strano says.

Applications for this technology include screening different species of plants for their ability to resist mechanical damage, light, heat, and other forms of stress, Strano says. It could also be used to study how different species respond to pathogens, such as the bacteria that cause citrus greening and the fungus that causes coffee rust.

“One of the things I’m interested in doing is understanding why some types of plants exhibit certain immunity to these pathogens and others don’t,” he says.

Strano and his colleagues in the Disruptive and Sustainable Technology for Agricultural Precision interdisciplinary research group at the Singapore-MIT Alliance for Research and Technology (SMART), MIT’s research enterprise in Singapore, are also interested in studying is how plants respond to different growing conditions in urban farms.

One problem they hope to address is shade avoidance, which is seen in many species of plants when they are grown at high density. Such plants turn on a stress response that diverts their resources into growing taller, instead of putting energy into producing crops. This lowers the overall crop yield, so agricultural researchers are interested in engineering plants so that don’t turn on that response.

“Our sensor allows us to intercept that stress signal and to understand exactly the conditions and the mechanism that are happening upstream and downstream in the plant that gives rise to the shade avoidance,” Strano says.

The research was funded by the National Research Foundation of Singapore, the Singapore Agency for Science, Technology, and Research (A*STAR), and the U.S. Department of Energy Computational Science Graduate Fellowship Program."
14;machinelearningmastery.com;http://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/;2016-03-13;Parametric and Nonparametric Machine Learning Algorithms;"Tweet Share Share

Last Updated on October 25, 2019

What is a parametric machine learning algorithm and how is it different from a nonparametric machine learning algorithm?

In this post you will discover the difference between parametric and nonparametric machine learning algorithms.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Learning a Function

Machine learning can be summarized as learning a function (f) that maps input variables (X) to output variables (Y).

Y = f(x)

An algorithm learns this target mapping function from training data.

The form of the function is unknown, so our job as machine learning practitioners is to evaluate different machine learning algorithms and see which is better at approximating the underlying function.

Different algorithms make different assumptions or biases about the form of the function and how it can be learned.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Parametric Machine Learning Algorithms

Assumptions can greatly simplify the learning process, but can also limit what can be learned. Algorithms that simplify the function to a known form are called parametric machine learning algorithms.

A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.

— Artificial Intelligence: A Modern Approach, page 737

The algorithms involve two steps:

Select a form for the function. Learn the coefficients for the function from the training data.

An easy to understand functional form for the mapping function is a line, as is used in linear regression:

b0 + b1*x1 + b2*x2 = 0

Where b0, b1 and b2 are the coefficients of the line that control the intercept and slope, and x1 and x2 are two input variables.

Assuming the functional form of a line greatly simplifies the learning process. Now, all we need to do is estimate the coefficients of the line equation and we have a predictive model for the problem.

Often the assumed functional form is a linear combination of the input variables and as such parametric machine learning algorithms are often also called “linear machine learning algorithms“.

The problem is, the actual unknown underlying function may not be a linear function like a line. It could be almost a line and require some minor transformation of the input data to work right. Or it could be nothing like a line in which case the assumption is wrong and the approach will produce poor results.

Some more examples of parametric machine learning algorithms include:

Logistic Regression

Linear Discriminant Analysis

Perceptron

Naive Bayes

Simple Neural Networks

Benefits of Parametric Machine Learning Algorithms:

Simpler : These methods are easier to understand and interpret results.

: These methods are easier to understand and interpret results. Speed : Parametric models are very fast to learn from data.

: Parametric models are very fast to learn from data. Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect.

Limitations of Parametric Machine Learning Algorithms:

Constrained : By choosing a functional form these methods are highly constrained to the specified form.

: By choosing a functional form these methods are highly constrained to the specified form. Limited Complexity : The methods are more suited to simpler problems.

: The methods are more suited to simpler problems. Poor Fit: In practice the methods are unlikely to match the underlying mapping function.

Nonparametric Machine Learning Algorithms

Algorithms that do not make strong assumptions about the form of the mapping function are called nonparametric machine learning algorithms. By not making assumptions, they are free to learn any functional form from the training data.

Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features.

— Artificial Intelligence: A Modern Approach, page 757

Nonparametric methods seek to best fit the training data in constructing the mapping function, whilst maintaining some ability to generalize to unseen data. As such, they are able to fit a large number of functional forms.

An easy to understand nonparametric model is the k-nearest neighbors algorithm that makes predictions based on the k most similar training patterns for a new data instance. The method does not assume anything about the form of the mapping function other than patterns that are close are likely to have a similar output variable.

Some more examples of popular nonparametric machine learning algorithms are:

k-Nearest Neighbors

Decision Trees like CART and C4.5

Support Vector Machines

Benefits of Nonparametric Machine Learning Algorithms:

Flexibility : Capable of fitting a large number of functional forms.

: Capable of fitting a large number of functional forms. Power : No assumptions (or weak assumptions) about the underlying function.

: No assumptions (or weak assumptions) about the underlying function. Performance: Can result in higher performance models for prediction.

Limitations of Nonparametric Machine Learning Algorithms:

More data : Require a lot more training data to estimate the mapping function.

: Require a lot more training data to estimate the mapping function. Slower : A lot slower to train as they often have far more parameters to train.

: A lot slower to train as they often have far more parameters to train. Overfitting: More of a risk to overfit the training data and it is harder to explain why specific predictions are made.

Further Reading

This section lists some resources if you are looking to learn more about the difference between parametric and non-parametric machine learning algorithms.

Books

Posts

Summary

In this post you have discovered the difference between parametric and nonparametric machine learning algorithms.

You learned that parametric methods make large assumptions about the mapping of the input variables to the output variable and in turn are faster to train, require less data but may not be as powerful.

You also learned that nonparametric methods make few or no assumptions about the target function and in turn require a lot more data, are slower to train and have a higher model complexity but can result in more powerful models.

If you have any questions about parametric or nonparametric machine learning algorithms or this post, leave a comment and I will do my best to answer them.

Update: I originally had some algorithms listed under the wrong sections like neural nets and naive bayes, which made things confusing. All fixed now.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
15;news.mit.edu;http://news.mit.edu/2020/bringing-deep-learning-to-life-0224;;Bringing deep learning to life;"Gaby Ecanow loves listening to music, but never considered writing her own until taking 6.S191 (Introduction to Deep Learning). By her second class, the second-year MIT student had composed an original Irish folk song with the help of a recurrent neural network, and was considering how to adapt the model to create her own Louis the Child-inspired dance beats.

“It was cool,” she says. “It didn’t sound at all like a machine had made it.”

This year, 6.S191 kicked off as usual, with students spilling into the aisles of Stata Center’s Kirsch Auditorium during Independent Activities Period (IAP). But the opening lecture featured a twist: a recorded welcome from former President Barack Obama. The video was quickly revealed to be an AI-generated fabrication, one of many twists that Alexander Amini ’17 and Ava Soleimany ’16 introduce throughout their for-credit course to make the equations and code come alive.

As hundreds of their peers look on, Amini and Soleimany take turns at the podium. If they appear at ease, it’s because they know the material cold; they designed the curriculum themselves, and have taught it for the past three years. The course covers the technical foundations of deep learning and its societal implications through lectures and software labs focused on real-world applications. On the final day, students compete for prizes by pitching their own ideas for research projects. In the weeks leading up to class, Amini and Soleimany spend hours updating the labs, refreshing their lectures, and honing their presentations.

A branch of machine learning, deep learning harnesses massive data and algorithms modeled loosely on how the brain processes information to make predictions. The class has been credited with helping to spread machine-learning tools into research labs across MIT. That’s by design, says Amini, a graduate student in MIT’s Department of Electrical Engineering and Computer Science (EECS), and Soleimany, a graduate student at MIT and Harvard University.

Both are using machine learning in their own research — Amini in engineering robots, and Soleimany in developing diagnostic tools for cancer — and they wanted to make sure the curriculum would prepare students to do the same. In addition to the lab on developing a music-generating AI, they offer labs on building a face-recognition model with convolutional neural networks and a bot that uses reinforcement learning to play the vintage Atari video game, Pong. After students master the basics, those taking the class for credit go on to create applications of their own.

This year, 23 teams presented projects. Among the prize winners was Carmen Martin, a graduate student in the Harvard-MIT Program in Health Sciences and Technology (HST), who proposed using a type of neural net called a graph convolutional network to predict the spread of coronavirus. She combined several data streams: airline ticketing data to measure population fluxes, real-time confirmation of new infections, and a ranking of how well countries are equipped to prevent and respond to a pandemic.

“The goal is to train the model to predict cases to guide national governments and the World Health Organization in their recommendations to limit new cases and save lives,” she says.

A second prize winner, EECS graduate student Samuel Sledzieski, proposed building a model to predict protein interactions using only their amino acid sequences. Predicting protein behavior is key to designing drug targets, among other clinical applications, and Sledzieski wondered if deep learning could speed up the search for viable protein pairs.

“There’s still work to be done, but I’m excited by how far I was able to get in three days,” he says. “Having easy-to-follow examples in TensorFlow and Keras helped me understand how to actually build and train these models myself.” He plans to continue the work in his current lab rotation with Bonnie Berger, the Simons Professor of Mathematics in EECS and the Computer Science and Artificial Intelligence Laboratory (CSAIL).

Each year, students also hear about emerging deep-learning applications from companies sponsoring the course. David Cox, co-director of the MIT-IBM Watson AI Lab, covered neuro-symbolic AI, a hybrid approach that combines symbolic programs with deep learning’s expert pattern-matching ability. Alex Wiltschko, a senior researcher at Google Brain, spoke about using a network analysis tool to predict the scent of small molecules. Chuan Li, chief scientific officer at Lambda Labs, discussed neural rendering, a tool for reconstructing and generating graphics scenes. Animesh Garg, a senior researcher at NVIDIA, covered strategies for developing robots that perceive and act more human-like.

With 350 students taking the live course each year, and more than a million people who have watched the lectures online, Amini and Soleimany have become prominent ambassadors for deep learning. Yet, it was tennis that first brought them together.

Amini competed nationally as a high school student in Ireland and built an award-winning AI model to help amateur and pro tennis players improve their strokes; Soleimany was a two-time captain of the MIT women’s tennis team. They met on the court as undergraduates and discovered they shared a passion for machine learning.

After finishing their undergraduate degrees, they decided to challenge themselves and fill what they saw as an increasing need at MIT for a foundational course in deep learning. 6.S191 was launched in 2017 by two grad students, Nick Locascio and Harini Suresh, and Amini and Soleimany had a vision for transforming the course into something more. They created a series of software labs, introduced new cutting-edge topics like robust and ethical AI, and added content to appeal to a broad range of students, from computer scientists to aerospace engineers and MBAs.

“Alexander and I are constantly brainstorming, and those discussions are key to how 6.S191 and some of our own collaborative research projects have developed,” says Soleimany.

They cover one of those research collaborations in class. During the computer vision lab, students learn about algorithmic bias and how to test for and address racial and gender bias in face-recognition tools. The lab is based on an algorithm that Amini and Soleimany developed with their respective advisors, Daniela Rus, director of CSAIL, and Sangeeta Bhatia, the John J. and Dorothy Wilson Professor of HST and EECS. This year they also covered hot topics in robotics, including recent work of Amini’s on driverless cars.

But they don’t plan to stop there. “We’re committed to making 6.S191 the best that it can be, each year we teach it,” says Amini “and that means moving the course forward as deep learning continues to evolve.”"
16;machinelearningmastery.com;http://machinelearningmastery.com/use-classification-machine-learning-algorithms-weka/;2016-07-24;How To Use Classification Machine Learning Algorithms in Weka;"Tweet Share Share

Last Updated on August 22, 2019

Weka makes a large number of classification algorithms available.

The large number of machine learning algorithms available is one of the benefits of using the Weka platform to work through your machine learning problems.

In this post you will discover how to use 5 top machine learning algorithms in Weka.

After reading this post you will know:

About 5 top machine learning algorithms that you can use on your classification problems.

How to use 5 top classification algorithms in Weka.

The key configuration parameters for 5 top classification algorithms.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Classification Algorithm Tour Overview

We are going to take a tour of 5 top classification algorithms in Weka.

Each algorithm that we cover will be briefly described in terms of how it works, key algorithm parameters will be highlighted and the algorithm will be demonstrated in the Weka Explorer interface.

The 5 algorithms that we will review are:

Logistic Regression Naive Bayes Decision Tree k-Nearest Neighbors Support Vector Machines

These are 5 algorithms that you can try on your classification problem as a starting point.

A standard machine learning classification problem will be used to demonstrate each algorithm. Specifically, the Ionosphere binary classification problem. This is a good dataset to demonstrate classification algorithms because the input variables are numeric and all have the same scale the problem only has two classes to discriminate.

Each instance describes the properties of radar returns from the atmosphere and the task is to predict whether or not there is structure in the ionosphere or not. There are 34 numerical input variables of generally the same scale. You can learn more about this dataset on the UCI Machine Learning Repository. Top results are in the order of 98% accuracy.

Start the Weka Explorer:

Open the Weka GUI Chooser. Click the “Explorer” button to open the Weka Explorer. Load the Ionosphere dataset from the data/ionosphere.arff file. Click “Classify” to open the Classify tab.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Logistic Regression

Logistic regression is a binary classification algorithm.

It assumes the input variables are numeric and have a Gaussian (bell curve) distribution. This last point does not have to be true, as logistic regression can still achieve good results if your data is not Gaussian. In the case of the Ionosphere dataset, some input attributes have a Gaussian-like distribution, but many do not.

The algorithm learns a coefficient for each input value, which are linearly combined into a regression function and transformed using a logistic (s-shaped) function. Logistic regression is a fast and simple technique, but can be very effective on some problems.

The logistic regression only supports binary classification problems, although the Weka implementation has been adapted to support multi-class classification problems.

Choose the logistic regression algorithm:

Click the “Choose” button and select “Logistic” under the “functions” group. Click on the name of the algorithm to review the algorithm configuration.

The algorithm can run for a fixed number of iterations (maxIts), but by default will run until it is estimated that the algorithm has converged.

The implementation uses a ridge estimator which is a type of regularization. This method seeks to simplify the model during training by minimizing the coefficients learned by the model. The ridge parameter defines how much pressure to put on the algorithm to reduce the size of the coefficients. Setting this to 0 will turn off this regularization.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Ionosphere dataset.

You can see that with the default configuration that logistic regression achieves an accuracy of 88%.

Naive Bayes

Naive Bayes is a classification algorithm. Traditionally it assumes that the input values are nominal, although it numerical inputs are supported by assuming a distribution.

Naive Bayes uses a simple implementation of Bayes Theorem (hence naive) where the prior probability for each class is calculated from the training data and assumed to be independent of each other (technically called conditionally independent).

This is an unrealistic assumption because we expect the variables to interact and be dependent, although this assumption makes the probabilities fast and easy to calculate. Even under this unrealistic assumption, Naive Bayes has been shown to be a very effective classification algorithm.

Naive Bayes calculates the posterior probability for each class and makes a prediction for the class with the highest probability. As such, it supports both binary classification and multi-class classification problems.

Choose the Naive Bayes algorithm:

Click the “Choose” button and select “NaiveBayes” under the “bayes” group. Click on the name of the algorithm to review the algorithm configuration.

By default a Gaussian distribution is assumed for each numerical attributes.

You can change the algorithm to use a kernel estimator with the useKernelEstimator argument that may better match the actual distribution of the attributes in your dataset. Alternately, you can automatically convert numerical attributes to nominal attributes with the useSupervisedDiscretization parameter.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Ionosphere dataset.

You can see that with the default configuration that Naive Bayes achieves an accuracy of 82%.

There are a number of other flavors of naive bayes algorithms that you could work with.

Decision Tree

Decision trees can support classification and regression problems.

Decision trees are more recently referred to as Classification And Regression Trees (CART). They work by creating a tree to evaluate an instance of data, start at the root of the tree and moving town to the leaves (roots) until a prediction can be made. The process of creating a decision tree works by greedily selecting the best split point in order to make predictions and repeating the process until the tree is a fixed depth.

After the tree is constructed, it is pruned in order to improve the model’s ability to generalize to new data.

Choose the decision tree algorithm:

Click the “Choose” button and select “REPTree” under the “trees” group. Click on the name of the algorithm to review the algorithm configuration.

The depth of the tree is defined automatically, but a depth can be specified in the maxDepth attribute.

You can also choose to turn of pruning by setting the noPruning parameter to True, although this may result in worse performance.

The minNum parameter defines the minimum number of instances supported by the tree in a leaf node when constructing the tree from the training data.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Ionosphere dataset.

You can see that with the default configuration that the decision tree algorithm achieves an accuracy of 89%.

Another more advanced decision tree algorithm that you can use is the C4.5 algorithm, called J48 in Weka.

You can review a visualization of a decision tree prepared on the entire training data set by right clicking on the “Result list” and clicking “Visualize Tree”.

k-Nearest Neighbors

The k-nearest neighbors algorithm supports both classification and regression. It is also called kNN for short.

It works by storing the entire training dataset and querying it to locate the k most similar training patterns when making a prediction. As such, there is no model other than the raw training dataset and the only computation performed is the querying of the training dataset when a prediction is requested.

It is a simple algorithm, but one that does not assume very much about the problem other than that the distance between data instances is meaningful in making predictions. As such, it often achieves very good performance.

When making predictions on classification problems, KNN will take the mode (most common class) of the k most similar instances in the training dataset.

Choose the k-Nearest Neighbors algorithm:

Click the “Choose” button and select “IBk” under the “lazy” group. Click on the name of the algorithm to review the algorithm configuration.

The size of the neighborhood is controlled by the k parameter.

For example, if k is set to 1, then predictions are made using the single most similar training instance to a given new pattern for which a prediction is requested. Common values for k are 3, 7, 11 and 21, larger for larger dataset sizes. Weka can automatically discover a good value for k using cross validation inside the algorithm by setting the crossValidate parameter to True.

Another important parameter is the distance measure used. This is configured in the nearestNeighbourSearchAlgorithm which controls the way in which the training data is stored and searched.

The default is a LinearNNSearch. Clicking the name of this search algorithm will provide another configuration window where you can choose a distanceFunction parameter. By default, Euclidean distance is used to calculate the distance between instances, which is good for numerical data with the same scale. Manhattan distance is good to use if your attributes differ in measures or type.

It is a good idea to try a suite of different k values and distance measures on your problem and see what works best.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Ionosphere dataset.

You can see that with the default configuration that the kNN algorithm achieves an accuracy of 86%.

Support Vector Machines

Support Vector Machines were developed for binary classification problems, although extensions to the technique have been made to support multi-class classification and regression problems. The algorithm is often referred to as SVM for short.

SVM was developed for numerical input variables, although will automatically convert nominal values to numerical values. Input data is also normalized before being used.

SVM work by finding a line that best separates the data into the two groups. This is done using an optimization process that only considers those data instances in the training dataset that are closest to the line that best separates the classes. The instances are called support vectors, hence the name of the technique.

In almost all problems of interest, a line cannot be drawn to neatly separate the classes, therefore a margin is added around the line to relax the constraint, allowing some instances to be misclassified but allowing a better result overall.

Finally, few datasets can be separated with just a straight line. Sometimes a line with curves or even polygonal regions need to be marked out. This is achieved with SVM by projecting the data into a higher dimensional space in order to draw the lines and make predictions. Different kernels can be used to control the projection and the amount of flexibility in separating the classes.

Choose the SVM algorithm:

Click the “Choose” button and select “SMO” under the “function” group. Click on the name of the algorithm to review the algorithm configuration.

SMO refers to the specific efficient optimization algorithm used inside the SVM implementation, which stands for Sequential Minimal Optimization.

The C parameter, called the complexity parameter in Weka controls how flexible the process for drawing the line to separate the classes can be. A value of 0 allows no violations of the margin, whereas the default is 1.

A key parameter in SVM is the type of Kernel to use. The simplest kernel is a Linear kernel that separates data with a straight line or hyperplane. The default in Weka is a Polynomial Kernel that will separate the classes using a curved or wiggly line, the higher the polynomial, the more wiggly (the exponent value).

A popular and powerful kernel is the RBF Kernel or Radial Basis Function Kernel that is capable of learning closed polygons and complex shapes to separate the classes.

It is a good idea to try a suite of different kernels and C (complexity) values on your problem and see what works best.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Ionosphere dataset.

You can see that with the default configuration that the SVM algorithm achieves an accuracy of 88%.

Summary

In this post you discovered how to use top classification machine learning algorithms in Weka.

Specifically, you learned:

5 top classification algorithms you can try on your own problems.

The key configuration parameters to tune for each algorithm.

How to use each algorithm in Weka.

Do you have any questions about classification algorithms in Weka or about this post? Ask your questions in the comments and I will do my best to answer.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
17;machinelearningmastery.com;http://machinelearningmastery.com/compare-the-performance-of-machine-learning-algorithms-in-r/;2016-02-25;Compare The Performance of Machine Learning Algorithms in R;"# prepare training scheme

control < - trainControl ( method = ""repeatedcv"" , number = 10 , repeats = 3 )

# CART

set . seed ( 7 )

fit . cart < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""rpart"" , trControl = control )

# LDA

set . seed ( 7 )

fit . lda < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""lda"" , trControl = control )

# SVM

set . seed ( 7 )

fit . svm < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""svmRadial"" , trControl = control )

# kNN

set . seed ( 7 )

fit . knn < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""knn"" , trControl = control )

# Random Forest

set . seed ( 7 )

fit . rf < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""rf"" , trControl = control )

# collect resamples"
18;machinelearningmastery.com;https://machinelearningmastery.com/how-to-handle-big-p-little-n-p-n-in-machine-learning/#comments;2020-04-14;How to Handle Big-p, Little-n (p >> n) in Machine Learning;"Tweet Share Share

What if I have more Columns than Rows in my dataset?

Machine learning datasets are often structured or tabular data comprised of rows and columns.

The columns that are fed as input to a model are called predictors or “p” and the rows are samples “n“. Most machine learning algorithms assume that there are many more samples than there are predictors, denoted as p << n.

Sometimes, this is not the case, and there are many more predictors than samples in the dataset, referred to as “big-p, little-n” and denoted as p >> n. These problems often require specialized data preparation and modeling algorithms to address them correctly.

In this tutorial, you will discover the challenge of big-p, little n or p >> n machine learning problems.

After completing this tutorial, you will know:

Most machine learning problems have many more samples than predictors and most machine learning algorithms make this assumption during the training process.

Some modeling problems have many more predictors than samples, referred to as p >> n.

Algorithms to explore when modeling machine learning datasets with more predictors than samples.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Predictors (p) and Samples (n)

Machine Learning Assumes p << n

How to Handle p >> n

Predictors (p) and Samples (n)

Consider a predictive modeling problem, such as classification or regression.

The dataset is structured data or tabular data, like what you might see in an Excel spreadsheet.

There are columns and rows. Most of the columns would be used as inputs to a model and one column would represent the output or variable to be predicted.

The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable—in this case, sales—is often called the response or dependent variable, and is typically denoted using the symbol Y.

— Page 15, An Introduction to Statistical Learning with Applications in R, 2017.

Each column represents a variable or one aspect of a sample. The columns that represent the inputs to the model are called predictors.

Each row represents one sample with values across each of the columns or features.

Predictors : Input columns of a dataset, also called input variables or features.

: Input columns of a dataset, also called input variables or features. Samples: Rows of a dataset, also called an observation, example, or instance.

It is common to describe a training dataset in machine learning in terms of the predictors and samples.

The number of predictors in a dataset is described using the term “p” and the number of samples in a dataset is described using the term “n” or sometimes “N“.

p : The number of predictors in a dataset.

: The number of predictors in a dataset. n: The number of samples in a dataset.

To make this concrete, let’s take a look at the iris flowers classification problem.

Below is a sample of the first five rows of this dataset.

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ... 1 2 3 4 5 6 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ...

This dataset has five columns and 150 rows.

The first four columns are inputs and the fifth column is the output, meaning that there are four predictors.

We would describe the iris flowers dataset as:

p=4, n=150.

Machine Learning Assumes p << n

It is almost always the case that the number of predictors (p) will be smaller than the number of samples (n).

Often much smaller.

We can summarize this expectation as p << n, where “<<” is a mathematical inequality that means “much less than.”

p << n: Typically we have fewer predictors than samples.

To demonstrate this, let’s look at a few more standard machine learning datasets:

Most machine learning algorithms operate based on the assumption that there are many more samples than predictors.

One way to think about predictors and samples is to take a geometrical perspective.

Consider a hypercube where the number of predictors (p) defines the number of dimensions of the hypercube. The volume of this hypercube is the scope of possible samples that could be drawn from the domain. The number of samples (n) are the actual samples drawn from the domain that you must use to model your predictive modeling problem.

This is a rationale for the axiom “get as much data as possible” in applied machine learning. It is a desire to gather a sufficiently representative sample of the p-dimensional problem domain.

As the number of dimensions (p) increases, the volume of the domain increases exponentially. This, in turn, requires more samples (n) from the domain to provide effective coverage of the domain for a learning algorithm. We don’t need full coverage of the domain, just what is likely to be observable.

This challenge of effectively sampling high-dimensional spaces is generally referred to as the curse of dimensionality.

Machine learning algorithms overcome the curse of dimensionality by making assumptions about the data and structure of the mapping function from inputs to outputs. They add a bias.

The fundamental reason for the curse of dimensionality is that high-dimensional functions have the potential to be much more complicated than low-dimensional ones, and that those complications are harder to discern. The only way to beat the curse is to incorporate knowledge about the data that is correct.

— Page 15, Pattern Classification, 2000.

Many machine learning algorithms that use distance measures and other local models (in feature space) often degrade in performance as the number of predictors is increased.

When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large.

— Page 168, An Introduction to Statistical Learning with Applications in R, 2017.

It is not always the case that the number of predictors is less than the number of samples.

How to Handle p >> n

Some predictive modeling problems have more predictors than samples by definition.

Often many more predictors than samples.

This is often described as “big-p, little-n,” “large-p, small-n,” or more compactly as “p >> n”, where the “>>” is a mathematical inequality operator that means “much more than.”

… prediction problems in which the number of features p is much larger than the number of observations N, often written p >> N.

— Page 649, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2016.

Consider this from a geometrical perspective.

Now, instead of having a domain with tens of dimensions (or fewer), the domain has many thousands of dimensions and only a few tens of samples from this space. We cannot expect to have anything like a representative sample of the domain.

Many examples of p >> n problems come from the field of medicine, where there is a small patient population and a large number of descriptive characteristics.

At the same time, applications have emerged in which the number of experimental units is comparatively small but the underlying dimension is massive; illustrative examples might include image analysis, microarray analysis, document classification, astronomy and atmospheric science.

— Statistical challenges of high-dimensional data, 2009.

A common example of a p >> n problem is gene expression arrays, where there may be thousands of genes (predictors) and only tens of samples.

Gene expression arrays typically have 50 to 100 samples and 5,000 to 20,000 variables (genes).

— Expression Arrays and the p >> n Problem, 2003.

Given that most machine learning algorithms assume many more samples than predictors, this introduces a challenge when modeling.

Specifically, the assumptions made by standard machine learning models may cause the models to behave unexpectedly, provide misleading results, or fail completely.

… models cannot be used “out of the box”, since the standard fitting algorithms all require p<n; in fact the usual rule of thumb is that there be five or ten times as many samples as variables.

— Expression Arrays and the p >> n Problem, 2003.

A major problem with p >> n problems when using machine learning models is overfitting the training dataset.

Given the lack of samples, most models are unable to generalize and instead learn the statistical noise in the training data. This makes the model perform well on the training dataset but perform poorly on new examples from the problem domain.

This is also a hard problem to diagnose, as the lack of samples does not allow for a test or validation dataset by which model overfitting can be evaluated. As such, it is common to use leave-one-out style cross-validation (LOOCV) when evaluating models on p >> n problems.

There are many ways to approach a p >> n type classification or regression problem.

Some examples include:

Ignore p and n

One approach is to ignore the p and n relationship and evaluate standard machine learning models.

This might be considered the baseline method by which any other more specialized interventions may be compared.

Feature Selection

Feature selection involves selecting a subset of predictors to use as input to predictive models.

Common techniques include filter methods that select features based on their statistical relationship to the target variable (e.g. correlation), and wrapper methods that select features based on their contribution to a model when predicting the target variable (e.g. RFE).

A suite of feature selection methods could be evaluated and compared, perhaps applied in an aggressive manner to dramatically reduce the number of input features to those determined to be most critical.

Feature selection is an important scientific requirement for a classifier when p is large.

— Page 658, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2016.

For more on feature selection see the tutorial:

Projection Methods

Projection methods create a lower-dimensionality representation of samples that preserves the relationships observed in the data.

They are often used for visualization, although the dimensionality reduction nature of the techniques may also make them useful as a data transform to reduce the number of predictors.

This might include techniques from linear algebra, such as SVD and PCA.

When p > N, the computations can be carried out in an N-dimensional space, rather than p, via the singular value decomposition …

— Page 659, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2016.

It might also include manifold learning algorithms often used for visualization such as t-SNE.

Regularized Algorithms

Standard machine learning algorithms may be adapted to use regularization during the training process.

This will penalize models based on the number of features used or weighting of features, encouraging the model to perform well and minimize the number of predictors used in the model.

This can act as a type of automatic feature selection during training and may involve augmenting existing models (e.g regularized linear regression and regularized logistic regression) or the use of specialized methods such as LARS and LASSO.

There is no best method and it is recommended to use controlled experiments to test a suite of different methods.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

Books

Articles

Summary

In this tutorial, you discovered the challenge of big-p, little n or p >> n machine learning problems.

Specifically, you learned:

Machine learning datasets can be described in terms of the number of predictors (p) and the number of samples (n).

Most machine learning problems have many more samples than predictors and most machine learning algorithms make this assumption during the training process.

Some modeling problems have many more predictors than samples, such as problems from medicine, referred to as p >> n, and may require the use of specialized algorithms.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
19;machinelearningmastery.com;http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/;2015-08-20;Practice Machine Learning with Datasets from the UCI Machine Learning Repository;"Tweet Share Share

Last Updated on July 5, 2019

Where can you get good datasets to practice machine learning?

Datasets that are real-world so that they are interesting and relevant, although small enough for you to review in Excel and work through on your desktop.

In this post you will discover a database of high-quality, real-world, and well understood machine learning datasets that you can use to practice applied machine learning.

This database is called the UCI machine learning repository and you can use it to structure a self-study program and build a solid foundation in machine learning.

Why Do We Need Practice Datasets?

If you are interested in practicing applied machine learning, you need datasets on which to practice.

This problem can stop you dead.

Which dataset should you use?

Should you collect your own or use one off the shelf?

Which one and why?

I teach a top-down approach to machine learning where I encourage you to learn a process for working a problem end-to-end, map that process onto a tool and practice the process on data in a targeted way. For more information see my post “Machine Learning for Programmers: Leap from developer to machine learning practitioner“.

So How Do You Practice In A Targeted Way?

I teach that the best way to get started is to practice on datasets that have specific traits.

I recommend you select traits that you will encounter and need to address when you start working on problems of your own such as:

Different types of supervised learning such as classification and regression.

Different sized datasets from tens, hundreds, thousands and millions of instances.

Different numbers of attributes from less than ten, tens, hundreds and thousands of attributes

Different attribute types from real, integer, categorical, ordinal and mixtures

Different domains that force you to quickly understand and characterize a new problem in which you have no previous experience.

You can create a program of traits to study and learn about and the algorithm you need to address them, by designing a program of test problem datasets to work through.

Such a program has a number of practical requirements, for example:

Real-World : The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.

: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data. Small : The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.

: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle. Well-Understood : There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.

: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation. Baseline : It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).

: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken). Plentiful: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests.

For beginners, you can get everything you need and more in terms of datasets to practice on from the UCI Machine Learning Repository.

What is the UCI Machine Learning Repository?

The UCI Machine Learning Repository is a database of machine learning problems that you can access for free.

It is hosted and maintained by the Center for Machine Learning and Intelligent Systems at the University of California, Irvine. It was originally created by David Aha as a graduate student at UC Irvine.

For more than 25 years it has been the go-to place for machine learning researchers and machine learning practitioners that need a dataset.

Each dataset gets its own webpage that lists all the details known about it including any relevant publications that investigate it. The datasets themselves can be downloaded as ASCII files, often the useful CSV format.

For example, here is the webpage for the Abalone Data Set that requires the prediction of the age of abalone from their physical measurements.

Benefits of the Repository

Some beneficial features of the library include:

Almost all datasets are drawn from the domain (as opposed to being synthetic), meaning that they have real-world qualities.

Datasets cover a wide range of subject matter from biology to particle physics.

The details of datasets are summarized by aspects like attribute types, number of instances, number of attributes and year published that can be sorted and searched.

Datasets are well studied which means that they are well known in terms of interesting properties and expected “good” results. This can provide a useful baseline for comparison.

Most datasets are small (hundreds to thousands of instances) meaning that you can readily load them in a text editor or MS Excel and review them, you can also easily model them quickly on your workstation.

Browse the 300+ datasets using this handy table that supports sorting and searching.

Criticisms of the Repository

Some criticisms of the repository include:

The datasets are cleaned, meaning that the researchers that prepared them have often already performed some pre-processing in terms of the the selection of attributes and instances.

The datasets are small, this is not helpful if you are interested in investigating larger scale problems and techniques.

There are so many to choose from that you can be frozen by indecision and over-analysis. It can be hard to just pick a dataset and get started when you are unsure if it is a “good dataset” for what you’re investigating.

Datasets are limited to tabular data, primarily for classification (although clustering and regression datasets are listed). This is limiting for those interested in natural language, computer vision, recommender and other data.

Take a look at the repository homepage as it shows featured datasets, the newest datasets as well as which datasets are currently the most popular.

A Self-Study Program

So, how can you make the best use of the UCI machine learning repository?

I would advise you to think about the traits in problem datasets that you would like to learn about.

These may be traits that you would like to model (like regression), or algorithms that model these traits that you would like to get more skillful at using (like random forest for multi-class classification).

An example program might look like the following:

This is just a list of traits, can pick and choose your own traits to investigate.

I have listed one dataset for each trait, but you could pick 2-3 different datasets and complete a few small projects to improve your understanding and put in more practice.

For each problem, I would advise that you work it systematically from end-to-end, for example, go through the following steps in the applied machine learning process:

Define the problem Prepare data Evaluate algorithms Improve results Write-up results

For more on the process of working through a machine learning problem systematically, see my post titled “Process for working through Machine Learning Problems“.

The write-up is a key part.

It allows you to build up a portfolio of projects that you refer back to as a reference on future projects and get a jump-start, as well as use as a public resume or your growing skills and capabilities in applied machine learning.

For more on building a portfolio of projects, see my post “Build a Machine Learning Portfolio: Complete Small Focused Projects and Demonstrate Your Skills“.

But, What If…

I don’t know a machine learning tool.

Pick a tool or platform (like Weka, R or scikit-learn) and use this process to learn a tool. Cover off both practicing machine learning and getting good at your tool at the same time.

I don’t know how to program (or code very well).

Use Weka. It has a graphical user interface and no programming is required. I would recommend this to beginners regardless of whether they can program or not because the process of working machine learning problems maps so well onto the platform.

I don’t have the time.

With a strong systematic process and a good tool that covers the whole process, I think that you could work through a problem in one-or-two hours. This means you could complete one project in an evening or over two evenings.

You choose the level of detail to investigate and it is a good idea to keep it light and simple when just starting out.

I don’t have a background in the domain I’m modeling.

The dataset pages provide some background on the dataset. Often you can dive deeper by looking at publications or the information files accompanying the main dataset.

I have little to no experience working through machine learning problems.

Now is your time to start. Pick a systematic process, pick a simple dataset and a tool like Weka and work through your first problem. Place that first stone in your machine learning foundation.

I have no experience at data analysis.

No experience in data analysis is required. The datasets are simple, easy to understand and well explained. You simply need to read up on them using the data sets home page and by looking at the data files themselves.

Action Step

Select a dataset and get started.

If you are serious about your self-study, consider designing a modest list of traits and corresponding datasets to investigate.

You will learn a lot and build a valuable foundation for diving into more complex and interesting problems.

Did you find this post useful? Leave a comment and let me know."
20;news.mit.edu;http://news.mit.edu/2020/origins-earth-magnetic-field-mystery-0408;;Origins of Earth’s magnetic field remain a mystery;"Microscopic minerals excavated from an ancient outcrop of Jack Hills, in Western Australia, have been the subject of intense geological study, as they seem to bear traces of the Earth’s magnetic field reaching as far back as 4.2 billion years ago. That’s almost 1 billion years earlier than when the magnetic field was previously thought to originate, and nearly back to the time when the planet itself was formed.

But as intriguing as this origin story may be, an MIT-led team has now found evidence to the contrary. In a paper published today in Science Advances, the team examined the same type of crystals, called zircons, excavated from the same outcrop, and have concluded that zircons they collected are unreliable as recorders of ancient magnetic fields.

In other words, the jury is still out on whether the Earth’s magnetic field existed earlier than 3.5 billion years ago.

“There is no robust evidence of a magnetic field prior to 3.5 billion years ago, and even if there was a field, it will be very difficult to find evidence for it in Jack Hills zircons,” says Caue Borlina, a graduate student in MIT’s Department of Earth, Atmospheric, and Planetary Sciences (EAPS). “It’s an important result in the sense that we know what not to look for anymore.”

Borlina is the paper’s first author, which also includes EAPS Professor Benjamin Weiss, Principal Research Scientist Eduardo Lima, and Research Scientist Jahandar Ramezan of MIT, along with others from Cambridge University, Harvard University, the University of California at Los Angeles, the University of Alabama, and Princeton University.

A field, stirred up

Earth’s magnetic field is thought to play an important role in making the planet habitable. Not only does a magnetic field set the direction of our compass needles, it also acts as a shield of sorts, deflecting away solar wind that might otherwise eat away at the atmosphere.

Scientists know that today the Earth’s magnetic field is powered by the solidification of the planet’s liquid iron core. The cooling and crystallization of the core stirs up the surrounding liquid iron, creating powerful electric currents that generate a magnetic field stretching far out into space. This magnetic field is known as the geodynamo.

Multiple lines of evidence have shown that the Earth’s magnetic field existed at least 3.5 billion years ago. However, the planet’s core is thought to have started solidifying just 1 billion years ago, meaning that the magnetic field must have been driven by some other mechanism prior to 1 billion years ago. Pinning down exactly when the magnetic field formed could help scientists figure out what generated it to begin with.

Borlina says the origin of Earth’s magnetic field could also illuminate the early conditions in which Earth’s first life forms took hold.

“In the Earth’s first billion years, between 4.4 billion and 3.5 billion years, that’s when life was emerging,” Borlina says. “Whether you have a magnetic field at that time has different implications for the environment in which life emerged on Earth. That’s the motivation for our work.”

“Can’t trust zircon”

Scientists have traditionally used minerals in ancient rocks to determine the orientation and intensity of Earth’s magnetic field back through time. As rocks form and cool, the electrons within individual grains can shift in the direction of the surrounding magnetic field. Once the rock cools past a certain temperature, known as the Curie temperature, the orientations of the electrons are set in stone, so to speak. Scientists can determine their age and use standard magnetometers to measure their orientation, to estimate the strength and orientation of the Earth’s magnetic field at a given point in time.

Since 2001, Weiss and his group have been studying the magnetization of the Jack Hills rocks and zircon grains, with the challenging goal of establishing whether they contain ancient records of the Earth’s magnetic field.

“The Jack Hills zircons are some of the most weakly magnetic objects studied in the history of paleomagnetism,” Weiss says. “Furthermore, these zircons include the oldest known Earth materials, meaning that there are many geological events that could have reset their magnetic records.”

In 2015, a separate research group that had also started studying the Jack Hills zircons argued that they found evidence of magnetic material in zircons that they dated to be 4.2 billion years old — the first evidence that Earth’s magnetic field may have existed prior to 3.5 billion years ago.

But Borlina notes that the team did not confirm whether the magnetic material they detected actually formed during or after the zircon crystal formed 4.2 billion years ago — a goal that he and his team took on for their new paper.

Borlina, Weiss, and their colleagues had collected rocks from the same Jack Hills outcrop, and from those samples, extracted 3,754 zircon grains, each around 150 micrometers long — about the width of a human hair. Using standard dating techniques, they determined the age of each zircon grain, which ranged from 1 billion to 4.2 billion years old.

Around 250 crystals were older than 3.5 billion years. The team isolated and imaged those samples, looking for signs of cracks or secondary materials, such as minerals that may have been deposited on or within the crystal after it had fully formed, and searched for evidence that they were significantly heated over the last few billion years since they formed. Of these 250, they identified just three zircons that were relatively free of such impurities and therefore could contain suitable magnetic records.

The team then carried out detailed experiments on these three zircons to determine what kinds of magnetic materials they might contain. They eventually determined that a magnetic mineral called magnetite was present in two of the three zircons. Using a high-resolution quantum diamond magnetometer, the team looked at cross-sections of each of the two zircons to map the location of the magnetite in each crystal.

They discovered magnetite lying along cracks or damaged zones within the zircons. Such cracks, Borlina says, are pathways that allow water and other elements inside the rock. Such cracks could have let in secondary magnetite that settled into the crystal much later than when the zircon originally formed. Either way, Borlina says the evidence is clear: These zircons cannot be used as a reliable recorder for Earth’s magnetic field.

“This is evidence we can’t trust these zircon measurements for the record of the Earth’s magnetic field,” Borlina says. “We’ve shown that, before 3.5 billion years ago, we still have no idea when Earth’s magnetic field started.”

“For me, these results cast a great deal of doubt on the potential of Jack Hills zircons to faithfully record the palaeomagnetic field intensity prior to 3.5 billion years,” says Andy Biggin, professor of paleomagnetism at the University of Liverpool, who was not involved in the research. “That said, this debate has been raging, like the palaeomagnetic equivalent to Brexit, since 2015 and I would be very surprised if this were the last word on the matter. It is nigh on impossible to prove a negative and neither methods nor interpretations are ever beyond question.”

Despite these new results, Weiss stresses that previous magnetic analyses of these zircons are still highly valuable.

“The team that reported the original zircon magnetic study deserves a lot of credit for trying to tackle this enormously challenging problem,” Weiss says. “As a result of all the work from both groups, we now understand much better how to study the magnetism of ancient geological materials. We now can begin to apply this knowledge to other mineral grains and to grains from other planetary bodies.”

This research was supported in part by the National Science Foundation."
21;news.mit.edu;http://news.mit.edu/2020/amid-shutdowns-supply-chains-pivot-demand-for-specialized-talent-intensifies-mitx-micromasters-0413;;Amid shutdowns, supply chains pivot and global demand for specialized talent intensifies;"The global landscape of supply chain management has changed drastically in the past several weeks. Businesses, organizations, and people are rapidly innovating to improve supply chains and upskill and reskill the workforce and themselves to accommodate disruptions caused by the global Covid-19 health crisis. Online retailers and logistics providers are announcing vast hiring initiatives, while companies and organizations grapple with the logistics demands of supplying for vital services.

Even in the middle of these disruptions, a cohort of 383 dedicated online learners concluded nine to 18 months of learning to pass their comprehensive final exams, earning their MITx MicroMasters program credentials in supply chain management. These new credential-holders bring the total number of holders to 2,243 from 115 countries. The majority of credential-holders hail from the United States, India, Brazil, Spain, and China, some of the world’s most influential economies. While credential holders’ median age is 31, holders range in age from 21 to 74, practicing diverse business functions. They are currently employed at more than 700 companies worldwide, ranging from the largest multinational corporations to local, family-owned businesses.

Given the volatile nature of logistics during disruptions, the comprehensive theoretical and practical knowledge gained from these courses is already having an impact. “The program significantly changed my mindset to be proactive. This helped me improvise ahead of the current pandemic challenges to provide visibility across my supply chain,” says learner Mohamed El Tayeb, a demand planner in Saudi Arabia. “Technically, everything I learned in the program is coming in handy now.” Similarly, Matthias Stolz, a supply chain management project manager from Germany, claims “The MicroMasters program helped me to be able to make back-of-the-envelope calculations to quantify effects and evaluate risks and opportunities fast. This allowed me to confidently prepare decisions for the top management which have already enabled the company to respond quickly.”

Learners like El Tayeb and Stolz are leading the way on the ground, along with contributors from across the supply chain, to be cited as everyday heroes by MIT’s Center for Transportation and Logistics and the U.S. Department of Agriculture, among many others.

MIT will recognize the contributions of credential holders and program participants in a public online completion celebration on April 15 at 11 a.m. EDT. “Our goal is to pioneer supply chain digital education to shape the leaders of the future,” says program Director Eva Ponce. “We are bringing MIT education to anyone from anywhere to improve the capabilities and prospects of professionals through our massive open online courses. It is my distinct pleasure to thank the committed and passionate team responsible for the development and delivery of this program and to welcome this learner cohort to the credential-holder community, who are the future of the supply chain profession.”

As a new normal becomes apparent in the foreseeable future, experts agree that the global disruptions should serve as a wake-up call for supply chain and logistics managers. They foresee that practitioners will need an array of practical and analytical tools at their disposal to accommodate rapidly changing demands. Teaching supply chain management online is one strategy to meet this dynamic demand. The MITx MicroMasters Program in Supply Chain Management is becoming recognized as a go-to knowledge baseline for individuals and organizations to meet their global demand for talent."
22;machinelearningmastery.com;https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/;2020-01-14;Random Oversampling and Undersampling for Imbalanced Classification;"# example of random oversampling to balance the class distribution

from collections import Counter

from sklearn . datasets import make_classification

from imblearn . over_sampling import RandomOverSampler

# define dataset

X , y = make_classification ( n_samples = 10000 , weights = [ 0.99 ] , flip_y = 0 )

# summarize class distribution

print ( Counter ( y ) )

# define oversampling strategy

oversample = RandomOverSampler ( sampling_strategy = 'minority' )

# fit and apply the transform

X_over , y_over = oversample . fit_resample ( X , y )

# summarize class distribution"
23;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/;2018-11-11;How to Develop Convolutional Neural Network Models for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Convolutional Neural Network models, or CNNs for short, can be applied to time series forecasting.

There are many types of CNN models that can be used for each specific type of time series forecasting problem.

In this tutorial, you will discover how to develop a suite of CNN models for a range of standard time series forecasting problems.

The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem.

After completing this tutorial, you will know:

How to develop CNN models for univariate time series forecasting.

How to develop CNN models for multivariate time series forecasting.

How to develop CNN models for multi-step time series forecasting.

This is a large and important post; you may want to bookmark it for future reference.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

In this tutorial, we will explore how to develop a suite of different types of CNN models for time series forecasting.

The models are demonstrated on small contrived time series problems intended to give the flavor of the type of time series problem being addressed. The chosen configuration of the models is arbitrary and not optimized for each problem; that was not the goal.

This tutorial is divided into four parts; they are:

Univariate CNN Models Multivariate CNN Models Multi-Step CNN Models Multivariate Multi-Step CNN Models

Univariate CNN Models

Although traditionally developed for two-dimensional image data, CNNs can be used to model univariate time series forecasting problems.

Univariate time series are datasets comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence.

This section is divided into two parts; they are:

Data Preparation CNN Model

Data Preparation

Before a univariate series can be modeled, it must be prepared.

The CNN model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn.

Consider a given univariate sequence:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.

X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ... 1 2 3 4 5 X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ...

The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.

# split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on our small contrived dataset above.

The complete example is listed below.

# univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step.

[10 20 30] 40 [20 30 40] 50 [30 40 50] 60 [40 50 60] 70 [50 60 70] 80 [60 70 80] 90 1 2 3 4 5 6 [10 20 30] 40 [20 30 40] 50 [30 40 50] 60 [40 50 60] 70 [50 60 70] 80 [60 70 80] 90

Now that we know how to prepare a univariate series for modeling, let’s look at developing a CNN model that can learn the mapping of inputs to outputs.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

CNN Model

A one-dimensional CNN is a CNN model that has a convolutional hidden layer that operates over a 1D sequence. This is followed by perhaps a second convolutional layer in some cases, such as very long input sequences, and then a pooling layer whose job it is to distill the output of the convolutional layer to the most salient elements.

The convolutional and pooling layers are followed by a dense fully connected layer that interprets the features extracted by the convolutional part of the model. A flatten layer is used between the convolutional layers and the dense layer to reduce the feature maps to a single one-dimensional vector.

We can define a 1D CNN Model for univariate time series forecasting as follows.

# define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 7 8 # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features.

We are working with a univariate series, so the number of features is one, for one variable.

The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function.

The input shape for each sample is specified in the input_shape argument on the definition of the first hidden layer.

We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape:

[samples, timesteps, features] 1 [samples, timesteps, features]

Our split_sequence() function in the previous section outputs the X with the shape [samples, timesteps], so we can easily reshape it to have an additional dimension for the one feature.

# reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) 1 2 3 # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) )

The CNN does not actually view the data as having time steps, instead, it is treated as a sequence over which convolutional read operations can be performed, like a one-dimensional image.

In this example, we define a convolutional layer with 64 filter maps and a kernel size of 2. This is followed by a max pooling layer and a dense layer to interpret the input feature. An output layer is specified that predicts a single numerical value.

The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ‘mse‘, loss function.

Once the model is defined, we can fit it on the training dataset.

# fit model model.fit(X, y, epochs=1000, verbose=0) 1 2 # fit model model . fit ( X , y , epochs = 1000 , verbose = 0 )

After the model is fit, we can use it to make a prediction.

We can predict the next value in the sequence by providing the input:

[70, 80, 90] 1 [70, 80, 90]

And expecting the model to predict something like:

[100] 1 [100]

The model expects the input shape to be three-dimensional with [samples, timesteps, features], therefore, we must reshape the single input sample before making the prediction.

# demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

We can tie all of this together and demonstrate how to develop a 1D CNN model for univariate time series forecasting and make a single prediction.

# univariate cnn example from numpy import array from keras.models import Sequential from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) # define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=1000, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # univariate cnn example from numpy import array from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) ) # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 1000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

Your results may vary given the stochastic nature of the algorithm; try running the example a few times.

We can see that the model predicts the next value in the sequence.

[[101.67965]] 1 [[101.67965]]

Multivariate CNN Models

Multivariate time series data means data where there is more than one observation for each time step.

There are two main models that we may require with multivariate time series data; they are:

Multiple Input Series. Multiple Parallel Series.

Let’s take a look at each in turn.

Multiple Input Series

A problem may have two or more parallel input time series and an output time series that is dependent on the input time series.

The input time series are parallel because each series has observations at the same time steps.

We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series.

# define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) 1 2 3 4 # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] )

We can reshape these three arrays of data as a single dataset where each row is a time step and each column is a separate time series.

This is a standard way of storing parallel time series in a CSV file.

# convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) 1 2 3 4 5 6 # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) )

The complete example is listed below.

# multivariate data preparation from numpy import array from numpy import hstack # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) print(dataset) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # multivariate data preparation from numpy import array from numpy import hstack # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) print ( dataset )

Running the example prints the dataset with one row per time step and one column for each of the two input and one output parallel time series.

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

As with the univariate time series, we must structure these data into samples with input and output samples.

A 1D CNN model needs sufficient context to learn a mapping from an input sequence to an output value. CNNs can support parallel input time series as separate channels, like red, green, and blue components of an image. Therefore, we need to split the data into samples maintaining the order of observations across the two input sequences.

If we chose three input time steps, then the first sample would look as follows:

Input:

10, 15 20, 25 30, 35 1 2 3 10, 15 20, 25 30, 35

Output:

65 1 65

That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case, 65.

We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used.

We can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input/output samples.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can test this function on our dataset using three time steps for each input time series as input.

The complete example is listed below.

# multivariate data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # multivariate data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the X and y components.

We can see that the X component has a three-dimensional structure.

The first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series.

This is the exact three-dimensional structure expected by a 1D CNN as input. The data is ready to use without further reshaping.

We can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample.

(7, 3, 2) (7,) [[10 15] [20 25] [30 35]] 65 [[20 25] [30 35] [40 45]] 85 [[30 35] [40 45] [50 55]] 105 [[40 45] [50 55] [60 65]] 125 [[50 55] [60 65] [70 75]] 145 [[60 65] [70 75] [80 85]] 165 [[70 75] [80 85] [90 95]] 185 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 (7, 3, 2) (7,) [[10 15] [20 25] [30 35]] 65 [[20 25] [30 35] [40 45]] 85 [[30 35] [40 45] [50 55]] 105 [[40 45] [50 55] [60 65]] 125 [[50 55] [60 65] [70 75]] 145 [[60 65] [70 75] [80 85]] 165 [[70 75] [80 85] [90 95]] 185

We are now ready to fit a 1D CNN model on this data, specifying the expected number of time steps and features to expect for each input sample, in this case three and two respectively.

# define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 7 8 # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

When making a prediction, the model expects three time steps for two input time series.

We can predict the next value in the output series providing the input values of:

80, 85 90, 95 100, 105 1 2 3 80, 85 90, 95 100, 105

The shape of the one sample with three time steps and two variables must be [1, 3, 2].

We would expect the next value in the sequence to be 100 + 105 or 205.

# demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

The complete example is listed below.

# multivariate cnn example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=1000, verbose=0) # demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # multivariate cnn example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 1000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[[206.0161]] 1 [[206.0161]]

There is another, more elaborate way to model the problem.

Each input series can be handled by a separate CNN and the output of each of these submodels can be combined before a prediction is made for the output sequence.

We can refer to this as a multi-headed CNN model. It may offer more flexibility or better performance depending on the specifics of the problem that is being modeled. For example, it allows you to configure each sub-model differently for each input series, such as the number of filter maps and the kernel size.

This type of model can be defined in Keras using the Keras functional API.

First, we can define the first input model as a 1D CNN with an input layer that expects vectors with n_steps and 1 feature.

# first input model visible1 = Input(shape=(n_steps, n_features)) cnn1 = Conv1D(filters=64, kernel_size=2, activation='relu')(visible1) cnn1 = MaxPooling1D(pool_size=2)(cnn1) cnn1 = Flatten()(cnn1) 1 2 3 4 5 # first input model visible1 = Input ( shape = ( n_steps , n_features ) ) cnn1 = Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' ) ( visible1 ) cnn1 = MaxPooling1D ( pool_size = 2 ) ( cnn1 ) cnn1 = Flatten ( ) ( cnn1 )

We can define the second input submodel in the same way.

# second input model visible2 = Input(shape=(n_steps, n_features)) cnn2 = Conv1D(filters=64, kernel_size=2, activation='relu')(visible2) cnn2 = MaxPooling1D(pool_size=2)(cnn2) cnn2 = Flatten()(cnn2) 1 2 3 4 5 # second input model visible2 = Input ( shape = ( n_steps , n_features ) ) cnn2 = Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' ) ( visible2 ) cnn2 = MaxPooling1D ( pool_size = 2 ) ( cnn2 ) cnn2 = Flatten ( ) ( cnn2 )

Now that both input submodels have been defined, we can merge the output from each model into one long vector which can be interpreted before making a prediction for the output sequence.

# merge input models merge = concatenate([cnn1, cnn2]) dense = Dense(50, activation='relu')(merge) output = Dense(1)(dense) 1 2 3 4 # merge input models merge = concatenate ( [ cnn1 , cnn2 ] ) dense = Dense ( 50 , activation = 'relu' ) ( merge ) output = Dense ( 1 ) ( dense )

We can then tie the inputs and outputs together.

model = Model(inputs=[visible1, visible2], outputs=output) 1 model = Model ( inputs = [ visible1 , visible2 ] , outputs = output )

The image below provides a schematic for how this model looks, including the shape of the inputs and outputs of each layer.

This model requires input to be provided as a list of two elements where each element in the list contains data for one of the submodels.

In order to achieve this, we can split the 3D input data into two separate arrays of input data; that is from one array with the shape [7, 3, 2] to two 3D arrays with [7, 3, 1]

# one time series per head n_features = 1 # separate input data X1 = X[:, :, 0].reshape(X.shape[0], X.shape[1], n_features) X2 = X[:, :, 1].reshape(X.shape[0], X.shape[1], n_features) 1 2 3 4 5 # one time series per head n_features = 1 # separate input data X1 = X [ : , : , 0 ] . reshape ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) X2 = X [ : , : , 1 ] . reshape ( X . shape [ 0 ] , X . shape [ 1 ] , n_features )

These data can then be provided in order to fit the model.

# fit model model.fit([X1, X2], y, epochs=1000, verbose=0) 1 2 # fit model model . fit ( [ X1 , X2 ] , y , epochs = 1000 , verbose = 0 )

Similarly, we must prepare the data for a single sample as two separate two-dimensional arrays when making a single one-step prediction.

x_input = array([[80, 85], [90, 95], [100, 105]]) x1 = x_input[:, 0].reshape((1, n_steps, n_features)) x2 = x_input[:, 1].reshape((1, n_steps, n_features)) 1 2 3 x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x1 = x_input [ : , 0 ] . reshape ( ( 1 , n_steps , n_features ) ) x2 = x_input [ : , 1 ] . reshape ( ( 1 , n_steps , n_features ) )

We can tie all of this together; the complete example is listed below.

# multivariate multi-headed 1d cnn example from numpy import array from numpy import hstack from keras.models import Model from keras.layers import Input from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D from keras.layers.merge import concatenate # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # one time series per head n_features = 1 # separate input data X1 = X[:, :, 0].reshape(X.shape[0], X.shape[1], n_features) X2 = X[:, :, 1].reshape(X.shape[0], X.shape[1], n_features) # first input model visible1 = Input(shape=(n_steps, n_features)) cnn1 = Conv1D(filters=64, kernel_size=2, activation='relu')(visible1) cnn1 = MaxPooling1D(pool_size=2)(cnn1) cnn1 = Flatten()(cnn1) # second input model visible2 = Input(shape=(n_steps, n_features)) cnn2 = Conv1D(filters=64, kernel_size=2, activation='relu')(visible2) cnn2 = MaxPooling1D(pool_size=2)(cnn2) cnn2 = Flatten()(cnn2) # merge input models merge = concatenate([cnn1, cnn2]) dense = Dense(50, activation='relu')(merge) output = Dense(1)(dense) model = Model(inputs=[visible1, visible2], outputs=output) model.compile(optimizer='adam', loss='mse') # fit model model.fit([X1, X2], y, epochs=1000, verbose=0) # demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x1 = x_input[:, 0].reshape((1, n_steps, n_features)) x2 = x_input[:, 1].reshape((1, n_steps, n_features)) yhat = model.predict([x1, x2], verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 # multivariate multi-headed 1d cnn example from numpy import array from numpy import hstack from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D from keras . layers . merge import concatenate # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # one time series per head n_features = 1 # separate input data X1 = X [ : , : , 0 ] . reshape ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) X2 = X [ : , : , 1 ] . reshape ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) # first input model visible1 = Input ( shape = ( n_steps , n_features ) ) cnn1 = Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' ) ( visible1 ) cnn1 = MaxPooling1D ( pool_size = 2 ) ( cnn1 ) cnn1 = Flatten ( ) ( cnn1 ) # second input model visible2 = Input ( shape = ( n_steps , n_features ) ) cnn2 = Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' ) ( visible2 ) cnn2 = MaxPooling1D ( pool_size = 2 ) ( cnn2 ) cnn2 = Flatten ( ) ( cnn2 ) # merge input models merge = concatenate ( [ cnn1 , cnn2 ] ) dense = Dense ( 50 , activation = 'relu' ) ( merge ) output = Dense ( 1 ) ( dense ) model = Model ( inputs = [ visible1 , visible2 ] , outputs = output ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( [ X1 , X2 ] , y , epochs = 1000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x1 = x_input [ : , 0 ] . reshape ( ( 1 , n_steps , n_features ) ) x2 = x_input [ : , 1 ] . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( [ x1 , x2 ] , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[[205.871]] 1 [[205.871]]

Multiple Parallel Series

An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each.

For example, given the data from the previous section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may want to predict the value for each of the three time series for the next time step.

This might be referred to as multivariate forecasting.

Again, the data must be split into input/output samples in order to train a model.

The first sample of this dataset would be:

Input:

10, 15, 25 20, 25, 45 30, 35, 65 1 2 3 10, 15, 25 20, 25, 45 30, 35, 65

Output:

40, 45, 85 1 40, 45, 85

The split_sequences() function below will split multiple parallel time series with rows for time steps and one series per column into the required input/output shape.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this on the contrived problem; the complete example is listed below.

# multivariate output data prep from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # multivariate output data prep from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared X and y components.

The shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3).

The shape of y is two-dimensional as we might expect for the number of samples (6) and the number of time variables per sample to be predicted (3).

The data is ready to use in a 1D CNN model that expects three-dimensional input and two-dimensional output shapes for the X and y components of each sample.

Then, each of the samples is printed showing the input and output components of each sample.

(6, 3, 3) (6, 3) [[10 15 25] [20 25 45] [30 35 65]] [40 45 85] [[20 25 45] [30 35 65] [40 45 85]] [ 50 55 105] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [ 60 65 125] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [ 70 75 145] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [ 80 85 165] [[ 60 65 125] [ 70 75 145] [ 80 85 165]] [ 90 95 185] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (6, 3, 3) (6, 3) [[10 15 25] [20 25 45] [30 35 65]] [40 45 85] [[20 25 45] [30 35 65] [40 45 85]] [ 50 55 105] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [ 60 65 125] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [ 70 75 145] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [ 80 85 165] [[ 60 65 125] [ 70 75 145] [ 80 85 165]] [ 90 95 185]

We are now ready to fit a 1D CNN model on this data.

In this model, the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument.

The number of parallel series is also used in the specification of the number of values to predict by the model in the output layer; again, this is three.

# define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(n_features)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 7 8 # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( n_features ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

We can predict the next value in each of the three parallel series by providing an input of three time steps for each series.

70, 75, 145 80, 85, 165 90, 95, 185 1 2 3 70, 75, 145 80, 85, 165 90, 95, 185

The shape of the input for making a single prediction must be 1 sample, 3 time steps, and 3 features, or [1, 3, 3].

# demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

We would expect the vector output to be:

[100, 105, 205] 1 [100, 105, 205]

We can tie all of this together and demonstrate a 1D CNN for multivariate output time series forecasting below.

# multivariate output 1d cnn example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(n_features)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=3000, verbose=0) # demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 # multivariate output 1d cnn example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( n_features ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 3000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model and makes a prediction.

[[100.11272 105.32213 205.53436]] 1 [[100.11272 105.32213 205.53436]]

As with multiple input series, there is another more elaborate way to model the problem.

Each output series can be handled by a separate output CNN model.

We can refer to this as a multi-output CNN model. It may offer more flexibility or better performance depending on the specifics of the problem that is being modeled.

This type of model can be defined in Keras using the Keras functional API.

First, we can define the first input model as a 1D CNN model.

# define model visible = Input(shape=(n_steps, n_features)) cnn = Conv1D(filters=64, kernel_size=2, activation='relu')(visible) cnn = MaxPooling1D(pool_size=2)(cnn) cnn = Flatten()(cnn) cnn = Dense(50, activation='relu')(cnn) 1 2 3 4 5 6 # define model visible = Input ( shape = ( n_steps , n_features ) ) cnn = Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' ) ( visible ) cnn = MaxPooling1D ( pool_size = 2 ) ( cnn ) cnn = Flatten ( ) ( cnn ) cnn = Dense ( 50 , activation = 'relu' ) ( cnn )

We can then define one output layer for each of the three series that we wish to forecast, where each output submodel will forecast a single time step.

# define output 1 output1 = Dense(1)(cnn) # define output 2 output2 = Dense(1)(cnn) # define output 3 output3 = Dense(1)(cnn) 1 2 3 4 5 6 # define output 1 output1 = Dense ( 1 ) ( cnn ) # define output 2 output2 = Dense ( 1 ) ( cnn ) # define output 3 output3 = Dense ( 1 ) ( cnn )

We can then tie the input and output layers together into a single model.

# tie together model = Model(inputs=visible, outputs=[output1, output2, output3]) model.compile(optimizer='adam', loss='mse') 1 2 3 # tie together model = Model ( inputs = visible , outputs = [ output1 , output2 , output3 ] ) model . compile ( optimizer = 'adam' , loss = 'mse' )

To make the model architecture clear, the schematic below clearly shows the three separate output layers of the model and the input and output shapes of each layer.

When training the model, it will require three separate output arrays per sample. We can achieve this by converting the output training data that has the shape [7, 3] to three arrays with the shape [7, 1].

# separate output y1 = y[:, 0].reshape((y.shape[0], 1)) y2 = y[:, 1].reshape((y.shape[0], 1)) y3 = y[:, 2].reshape((y.shape[0], 1)) 1 2 3 4 # separate output y1 = y [ : , 0 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y2 = y [ : , 1 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y3 = y [ : , 2 ] . reshape ( ( y . shape [ 0 ] , 1 ) )

These arrays can be provided to the model during training.

# fit model model.fit(X, [y1,y2,y3], epochs=2000, verbose=0) 1 2 # fit model model . fit ( X , [ y1 , y2 , y3 ] , epochs = 2000 , verbose = 0 )

Tying all of this together, the complete example is listed below.

# multivariate output 1d cnn example from numpy import array from numpy import hstack from keras.models import Model from keras.layers import Input from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # separate output y1 = y[:, 0].reshape((y.shape[0], 1)) y2 = y[:, 1].reshape((y.shape[0], 1)) y3 = y[:, 2].reshape((y.shape[0], 1)) # define model visible = Input(shape=(n_steps, n_features)) cnn = Conv1D(filters=64, kernel_size=2, activation='relu')(visible) cnn = MaxPooling1D(pool_size=2)(cnn) cnn = Flatten()(cnn) cnn = Dense(50, activation='relu')(cnn) # define output 1 output1 = Dense(1)(cnn) # define output 2 output2 = Dense(1)(cnn) # define output 3 output3 = Dense(1)(cnn) # tie together model = Model(inputs=visible, outputs=[output1, output2, output3]) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, [y1,y2,y3], epochs=2000, verbose=0) # demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 # multivariate output 1d cnn example from numpy import array from numpy import hstack from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # separate output y1 = y [ : , 0 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y2 = y [ : , 1 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y3 = y [ : , 2 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) # define model visible = Input ( shape = ( n_steps , n_features ) ) cnn = Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' ) ( visible ) cnn = MaxPooling1D ( pool_size = 2 ) ( cnn ) cnn = Flatten ( ) ( cnn ) cnn = Dense ( 50 , activation = 'relu' ) ( cnn ) # define output 1 output1 = Dense ( 1 ) ( cnn ) # define output 2 output2 = Dense ( 1 ) ( cnn ) # define output 3 output3 = Dense ( 1 ) ( cnn ) # tie together model = Model ( inputs = visible , outputs = [ output1 , output2 , output3 ] ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , [ y1 , y2 , y3 ] , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[array([[100.96118]], dtype=float32), array([[105.502686]], dtype=float32), array([[205.98045]], dtype=float32)] 1 2 3 [array([[100.96118]], dtype=float32), array([[105.502686]], dtype=float32), array([[205.98045]], dtype=float32)]

Multi-Step CNN Models

In practice, there is little difference to the 1D CNN model in predicting a vector output that represents different output variables (as in the previous example), or a vector output that represents multiple time steps of one variable.

Nevertheless, there are subtle and important differences in the way the training data is prepared. In this section, we will demonstrate the case of developing a multi-step forecast model using a vector model.

Before we look at the specifics of the model, let’s first look at the preparation of data for multi-step forecasting.

Data Preparation

As with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components.

Both the input and output components will be comprised of multiple time steps and may or may not have the same number of steps.

For example, given the univariate time series:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We could use the last three time steps as input and forecast the next two time steps.

The first sample would look as follows:

Input:

[10, 20, 30] 1 [10, 20, 30]

Output:

[40, 50] 1 [40, 50]

The split_sequence() function below implements this behavior and will split a given univariate time series into samples with a specified number of input and output time steps.

# split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on the small contrived dataset.

The complete example is listed below.

# multi-step data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # split into samples X, y = split_sequence(raw_seq, n_steps_in, n_steps_out) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # multi-step data preparation from numpy import array # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # split into samples X , y = split_sequence ( raw_seq , n_steps_in , n_steps_out ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example splits the univariate series into input and output time steps and prints the input and output components of each.

[10 20 30] [40 50] [20 30 40] [50 60] [30 40 50] [60 70] [40 50 60] [70 80] [50 60 70] [80 90] 1 2 3 4 5 [10 20 30] [40 50] [20 30 40] [50 60] [30 40 50] [60 70] [40 50 60] [70 80] [50 60 70] [80 90]

Now that we know how to prepare data for multi-step forecasting, let’s look at a 1D CNN model that can learn this mapping.

Vector Output Model

The 1D CNN can output a vector directly that can be interpreted as a multi-step forecast.

This approach was seen in the previous section were one time step of each output time series was forecasted as a vector.

As with the 1D CNN models for univariate data in a prior section, the prepared samples must first be reshaped. The CNN expects data to have a three-dimensional structure of [samples, timesteps, features], and in this case, we only have one feature so the reshape is straightforward.

# reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) 1 2 3 # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) )

With the number of input and output steps specified in the n_steps_in and n_steps_out variables, we can define a multi-step time-series forecasting model.

# define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 7 8 # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps_in , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

The model can make a prediction for a single sample. We can predict the next two steps beyond the end of the dataset by providing the input:

[70, 80, 90] 1 [70, 80, 90]

We would expect the predicted output to be:

[100, 110] 1 [100, 110]

As expected by the model, the shape of the single sample of input data when making the prediction must be [1, 3, 1] for the 1 sample, 3 time steps of the input, and the single feature.

# demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

Tying all of this together, the 1D CNN for multi-step forecasting with a univariate time series is listed below.

# univariate multi-step vector-output 1d cnn example from numpy import array from keras.models import Sequential from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # split into samples X, y = split_sequence(raw_seq, n_steps_in, n_steps_out) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) # define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # univariate multi-step vector-output 1d cnn example from numpy import array from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # split into samples X , y = split_sequence ( raw_seq , n_steps_in , n_steps_out ) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) ) # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps_in , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example forecasts and prints the next two time steps in the sequence.

[[102.86651 115.08979]] 1 [[102.86651 115.08979]]

Multivariate Multi-Step CNN Models

In the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting.

It is possible to mix and match the different types of 1D CNN models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging.

In this section, we will explore short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically:

Multiple Input Multi-Step Output. Multiple Parallel Input and Multi-Step Output.

Perhaps the biggest stumbling block is in the preparation of data, so this is where we will focus our attention.

Multiple Input Multi-Step Output

There are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series.

For example, consider our multivariate time series from a prior section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may use three prior time steps of each of the two input time series to predict two time steps of the output time series.

Input:

10, 15 20, 25 30, 35 1 2 3 10, 15 20, 25 30, 35

Output:

65 85 1 2 65 85

The split_sequences() function below implements this behavior.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this on our contrived dataset. The complete example is listed below.

# multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared training data.

We can see that the shape of the input portion of the samples is three-dimensional, comprised of six samples, with three time steps and two variables for the two input time series.

The output portion of the samples is two-dimensional for the six samples and the two time steps for each sample to be predicted.

The prepared samples are then printed to confirm that the data was prepared as we specified.

(6, 3, 2) (6, 2) [[10 15] [20 25] [30 35]] [65 85] [[20 25] [30 35] [40 45]] [ 85 105] [[30 35] [40 45] [50 55]] [105 125] [[40 45] [50 55] [60 65]] [125 145] [[50 55] [60 65] [70 75]] [145 165] [[60 65] [70 75] [80 85]] [165 185] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (6, 3, 2) (6, 2) [[10 15] [20 25] [30 35]] [65 85] [[20 25] [30 35] [40 45]] [ 85 105] [[30 35] [40 45] [50 55]] [105 125] [[40 45] [50 55] [60 65]] [125 145] [[50 55] [60 65] [70 75]] [145 165] [[60 65] [70 75] [80 85]] [165 185]

We can now develop a 1D CNN model for multi-step predictions.

In this case, we will demonstrate a vector output model. The complete example is listed below.

# multivariate multi-step 1d cnn example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([[70, 75], [80, 85], [90, 95]]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 # multivariate multi-step 1d cnn example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps_in , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 ] , [ 80 , 85 ] , [ 90 , 95 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example fits the model and predicts the next two time steps of the output sequence beyond the dataset.

We would expect the next two steps to be [185, 205].

It is a challenging framing of the problem with very little data, and the arbitrarily configured version of the model gets close.

[[185.57011 207.77893]] 1 [[185.57011 207.77893]]

Multiple Parallel Input and Multi-Step Output

A problem with parallel time series may require the prediction of multiple time steps of each time series.

For example, consider our multivariate time series from a prior section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may use the last three time steps from each of the three time series as input to the model, and predict the next time steps of each of the three time series as output.

The first sample in the training dataset would be the following.

Input:

10, 15, 25 20, 25, 45 30, 35, 65 1 2 3 10, 15, 25 20, 25, 45 30, 35, 65

Output:

40, 45, 85 50, 55, 105 1 2 40, 45, 85 50, 55, 105

The split_sequences() function below implements this behavior.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on the small contrived dataset.

The complete example is listed below.

# multivariate multi-step data preparation from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import RepeatVector from keras.layers import TimeDistributed # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # multivariate multi-step data preparation from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import RepeatVector from keras . layers import TimeDistributed # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared training dataset.

We can see that both the input (X) and output (Y) elements of the dataset are three dimensional for the number of samples, time steps, and variables or parallel time series respectively.

The input and output elements of each series are then printed side by side so that we can confirm that the data was prepared as we expected.

(5, 3, 3) (5, 2, 3) [[10 15 25] [20 25 45] [30 35 65]] [[ 40 45 85] [ 50 55 105]] [[20 25 45] [30 35 65] [40 45 85]] [[ 50 55 105] [ 60 65 125]] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [[ 60 65 125] [ 70 75 145]] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [[ 70 75 145] [ 80 85 165]] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [[ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (5, 3, 3) (5, 2, 3) [[10 15 25] [20 25 45] [30 35 65]] [[ 40 45 85] [ 50 55 105]] [[20 25 45] [30 35 65] [40 45 85]] [[ 50 55 105] [ 60 65 125]] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [[ 60 65 125] [ 70 75 145]] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [[ 70 75 145] [ 80 85 165]] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [[ 80 85 165] [ 90 95 185]]

We can now develop a 1D CNN model for this dataset.

We will use a vector-output model in this case. As such, we must flatten the three-dimensional structure of the output portion of each sample in order to train the model. This means, instead of predicting two steps for each series, the model is trained on and expected to predict a vector of six numbers directly.

# flatten output n_output = y.shape[1] * y.shape[2] y = y.reshape((y.shape[0], n_output)) 1 2 3 # flatten output n_output = y . shape [ 1 ] * y . shape [ 2 ] y = y . reshape ( ( y . shape [ 0 ] , n_output ) )

The complete example is listed below.

# multivariate output multi-step 1d cnn example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense from keras.layers import Flatten from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) # flatten output n_output = y.shape[1] * y.shape[2] y = y.reshape((y.shape[0], n_output)) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features))) model.add(MaxPooling1D(pool_size=2)) model.add(Flatten()) model.add(Dense(50, activation='relu')) model.add(Dense(n_output)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=7000, verbose=0) # demonstrate prediction x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # multivariate output multi-step 1d cnn example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) # flatten output n_output = y . shape [ 1 ] * y . shape [ 2 ] y = y . reshape ( ( y . shape [ 0 ] , n_output ) ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # define model model = Sequential ( ) model . add ( Conv1D ( filters = 64 , kernel_size = 2 , activation = 'relu' , input_shape = ( n_steps_in , n_features ) ) ) model . add ( MaxPooling1D ( pool_size = 2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( n_output ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 7000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 60 , 65 , 125 ] , [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example fits the model and predicts the values for each of the three time steps for the next two time steps beyond the end of the dataset.

We would expect the values for these series and time steps to be as follows:

90, 95, 185 100, 105, 205 1 2 90, 95, 185 100, 105, 205

We can see that the model forecast gets reasonably close to the expected values.

[[ 90.47855 95.621284 186.02629 100.48118 105.80815 206.52821 ]] 1 [[ 90.47855 95.621284 186.02629 100.48118 105.80815 206.52821 ]]

Summary

In this tutorial, you discovered how to develop a suite of CNN models for a range of standard time series forecasting problems.

Specifically, you learned:

How to develop CNN models for univariate time series forecasting.

How to develop CNN models for multivariate time series forecasting.

How to develop CNN models for multi-step time series forecasting.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
24;machinelearningmastery.com;http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/;2016-08-08;How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras;"# Use scikit-learn to grid search the number of neurons

import numpy

from sklearn . model_selection import GridSearchCV

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Dropout

from keras . wrappers . scikit_learn import KerasClassifier

from keras . constraints import maxnorm

# Function to create model, required for KerasClassifier

def create_model ( neurons = 1 ) :

# create model

model = Sequential ( )

model . add ( Dense ( neurons , input_dim = 8 , kernel_initializer = 'uniform' , activation = 'linear' , kernel_constraint = maxnorm ( 4 ) ) )

model . add ( Dropout ( 0.2 ) )

model . add ( Dense ( 1 , kernel_initializer = 'uniform' , activation = 'sigmoid' ) )

# Compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

return model

# fix random seed for reproducibility

seed = 7

numpy . random . seed ( seed )

# load dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# create model

model = KerasClassifier ( build_fn = create_model , epochs = 100 , batch_size = 10 , verbose = 0 )

# define the grid search parameters

neurons = [ 1 , 5 , 10 , 15 , 20 , 25 , 30 ]

param_grid = dict ( neurons = neurons )

grid = GridSearchCV ( estimator = model , param_grid = param_grid , n_jobs = - 1 , cv = 3 )

grid_result = grid . fit ( X , Y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :"
25;machinelearningmastery.com;https://machinelearningmastery.com/scale-machine-learning-data-scratch-python/;2016-10-13;How to Scale Machine Learning Data From Scratch With Python;"from csv import reader

from math import sqrt

# Load a CSV file

def load_csv ( filename ) :

file = open ( filename , ""rb"" )

lines = reader ( file )

dataset = list ( lines )

return dataset

# Convert string column to float

def str_column_to_float ( dataset , column ) :

for row in dataset :

row [ column ] = float ( row [ column ] . strip ( ) )

# calculate column means

def column_means ( dataset ) :

means = [ 0 for i in range ( len ( dataset [ 0 ] ) ) ]

for i in range ( len ( dataset [ 0 ] ) ) :

col_values = [ row [ i ] for row in dataset ]

means [ i ] = sum ( col_values ) / float ( len ( dataset ) )

return means

# calculate column standard deviations

def column_stdevs ( dataset , means ) :

stdevs = [ 0 for i in range ( len ( dataset [ 0 ] ) ) ]

for i in range ( len ( dataset [ 0 ] ) ) :

variance = [ pow ( row [ i ] - means [ i ] , 2 ) for row in dataset ]

stdevs [ i ] = sum ( variance )

stdevs = [ sqrt ( x / ( float ( len ( dataset ) - 1 ) ) ) for x in stdevs ]

return stdevs

# standardize dataset

def standardize_dataset ( dataset , means , stdevs ) :

for row in dataset :

for i in range ( len ( row ) ) :

row [ i ] = ( row [ i ] - means [ i ] ) / stdevs [ i ]

# Load pima-indians-diabetes dataset

filename = 'pima-indians-diabetes.csv'

dataset = load_csv ( filename )

print ( 'Loaded data file {0} with {1} rows and {2} columns' ) . format ( filename , len ( dataset ) , len ( dataset [ 0 ] ) )

# convert string columns to float

for i in range ( len ( dataset [ 0 ] ) ) :

str_column_to_float ( dataset , i )

print ( dataset [ 0 ] )

# Estimate mean and standard deviation

means = column_means ( dataset )

stdevs = column_stdevs ( dataset , means )

# standardize dataset

standardize_dataset ( dataset , means , stdevs )"
26;machinelearningmastery.com;https://machinelearningmastery.com/gentle-introduction-generative-long-short-term-memory-networks/;2017-08-24;Gentle Introduction to Generative Long Short-Term Memory Networks;"Tweet Share Share

Last Updated on August 14, 2019

The Long Short-Term Memory recurrent neural network was developed for sequence prediction.

In addition to sequence prediction problems. LSTMs can also be used as a generative model

In this post, you will discover how LSTMs can be used as generative models.

After completing this post, you will know:

About generative models, with a focus on generative models for text called language modeling.

Examples of applications where LSTM Generative models have been used.

Examples of how to model text for generative models with LSTMs.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Generative Models

LSTMs can be used as a generative model.

Given a large corpus of sequence data, such as text documents, LSTM models can be designed to learn the general structural properties of the corpus, and when given a seed input, can generate new sequences that are representative of the original corpus.

The problem of developing a model to generalize a corpus of text is called language modeling in the field of natural language processing. A language model may work at the word level and learn the probabilistic relationships between words in a document in order to accurately complete a sentence and generate entirely new sentences. At its most challenging, language models work at the character level, learning from sequences of characters, and generating new sequences one character at a time.

The goal of character-level language modeling is to predict the next character in a sequence.

— Generating Text with Recurrent Neural Networks, 2011.

Although more challenging, the added flexibility of a character-level model allows new words to be generated, punctuation added, and the generation of any other structures that may exist in the text data.

… predicting one character at a time is more interesting from the perspective of sequence generation, because it allows the network to invent novel words and strings.

— Generating Sequences With Recurrent Neural Networks, 2013.

Language modeling is by far the most studied application of Generative LSTMs, perhaps because of the use of standard datasets where model performance can be quantified and compared. This approach has been used to generate text on a suite of interesting language modeling problems, such as:

Generating Wikipedia articles (including markup).

Generating snippets from great authors like Shakespeare.

Generating technical manuscripts (including markup).

Generating computer source code.

Generating article headlines.

The quality of the results vary; for example, the markup or source code may require manual intervention to render or compile. Nevertheless, the results are impressive.

The approach has also been applied to different domains where a large corpus of existing sequence information is available and new sequences can be generated one step at a time, such as:

Handwriting generation.

Music generation.

Speech generation.

Generative LSTMs

A Generative LSTM is not really architecture, it is more a change in perspective about what an LSTM predictive model learns and how the model is used.

We could conceivably use any LSTM architecture as a generative model. In this case, we will use a simple Vanilla LSTM.

In the case of a character-level language model, the alphabet of all possible characters is fixed. A one hot encoding is used both for learning input sequences and predicting output sequences.

A one-to-one model is used where one step is predicted for each input time step. This means that input sequences may require specialized handling in order to be vectorized or formatted for efficiently training a supervised model.

For example, given the sequence:

""hello world"" 1 ""hello world""

A dataset would need to be constructed such as:

'h' => 'e' 'e' => 'l' 'l' => 'l' ... 1 2 3 4 'h' => 'e' 'e' => 'l' 'l' => 'l' ...

This could be presented as-is as a dataset of one time step samples, which could be quite limiting to the network (e.g. no BPTT).

Alternately, it could be vectorized to a fixed-length input sequence for a many-to-one time step model, such as:

['h', 'e', 'l'] => 'l' ['e', 'l', 'l'] => 'o' ['l', 'l', 'o'] => ' ' ... 1 2 3 4 ['h', 'e', 'l'] => 'l' ['e', 'l', 'l'] => 'o' ['l', 'l', 'o'] => ' ' ...

Or, a fixed-length output sequence for a one-to-many time step model:

'h' => ['e', 'l', 'l'] 'e' => ['l', 'l', 'o'] 'l' => ['l', 'o', ' '] ... 1 2 3 4 'h' => ['e', 'l', 'l'] 'e' => ['l', 'l', 'o'] 'l' => ['l', 'o', ' '] ...

Or some variation on these approaches.

Note that the same vectorized representation would be required when making predictions, meaning that predicted characters would need to be presented as input for subsequent samples. This could be quite clumsy in implementation.

The internal state of the network may need careful management, perhaps reset at choice locations in the input sequence (e.g. end of paragraph, page, or chapter) rather than at the end of each input sequence.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Papers

Posts

Summary

In this post, you discovered the use of LSTMs as generative models.

Specifically, you learned:

About generative models, with a focus on generative models for text called language modeling.

Examples of applications where LSTM Generative models have been used.

Examples of how to model text for generative models with LSTMs.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside"
27;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/;2019-05-12;How to Develop a CNN From Scratch for CIFAR-10 Photo Classification;"# baseline model with dropout and data augmentation on the cifar10 dataset

import sys

from matplotlib import pyplot

from keras . datasets import cifar10

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Conv2D

from keras . layers import MaxPooling2D

from keras . layers import Dense

from keras . layers import Flatten

from keras . optimizers import SGD

from keras . preprocessing . image import ImageDataGenerator

from keras . layers import Dropout

from keras . layers import BatchNormalization

# load train and test dataset

def load_dataset ( ) :

# load dataset

( trainX , trainY ) , ( testX , testY ) = cifar10 . load_data ( )

# one hot encode target values

trainY = to_categorical ( trainY )

testY = to_categorical ( testY )

return trainX , trainY , testX , testY

# scale pixels

def prep_pixels ( train , test ) :

# convert from integers to floats

train_norm = train . astype ( 'float32' )

test_norm = test . astype ( 'float32' )

# normalize to range 0-1

train_norm = train_norm / 255.0

test_norm = test_norm / 255.0

# return normalized images

return train_norm , test_norm

# define cnn model

def define_model ( ) :

model = Sequential ( )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 32 , 32 , 3 ) ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.2 ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.3 ) )

model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( BatchNormalization ( ) )

model . add ( Dropout ( 0.5 ) )

model . add ( Dense ( 10 , activation = 'softmax' ) )

# compile model

opt = SGD ( lr = 0.001 , momentum = 0.9 )

model . compile ( optimizer = opt , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )

return model

# plot diagnostic learning curves

def summarize_diagnostics ( history ) :

# plot loss

pyplot . subplot ( 211 )

pyplot . title ( 'Cross Entropy Loss' )

pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' )

# plot accuracy

pyplot . subplot ( 212 )

pyplot . title ( 'Classification Accuracy' )

pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' )

pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' )

# save plot to file

filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ]

pyplot . savefig ( filename + '_plot.png' )

pyplot . close ( )

# run the test harness for evaluating a model

def run_test_harness ( ) :

# load dataset

trainX , trainY , testX , testY = load_dataset ( )

# prepare pixel data

trainX , testX = prep_pixels ( trainX , testX )

# define model

model = define_model ( )

# create data generator

datagen = ImageDataGenerator ( width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True )

# prepare iterator

it_train = datagen . flow ( trainX , trainY , batch_size = 64 )

# fit model

steps = int ( trainX . shape [ 0 ] / 64 )

history = model . fit_generator ( it_train , steps_per_epoch = steps , epochs = 400 , validation_data = ( testX , testY ) , verbose = 0 )

# evaluate model

_ , acc = model . evaluate ( testX , testY , verbose = 0 )

print ( '> %.3f' % ( acc * 100.0 ) )

# learning curves

summarize_diagnostics ( history )

# entry point, run the test harness"
28;news.mit.edu;http://news.mit.edu/2020/engineers-3d-print-brain-implants-0330;;Engineers 3D print soft, rubbery brain implants;"The brain is one of our most vulnerable organs, as soft as the softest tofu. Brain implants, on the other hand, are typically made from metal and other rigid materials that over time can cause inflammation and the buildup of scar tissue.

MIT engineers are working on developing soft, flexible neural implants that can gently conform to the brain’s contours and monitor activity over longer periods, without aggravating surrounding tissue. Such flexible electronics could be softer alternatives to existing metal-based electrodes designed to monitor brain activity, and may also be useful in brain implants that stimulate neural regions to ease symptoms of epilepsy, Parkinson’s disease, and severe depression.

Led by Xuanhe Zhao, a professor of mechanical engineering and of civil and environmental engineering, the research team has now developed a way to 3D print neural probes and other electronic devices that are as soft and flexible as rubber.

The devices are made from a type of polymer, or soft plastic, that is electrically conductive. The team transformed this normally liquid-like conducting polymer solution into a substance more like viscous toothpaste — which they could then feed through a conventional 3D printer to make stable, electrically conductive patterns.

The team printed several soft electronic devices, including a small, rubbery electrode, which they implanted in the brain of a mouse. As the mouse moved freely in a controlled environment, the neural probe was able to pick up on the activity from a single neuron. Monitoring this activity can give scientists a higher-resolution picture of the brain’s activity, and can help in tailoring therapies and long-term brain implants for a variety of neurological disorders.

“We hope by demonstrating this proof of concept, people can use this technology to make different devices, quickly,” says Hyunwoo Yuk, a graduate student in Zhao’s group at MIT. “They can change the design, run the printing code, and generate a new design in 30 minutes. Hopefully this will streamline the development of neural interfaces, fully made of soft materials.”

Yuk and Zhao have published their results today in the journal Nature Communications. Their co-authors include Baoyang Lu and Jingkun Xu of the Jiangxi Science and Technology Normal University, along with Shen Lin and Jianhong Luo of Zheijiang University’s School of Medicine.

The team printed several soft electronic devices, including a small, rubbery electrode.

From soap water to toothpaste

Conducting polymers are a class of materials that scientists have eagerly explored in recent years for their unique combination of plastic-like flexibility and metal-like electrical conductivity. Conducting polymers are used commercially as antistatic coatings, as they can effectively carry away any electrostatic charges that build up on electronics and other static-prone surfaces.

“These polymer solutions are easy to spray on electrical devices like touchscreens,” Yuk says. “But the liquid form is mostly for homogenous coatings, and it’s difficult to use this for any two-dimensional, high-resolution patterning. In 3D, it’s impossible.”

Yuk and his colleagues reasoned that if they could develop a printable conducting polymer, they could then use the material to print a host of soft, intricately patterned electronic devices, such as flexible circuits, and single-neuron electrodes.

In their new study, the team report modifying poly (3,4-ethylenedioxythiophene) polystyrene sulfonate, or PEDOT:PSS, a conducting polymer typically supplied in the form of an inky, dark-blue liquid. The liquid is a mixture of water and nanofibers of PEDOT:PSS. The liquid gets its conductivity from these nanofibers, which, when they come in contact, act as a sort of tunnel through which any electrical charge can flow.

If the researchers were to feed this polymer into a 3D printer in its liquid form, it would simply bleed across the underlying surface. So the team looked for a way to thicken the polymer while retaining the material’s inherent electrical conductivity.

They first freeze-dried the material, removing the liquid and leaving behind a dry matrix, or sponge, of nanofibers. Left alone, these nanofibers would become brittle and crack. So the researchers then remixed the nanofibers with a solution of water and an organic solvent, which they had previously developed, to form a hydrogel — a water-based, rubbery material embedded with nanofibers.

They made hydrogels with various concentrations of nanofibers, and found that a range between 5 to 8 percent by weight of nanofibers produced a toothpaste-like material that was both electrically conductive and suitable for feeding into a 3D printer.

“Initially, it’s like soap water,” Zhao says. “We condense the nanofibers and make it viscous like toothpaste, so we can squeeze it out as a thick, printable liquid.”

Implants on demand

The researchers fed the new conducting polymer into a conventional 3D printer and found they could produce intricate patterns that remained stable and electrically conductive.

As a proof of concept, they printed a small, rubbery electrode, about the size of a piece of confetti. The electrode consists of a layer of flexible, transparent polymer, over which they then printed the conducting polymer, in thin, parallel lines that converged at a tip, measuring about 10 microns wide — small enough to pick up electrical signals from a single neuron.

MIT researchers print flexible circuits (shown here) and other soft electrical devices using new 3-D-printing technique and conducting polymer ink.

The team implanted the electrode in the brain of a mouse and found it could pick up electrical signals from a single neuron.

“Traditionally, electrodes are rigid metal wires, and once there are vibrations, these metal electrodes could damage tissue,” Zhao says. “We’ve shown now that you could insert a gel probe instead of a needle.”

In principle, such soft, hydrogel-based electrodes might even be more sensitive than conventional metal electrodes. That’s because most metal electrodes conduct electricity in the form of electrons, whereas neurons in the brain produce electrical signals in the form of ions. Any ionic current produced by the brain needs to be converted into an electrical signal that a metal electrode can register — a conversion that can result in some part of the signal getting lost in translation. What’s more, ions can only interact with a metal electrode at its surface, which can limit the concentration of ions that the electrode can detect at any given time.

In contrast, the team’s soft electrode is made from electron-conducting nanofibers, embedded in a hydrogel — a water-based material that ions can freely pass through.

“The beauty of a conducting polymer hydrogel is, on top of its soft mechanical properties, it is made of hydrogel, which is ionically conductive, and also a porous sponge of nanofibers, which the ions can flow in and out of,” Lu says. “Because the electrode’s whole volume is active, its sensitivity is enhanced.”

In addition to the neural probe, the team also fabricated a multielectrode array — a small, Post-it-sized square of plastic, printed with very thin electrodes, over which the researchers also printed a round plastic well. Neuroscientists typically fill the wells of such arrays with cultured neurons, and can study their activity through the signals that are detected by the device’s underlying electrodes.

For this demonstration, the group showed they could replicate the complex designs of such arrays using 3D printing, versus traditional lithography techniques, which

involve carefully etching metals, such as gold, into prescribed patterns, or masks — a process that can take days to complete a single device.

“We make the same geometry and resolution of this device using 3D printing, in less than an hour,” Yuk says. “This process may replace or supplement lithography techniques, as a simpler and cheaper way to make a variety of neurological devices, on demand.”"
29;news.mit.edu;http://news.mit.edu/2020/warning-labels-fake-news-trustworthy-0303;;The catch to putting warning labels on fake news;"After the 2016 U.S. presidential election, Facebook began putting warning tags on news stories fact-checkers judged to be false. But there’s a catch: Tagging some stories as false makes readers more willing to believe other stories and share them with friends, even if those additional, untagged stories also turn out to be false.

That is the main finding of a new study co-authored by an MIT professor, based on multiple experiments with news consumers. The researchers call this unintended consequence — in which the selective labeling of false news makes other news stories seem more legitimate — the “implied-truth effect” in news consumption.

“Putting a warning on some content is going to make you think, to some extent, that all of the other content without the warning might have been checked and verified,” says David Rand, the Erwin H. Schell Professor at the MIT Sloan School of Management and co-author of a newly published paper detailing the study.

“There’s no way the fact-checkers can keep up with the stream of misinformation, so even if the warnings do really reduce belief in the tagged stories, you still have a problem, because of the implied truth effect,” Rand adds.

Moreover, Rand observes, the implied truth effect “is actually perfectly rational” on the part of readers, since there is ambiguity about whether untagged stories were verified or just not yet checked. “That makes these warnings potentially problematic,” he says. “Because people will reasonably make this inference.”

Even so, the findings also suggest a solution: Placing “Verified” tags on stories found to be true eliminates the problem.

The paper, “The Implied Truth Effect,” has just appeared in online form in the journal Management Science. In addition to Rand, the authors are Gordon Pennycook, an assistant professor of psychology at the University of Regina; Adam Bear, a postdoc in the Cushman Lab at Harvard University; and Evan T. Collins, an undergraduate researcher on the project from Yale University.

BREAKING: More labels are better

To conduct the study, the researchers conducted a pair of online experiments with a total of 6,739 U.S. residents, recruited via Amazon’s Mechanical Turk platform. Participants were given a variety of true and false news headlines in a Facebook-style format. The false stories were chosen from the website Snopes.com and included headlines such as “BREAKING NEWS: Hillary Clinton Filed for Divorce in New York Courts” and “Republican Senator Unveils Plan To Send All Of America’s Teachers Through A Marine Bootcamp.”

The participants viewed an equal mix of true stories and false stories, and were asked whether they would consider sharing each story on social media. Some participants were assigned to a control group in which no stories were labeled; others saw a set of stories where some of the false ones displayed a “FALSE” label; and some participants saw a set of stories with warning labels on some false stories and “TRUE” verification labels for some true stories.

In the first place, stamping warnings on false stories does make people less likely to consider sharing them. For instance, with no labels being used at all, participants considered sharing 29.8 percent of false stories in the sample. That figure dropped to 16.1 percent of false stories that had a warning label attached.

However, the researchers also saw the implied truth effect take effect. Readers were willing to share 36.2 percent of the remaining false stories that did not have warning labels, up from 29.8 percent.

“We robustly observe this implied-truth effect, where if false content doesn’t have a warning, people believe it more and say they would be more likely to share it,” Rand notes.

But when the warning labels on some false stories were complemented with verification labels on some of the true stories, participants were less likely to consider sharing false stories, across the board. In those circumstances, they shared only 13.7 percent of the headlines labeled as false, and just 26.9 percent of the nonlabeled false stories.

“If, in addition to putting warnings on things fact-checkers find to be false, you also put verification panels on things fact-checkers find to be true, then that solves the problem, because there’s no longer any ambiguity,” Rand says. “If you see a story without a label, you know it simply hasn’t been checked.”

Policy implications

The findings come with one additional twist that Rand emphasizes, namely, that participants in the survey did not seem to reject warnings on the basis of ideology. They were still likely to change their perceptions of stories with warning or verifications labels, even if discredited news items were “concordant” with their stated political views.

“These results are not consistent with the idea that our reasoning powers are hijacked by our partisanship,” Rand says.

Rand notes that, while continued research on the subject is important, the current study suggests a straightforward way that social media platforms can take action to further improve their systems of labeling online news content.

“I think this has clear policy implications when platforms are thinking about attaching warnings,” he says. “They should be very careful to check not just the effect of the warnings on the content with the tag, but also check the effects on all the other content.”

Support for the research was provided, in part, by the Ethics and Governance of Artificial Intelligence Initiative of the Miami Foundation, and the Social Sciences and Humanities Research Council of Canada."
30;machinelearningmastery.com;https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/;2018-12-09;Use Early Stopping to Halt the Training of Neural Networks At the Right Time;"# generate two moons dataset

from sklearn . datasets import make_moons

from matplotlib import pyplot

from pandas import DataFrame

# generate 2d classification dataset

X , y = make_moons ( n_samples = 100 , noise = 0.2 , random_state = 1 )

# scatter plot, dots colored by class value

df = DataFrame ( dict ( x = X [ : , 0 ] , y = X [ : , 1 ] , label = y ) )

colors = { 0 : 'red' , 1 : 'blue' }

fig , ax = pyplot . subplots ( )

grouped = df . groupby ( 'label' )

for key , group in grouped :

group . plot ( ax = ax , kind = 'scatter' , x = 'x' , y = 'y' , label = key , color = colors [ key ] )"
31;machinelearningmastery.com;http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/;2016-03-20;Overfitting and Underfitting With Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

The cause of poor performance in machine learning is either overfitting or underfitting the data.

In this post, you will discover the concept of generalization in machine learning and the problems of overfitting and underfitting that go along with it.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Approximate a Target Function in Machine Learning

Supervised machine learning is best understood as approximating a target function (f) that maps input variables (X) to an output variable (Y).

Y = f(X)

This characterization describes the range of classification and prediction problems and the machine algorithms that can be used to address them.

An important consideration in learning the target function from the training data is how well the model generalizes to new data. Generalization is important because the data we collect is only a sample, it is incomplete and noisy.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Generalization in Machine Learning

In machine learning we describe the learning of the target function from training data as inductive learning.

Induction refers to learning general concepts from specific examples which is exactly the problem that supervised machine learning problems aim to solve. This is different from deduction that is the other way around and seeks to learn specific concepts from general rules.

Generalization refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning.

The goal of a good machine learning model is to generalize well from the training data to any data from the problem domain. This allows us to make predictions in the future on data the model has never seen.

There is a terminology used in machine learning when we talk about how well a machine learning model learns and generalizes to new data, namely overfitting and underfitting.

Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms.

Statistical Fit

In statistics, a fit refers to how well you approximate a target function.

This is good terminology to use in machine learning, because supervised machine learning algorithms seek to approximate the unknown underlying mapping function for the output variables given the input variables.

Statistics often describe the goodness of fit which refers to measures used to estimate how well the approximation of the function matches the target function.

Some of these methods are useful in machine learning (e.g. calculating the residual errors), but some of these techniques assume we know the form of the target function we are approximating, which is not the case in machine learning.

If we knew the form of the target function, we would use it directly to make predictions, rather than trying to learn an approximation from samples of noisy training data.

Overfitting in Machine Learning

Overfitting refers to a model that models the training data too well.

Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.

Overfitting is more likely with nonparametric and nonlinear models that have more flexibility when learning a target function. As such, many nonparametric machine learning algorithms also include parameters or techniques to limit and constrain how much detail the model learns.

For example, decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up.

Underfitting in Machine Learning

Underfitting refers to a model that can neither model the training data nor generalize to new data.

An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.

Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.

A Good Fit in Machine Learning

Ideally, you want to select a model at the sweet spot between underfitting and overfitting.

This is the goal, but is very difficult to do in practice.

To understand this goal, we can look at the performance of a machine learning algorithm over time as it is learning a training data. We can plot both the skill on the training data and the skill on a test dataset we have held back from the training process.

Over time, as the algorithm learns, the error for the model on the training data goes down and so does the error on the test dataset. If we train for too long, the performance on the training dataset may continue to decrease because the model is overfitting and learning the irrelevant detail and noise in the training dataset. At the same time the error for the test set starts to rise again as the model’s ability to generalize decreases.

The sweet spot is the point just before the error on the test dataset starts to increase where the model has good skill on both the training dataset and the unseen test dataset.

You can perform this experiment with your favorite machine learning algorithms. This is often not useful technique in practice, because by choosing the stopping point for training using the skill on the test dataset it means that the testset is no longer “unseen” or a standalone objective measure. Some knowledge (a lot of useful knowledge) about that data has leaked into the training procedure.

There are two additional techniques you can use to help find the sweet spot in practice: resampling methods and a validation dataset.

How To Limit Overfitting

Both overfitting and underfitting can lead to poor model performance. But by far the most common problem in applied machine learning is overfitting.

Overfitting is such a problem because the evaluation of machine learning algorithms on training data is different from the evaluation we actually care the most about, namely how well the algorithm performs on unseen data.

There are two important techniques that you can use when evaluating machine learning algorithms to limit overfitting:

Use a resampling technique to estimate model accuracy. Hold back a validation dataset.

The most popular resampling technique is k-fold cross validation. It allows you to train and test your model k-times on different subsets of training data and build up an estimate of the performance of a machine learning model on unseen data.

A validation dataset is simply a subset of your training data that you hold back from your machine learning algorithms until the very end of your project. After you have selected and tuned your machine learning algorithms on your training dataset you can evaluate the learned models on the validation dataset to get a final objective idea of how the models might perform on unseen data.

Using cross validation is a gold standard in applied machine learning for estimating model accuracy on unseen data. If you have the data, using a validation dataset is also an excellent practice.

Further Reading

This section lists some recommended resources if you are looking to learn more about generalization, overfitting and underfitting in machine learning.

Summary

In this post, you discovered that machine learning is solving problems by the method of induction.

You learned that generalization is a description of how well the concepts learned by a model apply to new data. Finally, you learned about the terminology of generalization in machine learning of overfitting and underfitting:

Overfitting : Good performance on the training data, poor generliazation to other data.

: Good performance on the training data, poor generliazation to other data. Underfitting: Poor performance on the training data and poor generalization to other data

Do you have any questions about overfitting, underfitting or this post? Leave a comment and ask your question and I will do my best to answer it.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
32;machinelearningmastery.com;http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/;2016-04-14;K-Nearest Neighbors for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

In this post you will discover the k-Nearest Neighbors (KNN) algorithm for classification and regression. After reading this post you will know.

The model representation used by KNN.

How a model is learned using KNN (hint, it’s not).

How to make predictions using KNN

The many names for KNN including how different fields refer to it.

How to prepare your data to get the most from KNN.

Where to look to learn more about the KNN algorithm.

This post was written for developers and assumes no background in statistics or mathematics. The focus is on how the algorithm works and how to use it for predictive modeling problems. If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

KNN Model Representation

The model representation for KNN is the entire training dataset.

It is as simple as that.

KNN has no model other than storing the entire dataset, so there is no learning required.

Efficient implementations can store the data using complex data structures like k-d trees to make look-up and matching of new patterns during prediction efficient.

Because the entire training dataset is stored, you may want to think carefully about the consistency of your training data. It might be a good idea to curate it, update it often as new data becomes available and remove erroneous and outlier data.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Making Predictions with KNN

KNN makes predictions using the training dataset directly.

Predictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.

To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance.

Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j.

EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )

Other popular distance measures include:

Hamming Distance : Calculate the distance between binary vectors (more).

: Calculate the distance between binary vectors (more). Manhattan Distance : Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more).

: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more). Minkowski Distance: Generalization of Euclidean and Manhattan distance (more).

There are many other distance measures that can be used, such as Tanimoto, Jaccard, Mahalanobis and cosine distance. You can choose the best distance metric based on the properties of your data. If you are unsure, you can experiment with different distance metrics and different values of K together and see which mix results in the most accurate models.

Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.).

The value for K can be found by algorithm tuning. It is a good idea to try many different values for K (e.g. values from 1 to 21) and see what works best for your problem.

The computational complexity of KNN increases with the size of the training dataset. For very large training sets, KNN can be made stochastic by taking a sample from the training dataset from which to calculate the K-most similar instances.

KNN has been around for a long time and has been very well studied. As such, different disciplines have different names for it, for example:

Instance-Based Learning : The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain).

: The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain). Lazy Learning : No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm.

: No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm. Non-Parametric: KNN makes no assumptions about the functional form of the problem being solved. As such KNN is referred to as a non-parametric machine learning algorithm.

KNN can be used for regression and classification problems.

KNN for Regression

When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.

KNN for Classification

When KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction.

Class probabilities can be calculated as the normalized frequency of samples that belong to each class in the set of K most similar instances for a new data instance. For example, in a binary classification problem (class is 0 or 1):

p(class=0) = count(class=0) / (count(class=0)+count(class=1))

If you are using K and you have an even number of classes (e.g. 2) it is a good idea to choose a K value with an odd number to avoid a tie. And the inverse, use an even number for K when you have an odd number of classes.

Ties can be broken consistently by expanding K by 1 and looking at the class of the next most similar instance in the training dataset.

Curse of Dimensionality

KNN works well with a small number of input variables (p), but struggles when the number of inputs is very large.

Each input variable can be considered a dimension of a p-dimensional input space. For example, if you had two input variables x1 and x2, the input space would be 2-dimensional.

As the number of dimensions increases the volume of the input space increases at an exponential rate.

In high dimensions, points that may be similar may have very large distances. All points will be far away from each other and our intuition for distances in simple 2 and 3-dimensional spaces breaks down. This might feel unintuitive at first, but this general problem is called the “Curse of Dimensionality“.

Best Prepare Data for KNN

Rescale Data : KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution.

: KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution. Address Missing Data : Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.

: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed. Lower Dimensionality: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space.

Further Reading

If you are interested in implementing KNN from scratch in Python, checkout the post:

Below are some good machine learning texts that cover the KNN algorithm from a predictive modeling perspective.

Also checkout K-Nearest Neighbors on Wikipedia.

Summary

In this post you discovered the KNN machine learning algorithm. You learned that:

KNN stores the entire training dataset which it uses as its representation.

KNN does not learn any model.

KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance.

There are many distance measures to choose from to match the structure of your input data.

That it is a good idea to rescale your data, such as using normalization, when using KNN.

If you have any questions about this post or the KNN algorithm ask in the comments and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
33;machinelearningmastery.com;https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/;2020-01-02;How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification;"# calculates precision for 1:100 dataset with 90 tp and 30 fp

from sklearn . metrics import precision_score

# define actual

act_pos = [ 1 for _ in range ( 100 ) ]

act_neg = [ 0 for _ in range ( 10000 ) ]

y_true = act_pos + act_neg

# define predictions

pred_pos = [ 0 for _ in range ( 10 ) ] + [ 1 for _ in range ( 90 ) ]

pred_neg = [ 1 for _ in range ( 30 ) ] + [ 0 for _ in range ( 9970 ) ]

y_pred = pred_pos + pred_neg

# calculate prediction

precision = precision_score ( y_true , y_pred , average = 'binary' )"
34;machinelearningmastery.com;https://machinelearningmastery.com/statistical-tolerance-intervals-in-machine-learning/;2018-05-31;A Gentle Introduction to Statistical Tolerance Intervals in Machine Learning;"# parametric tolerance interval

from numpy . random import seed

from numpy . random import randn

from numpy import mean

from numpy import sqrt

from scipy . stats import chi2

from scipy . stats import norm

# seed the random number generator

seed ( 1 )

# generate dataset

data = 5 * randn ( 100 ) + 50

# specify degrees of freedom

n = len ( data )

dof = n - 1

# specify data coverage

prop = 0.95

prop_inv = ( 1.0 - prop ) / 2.0

gauss_critical = norm . isf ( prop_inv )

print ( 'Gaussian critical value: %.3f (coverage=%d%%)' % ( gauss_critical , prop* 100 ) )

# specify confidence

prob = 0.99

chi_critical = chi2 . isf ( q = prob , df = dof )

print ( 'Chi-Squared critical value: %.3f (prob=%d%%, dof=%d)' % ( chi_critical , prob* 100 , dof ) )

# tolerance

interval = sqrt ( ( dof * ( 1 + ( 1 / n ) ) * gauss_critical* * 2 ) / chi_critical )

print ( 'Tolerance Interval: %.3f' % interval )

# summarize

data_mean = mean ( data )

lower , upper = data_mean - interval , data_mean + interval"
35;news.mit.edu;http://news.mit.edu/2020/searching-covid-19-protein-test-hadley-sikes-0417;;3 Questions: Hadley Sikes on searching for a Covid-19 protein test;"Before the world was alerted to the threat of a novel coronavirus spreading out from China, Hadley Sikes was already well acquainted with developing molecular technology to improve diagnosis and treatment of diseases. Now working on a crucial diagnostic test to find people with Covid-19, the Esther and Harold E. Edgerton Associate Professor of chemical engineering at MIT and principal investigator of the Antimicrobial Resistance Interdisciplinary Research Group at Singapore-MIT Alliance for Research and Technology (SMART), and her collaborators have managed to condense months of work into a matter of a few weeks.

Q: Where does your work fit in with the global Covid-19 research effort?

A: The Covid-19 pandemic has presented a huge challenge for the world’s capacity for diagnostic testing. It has given us a clearer picture of what our actual capabilities are because all the countries in this effort are as motivated as they could ever be to deploy technologies that can test populations as quickly and accurately as possible.

Scientists have only been able to deploy one kind of test so far to identify people who have Covid-19. The coronavirus that causes this illness is made out of proteins and RNA, and so far, we only detect its RNA. RNA tests are complicated and can take hours, or even days, for doctors to receive the results. A faster version of an RNA test was just announced, but it also requires laboratory equipment and it is difficult to produce as many tests are needed.

What I’ve been working on at MIT and SMART, MIT’s research enterprise in Singapore, is developing protein tests that are quick to run and don’t require laboratories. These tests can find out if viral proteins are present in bodily fluids and also if a patient’s immune system has responded to the SARS-CoV-2 virus.

That is information that is critical, especially in this situation whereby countries are shutting down. If you know who has had the infection and recovered, and thus now has immunity, you have the potential to keep things running without putting more people at risk.

Before this outbreak, we had been working with support from the Deshpande Center on protein tests to diagnose malaria, dengue, and tuberculosis. Our goal was to find a way to capture more of the proteins made by these pathogens by developing binding reagents that concentrate the proteins within a testing device. We also wanted our tests to be affordable and easy to produce in large quantities.

Developing the reagents is a slow but crucial part of the process of developing a clinical protein diagnostic, and typically takes longer than for nucleic acid tests.

Dr. Eric Miller, who is in my lab at MIT, and Dr. Patthara Kongsuphol at SMART, have been working on engineering reagents that capture more of the scarce viral proteins in a patient’s bodily fluids. If more of these viral proteins can be captured, the test can be more sensitive.

With the Covid-19 pandemic as his motivation, Dr. Miller figured out how to engineer these binding agents in just two weeks — much sooner than the several months it might typically take. On the SMART side, Dr. Kongsuphol has been leading our efforts in Singapore to integrate these agents into diverse test formats that can be challenged with clinical samples.

We are aiming to create a test that can work in 10 minutes and doesn’t require specialized instruments or laboratory infrastructure. In this way, it can be carried out at an airport or a clinic to accurately show if a person either has or is immune to Covid-19. It’s challenging to make a test that is sensitive and accurate enough, and also a huge challenge to scale up production of such a test fast enough to have an impact when a new pathogen emerges.

Q: What influence has Singapore had on your work?

A: I have been at MIT for 10 years and started working with SMART two years ago. Joining an interdisciplinary research team in Singapore has given me a really great chance to work on a pressing medical problem of our time, which is antimicrobial resistance. It allows me to work with world-class clinicians and government agencies that are international leaders in public health, and with top researchers at Nanyang Technological University, the National University of Singapore, and the Agency for Science, Technology and Research.

I spent January in Singapore and went back again at the beginning of March, just as the outbreak was emerging in the United States. I really wanted to learn more about how Singapore’s experience during the SARS outbreak in the early 2000s allowed them to respond so effectively to this outbreak, particularly with diagnostic testing.

It was nerve-wracking being separated from my family at this time. I have three young children and a husband who has a full-time job. Because of the 12-hour time difference with Boston, we had a lot of late-night and early-morning FaceTime chats.

They have been really supportive of the work my team and I are trying to do. I think they are glad I went to Singapore because they see that I am doing what I can to play a role in figuring out effective responses to this, and future public health crises. The mission provides a powerful sense of purpose.

The United States is fortunate that it had not experienced the SARS or MERS outbreaks Singapore and other Asian countries had been through, but this means we are lacking in the knowledge and experience these countries have gained. The United States and other countries can learn a lot from Singapore.

After that event, Singaporean officials analyzed everything that happened and put in place new public health measures designed to effectively manage and contain any future outbreaks. By doing this, they have developed a world-class response.

Q: What did you learn in Singapore?

A: I valued getting to speak to the doctors on the ground who were fighting the Covid-19 outbreak in Singapore and learning what they had seen during their case management amid the crisis.

When I arrived in Singapore, I was honored to get to speak with Dr. Sidney Yee, CEO of the Diagnostics Development Hub, who had worked to rapidly produce a high-performing RNA test and ensure it was quickly deployed as part of Singapore’s effective response to Covid-19.

I was also able to talk to my colleague at SMART, Dr. Tsin Wen Yeo, while he was doing shifts in the Intensive Care Unit, caring for Covid-19 patients. He gave me his views about what was required from a diagnostic protein test. I think it was an incredible opportunity to understand the needs of doctors in different settings and it focused the efforts of my team. Understanding how diagnostic tests will be used allows us to prioritize the things that doctors find most important.

You could make all sorts of diagnostic tests, but the ones we focus our effort on are the ones that are going to provide doctors with actionable information that will help them treat their patients.

This is a really interesting time now that there is a sudden emphasis on needing better, faster diagnostics for the world’s health-care systems. Engineers have a big role in providing these, for the benefit of patients and health workers, and also to help economies get back on their feet. I hope that this desire for more practical diagnostic research continues after we recover from this outbreak."
36;machinelearningmastery.com;https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/#comments;2020-03-31;Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost;"# gradient boosting for classification in scikit-learn

from numpy import mean

from numpy import std

from sklearn . datasets import make_classification

from sklearn . ensemble import GradientBoostingClassifier

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from matplotlib import pyplot

# define dataset

X , y = make_classification ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_redundant = 5 , random_state = 1 )

# evaluate the model

model = GradientBoostingClassifier ( )

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

n_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' )

print ( 'Accuracy: %.3f (%.3f)' % ( mean ( n_scores ) , std ( n_scores ) ) )

# fit the model on the whole dataset

model = GradientBoostingClassifier ( )

model . fit ( X , y )

# make a single prediction

row = [ [ 2.56999479 , - 0.13019997 , 3.16075093 , - 4.35936352 , - 1.61271951 , - 1.39352057 , - 2.48924933 , - 1.93094078 , 3.26130366 , 2.05692145 ] ]

yhat = model . predict ( row )"
37;machinelearningmastery.com;https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/;2020-02-11;Bagging and Random Forest for Imbalanced Classification;"# bagged decision trees on an imbalanced classification problem

from numpy import mean

from sklearn . datasets import make_classification

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from sklearn . ensemble import BaggingClassifier

# generate dataset

X , y = make_classification ( n_samples = 10000 , n_features = 2 , n_redundant = 0 ,

n_clusters_per_class = 1 , weights = [ 0.99 ] , flip_y = 0 , random_state = 4 )

# define model

model = BaggingClassifier ( )

# define evaluation procedure

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

# evaluate model

scores = cross_val_score ( model , X , y , scoring = 'roc_auc' , cv = cv , n_jobs = - 1 )

# summarize performance"
38;machinelearningmastery.com;http://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/;2014-02-06;Why you should be Spot-Checking Algorithms on your Machine Learning Problems;"Tweet Share Share

Last Updated on April 7, 2018

Spot-checking algorithms is about getting a quick assessment of a bunch of different algorithms on your machine learning problem so that you know what algorithms to focus on and what to discard.

In this post you will discover the 3 benefits of spot-checking algorithms, 5 tips for spot-checking on your next problem and the top 10 most popular data mining algorithms that you could use in your suite of algorithms to spot-check.

Spot-Checking Algorithms

Spot-checking algorithms is a part of the process of applied machine learning. On a new problem, you need to quickly determine which type or class of algorithms is good at picking out the structure in your problem and which are not.

The alternative to spot checking is that you feel overwhelmed by the vast number of algorithms and algorithm types that you could try that you end up trying very few or going with what has worked for you in the past. This results in wasted time and sub-par results.

Benefits of Spot-Checking Algorithms

There are 3 key benefits of spot-checking algorithms on your machine learning problems:

Speed : You could spend a lot of time playing around with different algorithms, tuning parameters and thinking about what algorithms will do well on your problem. I have been there and end up testing the same algorithms over and over because I have not been systematic. A single spot-check experiment can save hours, days and even weeks of noodling around.

: You could spend a lot of time playing around with different algorithms, tuning parameters and thinking about what algorithms will do well on your problem. I have been there and end up testing the same algorithms over and over because I have not been systematic. A single spot-check experiment can save hours, days and even weeks of noodling around. Objective : There is a tendency to go with what has worked for you before. We pick our favorite algorithm (or algorithms) and apply them to every problem we see. The power of machine learning is that there are so many different ways to approach a given problem. A spot-check experiment allows you to automatically and objectively discover those algorithms that are the best at picking out the structure in the problem so you can focus your attention.

: There is a tendency to go with what has worked for you before. We pick our favorite algorithm (or algorithms) and apply them to every problem we see. The power of machine learning is that there are so many different ways to approach a given problem. A spot-check experiment allows you to automatically and objectively discover those algorithms that are the best at picking out the structure in the problem so you can focus your attention. Results: Spot-checking algorithms gets you usable results, fast. You may discover a good enough solution in the first spot experiment. Alternatively, you may quickly learn that your dataset does not expose enough structure for any mainstream algorithm to do well. Spot-checking gives you the results you need to decide whether to move forward and optimize a given model or backward and revisit the presentation of the problem.

I think spot checking mainstream algorithms on your problem is a no-brainer first step.

Tips for Spot-Checking Algorithms

There are some things you can do when you are spot-checking algorithms to ensure you are getting useful and actionable results.

Below are 5 tips to ensure you are getting the most from spot-checking machine learning algorithms on your problem.

Algorithm Diversity : You want a good mix of algorithm types. I like to include instance based methods (live LVQ and knn), functions and kernels (like neural nets, regression and SVM), rule systems (like Decision Table and RIPPER) and decision trees (like CART, ID3 and C4.5).

: You want a good mix of algorithm types. I like to include instance based methods (live LVQ and knn), functions and kernels (like neural nets, regression and SVM), rule systems (like Decision Table and RIPPER) and decision trees (like CART, ID3 and C4.5). Best Foot Forward : Each algorithm needs to be given a chance to put it’s best foot forward. This does not mean performing a sensitivity analysis on the parameters of each algorithm, but using experiments and heuristics to give each algorithm a fair chance. For example if kNN is in the mix, give it 3 chances with k values of 1, 5 and 7.

: Each algorithm needs to be given a chance to put it’s best foot forward. This does not mean performing a sensitivity analysis on the parameters of each algorithm, but using experiments and heuristics to give each algorithm a fair chance. For example if kNN is in the mix, give it 3 chances with k values of 1, 5 and 7. Formal Experiment : Don’t play. There is a huge temptation to try lots of different things in an informal manner, to play around with algorithms on your problem. The idea of spot-checking is to get to the methods that do well on the problem, fast. Design the experiment, run it, then analyze the results. Be methodical. I like to rank algorithms by their statistical significant wins (in pairwise comparisons) and take the top 3-5 as a basis for tuning.

: Don’t play. There is a huge temptation to try lots of different things in an informal manner, to play around with algorithms on your problem. The idea of spot-checking is to get to the methods that do well on the problem, fast. Design the experiment, run it, then analyze the results. Be methodical. I like to rank algorithms by their statistical significant wins (in pairwise comparisons) and take the top 3-5 as a basis for tuning. Jumping-off Point : The best performing algorithms are a starting point not the solution to the problem. The algorithms that are shown to be effective may not be the best algorithms for the job. They are most likely to be useful pointers to types of algorithms that perform well on the problem. For example, if kNN does well, consider follow-up experiments on all the instance based methods and variations of kNN you can think of.

: The best performing algorithms are a starting point not the solution to the problem. The algorithms that are shown to be effective may not be the best algorithms for the job. They are most likely to be useful pointers to types of algorithms that perform well on the problem. For example, if kNN does well, consider follow-up experiments on all the instance based methods and variations of kNN you can think of. Build Your Short-list: As you learn and try many different algorithms you can add new algorithms to the suite of algorithms that you use in a spot-check experiment. When I discover a particularly powerful configuration of an algorithm, I like to generalize it and include it in my suite, making my suite more robust for the next problem.

Start building up your suite of algorithms for spot check experiments.

Top 10 Algorithms

There was a paper published in 2008 titled “Top 10 algorithms in data mining“. Who could go past a title like that? It was also turned into a book “The Top Ten Algorithms in Data Mining” and inspired the structure of another “Machine Learning in Action”.

This might be a good paper for you to jump start your short-list of algorithms to spot-check on your next machine learning problem. The top 10 algorithms for data mining listed in the paper were.

C4.5 This is a decision tree algorithm and includes descendent methods like the famous C5.0 and ID3 algorithms.

k-means. The go-to clustering algorithm.

Support Vector Machines. This is really a huge field of study.

Apriori. This is the go-to algorithm for rule extraction.

EM. Along with k-means, go-to clustering algorithm.

PageRank. I rarely touch graph-based problems.

AdaBoost. This is really the family of boosting ensemble methods.

knn (k-nearest neighbor). Simple and effective instance-based method.

Naive Bayes. Simple and robust use of Bayes theorem on data.

CART (classification and regression trees) another tree-based method.

There is also a great Quora question on this topic that you could mine for ideas of algorithms to try on your problem.

Resources

Which algorithms do you like to spot-check on problems? Do you have a favorite?"
39;machinelearningmastery.com;https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/;2017-09-19;7 Applications of Deep Learning for Natural Language Processing;"Tweet Share Share

Last Updated on August 7, 2019

The field of natural language processing is shifting from statistical methods to neural network methods.

There are still many challenging problems to solve in natural language. Nevertheless, deep learning methods are achieving state-of-the-art results on some specific language problems.

It is not just the performance of deep learning models on benchmark problems that is most interesting; it is the fact that a single model can learn word meaning and perform language tasks, obviating the need for a pipeline of specialized and hand-crafted methods.

In this post, you will discover 7 interesting natural language processing tasks where deep learning methods are achieving some headway.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we will look at the following 7 natural language processing problems.

Text Classification Language Modeling Speech Recognition Caption Generation Machine Translation Document Summarization Question Answering

I have tried to focus on the types of end-user problems that you may be interested in, as opposed to more academic or linguistic sub-problems where deep learning does well such as part-of-speech tagging, chunking, named entity recognition, and so on.

Each example provides a description of the problem, an example, and references to papers that demonstrate the methods and results. Most references are drawn from Goldberg’s excellent 2015 primer on deep learning for NLP researchers.

Do you have a favorite NLP application for deep learning that is not listed?

Let me know in the comments below.

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Text Classification

Given an example of text, predict a predefined class label.

The goal of text categorization is to classify the topic or theme of a document.

— Page 575, Foundations of Statistical Natural Language Processing, 1999.

A popular classification example is sentiment analysis where class labels represent the emotional tone of the source text such as “positive” or “negative“.

Below are a 3 more examples:

Spam filtering, classifying email text as spam or not.

Language identification, classifying the language of the source text.

Genre classification, classifying the genre of a fictional story.

Further, the problem may be framed in a way that requires multiple classes assigned to a text, so-called multi-label classification. Such as predicting multiple hashtags for a source tweet.

For more on the general topic, see:

Below are 3 examples of deep learning papers for text classification:

2. Language Modeling

Language modeling is really a subtask of more interesting natural language problems, specifically those that condition the language model on some other input.

… the problem is to predict the next word given the previous words. The task is fundamental to speech or optical character recognition, and is also used for spelling correction, handwriting recognition, and statistical machine translation.

— Page 191, Foundations of Statistical Natural Language Processing, 1999.

In addition to the academic interest in language modeling, it is a key component of many deep learning natural language processing architectures.

A language model learns the probabilistic relationship between words such that new sequences of words can be generated that are statistically consistent with the source text.

Alone, language models can be used for text or speech generation; for example:

Generating new article headlines.

Generating new sentences, paragraphs, or documents.

Generating suggested continuation of a sentence.

For more in language modeling, see:

Below is an example of deep learning for language modeling (only):

Language model of English texts, books and news articles. A Neural Probabilistic Language Model, 2003



3. Speech Recognition

Speech recognition is the problem of understanding what was said.

The task of speech recognition is to map an acoustic signal containing a spoken natural language utterance into the corresponding sequence of words intended by the speaker.

— Page 458, Deep Learning, 2016.

Given an utterance of text as audio data, the model must produce human readable text.

Given the automatic nature of the process, the problem may also be called Automatic Speech Recognition (ASR).

A language model is used to create the text output that is conditioned on the audio data.

Some examples include:

Transcribing a speech.

Creating text captions for a movie or TV show.

Issuing commands to the radio while driving.

For more on speech recognition, see:

Below are 3 examples of deep learning for speech recognition.

4. Caption Generation

Caption generation is the problem of describing the contents of an image.

Given a digital image, such as a photo, generate a textual description of the contents of the image.

A language model is used to create the caption that is conditioned on the image.

Some examples include:

Describing the contents of a scene.

Creating a caption for a photograph.

Describing a video.

This is not just an application for the hearing impaired, but also in generating human readable text for image and video data that can be searched, such as on the web.

Below are 3 examples of deep learning for caption generation:

5. Machine Translation

Machine translation is the problem of converting a source text in one language to another language.

Machine translation, the automatic translation of text or speech from one language to another, is one [of] the most important applications of NLP.

— Page 463, Foundations of Statistical Natural Language Processing, 1999.

Given that deep neural networks are used, the field is referred to as neural machine translation.

In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this int a sequence of symbols in another language. This is commonly applied to natural languages, such as translating from English to French. Deep learning has recently begun to have an important impact on this kind of task.

— Page 98, Deep Learning, 2016.

A language model is used to output the destination text in the second language, conditioned on the source text.

Some examples include:

Translating a text document from French to English.

Translating Spanish audio to German text.

Translating English text to Italian audio.

For more on neural machine translation, see:

Below are 3 examples of deep learning for machine translation:

6. Document Summarization

Document summarization is the task where a short description of a text document is created.

As above, a language model is used to output the summary conditioned on the full document.

Some examples of document summarization include:

Creating a heading for a document.

Creating an abstract of a document.

For more on the topic, see:

Below are 3 examples of deep learning for document summarization:

7. Question Answering

Question answering is the problem where given a subject, such as a document of text, answer a specific question about the subject.

… question answering systems which try to answer a user query that is formulated in the form of a question by return the appropriate none phrase such as a location, a person, or a date. For example, the question Why killed President Kennedy? might be answered with the noun phrase Oswald

— Page 377, Foundations of Statistical Natural Language Processing, 1999.

Some examples include:

For more information on question answering, see:

Answering questions about Wikipedia articles.

Answering questions about news articles.

Answering questions about medical records.

Below are 3 examples of deep learning for question answering:

Further Reading

This section provides more resources on deep learning applications for NLP if you are looking go deeper.

Summary

In this post, you discovered 7 applications of deep learning to natural language processing tasks.

Was your favorite example of deep learning for NLP missed?

Let me know in the comments.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside"
40;machinelearningmastery.com;https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/;2018-01-09;How to Develop a Neural Machine Translation System from Scratch;"from pickle import load

from numpy import array

from numpy import argmax

from keras . preprocessing . text import Tokenizer

from keras . preprocessing . sequence import pad_sequences

from keras . models import load_model

from nltk . translate . bleu_score import corpus_bleu

# load a clean dataset

def load_clean_sentences ( filename ) :

return load ( open ( filename , 'rb' ) )

# fit a tokenizer

def create_tokenizer ( lines ) :

tokenizer = Tokenizer ( )

tokenizer . fit_on_texts ( lines )

return tokenizer

# max sentence length

def max_length ( lines ) :

return max ( len ( line . split ( ) ) for line in lines )

# encode and pad sequences

def encode_sequences ( tokenizer , length , lines ) :

# integer encode sequences

X = tokenizer . texts_to_sequences ( lines )

# pad sequences with 0 values

X = pad_sequences ( X , maxlen = length , padding = 'post' )

return X

# map an integer to a word

def word_for_id ( integer , tokenizer ) :

for word , index in tokenizer . word_index . items ( ) :

if index == integer :

return word

return None

# generate target given source sequence

def predict_sequence ( model , tokenizer , source ) :

prediction = model . predict ( source , verbose = 0 ) [ 0 ]

integers = [ argmax ( vector ) for vector in prediction ]

target = list ( )

for i in integers :

word = word_for_id ( i , tokenizer )

if word is None :

break

target . append ( word )

return ' ' . join ( target )

# evaluate the skill of the model

def evaluate_model ( model , tokenizer , sources , raw_dataset ) :

actual , predicted = list ( ) , list ( )

for i , source in enumerate ( sources ) :

# translate encoded source text

source = source . reshape ( ( 1 , source . shape [ 0 ] ) )

translation = predict_sequence ( model , eng_tokenizer , source )

raw_target , raw_src = raw_dataset [ i ]

if i < 10 :

print ( 'src=[%s], target=[%s], predicted=[%s]' % ( raw_src , raw_target , translation ) )

actual . append ( [ raw_target . split ( ) ] )

predicted . append ( translation . split ( ) )

# calculate BLEU score

print ( 'BLEU-1: %f' % corpus_bleu ( actual , predicted , weights = ( 1.0 , 0 , 0 , 0 ) ) )

print ( 'BLEU-2: %f' % corpus_bleu ( actual , predicted , weights = ( 0.5 , 0.5 , 0 , 0 ) ) )

print ( 'BLEU-3: %f' % corpus_bleu ( actual , predicted , weights = ( 0.3 , 0.3 , 0.3 , 0 ) ) )

print ( 'BLEU-4: %f' % corpus_bleu ( actual , predicted , weights = ( 0.25 , 0.25 , 0.25 , 0.25 ) ) )

# load datasets

dataset = load_clean_sentences ( 'english-german-both.pkl' )

train = load_clean_sentences ( 'english-german-train.pkl' )

test = load_clean_sentences ( 'english-german-test.pkl' )

# prepare english tokenizer

eng_tokenizer = create_tokenizer ( dataset [ : , 0 ] )

eng_vocab_size = len ( eng_tokenizer . word_index ) + 1

eng_length = max_length ( dataset [ : , 0 ] )

# prepare german tokenizer

ger_tokenizer = create_tokenizer ( dataset [ : , 1 ] )

ger_vocab_size = len ( ger_tokenizer . word_index ) + 1

ger_length = max_length ( dataset [ : , 1 ] )

# prepare data

trainX = encode_sequences ( ger_tokenizer , ger_length , train [ : , 1 ] )

testX = encode_sequences ( ger_tokenizer , ger_length , test [ : , 1 ] )

# load model

model = load_model ( 'model.h5' )

# test on some training sequences

print ( 'train' )

evaluate_model ( model , eng_tokenizer , trainX , train )

# test on some test sequences

print ( 'test' )"
41;machinelearningmastery.com;http://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/;2014-02-18;How To Choose The Right Test Options When Evaluating Machine Learning Algorithms;"Tweet Share Share

Last Updated on June 21, 2016

The test options you use when evaluating machine learning algorithms can mean the difference between over-learning, a mediocre result and a usable state-of-the-art result that you can confidently shout from the roof tops (you really do feel like doing that sometimes).

In this post you will discover the standard test options you can use in your algorithm evaluation test harness and how to choose the right options next time.

Randomness

The root of the difficulty in choosing the right test options is randomness. Most (almost all) machine learning algorithms use randomness in some way. The randomness may be explicit in the algorithm or may be in the sample of the data selected to train the algorithm.

This does not mean that the algorithms produce random results, it means that they produce results with some noise or variance. We call this type of limited variance, stochastic and the algorithms that exploit it, stochastic algorithms.

Train and Test on Same Data

If you have a dataset, you may want to train the model on the dataset and then report the results of the model on that dataset. That’s how good the model is, right?

The problem with this approach of evaluating algorithms is that you indeed will know the performance of the algorithm on the dataset, but do not have any indication of how the algorithm will perform on data that the model was not trained on (so-called unseen data).

This matters, only if you want to use the model to make predictions on unseen data.

Split Test

A simple way to use one dataset to both train and estimate the performance of the algorithm on unseen data is to split the dataset. You take the dataset, and split it into a training dataset and a test dataset. For example, you randomly select 66% of the instances for training and use the remaining 34% as a test dataset.

The algorithm is run on the training dataset and a model is created and assessed on the test dataset and you get a performance accuracy, lets say 87% classification accuracy.

Spit tests are fast and great when you have a lot of data or when training a model is expensive (it resources or time). A split test on a very very large dataset can produce an accurate estimate of the actual performance of the algorithm.

How good is the algorithm on the data? Can we confidently say it can achieve an accuracy of 87%?

A problem is that if we spit the training dataset again into a different 66%/34% split, we would get a different result from our algorithm. This is called model variance.

Multiple Split Tests

A solution to our problem with the split test getting different results on different splits of the dataset is to reduce the variance of the random process and do it many times. We can collect the results from a fair number of runs (say 10) and take the average.

For example, let’s say we split our dataset 66%/34%, ran our algorithm and got an accuracy and we did this 10 times with 10 different splits. We might have 10 accuracy scores as follows: 87, 87, 88, 89, 88, 86, 88, 87, 88, 87.

The average performance of our model is 87.5, with a standard deviation of about 0.85.

A problem with multiple split tests is that it is possible that some data instance are never included for training or testing, where as others may be selected multiple times. The effect is that this may skew results and may not give an meaningful idea of the accuracy of the algorithm.

Cross Validation

A solution to the problem of ensuring each instance is used for training and testing an equal number of times while reducing the variance of an accuracy score is to use cross validation. Specifically k-fold cross validation, where k is the number of splits to make in the dataset.

For example, let’s choose a value of k=10 (very common). This will split the dataset into 10 parts (10 folds) and the algorithm will be run 10 times. Each time the algorithm is run, it will be trained on 90% of the data and tested on 10%, and each run of the algorithm will change which 10% of the data the algorithm is tested on.

In this example, each data instance will be used as a training instance exactly 9 times and as a test instance 1 time. The accuracy will not be a mean and a standard deviation, but instead will be an exact accuracy score of how many correct predictions were made.

The k-fold cross validation method is the go-to method for evaluating the performance of an algorithm on a dataset. You want to choose k-values that give you a good sized training and test dataset for your algorithm. Not too disproportionate (too large or small for training or test). If you have a lot of data, you may may have to resort to either sampling the data or reverting to a split test.

Cross validation does give an unbiased estimation of the algorithms performance on unseen data, but what if the algorithm itself uses randomness. The algorithm would produce different results for the same training data each time it was trained with a different random number seed (start of the sequence of pseudo-randomness). Cross validation does not account for variance in the algorithm’s predictions.

Another point of concern is that cross validation itself uses randomness to decide how to split the dataset into k folds. Cross validation does not estimate how the algorithm perform with different sets of folds.

This only matters if you want to understand how robust the algorithm is on the dataset.

Multiple Cross Validation

A way to account for the variance in the algorithm itself is to run cross validation multiple times and take the mean and the standard deviation of the algorithm accuracy from each run.

This will will give you an an estimate of the performance of the algorithm on the dataset and an estimation of how robust (the size of the standard deviation) the performance is.

If you have one mean and standard deviation for algorithm A and another mean and standard deviation for algorithm B and they differ (for example, algorithm A has a higher accuracy), how do you know if the difference is meaningful?

This only matters if you want to compare the results between algorithms.

Statistical Significance

A solution to comparing algorithm performance measures when using multiple runs of k-fold cross validation is to use statistical significance tests (like the Student’s t-test).

The results from multiple runs of k-fold cross validation is a list of numbers. We like to summarize these numbers using the mean and standard deviation. You can think of these numbers as a sample from an underlying population. A statistical significance test answers the question: are two samples drawn from the same population? (no difference). If the answer is “yes”, then, even if the mean and standard deviations differ, the difference can be said to be not statistically significant.

We can use statistical significance tests to give meaning to the differences (or lack there of) between algorithm results when using multiple runs (like multiple runs of k-fold cross validation with different random number seeds). This can when we want to make accurate claims about results (algorithm A was better than algorithm B and the difference was statistically significant)

This is not the end of the story, because there are different statistical significance tests (parametric and nonparametric) and parameters to those tests (p-value). I’m going to draw the line here because if you have followed me this far, you now know enough about selecting test options to produce rigorous (publishable!) results.

Summary

In this post you have discovered the difference between the main test options available to you when designing a test harness to evaluate machine learning algorithms.

Specifically, you learned the utility and problems with:

Training and testing on the same dataset

Split tests

Multiple split tests

Cross validation

Multiple cross validation

Statistical significance

When in doubt, use k-fold cross validation (k=10) and use multiple runs of k-fold cross validation with statistical significance tests when you want to meaningfully compare algorithms on your dataset."
42;news.mit.edu;http://news.mit.edu/2020/dreaming-big-small-country-misti-global-startup-labs-uruguay-0224;;Dreaming big in a small country;"When Miguel Brechner started planning a new ambitious plan to foster a new generation of data scientists in Uruguay and Latin America, he immediately thought of MIT. “There is no question that MIT is a world leader in science and technology. In Uruguay we are a small country, but we dream big.” Brechner is president of Plan Ceibal, an internationally awarded public initiative that has as main goals to distribute technology, promote knowledge, and generate social equity by widening access to digital technologies.

In 2019, Uruguayan public institutions like Plan Ceibal, ANII (Agencia Nacional de Investigación e Innovación), and UTEC (Universidad Tecnológica del Uruguay) began collaborating with MIT International Science and Technology Initiatives (MISTI) and the Abdul Latif Jameel World Education Lab (J-WEL). The partnership supports 60 Latin American students that are part of the Program in Data Science, a program which includes online courses from MITx and on-site workshops run by J-WEL and MISTI. Local students include CEOs, entrepreneurs, engineers, economists, medical professionals, and senior administrators.

The MISTI Global Startup Labs (GSL) program, now in its 20th year, has expanded its partnerships to include Uruguayan institutions to promote entrepreneurship and data science across Latin America. GSL is a unique program designed to offer the opportunity to blend digital technologies and entrepreneurship in emerging regions in the world. Since 1998, hundreds of MIT students have traveled to more than 15 countries to be part of the program that has benefited thousands of technology entrepreneurs around the world. GSL instructors are MIT graduate and undergraduate students, selected among many applicants from all over the institute. GSL programs in different countries are uniquely crafted based on the needs of the local partners, and MIT student instructors take the lead teaching app and web development, coding, data science, entrepreneurship, and intrapreneurship.

The new GSL, one of the first to be run over Independent Acitivities Period, took place during January in Montevideo. The Uruguay program focused specifically on machine learning and the business opportunities of the technology. The local student participants had previously taken courses from the MITx MicroMasters in Data Science, and the GSL workshop gave them the opportunity to experience project-based learning in data science. This hands-on experiential immersion in the subject matter is the core methodology of the GSL program.

More than 30 graduate and undergraduate students applied to be part of GSL in Uruguay this year, and 13 were selected to be part of the workshop in Montevideo. Eduardo Rivera, managing director for Uruguay, explained the process: “Recruiting students for GSL is always a challenge. We look for expertise and experience teaching, but also for team players and risk-takers. The team is composed of students from different disciplines and levels of studies, which makes the experience a unique opportunity for our students to learn from their MIT peers in new and challenging contexts.” Rivera adds, “At MIT, we are fortunate to have plenty of talented and passionate students, willing to cross borders and oceans to teach and learn.”

Over the course of a month, the local students were taught how to build prototypes, create business models, and pitch presentations. The class pursued projects ranging from predictive maintenance to autism detection to logistics optimization. The final results were presented in a pitch event hosted in Zonamerica, Uruguay's premier hub for technology and innovation.

""Working with our local students was a truly unique and unforgettable opportunity,"" says electrical engineering and computer science (EECS) senior Ryan Sander. ""I'm certain I learned just as much from the students as they learned from us. What really left an impression on me was observing not only how bright our students are, but also how passionate these people are about solving real-world problems with high impact.""

For MBA student Kenny Li, the opportunity to interact with the local students was broadening. “In today’s world, you need to be able to understand people’s cultures, how do they approach business, how they interact at work …GSL gave me a great learning opportunity to understand the global context of entrepreneurs.”

When not teaching classes, the MIT students were able to visit various places around Montevideo, including the beautiful beaches of Punta del Este, the neighboring city of Buenos Aires, and relaxing getaways to Colonia. After classes, the teaching team was steps away from the beach and could wind each day down with a beautiful sunset, soaking up the warm summer weather in January.

Rivera finds these cultural connections to be one of the major benefits of the program. “At MISTI, we are certain that international teaching activities contribute not only to the academic formation of the students but also give them valuable tools to interact in multicultural environments and confront new challenges in different locations. For future global leaders, this is a unique opportunity. We often hear from our students that MISTI experiences are life-changing, not only in professional life but also in their personal life.”

""The weekends and weekday evenings were a great way for us to bond with each other and our students,"" says Victoria Pisini, a senior in the MIT Sloan School of Management. “We went to beaches together, traveled to different cities, and shared a lot of unforgettable moments.""

The MIT students participating in this year’s GSL were Amauche Emenari (EECS PhD student), Devin Zhang (MBA student), Evan Mu (EECS PhD student), Geeticka Chauhan (EECS PhD student), Hans Nowak (MBA student), Julian Alverio (EECS MEng student), Kenny Li (MBA student), Madhav Kumar (MIT Sloan PhD candidate), Maya Murad (Leaders for Global Operations master's and PhD student), Ryan Sander (EECS student), Taylor Elise Baum (EECS PhD student), Tiffany Fung (MBA student), and Victoria Pisini (MBA student).

GSL programs are planned in multiple countries for summer 2020 and Independent Activities Period 2021, and there are still a small number of available opportunities for instructors this summer."
43;news.mit.edu;http://news.mit.edu/2020/arnold-demain-professor-emeritus-biology-dies-0414;;Professor Emeritus Arnold Demain, a pioneer in the development of antibiotics, dies at 92;"Arnold Lester Demain, professor emeritus of biology, passed away on Apr. 3 at the age of 92 from complications due to Covid-19. He was just shy of celebrating his 93rd birthday.

Demain advanced the field of fermentation biology, and made major contributions to the study of antibiotics like penicillin, cephalosporin, and beta-lactam. Over the course of his 60-year career, he made a name for himself as one of the world’s leading industrial microbiologists, and mentored hundreds of budding scientists around the world.

“Arny was a prolific industrial biologist, as well as a colleague and friend,” says Alan Grossman, head of the Department of Biology and the Praecis Professor of Biology. “His work on antibiotic fermentations spurred a new wave. He was kind and supportive to all, and a dedicated mentor to many students and postdocs.”

Demain was born on April 26, 1927 in Brooklyn, New York, and grew up during the Great Depression. He graduated from high school at the age of 16, and attended Michigan State College (now Michigan State University). At 17, he put his education on pause to join the U.S. Navy and fight in World War II. After the war ended, he returned to Michigan State and resumed his studies, earning his bachelor’s degree in 1949 and his master’s in microbiology in 1950, with a focus on food fermentation — specifically, how pickles spoil. During this time, Demain met his wife Joanna (Kaye) Demain, and they were married on August 2, 1952.

Demain began his PhD in food science at the University of California at Berkeley, but relocated to Davis when that campus opened. Under the guidance of his research advisor and prominent yeast scholar, Herman Jan Phaff, Demain studied the degradation of pectic acid by an extracellular enzyme in the yeast Klyveromyces fragilis, publishing four papers, including one in Nature. Demain and Phaff were also among the first researchers to perform affinity chromatography, which later became a standard biochemical procedure.

After earning his PhD in 1954, Demain was recruited by Merck Sharp & Dohme Research Laboratories, first to research penicillin biosynthesis and later to study cephalosporin C. In 1965, he established Merck’s Fermentation Microbiology Department.

After 16 years at Merck, Demain joined MIT’s former Department of Nutrition and Food Science, and in 1988 he joined the Department of Biology. When he first arrived, no one at MIT was conducting research on antibiotics. He was eager to continue investigating penicillins and cephalosporins, and his hard work culminated in the breakthrough discovery of a key enzyme in cephalosporin biosynthesis: deacetoxycephalosporin C synthase.

Rich Losick PhD ’69 was finishing his graduate work at MIT when Demain arrived on campus. Demain later interacted with Losick’s wife Janice Pero and former postdoc John Perkins because all three shared an interest in vitamin B2 research. “I was drawn to Arny due to his warm and engaging personality and my interest in microbiology,” Losick recalls. “He pioneered industrial production of vitamins, antibiotics, and fine chemicals, and was revered for his many contributions to industrial microbiology. He was a big-hearted human being, an excellent and productive scientist, and a dedicated teacher. He will be greatly missed.”

While at MIT, Demain also helped catalyze the biotech industry by serving as the founding consultant for the biotech company, Cetus Corporation. By the mid-1990s, he’d spearheaded a series of NASA-sponsored experiments to probe the effects of simulated microgravity on secondary metabolism. Toward the end of his 32 years at MIT, he began examining Clostridium tetani and Clostridium difficile bacteria in hopes of devising tetanus and antibiotic-associated diarrhea vaccines. He ultimately authored more than 500 publications and 21 U.S. patents.

“Arny had a keen mind and a gentle disposition that put you at ease,” says Gerald Fink, professor in the Department of Biology and founding member and former director of the Whitehead Institute. When Fink started at MIT, Demain was the first to greet him. “He dropped into my office in Building 56 and he said, ‘You are going to like it here,’” Fink recalls.

“Arny was a wonderful colleague,” adds Robert Sauer, the Salvador E. Luria Professor of Biology. “He was always upbeat and happy to talk about science or anything else on your mind.”

Professor of Biology Anthony Sinskey shared an office with Demain, and remembers him as a pioneer who applied genetics and biochemistry to improve antibiotic production processes. He says Demain was instrumental in forming important interdisciplinary programs at MIT — including using anaerobic microorganisms to convert cellulose to fuels, as well as strategies for cell free synthesis of antibiotics and other projects.

“I learned a tremendous amount from our interactions,” Sinskey says. “He taught industrial microbiology and fermentation technology to hundreds of students both at MIT and from industry.”

While Demain was at MIT, an informal group of students formed called Arny’s Army and Friends. Since his “early” retirement from MIT at age 75, Demain’s students have held the Arny’s Army and Friends Symposia in his honor every three years.

Later, Demain would recall that he “was very lucky ... to have had a fantastic group of bright and hardworking visiting scientists, postdoctoral associates, graduate students, undergraduate students and high school students. ... Success at MIT would not have been possible without them.”

Abraham L. “Linc” Sonenshein first crossed paths with Demain as a graduate student at MIT. From the beginning, Sonenshein could tell Demain was “a very important scientist to interact with,” because of the way he applied his knowledge of microbiology to prevent and treat infections. “I was amazed that he continued to contribute to science publication and training for decades after officially retiring as a full-time faculty member,” Sonenshein says. “The number of scientists he educated and trained is phenomenal.”

In 2000, Demain moved to Madison, New Jersey, and joined the Research Institute for Scientists Emeriti (RISE) at Drew University. He continued to conduct research and mentor students until May 2019, when he fully retired at the age of 92. That same year, Drew University established an endowed scholarship in his name.

Over the course of his career, Demain earned numerous awards, including one from the king of Spain and another from the emperor of Japan. He was also a member of the National Academy of Sciences, president of the Society for Industrial Microbiology, and on the Board of Governors for the American Academy of Microbiology.

He is survived by his wife of 68 years, Joanna (Jody) Demain; his daughter, Pamela Demain; his son, Jeffrey Demain; his daughter-in-law, Lauren Brener; his grandchildren; and his great-grandchildren. A memorial service for family, friends, colleagues, and former students will be held when it is safe to do so. Donations in his memory can be made to the Alzheimer's Foundation of America."
44;news.mit.edu;http://news.mit.edu/2020/age-founders-successful-startups-0320;;A business edge that comes with age;"Two years ago, MIT economist Pierre Azoulay started a lively discussion when a working paper he co-authored, “Age and High-Growth Entrepreneurship,” revealed a surprising fact about startup founders: Among firms in the top 1/10 of the top 1 percent, in terms of growth, the average founder’s age is 45. That’s contrary to the popular image of valuable startups being the sole domain of twentysomething founders, such as Mark Zuckerberg of Facebook.

The paper, written with Benjamin Jones of Northwestern University, J. Daniel Kim of the University of Pennsylvania, and Javier Miranda of the U.S. Bureau of the Census, has now been officially published, in the journal American Economics Review: Insights. MIT News spoke to Azoulay, the International Programs Professor of Management at the MIT Sloan School of Management, about the finding and the discussion it has generated.

Q: What has been the response of people to the study?

A: We’re documenting a fact, and that fact either accords with people’s intuitions, or it doesn’t. Some people are genuinely surprised because they’ve lived in the current zeitgeist, and then once they start thinking about it, they say, “Ah, it makes sense.” And then there are people who say, “Oh, I knew it all along!” But Silicon Valley venture capitalists have studiously avoided engaging with what we’ve done. And I don’t know why.

I think one line of [venture capitalist] skepticism is to say, “Well, you may well be right, but you’re studying the one-in-a-thousand firm, and we’re [investing in] the one-in-a-million firm.” Which is sort of like restating that Apple, Microsoft, and Google were founded by young people.

Yes, we already knew it is possible for very large, successful firms to be founded by very young people. The question is: Is it likely?

Q: As you continue to think about this subject, is it possible to say what accounts for the success of relatively older entrepreneurs? Experience, intellectual capital, greater business connections — what matters?

A: All of those things are not mutually exclusive, and they’re all likely to play a role. They just need to be studied separately, if you will. We have to remain agnostic. But there is one key point in my view. Forget experience: How about just knowledge? I like to say there is no such thing as a 25-year-old biotech entrepreneur. That person just doesn’t exist, because you need a PhD and three postdocs [to gain high-level knowledge]. There are lots of fields where if you want to make a contribution, you have to bring yourself to the frontier of knowledge in a domain, and that takes time. And that’s not going to be the realm of the 22-year-old.

Beyond the big platform IT companies, if you’re thinking about the broader swath of entrepreneurship across a multiplicity of sectors, then you have to acknowledge this point. In some sense, if you recognize the diversity of startups, the [founder’s] age number is going to be higher than if you’re only focused on super-high-value Silicon Valley internet companies. So we need a basic attitude adjustment.

Q. Even as people mythologize the young startup founder, there is also a tech-sector ethos that heralds serial entrepreneurship and tolerates failure, because you’re taking risks and learning by doing. Isn’t that one Silicon Valley notion that might correspond to what you discovered?

A: Yes, one thing that could explain our results is entrepreneurship being an activity you can learn to do better over time. That’s certainly something we’ve heard. We can’t pin it down, but if I had to think of the most likely stories, that’s certainly one. Even within a particular sector that demands a certain amount of intellectual capital, holding that constant, even within biotech or clean energy, you might think there is something about learning by founding, which is going to lead to a correlation between success and a higher age for founders."
45;machinelearningmastery.com;https://machinelearningmastery.com/how-to-predict-whether-eyes-are-open-or-closed-using-brain-waves/;2018-08-26;How to Predict Whether a Persons Eyes are Open or Closed Using Brain Waves;"# remove outliers from the EEG data

from pandas import read_csv

from numpy import mean

from numpy import std

from numpy import delete

from numpy import savetxt

# load the dataset.

data = read_csv ( 'EEG_Eye_State.csv' , header = None )

values = data . values

# step over each EEG column

for i in range ( values . shape [ 1 ] - 1 ) :

# calculate column mean and standard deviation

data_mean , data_std = mean ( values [ : , i ] ) , std ( values [ : , i ] )

# define outlier bounds

cut_off = data_std * 4

lower , upper = data_mean - cut_off , data_mean + cut_off

# remove too small

too_small = [ j for j in range ( values . shape [ 0 ] ) if values [ j , i ] < lower ]

values = delete ( values , too_small , 0 )

print ( '>deleted %d rows' % len ( too_small ) )

# remove too large

too_large = [ j for j in range ( values . shape [ 0 ] ) if values [ j , i ] > upper ]

values = delete ( values , too_large , 0 )

print ( '>deleted %d rows' % len ( too_large ) )

# save the results to a new file"
46;machinelearningmastery.com;https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/;2017-05-16;How to Use the TimeDistributed Layer in Keras;"from numpy import array

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# prepare sequence

length = 5

seq = array ( [ i / float ( length ) for i in range ( length ) ] )

X = seq . reshape ( len ( seq ) , 1 , 1 )

y = seq . reshape ( len ( seq ) , 1 )

# define LSTM configuration

n_neurons = length

n_batch = length

n_epoch = 1000

# create LSTM

model = Sequential ( )

model . add ( LSTM ( n_neurons , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

print ( model . summary ( ) )

# train LSTM

model . fit ( X , y , epochs = n_epoch , batch_size = n_batch , verbose = 2 )

# evaluate

result = model . predict ( X , batch_size = n_batch , verbose = 0 )

for value in result :"
47;news.mit.edu;http://news.mit.edu/2020/posh-chatbots-0417;;Deploying more conversational chatbots;"The comedian Bill Burr has said he refuses to call into automated customer service lines for fear that, years later on his death bed, all he’ll be able to think about are the moments he wasted dealing with chatbots.

Indeed, the frustrating experience of trying to complete even the most straightforward task through an automated customer service line is enough to make anyone question the purpose of life.

Now the startup Posh is trying to make conversations with chatbots more natural and less maddening. It’s accomplishing this with an artificial intelligence-powered system that uses “conversational memory” to help users complete tasks.

“We noticed bots in general would take what the user said at face value, without connecting the dots of what was said before in the conversation,” says Posh co-founder and CEO Karan Kashyap ’17, SM ’17. “If you think about your conversations with humans, especially in places like banks with tellers or in customer service, what you said in the past is very important, so we focused on making bots more humanlike by giving them the ability to remember historical information in a conversation.”

Posh’s chatbots are currently used by over a dozen credit unions across voice- and text-based channels. The well-defined customer base has allowed the company to train its system on only the most relevant data, improving performance.

The founders plan to gradually partner with companies in other sectors to gather industry-specific data and expand the use of their system without compromising performance. Down the line, Kashyap and Posh co-founder and CTO Matt McEachern ’17, SM ’18 plan to provide their chatbots as a platform for developers to build on.

The expansion plans should attract businesses in a variety of sectors: Kashyap says some credit unions have successfully resolved more than 90 percent of customer calls with Posh’s platform. The company’s expansion may also help alleviate the mind-numbing experience of calling into traditional customer service lines.

“When we deploy our telephone product, there’s no notion of ‘Press one or press two,’” Kashyap explains. “There’s no dial tone menu. We just say, ‘Welcome to whatever credit union, how can I help you today?’ In a few words, you let us know. We prompt users to describe their problems via natural speech instead of waiting for menu options to be read out.”

Bootstrapping better bots

Kashyap and McEachern became friends while pursuing their degrees in MIT’s Department of Electrical Engineering and Computer Science. They also worked together in the same research lab at the Computer Science and Artificial Intelligence Laboratory (CSAIL).

But their relationship quickly grew outside of MIT. In 2016, the students began software consulting, in part designing chatbots for companies to handle customer inquiries around medical devices, flight booking, personal fitness, and more. Kashyap says they used their time consulting to learn about and take business risks.

“That was a great learning experience, because we got real-world experience in designing these bots using the tools that were available,” Kashyap says. “We saw the market need for a bot platform and for better bot experiences.”

From the start, the founders executed a lean business strategy that made it clear the engineering undergrads were thinking long term. Upon graduation, the founders used their savings from consulting to fund Posh’s early operations, giving themselves salaries and even hiring some contacts from MIT.

It also helped that they were accepted into the delta v accelerator, run by the Martin Trust Center for MIT Entrepreneurship, which gave them a summer of guidance and free rent. Following delta v, Posh was accepted into the DCU Fintech Innovation Center, connecting it with one of the largest credit unions in the country and netting the company another 12 months of free rent.



With DCU serving as a pilot customer, the founders got a “crash course” in the credit union industry, Kashyap says. From there they began a calculated expansion to ensure they didn’t grow faster than Posh’s revenue allowed, freeing them from having to raise venture capital.

The disciplined growth strategy at times forced Posh to get creative. Last year, as the founders were looking to build out new features and grow their team, they secured about $1.5 million in prepayments from eight credit unions in exchange for discounts on their service along with a peer-driven profit-sharing incentive. In total, the company has raised $2.5 million using that strategy.

Now on more secure financial footing, the founders are poised to accelerate Posh’s growth.

Pushing the boundaries

Even referring to today’s automated messaging platforms as chatbots seems generous. Most of the ones on the market today are only designed to understand what a user is asking for, something known as intent recognition.

The result is that many of the virtual agents in our lives, from the robotic telecom operator to Amazon’s Alexa to the remote control, take directions but struggle to hold a conversation. Posh’s chatbots go beyond intent recognition, using what Kashyap calls context understanding to figure out what users are saying based on the history of the conversation. The founders have a patent pending for the approach.

“[Context understanding] allows us to more intelligently understand user inputs and handle things like changes in topics without having the bots break,” Kashyap says. “One of our biggest pet peeves was, in order to have a successful interaction with a bot, you as a user have to be very unnatural sometimes to convey what you want to convey or the bot won’t understand you.”

Kashyap says context understanding is a lot easier to accomplish when designing bots for specific industries. That’s why Posh’s founders decided to start by focusing on credit unions.

“The platforms on the market today are almost spreading themselves too thin to make a deep impact in a particular vertical,” Kashyap says. “If you have banks and telecos and health care companies all using the same [chatbot] service, it’s as if they’re all sharing the same customer service rep. It’s difficult to have one person trained across all of these domains meaningfully.”

To onboard a new credit union, Posh uses the customer’s conversational data to train its deep learning model.

“The bots continue to train even after they go live and have actual conversations,” Kashyap says. “We’re always improving it; I don’t think we’ll ever deploy a bot and say it’s done.”

Customers can use Posh’s bots for online chats, voice calls, SMS messaging, and through third party channels like Slack, WhatsApp, and Amazon Echo. Posh also offers an analytics platform to help customers analyze what users are calling about.

For now, Kashyap says he’s focused on quadrupling the number of credit unions using Posh over the next year. Then again, the founders’ have never let short term business goals cloud their larger vision for the company.

“Our perspective has always been that [the robot assistant] Jarvis from ‘Iron Man’ and the AI from the movie ‘Her’ are going to be reality sometime soon,” Kashyap says. “Someone has to pioneer the ability for bots to have contextual awareness and memory persistence. I think there’s a lot more that needs to go into bots overall, but we felt by pushing the boundaries a little bit, we’d succeed where other bots would fail, and ultimately people would like to use our bots more than others.”"
48;machinelearningmastery.com;https://machinelearningmastery.com/benefits-of-implementing-machine-learning-algorithms-from-scratch/;2014-09-09;Benefits of Implementing Machine Learning Algorithms From Scratch;"Tweet Share Share

Last Updated on August 12, 2019

Machine Learning can be difficult to understand when getting started. There are a lot of algorithms and processes that are prescribed and used, many with difficult to penetrate explanations for how and why the work.

It can feel overwhelming.

An approach that you can use to get handle on machine learning algorithms and practices is to implement them from scratch. This will give you a deep understanding of how the algorithm works and all of the micro decision points within the method that can be parameterized or modified to tune it to a specific problem.

In this post you will discover the benefits and limitations of implementing machine learning algorithms from scratch and how you can accelerate this process by completing algorithm tutorials.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Implement Machine Learning Algorithms

Implementing machine learning algorithms from scratch can give you a deep understanding of the algorithm and a sense of confidence and ownership that are difficult to achieve by just applying the method.

Benefits

The benefits of implementing algorithms from scratch are:

Understanding : You will gain a deep appreciate for how the algorithm works. You understand how the mathematical description of the method relates to vectors and matrices of numbers that you code operates on. You will also know how all of the parameters are used, their effects and even have insights into how it could be further parameterized to specialize it for a problem.

: You will gain a deep appreciate for how the algorithm works. You understand how the mathematical description of the method relates to vectors and matrices of numbers that you code operates on. You will also know how all of the parameters are used, their effects and even have insights into how it could be further parameterized to specialize it for a problem. Starting Point : Your implementation will provide the basis for more advanced extensions and even an operational system that uses the algorithm. Your deep knowledge of the algorithm and you implementation can give you advantages of knowing the space and time complexity of your own code over using an opaque off-the-shelf library.

: Your implementation will provide the basis for more advanced extensions and even an operational system that uses the algorithm. Your deep knowledge of the algorithm and you implementation can give you advantages of knowing the space and time complexity of your own code over using an opaque off-the-shelf library. Ownership: The implementation is your own giving you confidence with the method and ownership over how it is realized as a system. It is no longer just a machine learning algorithm, but a method that is now in your toolbox.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Extensions

Once you have implemented an algorithm you can explore making improvements to the implementation. Some examples of improvements you could explore include:

Experimentation : You can expose many of the micro-decisions you made in the algorithms implementation as parameters and perform studies on variations of those parameters. This can lead to new insights and disambiguation of algorithm implementations that you can share and promote.

: You can expose many of the micro-decisions you made in the algorithms implementation as parameters and perform studies on variations of those parameters. This can lead to new insights and disambiguation of algorithm implementations that you can share and promote. Optimization : You can explore opportunities to make the implementation more efficient by using tools, libraries, different languages, different data structures, patterns and internal algorithms. Knowledge you have of algorithms and data structures for classical computer science can be very beneficial in this type of work.

: You can explore opportunities to make the implementation more efficient by using tools, libraries, different languages, different data structures, patterns and internal algorithms. Knowledge you have of algorithms and data structures for classical computer science can be very beneficial in this type of work. Specialization : You may explore ways of making the algorithm more specific to a problem. This can be required when creating production systems and is a valuable skill. Making an algorithm more problem specific can also lead to increases in efficiency (such as running time) and efficacy (such as accuracy or other performance measures).

: You may explore ways of making the algorithm more specific to a problem. This can be required when creating production systems and is a valuable skill. Making an algorithm more problem specific can also lead to increases in efficiency (such as running time) and efficacy (such as accuracy or other performance measures). Generalization: Opportunities can be created by making a specific algorithm more general. Programmers (like mathematicians) are uniquely skilled in abstraction and you may be able to see how the algorithm could be applied to more general cases of a class of problem or other problems entirely.

Limitations of Implementing Algorithms

Implementing algorithms from scratch is an approach we have discussed before. It is one of the project types in my Small Projects Methodology. In this project type, I suggest that you perform your own literature survey and investigate how the algorithm works first, before implementing it.

This further lead to the algorithm description template, that provided you a tool on how to describe a machine learning algorithm effectively so that you deeply understand it.

The problem with all of this is that it is very time consuming. Researching an algorithm involves finding, reading and summarizing a large number of books, sample code and research papers and can take a good academic researcher many days to weeks of time to complete.

If you consider that you may want to implement a dozen machine learning algorithms, you could easily be required to invest more than half a year of your time.

Additionally, your own implementations of the code may have bugs that may be difficult for you to find (these algorithms have a way of working inspire of bugs, degrading performance). You may also get caught up with non-intuitive leaps in the mathematics that must be understood before you can implement the method in code.

Short-Cut The Process With Tutorials

You can short-cut this process by following along with and completing tutorials.

Machine learning algorithm tutorials explain the method and show you how to implement an algorithm step-by-step from scratch such that by the end, you have a working implementation. You get all of the benefits of an implementation of an algorithm from scratch, without having to research and decipher textbooks and academic papers.

A good tutorial has a number of principles:

Step-by-Step : The guide is discrete leading the reader one step at a time through the material, building on previous steps as it progresses.

: The guide is discrete leading the reader one step at a time through the material, building on previous steps as it progresses. Modular : The implementation is broken down into modular parts that are shown and demonstrated independently before being drawn together into a final working demonstration of the whole algorithm.

: The implementation is broken down into modular parts that are shown and demonstrated independently before being drawn together into a final working demonstration of the whole algorithm. Slow : The implementation is slow, introducing one new thing in each progressive step so that the whole can be understood as the sum of the parts.

: The implementation is slow, introducing one new thing in each progressive step so that the whole can be understood as the sum of the parts. Code : A complete working example for each step and for the whole tutorial. It’s obvious, but easy to forget or to mess up by not testing code. All code must be explained and it must execute.

: A complete working example for each step and for the whole tutorial. It’s obvious, but easy to forget or to mess up by not testing code. All code must be explained and it must execute. References : Additional resources and reading must be provided for those readers that want to dive deeper into the material.

: Additional resources and reading must be provided for those readers that want to dive deeper into the material. Extensions: After completing the tutorial, there must be suggestion of additional exercises the reader can take on if they are interested in taking the implementation further. A suggestion must be made as to how additional advanced elements can be integrated or how problems with the provided implementation can be addressed.

This is a popular way to learn algorithms and data structures in programming and is an approach that is easily overlooked for its simplicity. As such, there are few good machine learning algorithms tutorials available.

A good resource for python programmers is the book: Programming Collective Intelligence: Building Smart Web 2.0 Applications. This book takes you step-by-step through the creation of a number of machine learning systems, from scratch.

Summary

In this post you discovered the benefits of implementing machine learning algorithms from scratch and the confidence and sense of ownership it can provide over complex algorithms.

You discovered the limitations of the approach and how much time may be required in researching, distilling and summarizing algorithms from textbooks and research papers before they can be implemented.

Finally, you discovered that a short-cut is to follow machine learning algorithm tutorials that show you how to implement algorithms from scratch and give you the benefits and spare you from having to do the research.

I am currently preparing a collection of machine learning tutorials on how to implement algorithms from scratch. If this interests you, leave a comment and let me know.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
49;machinelearningmastery.com;https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/;2019-02-12;How to Control Neural Network Model Capacity With Nodes and Layers;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52

# study of mlp learning curves given different number of layers for multi-class classification from sklearn . datasets import make_blobs from keras . models import Sequential from keras . layers import Dense from keras . optimizers import SGD from keras . utils import to_categorical from matplotlib import pyplot # prepare multi-class classification dataset def create_dataset ( ) : # generate 2d classification dataset X , y = make_blobs ( n_samples = 1000 , centers = 20 , n_features = 100 , cluster_std = 2 , random_state = 2 ) # one hot encode output variable y = to_categorical ( y ) # split into train and test n_train = 500 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] return trainX , trainy , testX , testy # fit model with given number of layers, returns test set accuracy def evaluate_model ( n_layers , trainX , trainy , testX , testy ) : # configure the model based on the data n_input , n_classes = trainX . shape [ 1 ] , testy . shape [ 1 ] # define model model = Sequential ( ) model . add ( Dense ( 10 , input_dim = n_input , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) for _ in range ( 1 , n_layers ) : model . add ( Dense ( 10 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( n_classes , activation = 'softmax' ) ) # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] ) # fit model history = model . fit ( trainX , trainy , epochs = 100 , verbose = 0 ) # evaluate model on test set _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) return history , test_acc # get dataset trainX , trainy , testX , testy = create_dataset ( ) # evaluate model and plot learning curve of model with given number of layers all_history = list ( ) num_layers = [ 1 , 2 , 3 , 4 , 5 ] for n_layers in num_layers : # evaluate model with a given number of layers history , result = evaluate_model ( n_layers , trainX , trainy , testX , testy ) print ( 'layers=%d: %.3f' % ( n_layers , result ) ) # plot learning curve pyplot . plot ( history . history [ 'loss' ] , label = str ( n_layers ) ) pyplot . legend ( ) pyplot . show ( )"
50;machinelearningmastery.com;http://machinelearningmastery.com/normalize-standardize-time-series-data-python/;2016-12-11;How to Normalize and Standardize Time Series Data in Python;"# Normalize time series data

from pandas import read_csv

from sklearn . preprocessing import MinMaxScaler

# load the dataset and print the first 5 rows

series = read_csv ( 'daily-minimum-temperatures-in-me.csv' , header = 0 , index_col = 0 )

print ( series . head ( ) )

# prepare data for normalization

values = series . values

values = values . reshape ( ( len ( values ) , 1 ) )

# train the normalization

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

scaler = scaler . fit ( values )

print ( 'Min: %f, Max: %f' % ( scaler . data_min_ , scaler . data_max_ ) )

# normalize the dataset and print the first 5 rows

normalized = scaler . transform ( values )

for i in range ( 5 ) :

print ( normalized [ i ] )

# inverse transform and print the first 5 rows

inversed = scaler . inverse_transform ( normalized )

for i in range ( 5 ) :"
51;machinelearningmastery.com;https://machinelearningmastery.com/promise-recurrent-neural-networks-time-series-forecasting/;2017-05-21;The Promise of Recurrent Neural Networks for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Recurrent neural networks are a type of neural network that add the explicit handling of order in input observations.

This capability suggests that the promise of recurrent neural networks is to learn the temporal context of input sequences in order to make better predictions. That is, that the suite of lagged observations required to make a prediction no longer must be diagnosed and specified as in traditional time series forecasting, or even forecasting with classical neural networks. Instead, the temporal dependence can be learned, and perhaps changes to this dependence can also be learned.

In this post, you will discover the promised capability of recurrent neural networks for time series forecasting. After reading this post, you will know:

The focus and implicit, if not explicit, limitations on traditional time series forecasting methods.

The capabilities provided in using traditional feed-forward neural networks for time series forecasting.

The additional promise that recurrent neural networks make on top of traditional neural nets and hints of what this may mean in practice.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Time Series Forecasting

Time series forecasting is difficult.

Unlike the simpler problems of classification and regression, time series problems add the complexity of order or temporal dependence between observations.

This can be difficult as the specialized handling of the data is required when fitting and evaluating models. It also aids in modeling, providing additional structure like trends and seasonality that can be leveraged to improve model skill.

Traditionally, time series forecasting has been dominated by linear methods like ARIMA because they are well understood and effective on many problems. But these traditional methods also suffer from some limitations, such as:

Focus on complete data : missing or corrupt data is generally unsupported.

: missing or corrupt data is generally unsupported. Focus on linear relationships : assuming a linear relationship excludes more complex joint distributions.

: assuming a linear relationship excludes more complex joint distributions. Focus on fixed temporal dependence : the relationship between observations at different times, and in turn the number of lag observations provided as input, must be diagnosed and specified.

: the relationship between observations at different times, and in turn the number of lag observations provided as input, must be diagnosed and specified. Focus on univariate data : many real-world problems have multiple input variables.

: many real-world problems have multiple input variables. Focus on one-step forecasts: many real-world problems require forecasts with a long time horizon.

Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field.

— John Gamboa, Deep Learning for Time-Series Analysis, 2017

Note that some specialized techniques have been developed to address some of these limitations.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Neural Networks for Time Series

Neural networks approximate a mapping function from input variables to output variables.

This general capability is valuable for time series for a number of reasons.

Robust to Noise . Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values.

. Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values. Nonlinear. Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships.

… one important contribution of neural networks – namely their elegant ability to approximate arbitrary non-linear functions. This property is of high value in time series processing and promises more powerful applications, especially in the subfeld of forecasting …

— Georg Dorffner, Neural Networks for Time Series Processing, 1996.

More specifically, neural networks can be configured to support an arbitrary defined but fixed number of inputs and outputs in the mapping function. This means that:

Multivariate Inputs . An arbitrary number of input features can be specified, providing direct support for multivariate forecasting.

. An arbitrary number of input features can be specified, providing direct support for multivariate forecasting. Multi-Step Forecasts. An arbitrary number of output values can be specified, providing direct support for multi-step and even multivariate forecasting.

For these capabilities alone, feed-forward neural networks are widely used for time series forecasting.

Implicit in the usage of neural networks is the requirement that there is indeed a meaningful mapping from inputs to outputs to learn. Modeling a mapping of a random walk will perform no better than a persistence model (e.g. using the last seen observation as the forecast).

This expectation of a learnable mapping function also makes one of the limitations clear: the mapping function is fixed or static.

Fixed inputs . The number of lag input variables is fixed, in the same way as traditional time series forecasting methods.

. The number of lag input variables is fixed, in the same way as traditional time series forecasting methods. Fixed outputs. The number of output variables is also fixed; although a more subtle issue, it means that for each input pattern, one output must be produced.

Sequences pose a challenge for [deep neural networks] because they require that the dimensionality of the inputs and outputs is known and fixed.

— Ilya Sutskever, Oriol Vinyals, Quoc V. Le, Sequence to Sequence Learning with Neural Networks, 2014

Feed-forward neural networks do offer great capability but still suffer from this key limitation of having to specify the temporal dependence upfront in the design of the model.

This dependence is almost always unknown and must be discovered and teased out from detailed analysis in a fixed form.

Recurrent Neural Networks for Time Series

Recurrent neural networks like the Long Short-Term Memory network add the explicit handling of order between observations when learning a mapping function from inputs to outputs.

The addition of sequence is a new dimension to the function being approximated. Instead of mapping inputs to outputs alone, the network is capable of learning a mapping function for the inputs over time to an output.

This capability unlocks time series for neural networks.

Long Short-Term Memory (LSTM) is able to solve many time series tasks unsolvable by feed-forward networks using fixed size time windows.

— Felix A. Gers, Douglas Eck, Jürgen Schmidhuber, Applying LSTM to Time Series Predictable through Time-Window Approaches, 2001

In addition to the general benefits of using neural networks for time series forecasting, recurrent neural networks can also learn the temporal dependence from the data.

Learned Temporal Dependence. The context of observations over time is learned.

That is, in the simplest case, the network is shown one observation at a time from a sequence and can learn what observations it has seen previously are relevant and how they are relevant to forecasting.

Because of this ability to learn long term correlations in a sequence, LSTM networks obviate the need for a pre-specified time window and are capable of accurately modelling complex multivariate sequences.

— Pankaj Malhotra, et al., Long Short Term Memory Networks for Anomaly Detection in Time Series, 2015

The promise of recurrent neural networks is that the temporal dependence in the input data can be learned. That a fixed set of lagged observations does not need to be specified.

Implicit within this promise is that a temporal dependence that varies with circumstance can also be learned.

But, recurrent neural networks may be capable of more.

It is good practice to manually identify and remove such systematic structures from time series data to make the problem easier to model (e.g. make the series stationary), and this may still be a best practice when using recurrent neural networks. But, the general capability of these networks suggests that this may not be a requirement for a skillful model.

Technically, the available context may allow recurrent neural networks to learn:

Trend . An increasing or decreasing level to a time series and even variation in these changes.

. An increasing or decreasing level to a time series and even variation in these changes. Seasonality. Consistently repeating patterns over time.

What do you think the promise is for LSTMs on time series forecasting problems?

Summary

In this post, you discovered the promise of recurrent neural networks for time series forecasting.

Specifically, you learned:

Traditional time series forecasting methods focus on univariate data with linear relationships and fixed and manually-diagnosed temporal dependence.

Neural networks add the capability to learn possibly noisy and nonlinear relationships with arbitrarily defined but fixed numbers of inputs and outputs supporting multivariate and multi-step forecasting.

Recurrent neural networks add the explicit handling of ordered observations and the promise of learning temporal dependence from context.

Do you disagree with my thoughts on the promise of LSTMs for time series forecasting?

Leave a comment below and join the discussion.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
52;machinelearningmastery.com;https://machinelearningmastery.com/handle-missing-timesteps-sequence-prediction-problems-python/;2017-06-20;How to Handle Missing Timesteps in Sequence Prediction Problems with Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47

from random import random from numpy import array from pandas import concat from pandas import DataFrame from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import Masking # generate a sequence of random values def generate_sequence ( n_timesteps ) : return [ random ( ) for _ in range ( n_timesteps ) ] # generate data for the lstm def generate_data ( n_timesteps ) : # generate sequence sequence = generate_sequence ( n_timesteps ) sequence = array ( sequence ) # create lag df = DataFrame ( sequence ) df = concat ( [ df . shift ( 1 ) , df ] , axis = 1 ) # replace missing values with -1 df . fillna ( - 1 , inplace = True ) values = df . values # specify input and output data X , y = values , values [ : , 1 ] # reshape X = X . reshape ( len ( X ) , 2 , 1 ) y = y . reshape ( len ( y ) , 1 ) return X , y n_timesteps = 10 # define model model = Sequential ( ) model . add ( Masking ( mask_value = - 1 , input_shape = ( 2 , 1 ) ) ) model . add ( LSTM ( 5 ) ) model . add ( Dense ( 1 ) ) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) # fit model for i in range ( 500 ) : X , y = generate_data ( n_timesteps ) model . fit ( X , y , epochs = 1 , batch_size = 1 , verbose = 2 ) # evaluate model on new data X , y = generate_data ( n_timesteps ) yhat = model . predict ( X ) for i in range ( len ( X ) ) : print ( 'Expected' , y [ i , 0 ] , 'Predicted' , yhat [ i , 0 ] )"
53;machinelearningmastery.com;http://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/;2013-12-26;How to Evaluate Machine Learning Algorithms;"Tweet Share Share

Last Updated on March 22, 2020

Once you have defined your problem and prepared your data you need to apply machine learning algorithms to the data in order to solve your problem.

You can spend a lot of time choosing, running and tuning algorithms. You want to make sure you are using your time effectively to get closer to your goal.

In this post you will step through a process to rapidly test algorithms and discover whether or not there is structure in your problem for the algorithms to learn and which algorithms are effective.

Test Harness

You need to define a test harness. The test harness is the data you will train and test an algorithm against and the performance measure you will use to assess its performance. It is important to define your test harness well so that you can focus on evaluating different algorithms and thinking deeply about the problem.

The goal of the test harness is to be able to quickly and consistently test algorithms against a fair representation of the problem being solved. The outcome of testing multiple algorithms against the harness will be an estimation of how a variety of algorithms perform on the problem against a chosen performance measure. You will know which algorithms might be worth tuning on the problem and which should not be considered further.

The results will also give you an indication of how learnable the problem is. If a variety of different learning algorithms universally perform poorly on the problem, it may be an indication of a lack of structure available to algorithms to learn. This may be because there actually is a lack of learnable structure in the selected data or it may be an opportunity to try different transforms to expose the structure to the learning algorithms.

Performance Measure

The performance measure is the way you want to evaluate a solution to the problem. It is the measurement you will make of the predictions made by a trained model on the test dataset.

Performance measures are typically specialized to the class of problem you are working with, for example classification, regression, and clustering. Many standard performance measures will give you a score that is meaningful to your problem domain. For example, classification accuracy for classification (total correct correction divided by the total predictions made multiple by 100 to turn it into a percentage).

You may also want a more detailed breakdown of performance, for example, you may want to know about the false positives on a spam classification problem because good email will be marked as spam and cannot be read.

There are many standard performance measures to choose from. You rarely have to devise a new performance measure yourself as you can generally find or adapt one that best captures the requirements of the problem being solved. Look to similar problems you uncovered and at the performance measures used to see if any can be adopted.

Test and Train Datasets

From the transformed data, you will need to select a test set and a training set. An algorithm will be trained on the training dataset and will be evaluated against the test set. This may be as simple as selecting a random split of data (66% for training, 34% for testing) or may involve more complicated sampling methods.

A trained model is not exposed to the test dataset during training and any predictions made on that dataset are designed to be indicative of the performance of the model in general. As such you want to make sure the selection of your datasets are representative of the problem you are solving.

Cross Validation

A more sophisticated approach than using a test and train dataset is to use the entire transformed dataset to train and test a given algorithm. A method you could use in your test harness that does this is called cross validation.

It first involves separating the dataset into a number of equally sized groups of instances (called folds). The model is then trained on all folds exception one that was left out and the prepared model is tested on that left out fold. The process is repeated so that each fold get’s an opportunity at being left out and acting as the test dataset. Finally, the performance measures are averaged across all folds to estimate the capability of the algorithm on the problem.

For example, a 3-fold cross validation would involve training and testing a model 3 times:

#1: Train on folds 1+2, test on fold 3

#2: Train on folds 1+3, test on fold 2

#3: Train on folds 2+3, test on fold 1

The number of folds can vary based on the size of your dataset, but common numbers are 3, 5, 7 and 10 folds. The goal is to have a good balance between the size and representation of data in your train and test sets.

When you’re just getting started, stick with a simple split of train and test data (such as 66%/34%) and move onto cross validation once you have more confidence.

Testing Algorithms

When starting with a problem and having defined a test harness you are happy with, it is time to spot check a variety of machine learning algorithms. Spot checking is useful because it allows you to very quickly see if there is any learnable structures in the data and estimate which algorithms may be effective on the problem.

Spot checking also helps you work out any issues in your test harness and make sure the chosen performance measure is appropriate.

The best first algorithm to spot check is a random. Plug in a random number generator to generate predictions in the appropriate range. This should be the worst “algorithm result” you achieve and will be the measure by which all improvements can be assessed.

Select 5-10 standard algorithms that are appropriate for your problem and run them through your test harness. By standard algorithms, I mean popular methods no special configurations. Appropriate for your problem means that the algorithms can handle regression if you have a regression problem.

Choose methods from the groupings of algorithms we have already reviewed. I like to include a diverse mix and have 10-20 different algorithms drawn from a diverse range of algorithm types. Depending on the library I am using, I may spot check up to a 50+ popular methods to flush out promising methods quickly.

If you want to run a lot of methods, you may have to revisit data preparation and reduce the size of your selected dataset. This may reduce your confidence in the results, so test with various data set sizes. You may like to use a smaller size dataset for algorithm spot checking and a fuller dataset for algorithm tuning.

Summary

In this post you learned about the importance of setting up a trust worthy test harness that involves the selection of test and training datasets and a performance measure meaningful to your problem.

You also learned about the strategy of spot checking a diverse range of machine learning algorithms on your problem using your test harness. You discovered that this strategy can quickly highlight whether there is learnable structure in your dataset (and if not you can revisit data preparation) and which algorithms perform generally well on the problem (that may be candidates for further investigation and tuning).

Resources

If you are looking to dive deeper into this topic, you can learn more from the resources below."
54;machinelearningmastery.com;https://machinelearningmastery.com/memory-in-a-long-short-term-memory-network/;2017-05-11;Demonstration of Memory with a Long Short-Term Memory Network in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72

from pandas import DataFrame from keras . models import Sequential from keras . layers import Dense from keras . layers import LSTM # binary encode an input pattern, return a list of binary vectors def encode ( pattern , n_unique ) : encoded = list ( ) for value in pattern : row = [ 0.0 for x in range ( n_unique ) ] row [ value ] = 1.0 encoded . append ( row ) return encoded # create input/output pairs of encoded vectors, returns X, y def to_xy_pairs ( encoded ) : X , y = list ( ) , list ( ) for i in range ( 1 , len ( encoded ) ) : X . append ( encoded [ i - 1 ] ) y . append ( encoded [ i ] ) return X , y # convert sequence to x/y pairs ready for use with an LSTM def to_lstm_dataset ( sequence , n_unique ) : # one hot encode encoded = encode ( sequence , n_unique ) # convert to in/out patterns X , y = to_xy_pairs ( encoded ) # convert to LSTM friendly format dfX , dfy = DataFrame ( X ) , DataFrame ( y ) lstmX = dfX . values lstmX = lstmX . reshape ( lstmX . shape [ 0 ] , 1 , lstmX . shape [ 1 ] ) lstmY = dfy . values return lstmX , lstmY # define sequences seq1 = [ 3 , 0 , 1 , 2 , 3 ] seq2 = [ 4 , 0 , 1 , 2 , 4 ] # convert sequences into required data format n_unique = len ( set ( seq1 + seq2 ) ) seq1X , seq1Y = to_lstm_dataset ( seq1 , n_unique ) seq2X , seq2Y = to_lstm_dataset ( seq2 , n_unique ) # define LSTM configuration n_neurons = 20 n_batch = 1 n_epoch = 250 n_features = n_unique # create LSTM model = Sequential ( ) model . add ( LSTM ( n_neurons , batch_input_shape = ( n_batch , 1 , n_features ) , stateful = True ) ) model . add ( Dense ( n_unique , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' ) # train LSTM for i in range ( n_epoch ) : model . fit ( seq1X , seq1Y , epochs = 1 , batch_size = n_batch , verbose = 1 , shuffle = False ) model . reset_states ( ) model . fit ( seq2X , seq2Y , epochs = 1 , batch_size = n_batch , verbose = 0 , shuffle = False ) model . reset_states ( ) # test LSTM on sequence 1 print ( 'Sequence 1' ) result = model . predict_classes ( seq1X , batch_size = n_batch , verbose = 0 ) model . reset_states ( ) for i in range ( len ( result ) ) : print ( 'X=%.1f y=%.1f, yhat=%.1f' % ( seq1 [ i ] , seq1 [ i + 1 ] , result [ i ] ) ) # test LSTM on sequence 2 print ( 'Sequence 2' ) result = model . predict_classes ( seq2X , batch_size = n_batch , verbose = 0 ) model . reset_states ( ) for i in range ( len ( result ) ) : print ( 'X=%.1f y=%.1f, yhat=%.1f' % ( seq2 [ i ] , seq2 [ i + 1 ] , result [ i ] ) )"
55;machinelearningmastery.com;https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/;2018-12-04;How to Reduce Overfitting With Dropout Regularization in Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29

# mlp with dropout on the two circles dataset from sklearn . datasets import make_circles from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from matplotlib import pyplot # generate 2d classification dataset X , y = make_circles ( n_samples = 100 , noise = 0.1 , random_state = 1 ) # split into train and test n_train = 30 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] # define model model = Sequential ( ) model . add ( Dense ( 500 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dropout ( 0.4 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 4000 , verbose = 0 ) # evaluate the model _ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 ) _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) ) # plot history pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' ) pyplot . legend ( ) pyplot . show ( )"
56;machinelearningmastery.com;http://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/;2016-07-07;Crash Course in Recurrent Neural Networks for Deep Learning;"Tweet Share Share

Last Updated on August 14, 2019

There is another type of neural network that is dominating difficult machine learning problems that involve sequences of inputs called recurrent neural networks.

Recurrent neural networks have connections that have loops, adding feedback and memory to the networks over time. This memory allows this type of network to learn and generalize across sequences of inputs rather than individual patterns.

A powerful type of Recurrent Neural Network called the Long Short-Term Memory Network has been shown to be particularly effective when stacked into a deep configuration, achieving state-of-the-art results on a diverse array of problems from language translation to automatic captioning of images and videos.

In this post you will get a crash course in recurrent neural networks for deep learning, acquiring just enough understanding to start using LSTM networks in Python with Keras.

After reading this post, you will know:

The limitations of Multilayer Perceptrons that are addressed by recurrent neural networks.

The problems that must be addressed to make Recurrent Neural networks useful.

The details of the Long Short-Term Memory networks used in applied deep learning.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Support For Sequences in Neural Networks

There are some problem types that are best framed involving either a sequence as an input or an output.

For example, consider a univariate time series problem, like the price of a stock over time. This dataset can be framed as a prediction problem for a classical feedforward multilayer Perceptron network by defining a windows size (e.g. 5) and training the network to learn to make short term predictions from the fixed sized window of inputs.

This would work, but is very limited. The window of inputs adds memory to the problem, but is limited to just a fixed number of points and must be chosen with sufficient knowledge of the problem. A naive window would not capture the broader trends over minutes, hours and days that might be relevant to making a prediction. From one prediction to the next, the network only knows about the specific inputs it is provided.

Univariate time series prediction is important, but there are even more interesting problems that involve sequences.

Consider the following taxonomy of sequence problems that require a mapping of an input to an output (taken from Andrej Karpathy).

One-to-Many : sequence output, for image captioning.

: sequence output, for image captioning. Many-to-One : sequence input, for sentiment classification.

: sequence input, for sentiment classification. Many-to-Many : sequence in and out, for machine translation.

: sequence in and out, for machine translation. Synched Many-to-Many: synced sequences in and out, for video classification.

We can also see that a one-to-one example of input to output would be an example of a classical feedforward neural network for a prediction task like image classification.

Support for sequences in neural networks is an important class of problem and one where deep learning has recently shown impressive results State-of-the art results have been using a type of network specifically designed for sequence problems called recurrent neural networks.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Recurrent Neural Networks

Recurrent Neural Networks or RNNs are a special type of neural network designed for sequence problems.

Given a standard feed-forward multilayer Perceptron network, a recurrent neural network can be thought of as the addition of loops to the architecture. For example, in a given layer, each neuron may pass its signal latterly (sideways) in addition to forward to the next layer. The output of the network may feedback as an input to the network with the next input vector. And so on.

The recurrent connections add state or memory to the network and allow it to learn broader abstractions from the input sequences.

The field of recurrent neural networks is well established with popular methods. For the techniques to be effective on real problems, two major issues needed to be resolved for the network to be useful.

How to train the network with Backpropagation. How to stop gradients vanishing or exploding during training.

1. How to Train Recurrent Neural Networks

The staple technique for training feedforward neural networks is to back propagate error and update the network weights.

Backpropagation breaks down in a recurrent neural network, because of the recurrent or loop connections.

This was addressed with a modification of the Backpropagation technique called Backpropagation Through Time or BPTT.

Instead of performing backpropagation on the recurrent network as stated, the structure of the network is unrolled, where copies of the neurons that have recurrent connections are created. For example a single neuron with a connection to itself (A->A) could be represented as two neurons with the same weight values (A->B).

This allows the cyclic graph of a recurrent neural network to be turned into an acyclic graph like a classic feed-forward neural network, and Backpropagation can be applied.

2. How to Have Stable Gradients During Training

When Backpropagation is used in very deep neural networks and in unrolled recurrent neural networks, the gradients that are calculated in order to update the weights can become unstable.

They can become very large numbers called exploding gradients or very small numbers called the vanishing gradient problem. These large numbers in turn are used to update the weights in the network, making training unstable and the network unreliable.

This problem is alleviated in deep multilayer Perceptron networks through the use of the Rectifier transfer function, and even more exotic but now less popular approaches of using unsupervised pre-training of layers.

In recurrent neural network architectures, this problem has been alleviated using a new type of architecture called the Long Short-Term Memory Networks that allows deep recurrent networks to be trained.

Long Short-Term Memory Networks

The Long Short-Term Memory or LSTM network is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.

As such it can be used to create large (stacked) recurrent networks, that in turn can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.

Instead of neurons, LSTM networks have memory blocks that are connected into layers.

A block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A unit operates upon an input sequence and each gate within a unit uses the sigmoid activation function to control whether they are triggered or not, making the change of state and addition of information flowing through the unit conditional.

There are three types of gates within a memory unit:

Forget Gate : conditionally decides what information to discard from the unit.

: conditionally decides what information to discard from the unit. Input Gate : conditionally decides which values from the input to update the memory state.

: conditionally decides which values from the input to update the memory state. Output Gate: conditionally decides what to output based on input and the memory of the unit.

Each unit is like a mini state machine where the gates of the units have weights that are learned during the training procedure.

You can see how you may achieve a sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers.

Resources

We have covered a lot of ground in this post. Below are some resources that you can use to go deeper into the topic of recurrent neural networks for deep learning.

Resources to learn more about Recurrent Neural Networks and LSTMs.

Popular tutorials for implementing LSTMs.

Primary sources on LSTMs.

People to follow doing great work with LSTMs.

Summary

In this post you discovered sequence problems and recurrent neural networks that can be used to address them.

Specifically, you learned:

The limitations of classical feedforward neural networks and how recurrent neural networks can overcome these problems.

The practical problems in training recurrent neural networks and how they are overcome.

The Long Short-Term Memory network used to create deep recurrent neural networks.

Do you have any questions about deep recurrent neural networks, LSTMs or about this post? Ask your question in the comments and I will do my best to answer.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside"
57;machinelearningmastery.com;http://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/;2016-09-06;How to Tune the Number and Size of Decision Trees with XGBoost in Python;"# XGBoost on Otto dataset, Tune n_estimators

from pandas import read_csv

from xgboost import XGBClassifier

from sklearn . model_selection import GridSearchCV

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import LabelEncoder

import matplotlib

matplotlib . use ( 'Agg' )

from matplotlib import pyplot

# load data

data = read_csv ( 'train.csv' )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 94 ]

y = dataset [ : , 94 ]

# encode string class values as integers

label_encoded_y = LabelEncoder ( ) . fit_transform ( y )

# grid search

model = XGBClassifier ( )

n_estimators = range ( 50 , 400 , 50 )

param_grid = dict ( n_estimators = n_estimators )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 )

grid_search = GridSearchCV ( model , param_grid , scoring = ""neg_log_loss"" , n_jobs = - 1 , cv = kfold )

grid_result = grid_search . fit ( X , label_encoded_y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :

print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )

# plot

pyplot . errorbar ( n_estimators , means , yerr = stds )

pyplot . title ( ""XGBoost n_estimators vs Log Loss"" )

pyplot . xlabel ( 'n_estimators' )

pyplot . ylabel ( 'Log Loss' )"
58;news.mit.edu;http://news.mit.edu/2020/explained-cement-vs-concrete-understanding-differences-and-sustainability-opportunities-0403;;Explained: Cement vs. concrete — their differences, and opportunities for sustainability;"There’s a lot the average person doesn’t know about concrete. For example, it’s porous; it’s the world’s most-used material after water; and, perhaps most fundamentally, it’s not cement.

Though many use ""cement"" and ""concrete"" interchangeably, they actually refer to two different — but related — materials: Concrete is a composite made from several materials, one of which is cement.

Cement production begins with limestone, a sedimentary rock. Once quarried, it is mixed with a silica source, such as industrial byproducts slag or fly ash, and gets fired in a kiln at 2,700 degrees Fahrenheit. What comes out of the kiln is called clinker. Cement plants grind clinker down to an extremely fine powder and mix in a few additives. The final result is cement.

“Cement is then brought to sites where it is mixed with water, where it becomes cement paste,” explains Professor Franz-Josef Ulm, faculty director of the MIT Concrete Sustainability Hub (CSHub). “If you add sand to that paste it becomes mortar. And if you add to the mortar large aggregates — stones of a diameter of up to an inch — it becomes concrete.”

What makes concrete so strong is the chemical reaction that occurs when cement and water mix — a process known as hydration.

“Hydration occurs when cement and water react,” says Ulm. “During hydration, the clinker dissolves into the calcium and recombines with water and silica to form calcium silica hydrates.”

Calcium silica hydrates, or CSH, are the key to cement’s solidity. As they form, they combine, developing tight bonds that lend strength to the material. These connections have a surprising byproduct — they make cement incredibly porous.

Within the spaces between the bonds of CSH, tiny pores develop — on the scale of 3 nanometers. These are known as gel pores. On top of this, any water that hasn’t reacted to form CSH during the hydration process remains in the cement, creating another set of larger pores, called capillary pores.

According to research conducted by CSHub, the French National Center for Scientific Research, and Aix-Marseille University, cement paste is so porous that 96 percent of its pores are connected.

Despite this porosity, cement possesses excellent strength and binding properties. Of course, by decreasing this porosity, one can create a denser and even stronger final product.

Starting in the 1980s, engineers designed a material — high-performance concrete (HPC) — that did just that.

“High-performance concrete developed in the 1980s when people realized that the capillary pores can be reduced in part by reducing the water-to-cement ratio,” says Ulm. “With the addition of certain ingredients as well, this created more CSH and reduced the water that remained after hydration. Essentially, it reduced the larger pores filled with water and increased the strength of the material.”

Of course, notes Ulm, reducing the water-to-cement ratio for HPC also requires more cement. And depending on how that cement is produced, this can increase the material’s environmental impact. This is in part because when calcium carbonate is fired in a kiln to produce conventional cement, a chemical reaction occurs that produces carbon dioxide (CO 2 ).

Another source of cement’s CO 2 emissions come from heating cement kilns. This heating must be done using fossil fuels because of the extremely high temperatures required in the kiln (2,700 F). The electrification of kilns is being studied, but it is currently not technically or economically feasible.

Since concrete is the most popular material in the world and cement is the primary binder used in concrete, these two sources of CO 2 are the main reason that cement contributes around 8 percent of global emissions.

CSHub’s Executive Director Jeremy Gregory, however, sees concrete’s scale as an opportunity to mitigate climate change.

“Concrete is the most-used building material in the world. And because we use so much of it, any reductions we make in its footprint will have a big impact on global emissions.”

Many of the technologies needed to reduce concrete’s footprint exist today, he notes.

“When it comes to reducing the emissions of cement, we can increase the efficiency of cement kilns by increasing our use of waste materials as energy sources rather than fossil fuels,” explains Gregory.

“We can also use blended cements that have less clinker, such as Portland limestone cement, which mixes unheated limestone in the final grinding step of cement production. The last thing we can do is capture and store or utilize the carbon emitted during cement production.”

Carbon capture, utilization, and storage has significant potential to reduce cement and concrete’s environmental impact while creating large market opportunities. According to the Center for Climate and Energy Solutions, carbon utilization in concrete will have a $400 billion global market by 2030. Several companies, like Solidia Technologies and Carbon Cure, are getting ahead of the curve by designing cement and concrete that utilize and consequentially sequester CO 2 during the production process.

“What’s clear, though,” says Gregory, “is that low-carbon concrete mixtures will have to use many of these strategies. This means we need to rethink how we design our concrete mixtures.”

Currently, the exact specifications of concrete mixtures are prescribed ahead of time. While this reduces the risk for developers, it also hinders innovative mixes that lower emissions.

As a solution, Gregory advocates specifying a mix’s performance rather than its ingredients.

“Many prescriptive requirements limit the ability to improve concrete’s environmental impact — such as limits on the water-to-cement ratio and the use of waste materials in the mixture,” he explains. “Shifting to performance-based specifications is a key technique for encouraging more innovation and meeting cost and environmental impact targets.”

According to Gregory, this requires a culture shift. To transition to performance-based specifications, numerous stakeholders, such as architects, engineers, and specifiers, will have to collaborate to design the optimal mix for their project rather than rely on a predesigned mix.

To encourage other drivers of low-carbon concrete, says Gregory, “we [also] need to address barriers of risk and cost. We can mitigate risk by asking producers to report the environmental footprints of their products and by enabling performance-based specifications. To address cost, we need to support the development and deployment of carbon capture and low-carbon technologies.”

While innovations can reduce concrete’s initial emissions, concrete can also reduce emissions in other ways.

One way is through its use. The application of concrete in buildings and infrastructure can enable lower greenhouse gas emissions over time. Concrete buildings, for instance, can have high energy efficiency, while the surface and structural properties of concrete pavements allow cars to consume less fuel.

Concrete can also reduce some of its initial impact through exposure to the air.

“Something unique about concrete is that it actually absorbs carbon over its life during a natural chemical process called carbonation,” says Gregory.

Carbonation occurs gradually in concrete as CO 2 in the air reacts with cement to form water and calcium carbonate. A 2016 paper in Nature Geoscience found that since 1930, carbonation in concrete has offset 43 percent of the emissions from the chemical transformation of calcium carbonate to clinker during cement production.

Carbonation, though, has a drawback. It can lead to the corrosion of the steel rebar often set within concrete. Going forward, engineers may seek to maximize the carbon uptake of the carbonation process while also minimizing the durability issues it can pose.

Carbonation, as well as technologies like carbon capture, utilization, and storage and improved mixes, will all contribute to lower-carbon concrete. But making this possible will require the cooperation of academia, industry, and the government, says Gregory.

He sees this as an opportunity.

“Change doesn’t have to happen based on just technology,” he notes. “It can also happen by how we work together toward common objectives.”"
59;machinelearningmastery.com;http://machinelearningmastery.com/use-regression-machine-learning-algorithms-weka/;2016-07-21;How To Use Regression Machine Learning Algorithms in Weka;"Tweet Share Share

Last Updated on August 22, 2019

Weka has a large number of regression algorithms available on the platform.

The large number of machine learning algorithms supported by Weka is one of the biggest benefits of using the platform.

In this post you will discover how to use top regression machine learning algorithms in Weka.

After reading this post you will know:

About 5 top regression algorithms supported by Weka.

How to use regression machine learning algorithms for predictive modeling in Weka.

About the key configuration options of regression algorithms in Weka.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Regression Algorithms Overview

We are going to take a tour of 5 top regression algorithms in Weka.

Each algorithm that we cover will be briefly described in terms of how it works, key algorithm parameters will be highlighted and the algorithm will be demonstrated in the Weka Explorer interface.

The 5 algorithms that we will review are:

Linear Regression k-Nearest Neighbors Decision Tree Support Vector Machines Multi-Layer Perceptron

These are 5 algorithms that you can try on your regression problem as a starting point.

A standard machine learning regression problem will be used to demonstrate each algorithm.

Specifically, the Boston House Price Dataset. Each instance describes the properties of a Boston suburb and the task is to predict the house prices in thousands of dollars. There are 13 numerical input variables with varying scales describing the properties of suburbs. You can learn more about this dataset on the UCI Machine Learning Repository.

Start the Weka Explorer:

Open the Weka GUI Chooser. Click the “Explorer” button to open the Weka Explorer. Load the Boston house price dataset from the housing.arff file. Click “Classify” to open the Classify tab.

Let’s start things off by looking at the linear regression algorithm.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Linear Regression

Linear regression only supports regression type problems.

It works by estimating coefficients for a line or hyperplane that best fits the training data. It is a very simple regression algorithm, fast to train and can have great performance if the output variable for your data is a linear combination of your inputs.

It is good idea to evaluate linear regression on your problem before moving onto more complex algorithms in case it performs well.

Choose the linear regression algorithm:

Click the “Choose” button and select “LinearRegression” under the “functions” group. Click on the name of the algorithm to review the algorithm configuration.

The performance of linear regression can be reduced if your training data has input attributes that are highly correlated. Weka can detect and remove highly correlated input attributes automatically by setting eliminateColinearAttributes to True, which is the default.

Additionally, attributes that are unrelated to the output variable can also negatively impact performance. Weka can automatically perform feature selection to only select those relevant attributes by setting the attributeSelectionMethod. This is enabled by default and can be disabled.

Finally, the Weka implementation uses a ridge regularization technique in order to reduce the complexity of the learned model. It does this by minimizing the square of the absolute sum of the learned coefficients, which will prevent any specific coefficient from becoming too large (a sign of complexity in regression models).

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Boston house price dataset.

You can see that with the default configuration that linear regression achieves an RMSE of 4.9.

k-Nearest Neighbors

The k-nearest neighbors algorithm supports both classification and regression. It is also called kNN for short. It works by storing the entire training dataset and querying it to locate the k most similar training patterns when making a prediction.

As such, there is no model other than the raw training dataset and the only computation performed is the querying of the training dataset when a prediction is requested.

It is a simple algorithm, but one that does not assume very much about the problem other than that the distance between data instances is meaningful in making predictions. As such, it often achieves very good performance.

When making predictions on regression problems, KNN will take the mean of the k most similar instances in the training dataset. Choose the KNN algorithm:

Click the “Choose” button and select “IBk” under the “lazy” group. Click on the name of the algorithm to review the algorithm configuration.

In Weka KNN is called IBk which stands for Instance Based k.

The size of the neighborhood is controlled by the k parameter. For example, if set to 1, then predictions are made using the single most similar training instance to a given new pattern for which a prediction is requested. Common values for k are 3, 7, 11 and 21, larger for larger dataset sizes. Weka can automatically discover a good value for k using cross validation inside the algorithm by setting the crossValidate parameter to True.

Another important parameter is the distance measure used. This is configured in the nearestNeighbourSearchAlgorithm which controls the way in which the training data is stored and searched. The default is a LinearNNSearch. Clicking the name of this search algorithm will provide another configuration window where you can choose a distanceFunction parameter. By default, Euclidean distance is used to calculate the distance between instances, which is good for numerical data with the same scale. Manhattan distance is good to use if your attributes differ in measures or type.

It is a good idea to try a suite of different k values and distance measures on your problem and see what works best.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Boston house price dataset.

You can see that with the default configuration that KNN algorithm achieves an RMSE of 4.6.

Decision Tree

Decision trees can support classification and regression problems.

Decision trees are more recently referred to as Classification And Regression Trees or CART. They work by creating a tree to evaluate an instance of data, start at the root of the tree and moving town to the leaves (roots because the tree is drawn with an inverted prospective) until a prediction can be made. The process of creating a decision tree works by greedily selecting the best split point in order to make predictions and repeating the process until the tree is a fixed depth.

After the tree is construct, it is pruned in order to improve the model’s ability to generalize to new data.

Choose the decision tree algorithm:

Click the “Choose” button and select “REPTree” under the “trees” group. Click on the name of the algorithm to review the algorithm configuration.

The depth of the tree is defined automatically, but can specify a depth in the maxDepth attribute.

You can also choose to turn off pruning by setting the noPruning parameter to True, although this may result in worse performance.

The minNum parameter defines the minimum number of instances supported by the tree in a leaf node when constructing the tree from the training data.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Boston house price dataset.

You can see that with the default configuration that decision tree algorithm achieves an RMSE of 4.8.

Support Vector Regression

Support Vector Machines were developed for binary classification problems, although extensions to the technique have been made to support multi-class classification and regression problems. The adaptation of SVM for regression is called Support Vector Regression or SVR for short.

SVM was developed for numerical input variables, although will automatically convert nominal values to numerical values. Input data is also normalized before being used.

Unlike SVM that finds a line that best separates the training data into classes, SVR works by finding a line of best fit that minimizes the error of a cost function. This is done using an optimization process that only considers those data instances in the training dataset that are closest to the line with the minimum cost. These instances are called support vectors, hence the name of the technique.

In almost all problems of interest, a line cannot be drawn to best fit the data, therefore a margin is added around the line to relax the constraint, allowing some bad predictions to be tolerated but allowing a better result overall.

Finally, few datasets can be fit with just a straight line. Sometimes a line with curves or even polygonal regions need to be marked out. This is achieved by projecting the data into a higher dimensional space in order to draw the lines and make predictions. Different kernels can be used to control the projection and the amount of flexibility.

Choose the SVR algorithm:

Click the “Choose” button and select “SMOreg” under the “function” group. Click on the name of the algorithm to review the algorithm configuration.

The C parameter, called the complexity parameter in Weka controls how flexible the process for drawing the line to fit the data can be. A value of 0 allows no violations of the margin, whereas the default is 1.

A key parameter in SVM is the type of Kernel to use. The simplest kernel is a Linear kernel that separates data with a straight line or hyperplane. The default in Weka is a Polynomial Kernel that will fit the data using a curved or wiggly line, the higher the polynomial, the more wiggly (the exponent value).

The Polynomial Kernel has a default exponent of 1, which makes it equivalent to a linear kernel. A popular and powerful kernel is the RBF Kernel or Radial Basis Function Kernel that is capable of learning closed polygons and complex shapes to fit the training data.

It is a good idea to try a suite of different kernels and C (complexity) values on your problem and see what works best.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Boston house price dataset.

You can see that with the default configuration that SVR algorithm achieves an RMSE of 5.1.

Multi-Layer Perceptron

The Multi-Layer Perceptron algorithms supports both regression and classification problems.

It is also called artificial neural networks or simply neural networks for short.

Neural networks are a complex algorithm to use for predictive modeling because there are so many configuration parameters that can only be tuned effectively through intuition and a lot of trial and error.

It is an algorithm inspired by a model of biological neural networks in the brain where small processing units called neurons are organized into layers that if configured well are capable of approximating any function. In classification we are interested in approximating the underlying function to best discriminate between classes. In regression problems we are interested in approximating a function that best fits the real value output.

Choose the Multi-Layer Perceptron algorithm:

Click the “Choose” button and select “MultilayerPerceptron” under the “function” group. Click on the name of the algorithm to review the algorithm configuration.

You can manually specify the structure of the neural network that is used by the model, but this is not recommended for beginners.

The default will automatically design the network and train it on your dataset. The default will create a single hidden layer network. You can specify the number of hidden layers in the hiddenLayers parameter, set to automatic “a” by default.

You can also use a GUI to design the network structure. This can be fun, but it is recommended that you use the GUI with a simple train and test split of your training data, otherwise you will be asked to design a network for each of the 10 folds of cross validation.

You can configure the learning process by specifying how much to update the model each epoch by setting the learning rate. common values are small such as values between 0.3 (the default) and 0.1.

The learning process can be further tuned with a momentum (set to 0.2 by default) to continue updating the weights even when no changes need to be made, and a decay (set decay to True) which will reduce the learning rate over time to perform more learning at the beginning of training and less at the end.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Boston house price dataset.

You can see that with the default configuration that Multi-Layer Perceptron algorithm achieves an RMSE of 4.7.

Summary

In this post you discovered regression algorithms in Weka.

Specifically you learned:

About 5 top regression algorithms you can use for predictive modeling.

How to run regression algorithms in Weka.

About key configuration options for regression algorithms in Weka.

Do you have any questions about regression algorithms in Weka or about this post? Ask your questions in the comments and I will do my best to answer.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
60;machinelearningmastery.com;http://machinelearningmastery.com/introduction-python-deep-learning-library-tensorflow/;2016-05-04;Introduction to the Python Deep Learning Library TensorFlow;"import tensorflow as tf

import numpy as np

# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3

x_data = np . random . rand ( 100 ) . astype ( np . float32 )

y_data = x_data * 0.1 + 0.3

# Try to find values for W and b that compute y_data = W * x_data + b

# (We know that W should be 0.1 and b 0.3, but Tensorflow will

# figure that out for us.)

W = tf . Variable ( tf . random_uniform ( [ 1 ] , - 1.0 , 1.0 ) )

b = tf . Variable ( tf . zeros ( [ 1 ] ) )

y = W * x_data + b

# Minimize the mean squared errors.

loss = tf . reduce_mean ( tf . square ( y - y_data ) )

optimizer = tf . train . GradientDescentOptimizer ( 0.5 )

train = optimizer . minimize ( loss )

# Before starting, initialize the variables. We will 'run' this first.

init = tf . initialize_all_variables ( )

# Launch the graph.

sess = tf . Session ( )

sess . run ( init )

# Fit the line.

for step in xrange ( 201 ) :

sess . run ( train )

if step % 20 == 0 :

print ( step , sess . run ( W ) , sess . run ( b ) )"
61;machinelearningmastery.com;https://machinelearningmastery.com/why-training-a-neural-network-is-hard/;2019-02-28;Why Training a Neural Network Is Hard;"Tweet Share Share

Last Updated on August 6, 2019

Or, Why Stochastic Gradient Descent Is Used to Train Neural Networks.

Fitting a neural network involves using a training dataset to update the model weights to create a good mapping of inputs to outputs.

This training process is solved using an optimization algorithm that searches through a space of possible values for the neural network model weights for a set of weights that results in good performance on the training dataset.

In this post, you will discover the challenge of training a neural network framed as an optimization problem.

After reading this post, you will know:

Training a neural network involves using an optimization algorithm to find a set of weights to best map inputs to outputs.

The problem is hard, not least because the error surface is non-convex and contains local minima, flat spots, and is highly multidimensional.

The stochastic gradient descent algorithm is the best general algorithm to address this challenging problem.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

Learning as Optimization Challenging Optimization Features of the Error Surface Implications for Training

Learning as Optimization

Deep learning neural network models learn to map inputs to outputs given a training dataset of examples.

The training process involves finding a set of weights in the network that proves to be good, or good enough, at solving the specific problem.

This training process is iterative, meaning that it progresses step by step with small updates to the model weights each iteration and, in turn, a change in the performance of the model each iteration.

The iterative training process of neural networks solves an optimization problem that finds for parameters (model weights) that result in a minimum error or loss when evaluating the examples in the training dataset.

Optimization is a directed search procedure and the optimization problem that we wish to solve when training a neural network model is very challenging.

This raises the question as to what exactly is so challenging about this optimization problem?

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Challenging Optimization

Training deep learning neural networks is very challenging.

The best general algorithm known for solving this problem is stochastic gradient descent, where model weights are updated each iteration using the backpropagation of error algorithm.

Optimization in general is an extremely difficult task. […] When training neural networks, we must confront the general non-convex case.

— Page 282, Deep Learning, 2016.

An optimization process can be understood conceptually as a search through a landscape for a candidate solution that is sufficiently satisfactory.

A point on the landscape is a specific set of weights for the model, and the elevation of that point is an evaluation of the set of weights, where valleys represent good models with small values of loss.

This is a common conceptualization of optimization problems and the landscape is referred to as an “error surface.”

In general, E(w) [the error function of the weights] is a multidimensional function and impossible to visualize. If it could be plotted as a function of w [the weights], however, E [the error function] might look like a landscape with hills and valleys …

— Page 113, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

The optimization algorithm iteratively steps across this landscape, updating the weights and seeking out good or low elevation areas.

For simple optimization problems, the shape of the landscape is a big bowl and finding the bottom is easy, so easy that very efficient algorithms can be designed to find the best solution.

These types of optimization problems are referred to mathematically as convex.

The error surface we wish to navigate when optimizing the weights of a neural network is not a bowl shape. It is a landscape with many hills and valleys.

These type of optimization problems are referred to mathematically as non-convex.

In fact, there does not exist an algorithm to solve the problem of finding an optimal set of weights for a neural network in polynomial time. Mathematically, the optimization problem solved by training a neural network is referred to as NP-complete (e.g. they are very hard to solve).

We prove this problem NP-complete and thus demonstrate that learning in neural networks has no efficient general solution.

— Neural Network Design and the Complexity of Learning, 1988.

Key Features of the Error Surface

There are many types of non-convex optimization problems, but the specific type of problem we are solving when training a neural network is particularly challenging.

We can characterize the difficulty in terms of the features of the landscape or error surface that the optimization algorithm may encounter and must navigate in order to be able to deliver a good solution.

There are many aspects of the optimization of neural network weights that make the problem challenging, but three often-mentioned features of the error landscape are the presence of local minima, flat regions, and the high-dimensionality of the search space.

Backpropagation can be very slow particularly for multilayered networks where the cost surface is typically non-quadratic, non-convex, and high dimensional with many local minima and/or flat regions.

— Page 13, Neural Networks: Tricks of the Trade, 2012.

1. Local Minima

Local minimal or local optima refer to the fact that the error landscape contains multiple regions where the loss is relatively low.

These are valleys, where solutions in those valleys look good relative to the slopes and peaks around them. The problem is, in the broader view of the entire landscape, the valley has a relatively high elevation and better solutions may exist.

It is hard to know whether the optimization algorithm is in a valley or not, therefore, it is good practice to start the optimization process with a lot of noise, allowing the landscape to be sampled widely before selecting a valley to fall into.

By contrast, the lowest point in the landscape is referred to as the “global minima“.

Neural networks may have one or more global minima, and the challenge is that the difference between the local and global minima may not make a lot of difference.

The implication of this is that often finding a “good enough” set of weights is more tractable and, in turn, more desirable than finding a global optimal or best set of weights.

Nonlinear networks usually have multiple local minima of differing depths. The goal of training is to locate one of these minima.

— Page 14, Neural Networks: Tricks of the Trade, 2012.

A classical approach to addressing the problem of local minima is to restart the search process multiple times with a different starting point (random initial weights) and allow the optimization algorithm to find a different, and hopefully better, local minima. This is called “multiple restarts”.

Random Restarts: One of the simplest ways to deal with local minima is to train many different networks with different initial weights.

— Page 121, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

2. Flat Regions (Saddle Points)

A flat region or saddle point is a point on the landscape where the gradient is zero.

These are flat regions at the bottom of valleys or regions between peaks. The problem is that a zero gradient means that the optimization algorithm does not know which direction to move in order to improve the model.

… the presence of saddlepoints, or regions where the error function is very flat, can cause some iterative algorithms to become ‘stuck’ for extensive periods of time, thereby mimicking local minima.

— Page 255, Neural Networks for Pattern Recognition, 1995.

Nevertheless, recent work may suggest that perhaps local minima and flat regions may be less of a challenge than was previously believed.

Do neural networks enter and escape a series of local minima? Do they move at varying speed as they approach and then pass a variety of saddle points? […] we present evidence strongly suggesting that the answer to all of these questions is no.

— Qualitatively characterizing neural network optimization problems, 2015.

3. High-Dimensional

The optimization problem solved when training a neural network is high-dimensional.

Each weight in the network represents another parameter or dimension of the error surface. Deep neural networks often have millions of parameters, making the landscape to be navigated by the algorithm extremely high-dimensional, as compared to more traditional machine learning algorithms.

The problem of navigating a high-dimensional space is that the addition of each new dimension dramatically increases the distance between points in the space, or hypervolume. This is often referred to as the “curse of dimensionality”.

This phenomenon is known as the curse of dimensionality. Of particular concern is that the number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases.

— Page 155, Deep Learning, 2016.

Implications for Training

The challenging nature of optimization problems to be solved when using deep learning neural networks has implications when training models in practice.

In general, stochastic gradient descent is the best algorithm available, and this algorithm makes no guarantees.

There is no formula to guarantee that (1) the network will converge to a good solution, (2) convergence is swift, or (3) convergence even occurs at all.

— Page 13, Neural Networks: Tricks of the Trade, 2012.

We can summarize these implications as follows:

Possibly Questionable Solution Quality . The optimization process may or may not find a good solution and solutions can only be compared relatively, due to deceptive local minima.

. The optimization process may or may not find a good solution and solutions can only be compared relatively, due to deceptive local minima. Possibly Long Training Time . The optimization process may take a long time to find a satisfactory solution, due to the iterative nature of the search.

. The optimization process may take a long time to find a satisfactory solution, due to the iterative nature of the search. Possible Failure. The optimization process may fail to progress (get stuck) or fail to locate a viable solution, due to the presence of flat regions.

The task of effective training is to carefully configure, test, and tune the hyperparameters of the model and the learning process itself to best address this challenge.

Thankfully, modern advancements can dramatically simplify the search space and accelerate the search process, often discovering models much larger, deeper, and with better performance than previously thought possible.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Papers

Articles

Summary

In this post, you discovered the challenge of training a neural network framed as an optimization problem.

Specifically, you learned:

Training a neural network involves using an optimization algorithm to find a set of weights to best map inputs to outputs.

The problem is hard, not least because the error surface is non-convex and contains local minima, flat spots, and is highly multidimensional.

The stochastic gradient descent algorithm is the best general algorithm to address this challenging problem.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
62;machinelearningmastery.com;http://machinelearningmastery.com/process-for-working-through-machine-learning-problems/;2014-02-11;Applied Machine Learning Process;"Tweet Share Share

Last Updated on July 5, 2019

The Systematic Process For Working Through Predictive Modeling Problems

That Delivers Above Average Results

Over time, working on applied machine learning problems you develop a pattern or process for quickly getting to good robust results.

Once developed, you can use this process again and again on project after project. The more robust and developed your process, the faster you can get to reliable results.

In this post, I want to share with you the skeleton of my process for working a machine learning problem.

You can use this as a starting point or template on your next project.

5-Step Systematic Process

I liked to use a 5-step process:

Define the Problem Prepare Data Spot Check Algorithms Improve Results Present Results

There is a lot of flexibility in this process. For example, the “prepare data” step is typically broken down into analyze data (summarize and graph) and prepare data (prepare samples for experiments). The “Spot Checks” step may involve multiple formal experiments.

It’s a great big production line that I try to move through in a linear manner. The great thing in using automated tools is that you can go back a few steps (say from “Improve Results” back to “Prepare Data”) and insert a new transform of the dataset and re-run experiments in the intervening steps to see what interesting results come out and how they compare to the experiments you executed before.

The process I use has been adapted from the standard data mining process of knowledge discovery in databases (or KDD), See the post What is Data Mining and KDD for more details.

1. Define the Problem

I like to use a three step process to define the problem. I like to move quickly and I use this mini process to see the problem from a few different perspectives very quickly:

Step 1: What is the problem? Describe the problem informally and formally and list assumptions and similar problems.

Describe the problem informally and formally and list assumptions and similar problems. Step 2: Why does the problem need to be solved? List your motivation for solving the problem, the benefits a solution provides and how the solution will be used.

List your motivation for solving the problem, the benefits a solution provides and how the solution will be used. Step 3: How would I solve the problem? Describe how the problem would be solved manually to flush domain knowledge.

You can learn more about this process in the post:

2. Prepare Data

I preface data preparation with a data analysis phase that involves summarizing the attributes and visualizing them using scatter plots and histograms. I also like to describe in detail each attribute and relationships between attributes. This grunt work forces me to think about the data in the context of the problem before it is lost to the algorithms

The actual data preparation process is three step as follows:

Step 1: Data Selection : Consider what data is available, what data is missing and what data can be removed.

: Consider what data is available, what data is missing and what data can be removed. Step 2: Data Preprocessing : Organize your selected data by formatting, cleaning and sampling from it.

: Organize your selected data by formatting, cleaning and sampling from it. Step 3: Data Transformation: Transform preprocessed data ready for machine learning by engineering features using scaling, attribute decomposition and attribute aggregation.

You can learn more about this process for preparing data in the post:

3. Spot Check Algorithms

I use 10 fold cross validation in my test harnesses by default. All experiments (algorithm and dataset combinations) are repeated 10 times and the mean and standard deviation of the accuracy is collected and reported. I also use statistical significance tests to flush out meaningful results from noise. Box-plots are very useful for summarizing the distribution of accuracy results for each algorithm and dataset pair.

I spot check algorithms, which means loading up a bunch of standard machine learning algorithms into my test harness and performing a formal experiment. I typically run 10-20 standard algorithms from all the major algorithm families across all the transformed and scaled versions of the dataset I have prepared.

The goal of spot checking is to flush out the types of algorithms and dataset combinations that are good at picking out the structure of the problem so that they can be studied in more detail with focused experiments.

More focused experiments with well-performing families of algorithms may be performed in this step, but algorithm tuning is left for the next step.

You can discover more about defining your test harness in the post:

You can discover the importance of spot checking algorithms in the post:

4. Improve Results

After spot checking, it’s time to squeeze out the best result from the rig. I do this by running an automated sensitivity analysis on the parameters of the top performing algorithms. I also design and run experiments using standard ensemble methods of the top performing algorithms. I put a lot of time into thinking about how to get more out of the dataset or of the family of algorithms that have been shown to perform well.

Again, statistical significance of results is critical here. It is so easy to focus on the methods and play with algorithm configurations. The results are only meaningful if they are significant and all configuration are already thought out and the experiments are executed in batch. I also like to maintain my own personal leaderboard of top results on a problem.

In summary, the process of improving results involves:

Algorithm Tuning : where discovering the best models is treated like a search problem through model parameter space.

: where discovering the best models is treated like a search problem through model parameter space. Ensemble Methods : where the predictions made by multiple models are combined.

: where the predictions made by multiple models are combined. Extreme Feature Engineering: where the attribute decomposition and aggregation seen in data preparation is pushed to the limits.

You can discover more about this process in the post:

5. Present Results

The results of a complex machine learning problem are meaningless unless they are put to work. This typically means a presentation to stakeholders. Even if it is a competition or a problem I am working on for myself, I still go through the process of presenting the results. It’s a good practice and gives me clear learnings I can build upon next time.

The template I use to present results is below and may take the form of a text document, formal report or presentation slides.

Context (Why) : Define the environment in which the problem exists and set up the motivation for the research question.

: Define the environment in which the problem exists and set up the motivation for the research question. Problem (Question) : Concisely describe the problem as a question that you went out and answered.

: Concisely describe the problem as a question that you went out and answered. Solution (Answer) : Concisely describe the solution as an answer to the question you posed in the previous section. Be specific.

: Concisely describe the solution as an answer to the question you posed in the previous section. Be specific. Findings : Bulleted lists of discoveries you made along the way that interests the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey.

: Bulleted lists of discoveries you made along the way that interests the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey. Limitations : Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel.

: Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel. Conclusions (Why+Question+Answer): Revisit the “why”, research question and the answer you discovered in a tight little package that is easy to remember and repeat for yourself and others.

You can discover more about using the results of a machine learning project in the post:

Summary

In this post, you have learned my general template for processing a machine learning problem.

I use this process almost without fail and I use it across platforms, from Weka, R and scikit-learn and even new platforms I have been playing around with like pylearn2.

What is your process, leave a comment and share?

Will you copy this process, and if so, what changes will you make to it?"
63;machinelearningmastery.com;https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/;2019-04-16;How Do Convolutional Layers Work in Deep Learning Neural Networks?;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32

# example of calculation 2d convolutions from numpy import asarray from keras . models import Sequential from keras . layers import Conv2D # define input data data = [ [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ] data = asarray ( data ) data = data . reshape ( 1 , 8 , 8 , 1 ) # create model model = Sequential ( ) model . add ( Conv2D ( 1 , ( 3 , 3 ) , input_shape = ( 8 , 8 , 1 ) ) ) # define a vertical line detector detector = [ [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] , [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] , [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ] weights = [ asarray ( detector ) , asarray ( [ 0.0 ] ) ] # store the weights in the model model . set_weights ( weights ) # confirm they were stored print ( model . get_weights ( ) ) # apply filter to input data yhat = model . predict ( data ) for r in range ( yhat . shape [ 1 ] ) : # print each column in the row print ( [ yhat [ 0 , r , c , 0 ] for c in range ( yhat . shape [ 2 ] ) ] )"
64;machinelearningmastery.com;https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/;2019-04-21;A Gentle Introduction to Pooling Layers for Convolutional Neural Networks;"# example of vertical line detection with a convolutional layer

from numpy import asarray

from keras . models import Sequential

from keras . layers import Conv2D

# define input data

data = [ [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ]

data = asarray ( data )

data = data . reshape ( 1 , 8 , 8 , 1 )

# create model

model = Sequential ( )

model . add ( Conv2D ( 1 , ( 3 , 3 ) , activation = 'relu' , input_shape = ( 8 , 8 , 1 ) ) )

# summarize model

model . summary ( )

# define a vertical line detector

detector = [ [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ,

[ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ,

[ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ]

weights = [ asarray ( detector ) , asarray ( [ 0.0 ] ) ]

# store the weights in the model

model . set_weights ( weights )

# apply filter to input data

yhat = model . predict ( data )

# enumerate rows

for r in range ( yhat . shape [ 1 ] ) :

# print each column in the row"
65;news.mit.edu;http://news.mit.edu/2020/mit-christine-soh-integrates-computer-science-and-linguistics-0305;;MIT senior Christine Soh integrates computer science and linguistics;"Christine Soh fell in love with MIT the summer before her senior year of high school while attending the Women’s Technology Program run by MIT’s Department of Electrical Engineering and Computer Science. That’s when she discovered that learning to program in Python is just like learning a new language — and Soh loves languages.

Growing up in Colorado, Soh spoke both English and Korean; she learned French and Latin in school. This June, Soh will graduate from MIT, where she has happily combined her passions by majoring in computer science and engineering (Course 6-3) and linguistics (Course 24). She plans to begin working toward a PhD in linguistics next year.

With fluency in both technical and humanistic modes of thinking, Soh exemplifies a ""bilingual"" perspective. ""Dual competence is a good model for undergraduates at MIT,"" says engineer/historian David Mindell, who encourages MIT students to ""master two fundamental ways of thinking about the world, one technical and one humanistic or social. Sometimes these two modes will be at odds with each other, which raises critical questions. Other times they will be synergistic and energizing.""



The challenge of natural language and computation

“The really cool thing about language is that it’s universal,” says Soh, who has added ancient Greek, Chinese, and the programming language Java to her credits since that summer. “I can have a really interesting conversation with anybody, even if they don’t have a linguistics background, because everyone has experience with language.”

That said, natural language is difficult for computers to comprehend — something Soh finds fascinating. “It’s really interesting to think about how we understand language,” she says. “How is it that computers have such a hard time understanding what we find so easy?”

Tools from computational linguistics to improve speech

Pairing linguistics with computer science has allowed Soh to explore cutting-edge research combining the two disciplines. Thanks to MIT’s Advanced Undergraduate Research Opportunities Program, Soh got the chance to explore whether speech analysis software can be used as a tool for the clinical diagnosis of speech impairments.

“It’s very difficult to correctly diagnose a child because a speech impairment can be caused by a ton of different things,” says Soh. Working with the Speech Communication Group in MIT’s Research Laboratory of Electronics, Soh has been developing a tool that can listen to a child’s speech and extract linguistic information, such where in the mouth the sound was produced, thus identifying modifications from the proper formation of the word. “We can then use computational techniques to see if there are patterns to the modifications that have been made and see if these patterns can distinguish one underlying condition from another.”

A natural leader

Even if the team isn’t able to find such patterns, Soh says the tool could be used by speech pathologists to learn more about what linguistic modifications a child might need to make to improve speech. In December, Soh presented a poster on this work at the annual meeting of the Acoustical Society of America and was honored with a first-place prize in her category (signal processing in acoustics).

Exploring such real-world applications for computational linguistics helped inspire Soh to apply to doctoral programs in linguistics for next year. “I’ll be doing research that will be integrating computer science and linguistics,” she says, noting that possible applications of computational linguistics include working to improve speech-recognition software or to make machine-produced speech sound more natural. “I look forward to using the knowledge and skills I’ve learned at MIT in doing that research.”

“Christine’s unique interests, energy, and deep interests in both linguistics and computer science should enable her to accomplish great things,” says Suzanne Flynn, a professor of linguistics who has had Soh as a student. “She is a natural leader.”



From field methods to neurolinguistics

Looking back at her time at MIT, Soh recalls particularly enjoying two linguistics classes: 24.909 (Field Methods in Linguistics) which explores the structure of an unfamiliar language through direct work with a native speaker (in Soh’s year, the class centered on Wolof, which is spoken in Senegal, the Gambia, and Mauritania), and 24.906 (The Linguistic Study of Bilingualism).

In the latter class, Soh says, “We looked at neurolinguistics, what’s happening in the brain as the bilingual brain developed. We looked at topics in sociolinguistics: In communities that are bilingual, like Quebec, what kind of impact does it have on society, such as how schools are run? … We got to see a spectrum of linguistics. It was really cool.”

Building community at MIT

Outside class, Soh says she found community at MIT through the Asian Christian Fellowship and the Society of Women Engineers (SWE), which she served last year as vice president of membership. “SWE has also been a really awesome community and has opened up opportunities for conversation about what it means to be a woman engineer,” she says.

Interestingly, Soh almost didn’t apply to MIT at all, simply because her brother was already at the Institute. (Albert Soh ’18 is now a high school teacher of math and physics.) Fortunately, the Women’s Technology Program changed her mind, and as she nears graduation, Soh says, ""MIT has been absolutely fantastic.”



Story prepared by MIT SHASS Communications

Editorial and Design Director: Emily Hiestand

Senior Writer: Kathryn O'Neill

"
66;news.mit.edu;http://news.mit.edu/press/for-journalists;;For Journalists;"MIT is home to extraordinary work in science, technology, and other fields of inquiry, and to the world-class faculty and students who make that true. Those of us charged with sharing the Institute's news with the media hope that we can be your guides, whether you are interested in pursuing a specific research advance, or simply getting to know the Institute as a whole.

MIT News is the companion website to the MIT Media Relations website (which is where you are right now). It features articles about MIT teaching, research and innovation written by journalists on staff at the MIT News Office. The website was launched in September 2009, and it currently serves an international audience of about 400,000 monthly unique visitors.

To receive press releases and other MIT news

The MIT News Office's Media Relations team usually distributes several press releases tied into research advances each week, along with other MIT news and announcements. Please sign up here to receive press releases on a regular basis, and let us know what topics interest you.

Events

In any given week there are a variety of talks, symposia, and other more eclectic events happening on the MIT campus. We highlight some of these on our website, and will also notify you if we think there's something going on here that you'd want to know about. MIT also maintains an extensive events calendar.

Experts

Many of MIT's 1,000+ faculty members and researchers are available to comment on their research or other related topics. See Experts Guide.

Film crews

Film and documentary crews are welcome to film on the MIT campus, provided the focus of their work is MIT-related content and they adhere to certain guidelines.

General MIT background

See the MIT Facts page for general information about MIT, including enrollment numbers, admissions statistics, and a history of the Institute. Click here for a listing of MIT research areas by topic.

MIT studio (ISDN)

MIT Audio-Visual Services operates a studio in Building 46 equipped with an ISDN line for broadcast radio. For pricing and availability, producers should email avorders@mit.edu or call 617-253-2808.

MIT studio (satellite uplink)

MIT Video Productions (MVP) operates a studio in Building 24 that allows for MIT community members to be connected live to news outlets. The studio offers a number of visual backgrounds, including images that place the interviewee at MIT. This service is $600/hour with a one-hour minimum. ($750/hour during off hours) To arrange to use the studio, please contact MVP.

Photography

We can usually provide journalists with high-resolution images to illustrate research stories. These are available for download off of individual press releases, or by request. We also have an archive of thousands of additional images, so if you cannot find what you are looking for, please contact us."
67;news.mit.edu;http://news.mit.edu/2016/forbes-30-under-30-lists-0107;;25 from MIT named to Forbes 30 Under 30 lists in 2016;"According to Forbes magazine, their fifth annual 30 Under 30 lists showcase “America’s most important young entrepreneurs, creative leaders and brightest stars” who are less than than 30 years old. Twenty-five MIT students, researchers, and alumni made the 2016 lists.

Some 600 selections covering 20 categories were whittled down from an initial screening of more than 15,000 nominations. Following are the MIT affiliates who were selected.

Peter Bailis (Enterprise Tech)

Postdoc in the MIT Computer Science and Artificial Intelligence Laboratory

""26-year-old Bailis finished his PhD in computer science at Berkeley and accepted a tenure-track assistant professor position in Stanford's computer science department, starting in fall 2016. He wrote his PhD thesis on large-scale data management and is a recipient of a National Science Foundation Graduate Research Fellowship.""

Sampriti Bhattacharyya (Manufacturing and Industry)

Grad student in the Department of Mechanical Engineering and founder of Hydroswarm

“Bhattacharyya has developed underwater drones that are capable of scanning the ocean for lost planes, or measure oil spills or radiation under the sea.”

Jonathan Birnbaum ’08 (Finance)

Vice president at Morgan Stanley

“Chief operating officer of bank’s U.S. credit trading group, managing 100 bankers trading investment grade, high yield and distressed debt.”

Natalya Brikner PhD ’15, Louis Perna ’09, SM ’14 (Manufacturing and Industry)

Co-founders of Accion Systems

“Accion is working to commercialize ion propulsion technology for small satellites using a liquid ionic propellant that is non-toxic and non-explosive.”

Arnav Chhabra (Science)

Grad student with Harvard-MIT Health Sciences and Technology

“Chhabra published his first paper in high school. He's now pursuing his PhD, and his most recent thesis is focused on building a “liver on a chip” — a miniaturized liver model scientists hope could one day replace the use of animals for disease research.”

Abe Davis (Science)

Grad student in the Department of Electrical Engineering and Computer Science

“Davis is best known for the subject of his TED Talk, in which he showed that he could capture information from video based on vibrations in the room.”

Adam Elmachtoub PhD ’14 (Science)

Assistant professor at Columbia University

“Elmachtoub’s research is focused on using data science and optimizing algorithms to make businesses more efficient.”

Teasha Feldman-Fitzthum ’14 (Energy)

Cofounder of EverVest

“EverVest provides advanced software for analyzing, valuing, and financing renewable energy projects.”

Brian Fiske PhD ’15 (Healthcare)

Senior associate at Flagship Ventures

“This MIT PhD has put this technique to work making genetic alterations to cells to find proteins that can be hit with new drugs.”

Michael Grinich ’11, Christine Spang ’10 (Enterprise Tech)

Co-founders of Nylas

“While studying computer science and physics at MIT, Grinich wrote his thesis on the fundamental tools for syncing email. He founded Nylas (formerly known as Inbox), with fellow MIT alumna Spang in 2013 … Spang, who wrote the core mail synchronization engine, runs its platform team, while Grinich is CEO.”

Ben Harvatine ’12 (Healthcare)

Founder of Jolt

“Harvatine aims to catch concussions as they happen. His sensor can be attached to anything on the head — headband, helmet, hair clip — and sends feedback to smartphones in real time.”

Noel Hollingsworth ’13, MEng ’14 (Sports)

Director of data at Second Spectrum

“Hollingsworth won the 2014 Best Research Award at the MIT Sloan Sports Analytics Conference. Almost all NBA championship contenders use his work to gain an edge.”

Steven Keating SM ’12 (Healthcare)

Grad student in the department of Mechanical Engineering

“Keating found out that he had brain cancer after volunteering for an MRI experiment … the MIT researcher wants to know: why can my doctors see my tumor genome and not me?”

Andrew Leone ’09 (Finance)

Vice president at Nomura

“Heads one of Wall Street’s biggest VIX and structured volatility market making desks.”

Andrej Lenert SM ’10, PhD ’14 (Science)

Postdoc at the University of Michigan

“One avenue of his research was to develop a hybrid solar power system, combining the best of photovoltaic and solar thermal systems without their drawbacks.”

Maxim Lobovsky SM ’11 (Manufacturing and Industry)

Co-founder of Formlabs

“Formlabs’ printers are designed to allow for more precise parts to be created for more complicated 3-D printing projects.”

Carl Schoellhammer PhD ’15 (Healthcare)

CEO of Suono Bio

“Schoellhammer, a student of MIT professors Daniel Blankschtein and Robert Langer, invented a pill that can inject drugs directly into the gastrointestinal tract. The gadget helped him win the prestigious Lemelson-MIT Student Prize.”

Harbaljit Sohal (Science)

Postdoc at the McGovern Institute for Brain Research at MIT

""Sohal's research focus is on building neural implants for the treatment of brain disorders, disability and immunodisorders. He's developed microfabrication technologies that are capable of creating small, flexible electrodes and other devices that can be more easily integrated into the body.""

Reid Van Lehn ’09, PhD ’14 (Science)

Postdoc at Caltech

“Van Lehn’s research is focused on chemically engineered nanoparticles and studying how they interact with cell membranes.”

Ari Weinstein ’17 (Consumer Tech)

Co-founder and CEO of DeskConnect

“Weinstein is now behind Workflow, an app that won Apple’s Most Innovative App award for 2015 and which generates task recipes across iPad, iPhone or Watch.”

Sajith Wickramasekara ’15 (Healthcare)

Co-founder of Benchling

“Wickramasekara has created a digital version of every scientist’s Bible: The lab notebook. The system helps researchers contextualize and collaborate on genome engineering data.”

Natasha Wright SM ’14 (Energy)

Grad student in the Department of Mechanical Engineering

“Wright’s approach to removing salt from groundwater in rural India uses electrodialysis to extract the salt out of the water, and can run on solar power.”

Matt Zitzmann ’08 (Games)

Co-founder of Kamcord

“Kamcord’s mobile application allows users to record, share and watch the games they play on their phones.”"
68;machinelearningmastery.com;https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/;2020-03-31;Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost;"# gradient boosting for classification in scikit-learn

from numpy import mean

from numpy import std

from sklearn . datasets import make_classification

from sklearn . ensemble import GradientBoostingClassifier

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from matplotlib import pyplot

# define dataset

X , y = make_classification ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_redundant = 5 , random_state = 1 )

# evaluate the model

model = GradientBoostingClassifier ( )

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

n_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' )

print ( 'Accuracy: %.3f (%.3f)' % ( mean ( n_scores ) , std ( n_scores ) ) )

# fit the model on the whole dataset

model = GradientBoostingClassifier ( )

model . fit ( X , y )

# make a single prediction

row = [ [ 2.56999479 , - 0.13019997 , 3.16075093 , - 4.35936352 , - 1.61271951 , - 1.39352057 , - 2.48924933 , - 1.93094078 , 3.26130366 , 2.05692145 ] ]

yhat = model . predict ( row )"
69;machinelearningmastery.com;https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/;2018-02-25;How to Calculate the SVD from Scratch with Python;"from numpy import array

from numpy import diag

from numpy import zeros

from scipy . linalg import svd

# define a matrix

A = array ( [

[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] ,

[ 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 ] ,

[ 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 ] ] )

print ( A )

# Singular-value decomposition

U , s , VT = svd ( A )

# create m x n Sigma matrix

Sigma = zeros ( ( A . shape [ 0 ] , A . shape [ 1 ] ) )

# populate Sigma with n x n diagonal matrix

Sigma [ : A . shape [ 0 ] , : A . shape [ 0 ] ] = diag ( s )

# select

n_elements = 2

Sigma = Sigma [ : , : n_elements ]

VT = VT [ : n_elements , : ]

# reconstruct

B = U . dot ( Sigma . dot ( VT ) )

print ( B )

# transform

T = U . dot ( Sigma )

print ( T )

T = A . dot ( VT . T )"
70;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-machine-learning-models-for-multivariate-multi-step-air-pollution-time-series-forecasting/;2018-10-18;How to Develop Multivariate Multi-Step Time Series Forecasting Models for Air Pollution;"# spot check nonlinear algorithms

from numpy import load

from numpy import loadtxt

from numpy import nan

from numpy import isnan

from numpy import count_nonzero

from numpy import unique

from numpy import array

from sklearn . base import clone

from sklearn . neighbors import KNeighborsRegressor

from sklearn . tree import DecisionTreeRegressor

from sklearn . tree import ExtraTreeRegressor

from sklearn . svm import SVR

from sklearn . ensemble import AdaBoostRegressor

from sklearn . ensemble import BaggingRegressor

from sklearn . ensemble import RandomForestRegressor

from sklearn . ensemble import ExtraTreesRegressor

from sklearn . ensemble import GradientBoostingRegressor

# split the dataset by 'chunkID', return a list of chunks

def to_chunks ( values , chunk_ix = 0 ) :

chunks = list ( )

# get the unique chunk ids

chunk_ids = unique ( values [ : , chunk_ix ] )

# group rows by chunk id

for chunk_id in chunk_ids :

selection = values [ : , chunk_ix ] == chunk_id

chunks . append ( values [ selection , : ] )

return chunks

# return true if the array has any non-nan values

def has_data ( data ) :

return count_nonzero ( isnan ( data ) ) < len ( data )

# return a list of relative forecast lead times

def get_lead_times ( ) :

return [ 1 , 2 , 3 , 4 , 5 , 10 , 17 , 24 , 48 , 72 ]

# fit a single model

def fit_model ( model , X , y ) :

# clone the model configuration

local_model = clone ( model )

# fit the model

local_model . fit ( X , y )

return local_model

# fit one model for each variable and each forecast lead time [var][time][model]

def fit_models ( model , train ) :

# prepare structure for saving models

models = [ [ list ( ) for _ in range ( train . shape [ 1 ] ) ] for _ in range ( train . shape [ 0 ] ) ]

# enumerate vars

for i in range ( train . shape [ 0 ] ) :

# enumerate lead times

for j in range ( train . shape [ 1 ] ) :

# get data

data = train [ i , j ]

X , y = data [ : , : - 1 ] , data [ : , - 1 ]

# fit model

local_model = fit_model ( model , X , y )

models [ i ] [ j ] . append ( local_model )

return models

# return forecasts as [chunks][var][time]

def make_predictions ( models , test ) :

lead_times = get_lead_times ( )

predictions = list ( )

# enumerate chunks

for i in range ( test . shape [ 0 ] ) :

# enumerate variables

chunk_predictions = list ( )

for j in range ( test . shape [ 1 ] ) :

# get the input pattern for this chunk and target

pattern = test [ i , j ]

# assume a nan forecast

forecasts = array ( [ nan for _ in range ( len ( lead_times ) ) ] )

# check we can make a forecast

if has_data ( pattern ) :

pattern = pattern . reshape ( ( 1 , len ( pattern ) ) )

# forecast each lead time

forecasts = list ( )

for k in range ( len ( lead_times ) ) :

yhat = models [ j ] [ k ] [ 0 ] . predict ( pattern )

forecasts . append ( yhat [ 0 ] )

forecasts = array ( forecasts )

# save forecasts for each lead time for this variable

chunk_predictions . append ( forecasts )

# save forecasts for this chunk

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# convert the test dataset in chunks to [chunk][variable][time] format

def prepare_test_forecasts ( test_chunks ) :

predictions = list ( )

# enumerate chunks to forecast

for rows in test_chunks :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 3 , rows . shape [ 1 ] ) :

yhat = rows [ : , j ]

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# calculate the error between an actual and predicted value

def calculate_error ( actual , predicted ) :

# give the full actual value if predicted is nan

if isnan ( predicted ) :

return abs ( actual )

# calculate abs difference

return abs ( actual - predicted )

# evaluate a forecast in the format [chunk][variable][time]

def evaluate_forecasts ( predictions , testset ) :

lead_times = get_lead_times ( )

total_mae , times_mae = 0.0 , [ 0.0 for _ in range ( len ( lead_times ) ) ]

total_c , times_c = 0 , [ 0 for _ in range ( len ( lead_times ) ) ]

# enumerate test chunks

for i in range ( len ( test_chunks ) ) :

# convert to forecasts

actual = testset [ i ]

predicted = predictions [ i ]

# enumerate target variables

for j in range ( predicted . shape [ 0 ] ) :

# enumerate lead times

for k in range ( len ( lead_times ) ) :

# skip if actual in nan

if isnan ( actual [ j , k ] ) :

continue

# calculate error

error = calculate_error ( actual [ j , k ] , predicted [ j , k ] )

# update statistics

total_mae += error

times_mae [ k ] += error

total_c += 1

times_c [ k ] += 1

# normalize summed absolute errors

total_mae /= total_c

times_mae = [ times_mae [ i ] / times_c [ i ] for i in range ( len ( times_mae ) ) ]

return total_mae , times_mae

# summarize scores

def summarize_error ( name , total_mae ) :

print ( '%s: %.3f MAE' % ( name , total_mae ) )

# prepare a list of ml models

def get_models ( models = dict ( ) ) :

# non-linear models

models [ 'knn' ] = KNeighborsRegressor ( n_neighbors = 7 )

models [ 'cart' ] = DecisionTreeRegressor ( )

models [ 'extra' ] = ExtraTreeRegressor ( )

models [ 'svmr' ] = SVR ( )

# # ensemble models

n_trees = 100

models [ 'ada' ] = AdaBoostRegressor ( n_estimators = n_trees )

models [ 'bag' ] = BaggingRegressor ( n_estimators = n_trees )

models [ 'rf' ] = RandomForestRegressor ( n_estimators = n_trees )

models [ 'et' ] = ExtraTreesRegressor ( n_estimators = n_trees )

models [ 'gbm' ] = GradientBoostingRegressor ( n_estimators = n_trees )

print ( 'Defined %d models' % len ( models ) )

return models

# evaluate a suite of models

def evaluate_models ( models , train , test , actual ) :

for name , model in models . items ( ) :

# fit models

fits = fit_models ( model , train )

# make predictions

predictions = make_predictions ( fits , test )

# evaluate forecast

total_mae , _ = evaluate_forecasts ( predictions , actual )

# summarize forecast

summarize_error ( name , total_mae )

# load supervised datasets

train = load ( 'AirQualityPrediction/supervised_train.npy' , allow_pickle = True )

test = load ( 'AirQualityPrediction/supervised_test.npy' , allow_pickle = True )

print ( train . shape , test . shape )

# load test chunks for validation

testset = loadtxt ( 'AirQualityPrediction/naive_test.csv' , delimiter = ',' )

test_chunks = to_chunks ( testset )

actual = prepare_test_forecasts ( test_chunks )

# prepare list of models

models = get_models ( )

# evaluate models"
71;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-convolutional-neural-networks-for-multi-step-time-series-forecasting/;2018-10-07;How to Develop Convolutional Neural Networks for Multi-Step Time Series Forecasting;"# multi headed multi-step cnn

from math import sqrt

from numpy import split

from numpy import array

from pandas import read_csv

from sklearn . metrics import mean_squared_error

from matplotlib import pyplot

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers . convolutional import Conv1D

from keras . layers . convolutional import MaxPooling1D

from keras . models import Model

from keras . layers import Input

from keras . layers . merge import concatenate

# split a univariate dataset into train/test sets

def split_dataset ( data ) :

# split into standard weeks

train , test = data [ 1 : - 328 ] , data [ - 328 : - 6 ]

# restructure into windows of weekly data

train = array ( split ( train , len ( train ) / 7 ) )

test = array ( split ( test , len ( test ) / 7 ) )

return train , test

# evaluate one or more weekly forecasts against expected values

def evaluate_forecasts ( actual , predicted ) :

scores = list ( )

# calculate an RMSE score for each day

for i in range ( actual . shape [ 1 ] ) :

# calculate mse

mse = mean_squared_error ( actual [ : , i ] , predicted [ : , i ] )

# calculate rmse

rmse = sqrt ( mse )

# store

scores . append ( rmse )

# calculate overall RMSE

s = 0

for row in range ( actual . shape [ 0 ] ) :

for col in range ( actual . shape [ 1 ] ) :

s += ( actual [ row , col ] - predicted [ row , col ] ) * * 2

score = sqrt ( s / ( actual . shape [ 0 ] * actual . shape [ 1 ] ) )

return score , scores

# summarize scores

def summarize_scores ( name , score , scores ) :

s_scores = ', ' . join ( [ '%.1f' % s for s in scores ] )

print ( '%s: [%.3f] %s' % ( name , score , s_scores ) )

# convert history into inputs and outputs

def to_supervised ( train , n_input , n_out = 7 ) :

# flatten data

data = train . reshape ( ( train . shape [ 0 ] * train . shape [ 1 ] , train . shape [ 2 ] ) )

X , y = list ( ) , list ( )

in_start = 0

# step over the entire history one time step at a time

for _ in range ( len ( data ) ) :

# define the end of the input sequence

in_end = in_start + n_input

out_end = in_end + n_out

# ensure we have enough data for this instance

if out_end <= len ( data ) :

X . append ( data [ in_start : in_end , : ] )

y . append ( data [ in_end : out_end , 0 ] )

# move along one time step

in_start += 1

return array ( X ) , array ( y )

# plot training history

def plot_history ( history ) :

# plot loss

pyplot . subplot ( 2 , 1 , 1 )

pyplot . plot ( history . history [ 'loss' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' )

pyplot . title ( 'loss' , y = 0 , loc = 'center' )

pyplot . legend ( )

# plot rmse

pyplot . subplot ( 2 , 1 , 2 )

pyplot . plot ( history . history [ 'rmse' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_rmse' ] , label = 'test' )

pyplot . title ( 'rmse' , y = 0 , loc = 'center' )

pyplot . legend ( )

pyplot . show ( )

# train the model

def build_model ( train , n_input ) :

# prepare data

train_x , train_y = to_supervised ( train , n_input )

# define parameters

verbose , epochs , batch_size = 0 , 25 , 16

n_timesteps , n_features , n_outputs = train_x . shape [ 1 ] , train_x . shape [ 2 ] , train_y . shape [ 1 ]

# create a channel for each variable

in_layers , out_layers = list ( ) , list ( )

for i in range ( n_features ) :

inputs = Input ( shape = ( n_timesteps , 1 ) )

conv1 = Conv1D ( filters = 32 , kernel_size = 3 , activation = 'relu' ) ( inputs )

conv2 = Conv1D ( filters = 32 , kernel_size = 3 , activation = 'relu' ) ( conv1 )

pool1 = MaxPooling1D ( pool_size = 2 ) ( conv2 )

flat = Flatten ( ) ( pool1 )

# store layers

in_layers . append ( inputs )

out_layers . append ( flat )

# merge heads

merged = concatenate ( out_layers )

# interpretation

dense1 = Dense ( 200 , activation = 'relu' ) ( merged )

dense2 = Dense ( 100 , activation = 'relu' ) ( dense1 )

outputs = Dense ( n_outputs ) ( dense2 )

model = Model ( inputs = in_layers , outputs = outputs )

# compile model

model . compile ( loss = 'mse' , optimizer = 'adam' )

# fit network

input_data = [ train_x [ : , : , i ] . reshape ( ( train_x . shape [ 0 ] , n_timesteps , 1 ) ) for i in range ( n_features ) ]

model . fit ( input_data , train_y , epochs = epochs , batch_size = batch_size , verbose = verbose )

return model

# make a forecast

def forecast ( model , history , n_input ) :

# flatten data

data = array ( history )

data = data . reshape ( ( data . shape [ 0 ] * data . shape [ 1 ] , data . shape [ 2 ] ) )

# retrieve last observations for input data

input_x = data [ - n_input : , : ]

# reshape into n input arrays

input_x = [ input_x [ : , i ] . reshape ( ( 1 , input_x . shape [ 0 ] , 1 ) ) for i in range ( input_x . shape [ 1 ] ) ]

# forecast the next week

yhat = model . predict ( input_x , verbose = 0 )

# we only want the vector forecast

yhat = yhat [ 0 ]

return yhat

# evaluate a single model

def evaluate_model ( train , test , n_input ) :

# fit model

model = build_model ( train , n_input )

# history is a list of weekly data

history = [ x for x in train ]

# walk-forward validation over each week

predictions = list ( )

for i in range ( len ( test ) ) :

# predict the week

yhat_sequence = forecast ( model , history , n_input )

# store the predictions

predictions . append ( yhat_sequence )

# get real observation and add to history for predicting the next week

history . append ( test [ i , : ] )

# evaluate predictions days for each week

predictions = array ( predictions )

score , scores = evaluate_forecasts ( test [ : , : , 0 ] , predictions )

return score , scores

# load the new file

dataset = read_csv ( 'household_power_consumption_days.csv' , header = 0 , infer_datetime_format = True , parse_dates = [ 'datetime' ] , index_col = [ 'datetime' ] )

# split into train and test

train , test = split_dataset ( dataset . values )

# evaluate model and get scores

n_input = 14

score , scores = evaluate_model ( train , test , n_input )

# summarize scores

summarize_scores ( 'cnn' , score , scores )

# plot scores

days = [ 'sun' , 'mon' , 'tue' , 'wed' , 'thr' , 'fri' , 'sat' ]

pyplot . plot ( days , scores , marker = 'o' , label = 'cnn' )"
72;machinelearningmastery.com;https://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/;2014-01-10;How to Learn a Machine Learning Algorithm;"Tweet Share Share

Last Updated on August 12, 2019

The question of how to learn a machine learning algorithm has come up a few times on the email list.

In this post I’ll share with you the strategy I have been using for years to learn and build up a structured description of an algorithm in a step-by-step manner that I can add to, refine and refer back to again and again. I even used it to write a book.

This was just a strategy I used personally and I’ve been really surprised by the positive feedback.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Algorithm Descriptions are Broken

Learning a machine learning algorithm can be overwhelming. There are so many papers, books and websites describing how the algorithm works mathematically and textually. If you are really lucky you might find a pseudocode description of the algorithm.

If you are really really lucky you might find some suggested ways to configure the method for different situations. These descriptions are rare and typically buried deep in the original publication or in technical notes by the original authors.

A fact you learn quickly when you want to implement a method from research papers is that algorithms are almost never described in sufficient detail for you to reproduce them. The reasons vary, from the micro-decisions that are left out of the paper, to whole procedures that are summarized ambiguously in text, to symbols that are used inconsistently.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Piece it Together

To understand an algorithm you have to piece together an understanding yourself from disparate descriptions. This is the only tactic that I have found to be successful.

Disparate descriptions means resources such as the original descriptions of the method in the primary sources as well as authoritative secondary interpretations made of original descriptions in review papers and books.

It is common for there to be prototype implementations of a method released with the primary sources and reading this code (typically C, FORTRAN, R or Matlab) can be very enlightening for the details you need to reproduce an algorithm.

Algorithm Descriptions

An algorithm is an island of research and in all reality it can be difficult to pin down the canonical definition. For example, is it the version described in the primary source or is it the version that includes all the fixes and enhancements that are “best practice”.

A solution is to consider a given algorithm from multiple perspectives, each of which can serve a different purpose. For example, the abstract information processing description of the algorithm could be realized by a variety of different specific computational implementations.

I like this approach because it defends the need to telescope in on a specific case of the algorithm from many possible cases at each step of the description while also leaving the option open for the description of variations.

There are many descriptions you could use of varying specificity depending on your needs. Some that I like to use include: inspiration for the algorithm, metaphor or analogy for the strategy, information processing objectives, pseudocode and code.

Algorithm Prescriptions

When I started my own research projects, I thought the answer to this problem was to read everything on an algorithm and create the definitive implementation in code. Nice idea perhaps, but code is just one way to communicate an algorithm, and it is limited.

There is more to an algorithm description than the computation. There is meta information around an algorithm that can be invaluable for certain use cases.

For example, usage heuristics for an algorithm are embedded in papers. Having a summary of usage heuristics collected together in one place can mean the difference of getting a good enough result quickly and running sensitivity analysis on the algorithm for days or weeks.

Other examples include the standard experimental datasets used to test the algorithm, the general classes of problem to which the algorithm is suited, and known limitations that have been identified and described for the algorithm.

Design an Algorithm Description Template

An algorithm description template provides a structured way for you to learn about a machine learning algorithm.

You can start with a blank document and list out the section headings for the types of descriptions you need of the algorithm, for example applied, implementation, or your own personal reference cheat sheet.

To figure out what sections to include in your template, list out questions you would like to answer about the algorithm, or algorithms if you are looking to build up a reference. Some questions you could use include:

What is the standard and abbreviations used for the algorithm?

What is the information processing strategy of the algorithm?

What is the objective or goal for the algorithm?

What metaphors or analogies are commonly used to describe the behavior of the algorithm?

What is the pseudocode or flowchart description of the algorithm?

What are the heuristics or rules of thumb for using the algorithm?

What classes of problem is the algorithm well suited?

What are common benchmark or example datasets used to demonstrate the algorithm?

What are useful resources for learning more about the algorithm?

What are the primary references or resources in which the algorithm was first described?

Once you have settled on some questions, turn them into section headings.

For each section heading clearly define the requirements of the section and the form that the description in that section will take. I like to include motivating questions for each section that once answered will satisfy the section at the minimum level of detail.

Start Small and Build it Up

The beauty of this approach is that you don’t need to be an expert in the algorithm or in research. As long as you can find some resources that mention the algorithm, you can start to capture notes about an algorithm in the template.

You can start really simply and collect high-level descriptions of the algorithm, its names and abbreviations and the resources you have found and what they talk about. From here you can decide to expand the description further, or not.

You will end up with a one-to-two page description of the algorithm very quickly.

I Use It

I’ve been using algorithm templates for a long time. Some examples where I have found this strategy practically useful include:

Implementing machine learning algorithms using a descriptive-focused template.

Applying a machine learning algorithm using an applied-focused template.

Building a catalog of algorithms to use and refer to using a general purpose template.

In this last case, I turned my catalog into a book of 45 nature inspired algorithms which I published in early 2011. The book is called Clever Algorithms: Nature-Inspired Programming Recipes (affiliate link).

Summary

In this post you learned about using an algorithm description template as a strategy for learning a machine learning algorithm.

You learned that algorithm descriptions are broken and the answer to learning an algorithm effectively is to design an algorithm template that meets your needs and to fill in the template as you read and learn about the algorithm.

You learned that the template is an efficient and structured way to tackle an overwhelming problem.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
73;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-probabilistic-forecasting-model-to-predict-air-pollution-days/;2018-09-06;How to Develop a Probabilistic Forecasting Model to Predict Air Pollution Days;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68

# tune the gbm configuration from numpy import loadtxt from numpy import mean from matplotlib import pyplot from sklearn . base import clone from sklearn . metrics import brier_score_loss from sklearn . ensemble import BaggingClassifier from sklearn . ensemble import ExtraTreesClassifier from sklearn . ensemble import GradientBoostingClassifier from sklearn . ensemble import RandomForestClassifier # evaluate a sklearn model def evaluate_once ( bs_ref , template , trainX , trainy , testX , testy ) : # fit model model = clone ( template ) model . fit ( trainX , trainy ) # predict probabilities for 0 and 1 probs = model . predict_proba ( testX ) # keep the probabilities for class=1 only yhat = probs [ : , 1 ] # calculate brier score bs = brier_score_loss ( testy , yhat ) # calculate brier skill score bss = ( bs - bs_ref ) / ( 0 - bs_ref ) return bss # evaluate an sklearn model n times def evaluate ( bs_ref , model , trainX , trainy , testX , testy , n = 10 ) : scores = [ evaluate_once ( bs_ref , model , trainX , trainy , testX , testy ) for _ in range ( n ) ] print ( '>%s, bss=%.6f' % ( type ( model ) , mean ( scores ) ) ) return scores # load datasets train = loadtxt ( 'train.csv' , delimiter = ',' ) test = loadtxt ( 'test.csv' , delimiter = ',' ) # split into inputs/outputs trainX , trainy , testX , testy = train [ : , : - 1 ] , train [ : , - 1 ] , test [ : , : - 1 ] , test [ : , - 1 ] # estimate naive probabilistic forecast naive = sum ( train [ : , - 1 ] ) / train . shape [ 0 ] # forecast the test dataset yhat = [ naive for _ in range ( len ( test ) ) ] # calculate naive bs bs_ref = brier_score_loss ( testy , yhat ) # evaluate a suite of ensemble tree methods scores , names = list ( ) , list ( ) # base model = GradientBoostingClassifier ( learning_rate = 0.1 , n_estimators = 100 , subsample = 1.0 , max_depth = 3 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'base' ) # learning rate model = GradientBoostingClassifier ( learning_rate = 0.01 , n_estimators = 500 , subsample = 1.0 , max_depth = 3 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'lr' ) # depth model = GradientBoostingClassifier ( learning_rate = 0.1 , n_estimators = 100 , subsample = 0.7 , max_depth = 7 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'depth' ) # all model = GradientBoostingClassifier ( learning_rate = 0.01 , n_estimators = 500 , subsample = 0.7 , max_depth = 7 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'all' ) # plot results pyplot . boxplot ( scores , labels = names ) pyplot . show ( )"
74;machinelearningmastery.com;https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/;2019-05-26;How to Perform Object Detection With YOLOv3 in Keras;"# create a YOLOv3 Keras model and save it to file

# based on https://github.com/experiencor/keras-yolo3

import struct

import numpy as np

from keras . layers import Conv2D

from keras . layers import Input

from keras . layers import BatchNormalization

from keras . layers import LeakyReLU

from keras . layers import ZeroPadding2D

from keras . layers import UpSampling2D

from keras . layers . merge import add , concatenate

from keras . models import Model

def _conv_block ( inp , convs , skip = True ) :

x = inp

count = 0

for conv in convs :

if count == ( len ( convs ) - 2 ) and skip :

skip_connection = x

count += 1

if conv [ 'stride' ] > 1 : x = ZeroPadding2D ( ( ( 1 , 0 ) , ( 1 , 0 ) ) ) ( x ) # peculiar padding as darknet prefer left and top

x = Conv2D ( conv [ 'filter' ] ,

conv [ 'kernel' ] ,

strides = conv [ 'stride' ] ,

padding = 'valid' if conv [ 'stride' ] > 1 else 'same' , # peculiar padding as darknet prefer left and top

name = 'conv_' + str ( conv [ 'layer_idx' ] ) ,

use_bias = False if conv [ 'bnorm' ] else True ) ( x )

if conv [ 'bnorm' ] : x = BatchNormalization ( epsilon = 0.001 , name = 'bnorm_' + str ( conv [ 'layer_idx' ] ) ) ( x )

if conv [ 'leaky' ] : x = LeakyReLU ( alpha = 0.1 , name = 'leaky_' + str ( conv [ 'layer_idx' ] ) ) ( x )

return add ( [ skip_connection , x ] ) if skip else x

def make_yolov3_model ( ) :

input_image = Input ( shape = ( None , None , 3 ) )

# Layer 0 => 4

x = _conv_block ( input_image , [ { 'filter' : 32 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 0 } ,

{ 'filter' : 64 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 1 } ,

{ 'filter' : 32 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 2 } ,

{ 'filter' : 64 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 3 } ] )

# Layer 5 => 8

x = _conv_block ( x , [ { 'filter' : 128 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 5 } ,

{ 'filter' : 64 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 6 } ,

{ 'filter' : 128 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 7 } ] )

# Layer 9 => 11

x = _conv_block ( x , [ { 'filter' : 64 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 9 } ,

{ 'filter' : 128 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 10 } ] )

# Layer 12 => 15

x = _conv_block ( x , [ { 'filter' : 256 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 12 } ,

{ 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 13 } ,

{ 'filter' : 256 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 14 } ] )

# Layer 16 => 36

for i in range ( 7 ) :

x = _conv_block ( x , [ { 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 16 + i* 3 } ,

{ 'filter' : 256 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 17 + i* 3 } ] )

skip_36 = x

# Layer 37 => 40

x = _conv_block ( x , [ { 'filter' : 512 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 37 } ,

{ 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 38 } ,

{ 'filter' : 512 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 39 } ] )

# Layer 41 => 61

for i in range ( 7 ) :

x = _conv_block ( x , [ { 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 41 + i* 3 } ,

{ 'filter' : 512 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 42 + i* 3 } ] )

skip_61 = x

# Layer 62 => 65

x = _conv_block ( x , [ { 'filter' : 1024 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 62 } ,

{ 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 63 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 64 } ] )

# Layer 66 => 74

for i in range ( 3 ) :

x = _conv_block ( x , [ { 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 66 + i* 3 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 67 + i* 3 } ] )

# Layer 75 => 79

x = _conv_block ( x , [ { 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 75 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 76 } ,

{ 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 77 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 78 } ,

{ 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 79 } ] , skip = False )

# Layer 80 => 82

yolo_82 = _conv_block ( x , [ { 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 80 } ,

{ 'filter' : 255 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : False , 'leaky' : False , 'layer_idx' : 81 } ] , skip = False )

# Layer 83 => 86

x = _conv_block ( x , [ { 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 84 } ] , skip = False )

x = UpSampling2D ( 2 ) ( x )

x = concatenate ( [ x , skip_61 ] )

# Layer 87 => 91

x = _conv_block ( x , [ { 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 87 } ,

{ 'filter' : 512 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 88 } ,

{ 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 89 } ,

{ 'filter' : 512 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 90 } ,

{ 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 91 } ] , skip = False )

# Layer 92 => 94

yolo_94 = _conv_block ( x , [ { 'filter' : 512 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 92 } ,

{ 'filter' : 255 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : False , 'leaky' : False , 'layer_idx' : 93 } ] , skip = False )

# Layer 95 => 98

x = _conv_block ( x , [ { 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 96 } ] , skip = False )

x = UpSampling2D ( 2 ) ( x )

x = concatenate ( [ x , skip_36 ] )

# Layer 99 => 106

yolo_106 = _conv_block ( x , [ { 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 99 } ,

{ 'filter' : 256 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 100 } ,

{ 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 101 } ,

{ 'filter' : 256 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 102 } ,

{ 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 103 } ,

{ 'filter' : 256 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 104 } ,

{ 'filter' : 255 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : False , 'leaky' : False , 'layer_idx' : 105 } ] , skip = False )

model = Model ( input_image , [ yolo_82 , yolo_94 , yolo_106 ] )

return model

class WeightReader :

def __init__ ( self , weight_file ) :

with open ( weight_file , 'rb' ) as w_f :

major , = struct . unpack ( 'i' , w_f . read ( 4 ) )

minor , = struct . unpack ( 'i' , w_f . read ( 4 ) )

revision , = struct . unpack ( 'i' , w_f . read ( 4 ) )

if ( major* 10 + minor ) >= 2 and major < 1000 and minor < 1000 :

w_f . read ( 8 )

else :

w_f . read ( 4 )

transpose = ( major > 1000 ) or ( minor > 1000 )

binary = w_f . read ( )

self . offset = 0

self . all_weights = np . frombuffer ( binary , dtype = 'float32' )

def read_bytes ( self , size ) :

self . offset = self . offset + size

return self . all_weights [ self . offset - size : self . offset ]

def load_weights ( self , model ) :

for i in range ( 106 ) :

try :

conv_layer = model . get_layer ( 'conv_' + str ( i ) )

print ( ""loading weights of convolution #"" + str ( i ) )

if i not in [ 81 , 93 , 105 ] :

norm_layer = model . get_layer ( 'bnorm_' + str ( i ) )

size = np . prod ( norm_layer . get_weights ( ) [ 0 ] . shape )

beta = self . read_bytes ( size ) # bias

gamma = self . read_bytes ( size ) # scale

mean = self . read_bytes ( size ) # mean

var = self . read_bytes ( size ) # variance

weights = norm_layer . set_weights ( [ gamma , beta , mean , var ] )

if len ( conv_layer . get_weights ( ) ) > 1 :

bias = self . read_bytes ( np . prod ( conv_layer . get_weights ( ) [ 1 ] . shape ) )

kernel = self . read_bytes ( np . prod ( conv_layer . get_weights ( ) [ 0 ] . shape ) )

kernel = kernel . reshape ( list ( reversed ( conv_layer . get_weights ( ) [ 0 ] . shape ) ) )

kernel = kernel . transpose ( [ 2 , 3 , 1 , 0 ] )

conv_layer . set_weights ( [ kernel , bias ] )

else :

kernel = self . read_bytes ( np . prod ( conv_layer . get_weights ( ) [ 0 ] . shape ) )

kernel = kernel . reshape ( list ( reversed ( conv_layer . get_weights ( ) [ 0 ] . shape ) ) )

kernel = kernel . transpose ( [ 2 , 3 , 1 , 0 ] )

conv_layer . set_weights ( [ kernel ] )

except ValueError :

print ( ""no convolution #"" + str ( i ) )

def reset ( self ) :

self . offset = 0

# define the model

model = make_yolov3_model ( )

# load the model weights

weight_reader = WeightReader ( 'yolov3.weights' )

# set the model weights into the model

weight_reader . load_weights ( model )

# save the model to file"
75;news.mit.edu;http://news.mit.edu/2020/mit-csail-sprayabletech-sprayable-user-interfaces-0408;;Sprayable user interfaces;"For decades researchers have envisioned a world where digital user interfaces are seamlessly integrated with the physical environment, until the two are virtually indistinguishable from one another.

This vision, though, is held up by a few boundaries. First, it’s difficult to integrate sensors and display elements into our tangible world due to various design constraints. Second, most methods to do so are limited to smaller scales, bound by the size of the fabricating device.

Recently, a group of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with SprayableTech, a system that lets users create room-sized interactive surfaces with sensors and displays. The system, which uses airbrushing of functional inks, enables various displays, like interactive sofas with embedded sensors to control your television, and sensors for adjusting lighting and temperature through your walls.

SprayableTech lets users channel their inner Picassos: After designing your interactive artwork in the 3D editor, it automatically generates stencils for airbrushing the layout onto a surface. Once they’ve created the stencils from cardboard, a user can then add sensors to the desired surface, whether it’s a sofa, a wall, or even a building, to control various appliances like your lamp or television. (An alternate option to stenciling is projecting them digitally.)

“Since SprayableTech is so flexible in its application, you can imagine using this type of system beyond walls and surfaces to power larger-scale entities like interactive smart cities and interactive architecture in public places,” says Michael Wessely, postdoc in CSAIL and lead author on a new paper about SprayableTech. “We view this as a tool that will allow humans to interact with and use their environment in newfound ways.”

The race for the smartest home has now been in the works for some time, with a large interest in sensor technology. It’s a big advance from the enormous glass wall displays with quick-shifting images and screens we’ve seen in countless dystopian films.

The MIT researchers’ approach is focusing on scale, and creative expression. By using the airbrush technology, they’re no longer limited to the size of the printer, the area of the screen-printing net, or the size of the hydrographic bath — and there’s thousands of possible design options.

Let’s say a user wanted to design a tree symbol on their wall to control the ambient light in the room. To start the process, they would use a toolkit in a 3D editor to design their digital object, and customize for things like proximity sensors, touch buttons, sliders, and electroluminescent displays.

Then, the toolkit would output the choice of stencils: fabricated stencils cut from cardboard, which are great for high-precision spraying on simple, flat, surfaces, or projected stencils, which are less precise, but better for doubly-curved surfaces.

Designers can then spray on the functional ink, which is ink with electrically functional elements, using an airbrush. As a final step to get the system going, a microcontroller is attached that connects the interface to the board that runs the code for sensing and visual output.

The team tested the system on a variety of items, including:

a musical interface on a concrete pillar;

an interactive sofa that’s connected to a television;

a wall display for controlling light; and

a street post with a touchable display that provides audible information on subway stations and local attractions.

Since the stencils need to be created in advance via the digital editor, it reduces the opportunity for spontaneous exploration. Looking forward, the team wants to explore so-called “modular” stencils that create touch buttons of different sizes, as well as shape-changing stencils that adjust themselves based on a desired user interface shape.

“In the future, we aim to collaborate with graffiti artists and architects to explore the future potential for large-scale user interfaces in enabling the internet of things for smart cities and interactive homes,” says Wessely.

Wessely wrote the paper alongside MIT PhD student Ticha Sethapakdi, MIT undergraduate students Carlos Castillo and Jackson C. Snowden, MIT postdoc Isabel P.S. Qamar, MIT Professor Stefanie Mueller, University of Bristol PhD student Ollie Hanton, University of Bristol Professor Mike Fraser, and University of Bristol Associate Professor Anne Roudaut."
76;machinelearningmastery.com;https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/;2018-12-18;Ensemble Learning Methods for Deep Learning Neural Networks;"Tweet Share Share

Last Updated on August 6, 2019

How to Improve Performance By Combining Predictions From Multiple Models.

Deep learning neural networks are nonlinear methods.

They offer increased flexibility and can scale in proportion to the amount of training data available. A downside of this flexibility is that they learn via a stochastic training algorithm which means that they are sensitive to the specifics of the training data and may find a different set of weights each time they are trained, which in turn produce different predictions.

Generally, this is referred to as neural networks having a high variance and it can be frustrating when trying to develop a final model to use for making predictions.

A successful approach to reducing the variance of neural network models is to train multiple models instead of a single model and to combine the predictions from these models. This is called ensemble learning and not only reduces the variance of predictions but also can result in predictions that are better than any single model.

In this post, you will discover methods for deep learning neural networks to reduce variance and improve prediction performance.

After reading this post, you will know:

Neural network models are nonlinear and have a high variance, which can be frustrating when preparing a final model for making predictions.

Ensemble learning combines the predictions from multiple neural network models to reduce the variance of predictions and reduce generalization error.

Techniques for ensemble learning can be grouped by the element that is varied, such as training data, the model, and how predictions are combined.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

High Variance of Neural Network Models Reduce Variance Using an Ensemble of Models How to Ensemble Neural Network Models Summary of Ensemble Techniques

High Variance of Neural Network Models

Training deep neural networks can be very computationally expensive.

Very deep networks trained on millions of examples may take days, weeks, and sometimes months to train.

Google’s baseline model […] was a deep convolutional neural network […] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.

— Distilling the Knowledge in a Neural Network, 2015.

After the investment of so much time and resources, there is no guarantee that the final model will have low generalization error, performing well on examples not seen during training.

… train many different candidate networks and then to select the best, […] and to discard the rest. There are two disadvantages with such an approach. First, all of the effort involved in training the remaining networks is wasted. Second, […] the network which had best performance on the validation set might not be the one with the best performance on new test data.

— Pages 364-365, Neural Networks for Pattern Recognition, 1995.

Neural network models are a nonlinear method. This means that they can learn complex nonlinear relationships in the data. A downside of this flexibility is that they are sensitive to initial conditions, both in terms of the initial random weights and in terms of the statistical noise in the training dataset.

This stochastic nature of the learning algorithm means that each time a neural network model is trained, it may learn a slightly (or dramatically) different version of the mapping function from inputs to outputs, that in turn will have different performance on the training and holdout datasets.

As such, we can think of a neural network as a method that has a low bias and high variance. Even when trained on large datasets to satisfy the high variance, having any variance in a final model that is intended to be used to make predictions can be frustrating.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Reduce Variance Using an Ensemble of Models

A solution to the high variance of neural networks is to train multiple models and combine their predictions.

The idea is to combine the predictions from multiple good but different models.

A good model has skill, meaning that its predictions are better than random chance. Importantly, the models must be good in different ways; they must make different prediction errors.

The reason that model averaging works is that different models will usually not make all the same errors on the test set.

— Page 256, Deep Learning, 2016.

Combining the predictions from multiple neural networks adds a bias that in turn counters the variance of a single trained neural network model. The results are predictions that are less sensitive to the specifics of the training data, choice of training scheme, and the serendipity of a single training run.

In addition to reducing the variance in the prediction, the ensemble can also result in better predictions than any single best model.

… the performance of a committee can be better than the performance of the best single network used in isolation.

— Page 365, Neural Networks for Pattern Recognition, 1995.

This approach belongs to a general class of methods called “ensemble learning” that describes methods that attempt to make the best use of the predictions from multiple models prepared for the same problem.

Generally, ensemble learning involves training more than one network on the same dataset, then using each of the trained models to make a prediction before combining the predictions in some way to make a final outcome or prediction.

In fact, ensembling of models is a standard approach in applied machine learning to ensure that the most stable and best possible prediction is made.

For example, Alex Krizhevsky, et al. in their famous 2012 paper titled “Imagenet classification with deep convolutional neural networks” that introduced very deep convolutional neural networks for photo classification (i.e. AlexNet) used model averaging across multiple well-performing CNN models to achieve state-of-the-art results at the time. Performance of one model was compared to ensemble predictions averaged over two, five, and seven different models.

Averaging the predictions of five similar CNNs gives an error rate of 16.4%. […] Averaging the predictions of two CNNs that were pre-trained […] with the aforementioned five CNNs gives an error rate of 15.3%.

Ensembling is also the approach used by winners in machine learning competitions.

Another powerful technique for obtaining the best possible results on a task is model ensembling. […] If you look at machine-learning competitions, in particular on Kaggle, you’ll see that the winners use very large ensembles of models that inevitably beat any single model, no matter how good.

— Page 264, Deep Learning With Python, 2017.

How to Ensemble Neural Network Models

Perhaps the oldest and still most commonly used ensembling approach for neural networks is called a “committee of networks.”

A collection of networks with the same configuration and different initial random weights is trained on the same dataset. Each model is then used to make a prediction and the actual prediction is calculated as the average of the predictions.

The number of models in the ensemble is often kept small both because of the computational expense in training models and because of the diminishing returns in performance from adding more ensemble members. Ensembles may be as small as three, five, or 10 trained models.

The field of ensemble learning is well studied and there are many variations on this simple theme.

It can be helpful to think of varying each of the three major elements of the ensemble method; for example:

Training Data : Vary the choice of data used to train each model in the ensemble.

: Vary the choice of data used to train each model in the ensemble. Ensemble Models : Vary the choice of the models used in the ensemble.

: Vary the choice of the models used in the ensemble. Combinations: Vary the choice of the way that outcomes from ensemble members are combined.

Let’s take a closer look at each element in turn.

Varying Training Data

The data used to train each member of the ensemble can be varied.

The simplest approach would be to use k-fold cross-validation to estimate the generalization error of the chosen model configuration. In this procedure, k different models are trained on k different subsets of the training data. These k models can then be saved and used as members of an ensemble.

Another popular approach involves resampling the training dataset with replacement, then training a network using the resampled dataset. The resampling procedure means that the composition of each training dataset is different with the possibility of duplicated examples allowing the model trained on the dataset to have a slightly different expectation of the density of the samples, and in turn different generalization error.

This approach is called bootstrap aggregation, or bagging for short, and was designed for use with unpruned decision trees that have high variance and low bias. Typically a large number of decision trees are used, such as hundreds or thousands, given that they are fast to prepare.

… a natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. […] Of course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set.

— Pages 216-317, An Introduction to Statistical Learning with Applications in R, 2013.

An equivalent approach might be to use a smaller subset of the training dataset without regularization to allow faster training and some overfitting.

The desire for slightly under-optimized models applies to the selection of ensemble members more generally.

… the members of the committee should not individually be chosen to have optimal trade-off between bias and variance, but should have relatively smaller bias, since the extra variance can be removed by averaging.

— Page 366, Neural Networks for Pattern Recognition, 1995.

Other approaches may involve selecting a random subspace of the input space to allocate to each model, such as a subset of the hyper-volume in the input space or a subset of input features.

Ensemble Tutorials

For examples of deep learning ensembles that vary training data see:

Varying Models

Training the same under-constrained model on the same data with different initial conditions will result in different models given the difficulty of the problem, and the stochastic nature of the learning algorithm.

This is because the optimization problem that the network is trying to solve is so challenging that there are many “good” and “different” solutions to map inputs to outputs.

Most neural network algorithms achieve sub-optimal performance specifically due to the existence of an overwhelming number of sub-optimal local minima. If we take a set of neural networks which have converged to local minima and apply averaging we can construct an improved estimate. One way to understand this fact is to consider that, in general, networks which have fallen into different local minima will perform poorly in different regions of feature space and thus their error terms will not be strongly correlated.

— When networks disagree: Ensemble methods for hybrid neural networks, 1995.

This may result in a reduced variance, but may not dramatically improve generalization error. The errors made by the models may still be too highly correlated because the models all have learned similar mapping functions.

An alternative approach might be to vary the configuration of each ensemble model, such as using networks with different capacity (e.g. number of layers or nodes) or models trained under different conditions (e.g. learning rate or regularization).

The result may be an ensemble of models that have learned a more heterogeneous collection of mapping functions and in turn have a lower correlation in their predictions and prediction errors.

Differences in random initialization, random selection of minibatches, differences in hyperparameters, or different outcomes of non-deterministic implementations of neural networks are often enough to cause different members of the ensemble to make partially independent errors.

— Pages 257-258, Deep Learning, 2016.

Such an ensemble of differently configured models can be achieved through the normal process of developing the network and tuning its hyperparameters. Each model could be saved during this process and a subset of better models chosen to comprise the ensemble.

Slightly inferiorly trained networks are a free by-product of most tuning algorithms; it is desirable to use such extra copies even when their performance is significantly worse than the best performance found. Better performance yet can be achieved through careful planning for an ensemble classification by using the best available parameters and training different copies on different subsets of the available database.

— Neural Network Ensembles, 1990.

In cases where a single model may take weeks or months to train, another alternative may be to periodically save the best model during the training process, called snapshot or checkpoint models, then select ensemble members among the saved models. This provides the benefits of having multiple models trained on the same data, although collected during a single training run.

Snapshot Ensembling produces an ensemble of accurate and diverse models from a single training process. At the heart of Snapshot Ensembling is an optimization process which visits several local minima before converging to a final solution. We take model snapshots at these various minima, and average their predictions at test time.

— Snapshot Ensembles: Train 1, get M for free, 2017.

A variation on the Snapshot ensemble is to save models from a range of epochs, perhaps identified by reviewing learning curves of model performance on the train and validation datasets during training. Ensembles from such contiguous sequences of models are referred to as horizontal ensembles.

First, networks trained for a relatively stable range of epoch are selected. The predictions of the probability of each label are produced by standard classifiers [over] the selected epoch[s], and then averaged.

— Horizontal and vertical ensemble with deep representation for classification, 2013.

A further enhancement of the snapshot ensemble is to systematically vary the optimization procedure during training to force different solutions (i.e. sets of weights), the best of which can be saved to checkpoints. This might involve injecting an oscillating amount of noise over training epochs or oscillating the learning rate during training epochs. A variation of this approach called Stochastic Gradient Descent with Warm Restarts (SGDR) demonstrated faster learning and state-of-the-art results for standard photo classification tasks.

Our SGDR simulates warm restarts by scheduling the learning rate to achieve competitive results […] roughly two to four times faster. We also achieved new state-of-the-art results with SGDR, mainly by using even wider [models] and ensembles of snapshots from SGDR’s trajectory.

— SGDR: Stochastic Gradient Descent with Warm Restarts, 2016.

A benefit of very deep neural networks is that the intermediate hidden layers provide a learned representation of the low-resolution input data. The hidden layers can output their internal representations directly, and the output from one or more hidden layers from one very deep network can be used as input to a new classification model. This is perhaps most effective when the deep model is trained using an autoencoder model. This type of ensemble is referred to as a vertical ensemble.

This method ensembles a series of classifiers whose inputs are the representation of intermediate layers. A lower error rate is expected because these features seem diverse.

— Horizontal and vertical ensemble with deep representation for classification, 2013.

Ensemble Tutorials

For examples of deep learning ensembles that vary models see:

Varying Combinations

The simplest way to combine the predictions is to calculate the average of the predictions from the ensemble members.

This can be improved slightly by weighting the predictions from each model, where the weights are optimized using a hold-out validation dataset. This provides a weighted average ensemble that is sometimes called model blending.

… we might expect that some members of the committee will typically make better predictions than other members. We would therefore expect to be able to reduce the error still further if we give greater weight to some committee members than to others. Thus, we consider a generalized committee prediction given by a weighted combination of the predictions of the members …

— Page 367, Neural Networks for Pattern Recognition, 1995.

One further step in complexity involves using a new model to learn how to best combine the predictions from each ensemble member.

The model could be a simple linear model (e.g. much like the weighted average), but could be a sophisticated nonlinear method that also considers the specific input sample in addition to the predictions provided by each member. This general approach of learning a new model is called model stacking, or stacked generalization.

Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. […] When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question.

— Stacked generalization, 1992.

There are more sophisticated methods for stacking models, such as boosting where ensemble members are added one at a time in order to correct the mistakes of prior models. The added complexity means this approach is less often used with large neural network models.

Another combination that is a little bit different is to combine the weights of multiple neural networks with the same structure. The weights of multiple networks can be averaged, to hopefully result in a new single model that has better overall performance than any original model. This approach is called model weight averaging.

… suggests it is promising to average these points in weight space, and use a network with these averaged weights, instead of forming an ensemble by averaging the outputs of networks in model space

— Averaging Weights Leads to Wider Optima and Better Generalization, 2018.

Ensemble Tutorials

For examples of deep learning ensembles that vary combinations see:

Summary of Ensemble Techniques

In summary, we can list some of the more common and interesting ensemble methods for neural networks organized by each element of the method that can be varied, as follows:

There is no single best ensemble method; perhaps experiment with a few approaches or let the constraints of your project guide you.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Papers

Articles

Summary

In this post, you discovered ensemble methods for deep learning neural networks to reduce variance and improve prediction performance.

Specifically, you learned:

Neural network models are nonlinear and have a high variance, which can be frustrating when preparing a final model for making predictions.

Ensemble learning combines the predictions from multiple neural network models to reduce the variance of predictions and reduce generalization error.

Techniques for ensemble learning can be grouped by the element that is varied, such as training data, the model, and how predictions are combined.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
77;machinelearningmastery.com;https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/;2017-07-11;How to One Hot Encode Sequence Data in Python;"from numpy import argmax

# define input string

data = 'hello world'

print ( data )

# define universe of possible input values

alphabet = 'abcdefghijklmnopqrstuvwxyz '

# define a mapping of chars to integers

char_to_int = dict ( ( c , i ) for i , c in enumerate ( alphabet ) )

int_to_char = dict ( ( i , c ) for i , c in enumerate ( alphabet ) )

# integer encode input data

integer_encoded = [ char_to_int [ char ] for char in data ]

print ( integer_encoded )

# one hot encode

onehot_encoded = list ( )

for value in integer_encoded :

letter = [ 0 for _ in range ( len ( alphabet ) ) ]

letter [ value ] = 1

onehot_encoded . append ( letter )

print ( onehot_encoded )

# invert encoding

inverted = int_to_char [ argmax ( onehot_encoded [ 0 ] ) ]"
78;machinelearningmastery.com;http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/;2016-06-30;Object Classification with CNNs using the Keras Deep Learning Library;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55

# Large CNN model for the CIFAR-10 Dataset from keras . datasets import cifar10 from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from keras . layers import Flatten from keras . constraints import maxnorm from keras . optimizers import SGD from keras . layers . convolutional import Conv2D from keras . layers . convolutional import MaxPooling2D from keras . utils import np_utils # load data ( X_train , y_train ) , ( X_test , y_test ) = cifar10 . load_data ( ) # normalize inputs from 0-255 to 0.0-1.0 X_train = X_train . astype ( 'float32' ) X_test = X_test . astype ( 'float32' ) X_train = X_train / 255.0 X_test = X_test / 255.0 # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ] # Create the model model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , input_shape = ( 32 , 32 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Flatten ( ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Dense ( 1024 , activation = 'relu' , kernel_constraint = maxnorm ( 3 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Dense ( 512 , activation = 'relu' , kernel_constraint = maxnorm ( 3 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Dense ( num_classes , activation = 'softmax' ) ) # Compile model epochs = 25 lrate = 0.01 decay = lrate / epochs sgd = SGD ( lr = lrate , momentum = 0.9 , decay = decay , nesterov = False ) model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ] ) model . summary ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = epochs , batch_size = 64 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Accuracy: %.2f%%"" % ( scores [ 1 ] * 100 ) )"
79;news.mit.edu;http://news.mit.edu/2020/three-mit-awarded-2020-guggenheim-fellowships-0414;;Three from MIT awarded 2020 Guggenheim Fellowships;"MIT faculty members Sabine Iatridou, Jonathan Gruber, and Rebecca Saxe are among 175 scientists, artists, and scholars awarded 2020 fellowships from the John Simon Guggenheim Foundation. Appointed on the basis of prior achievement and exceptional promise, the 2020 Guggenheim Fellows were selected from almost 3,000 applicants.

“It’s exceptionally encouraging to be able to share such positive news at this terribly challenging time” says Edward Hirsch, president of the foundation. “A Guggenheim Fellowship has always offered practical assistance, helping fellows do their work, but for many of the new fellows, it may be a lifeline at a time of hardship, a survival tool as well as a creative one.”

Since 1925, the foundation has granted more the $375 million in fellowships to over 18,000 individuals, including Nobel laureates, Fields medalists, poets laureate, and winners of the Pulitzer Prize, among other internationally recognized honors. This year’s MIT recipients include a linguist, an economist, and a cognitive neuroscientist.

Sabine Iatridou is professor of linguistics in MIT's Department of Linguistics and Philosophy. Her work focuses on syntax and the syntax-semantics interface, as well as comparative linguistics. She is the author and coauthor of a series of innovative papers about tense and modality that opened up whole new domains of research for the field. Since those publications, she has made foundational contributions to many branches of linguistics that connect form with meaning. She is the recipient of the National Young Investigator Award (USA), of an honorary doctorate from the University of Crete in Greece, and of an award from the Royal Dutch Academy of Sciences. She was elected fellow of the Linguistic Society of America. She is co-founder and co-director of the CreteLing Summer School of Linguistics.

Jonathan Gruber is the Ford Professor of Economics at MIT, the director of the Health Care Program at the National Bureau of Economic Research, and the former president of the American Society of Health Economists. He has published more than 175 research articles, has edited six research volumes, and is the author of “Public Finance and Public Policy,” a leading undergraduate text; “Health Care Reform,” a graphic novel; and “Jump-Starting America: How Breakthrough Science Can Revive Economic Growth and the American Dream.” In 2006 he received the American Society of Health Economists Inaugural Medal for the best health economist in the nation aged 40 and under. He served as deputy sssistant secretary for economic policy at the U.S. Department of the Treasury. He was a key architect of Massachusetts' ambitious health reform effort, and became an inaugural member of the Health Connector Board, the main implementing body for that effort. He served as a technical consultant to the Obama administration and worked with both the administration and Congress to help craft the Affordable Care Act. In 2011, he was named “One of the Top 25 Most Innovative and Practical Thinkers of Our Time” by Slate magazine.

Rebecca Saxe is an associate investigator of the McGovern Institute and the John W. Jarve (1978) Professor in Brain and Cognitive Sciences. She studies human social cognition, using a combination of behavioral testing and brain imaging technologies. She is best known for her work on brain regions specialized for abstract concepts such as “theory of mind” tasks that involve understanding the mental states of other people. She also studies the development of the human brain during early infancy. She obtained her PhD from MIT and was a Harvard University junior fellow before joining the MIT faculty in 2006. Saxe was chosen in 2012 as a Young Global Leader by the World Economic Forum, and she received the 2014 Troland Award from the National Academy of Sciences. Her TED Talk, “How we read each other’s minds” has been viewed over 3 million times.

“As we grapple with the difficulties of the moment, it is also important to look to the future,” says Hirsch. “The artists, writers, scholars, and scientific researchers supported by the fellowship will help us understand and learn from what we are enduring individually and collectively, and it is an honor for the foundation to help them do their essential work.”"
80;machinelearningmastery.com;http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/;2016-07-03;How to Predict Sentiment From Movie Reviews Using Deep Learning (Text Classification);"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

# CNN for the IMDB problem from keras . datasets import imdb from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D from keras . layers . embeddings import Embedding from keras . preprocessing import sequence # load the dataset but only keep the top n words, zero the rest top_words = 5000 ( X_train , y_train ) , ( X_test , y_test ) = imdb . load_data ( num_words = top_words ) # pad dataset to a maximum review length in words max_words = 500 X_train = sequence . pad_sequences ( X_train , maxlen = max_words ) X_test = sequence . pad_sequences ( X_test , maxlen = max_words ) # create the model model = Sequential ( ) model . add ( Embedding ( top_words , 32 , input_length = max_words ) ) model . add ( Conv1D ( 32 , 3 , padding = 'same' , activation = 'relu' ) ) model . add ( MaxPooling1D ( ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 250 , activation = 'relu' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 2 , batch_size = 128 , verbose = 2 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Accuracy: %.2f%%"" % ( scores [ 1 ] * 100 ) )"
81;machinelearningmastery.com;https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/;2017-05-09;Multi-step Time Series Forecasting with Long Short-Term Memory Networks in Python;"from pandas import DataFrame

from pandas import Series

from pandas import concat

from pandas import read_csv

from pandas import datetime

from sklearn . metrics import mean_squared_error

from sklearn . preprocessing import MinMaxScaler

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from math import sqrt

from matplotlib import pyplot

from numpy import array

# date-time parsing function for loading the dataset

def parser ( x ) :

return datetime . strptime ( '190' + x , '%Y-%m' )

# convert time series into supervised learning problem

def series_to_supervised ( data , n_in = 1 , n_out = 1 , dropnan = True ) :

n_vars = 1 if type ( data ) is list else data . shape [ 1 ]

df = DataFrame ( data )

cols , names = list ( ) , list ( )

# input sequence (t-n, ... t-1)

for i in range ( n_in , 0 , - 1 ) :

cols . append ( df . shift ( i ) )

names += [ ( 'var%d(t-%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ]

# forecast sequence (t, t+1, ... t+n)

for i in range ( 0 , n_out ) :

cols . append ( df . shift ( - i ) )

if i == 0 :

names += [ ( 'var%d(t)' % ( j + 1 ) ) for j in range ( n_vars ) ]

else :

names += [ ( 'var%d(t+%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ]

# put it all together

agg = concat ( cols , axis = 1 )

agg . columns = names

# drop rows with NaN values

if dropnan :

agg . dropna ( inplace = True )

return agg

# create a differenced series

def difference ( dataset , interval = 1 ) :

diff = list ( )

for i in range ( interval , len ( dataset ) ) :

value = dataset [ i ] - dataset [ i - interval ]

diff . append ( value )

return Series ( diff )

# transform series into train and test sets for supervised learning

def prepare_data ( series , n_test , n_lag , n_seq ) :

# extract raw values

raw_values = series . values

# transform data to be stationary

diff_series = difference ( raw_values , 1 )

diff_values = diff_series . values

diff_values = diff_values . reshape ( len ( diff_values ) , 1 )

# rescale values to -1, 1

scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) )

scaled_values = scaler . fit_transform ( diff_values )

scaled_values = scaled_values . reshape ( len ( scaled_values ) , 1 )

# transform into supervised learning problem X, y

supervised = series_to_supervised ( scaled_values , n_lag , n_seq )

supervised_values = supervised . values

# split into train and test sets

train , test = supervised_values [ 0 : - n_test ] , supervised_values [ - n_test : ]

return scaler , train , test

# fit an LSTM network to training data

def fit_lstm ( train , n_lag , n_seq , n_batch , nb_epoch , n_neurons ) :

# reshape training into [samples, timesteps, features]

X , y = train [ : , 0 : n_lag ] , train [ : , n_lag : ]

X = X . reshape ( X . shape [ 0 ] , 1 , X . shape [ 1 ] )

# design network

model = Sequential ( )

model . add ( LSTM ( n_neurons , batch_input_shape = ( n_batch , X . shape [ 1 ] , X . shape [ 2 ] ) , stateful = True ) )

model . add ( Dense ( y . shape [ 1 ] ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

# fit network

for i in range ( nb_epoch ) :

model . fit ( X , y , epochs = 1 , batch_size = n_batch , verbose = 0 , shuffle = False )

model . reset_states ( )

return model

# make one forecast with an LSTM,

def forecast_lstm ( model , X , n_batch ) :

# reshape input pattern to [samples, timesteps, features]

X = X . reshape ( 1 , 1 , len ( X ) )

# make forecast

forecast = model . predict ( X , batch_size = n_batch )

# convert to array

return [ x for x in forecast [ 0 , : ] ]

# evaluate the persistence model

def make_forecasts ( model , n_batch , train , test , n_lag , n_seq ) :

forecasts = list ( )

for i in range ( len ( test ) ) :

X , y = test [ i , 0 : n_lag ] , test [ i , n_lag : ]

# make forecast

forecast = forecast_lstm ( model , X , n_batch )

# store the forecast

forecasts . append ( forecast )

return forecasts

# invert differenced forecast

def inverse_difference ( last_ob , forecast ) :

# invert first forecast

inverted = list ( )

inverted . append ( forecast [ 0 ] + last_ob )

# propagate difference forecast using inverted first value

for i in range ( 1 , len ( forecast ) ) :

inverted . append ( forecast [ i ] + inverted [ i - 1 ] )

return inverted

# inverse data transform on forecasts

def inverse_transform ( series , forecasts , scaler , n_test ) :

inverted = list ( )

for i in range ( len ( forecasts ) ) :

# create array from forecast

forecast = array ( forecasts [ i ] )

forecast = forecast . reshape ( 1 , len ( forecast ) )

# invert scaling

inv_scale = scaler . inverse_transform ( forecast )

inv_scale = inv_scale [ 0 , : ]

# invert differencing

index = len ( series ) - n_test + i - 1

last_ob = series . values [ index ]

inv_diff = inverse_difference ( last_ob , inv_scale )

# store

inverted . append ( inv_diff )

return inverted

# evaluate the RMSE for each forecast time step

def evaluate_forecasts ( test , forecasts , n_lag , n_seq ) :

for i in range ( n_seq ) :

actual = [ row [ i ] for row in test ]

predicted = [ forecast [ i ] for forecast in forecasts ]

rmse = sqrt ( mean_squared_error ( actual , predicted ) )

print ( 't+%d RMSE: %f' % ( ( i + 1 ) , rmse ) )

# plot the forecasts in the context of the original dataset

def plot_forecasts ( series , forecasts , n_test ) :

# plot the entire dataset in blue

pyplot . plot ( series . values )

# plot the forecasts in red

for i in range ( len ( forecasts ) ) :

off_s = len ( series ) - n_test + i - 1

off_e = off_s + len ( forecasts [ i ] ) + 1

xaxis = [ x for x in range ( off_s , off_e ) ]

yaxis = [ series . values [ off_s ] ] + forecasts [ i ]

pyplot . plot ( xaxis , yaxis , color = 'red' )

# show the plot

pyplot . show ( )

# load dataset

series = read_csv ( 'shampoo-sales.csv' , header = 0 , parse_dates = [ 0 ] , index_col = 0 , squeeze = True , date_parser = parser )

# configure

n_lag = 1

n_seq = 3

n_test = 10

n_epochs = 1500

n_batch = 1

n_neurons = 1

# prepare data

scaler , train , test = prepare_data ( series , n_test , n_lag , n_seq )

# fit model

model = fit_lstm ( train , n_lag , n_seq , n_batch , n_epochs , n_neurons )

# make forecasts

forecasts = make_forecasts ( model , n_batch , train , test , n_lag , n_seq )

# inverse transform forecasts and test

forecasts = inverse_transform ( series , forecasts , scaler , n_test + 2 )

actual = [ row [ n_lag : ] for row in test ]

actual = inverse_transform ( series , actual , scaler , n_test + 2 )

# evaluate forecasts

evaluate_forecasts ( actual , forecasts , n_lag , n_seq )

# plot forecasts"
82;news.mit.edu;http://news.mit.edu/2020/mit-idss-promoting-women-data-science-0417;;Annual Women in Data Science conference discusses fake news;"What do radiation waves from space, the U.S. Census, and the human genome have in common? All three, like so many things today, involve massive amounts of data. These data can unlock insights and lead to new solutions and better decision-making — for those who have the knowledge and tools to analyze it.

The impressive variety of applications for data science tools and techniques were on display at the Women in Data Science Conference (WiDS Cambridge), held at the Microsoft NERD Center in early March, before MIT and the Commonwealth of Massachusetts began to de-densify in response to the Covid-19 emergency. Co-hosted by the Institute for Data, Systems, and Society (IDSS), the Harvard Institute for Applied Computational Science, and Microsoft Research New England, WiDS Cambridge is one of dozens of satellite WiDS events around the world. The program showcases women who are not only using data science tools in their research or business, but who are leaders refining those tools and recruiting more women into the field.

The day’s signature event was a panel discussion on data science and fake news called “Data weaponized, data scrutinized: a war on information.” The panel was moderated by Manon Revel, a doctoral student in the IDSS Social and Engineering Systems (SES) program whose research has analyzed popup ads to see how exposure influences readers' assessment of news credibility. Addressing current challenges, Manon shared: “Understanding the effect of false information and combatting it is crucial. It requires thinking through the technology design, but also the regulatory framework and the political and social context.”

The panel also included Camille Francois, chief information officer for Graphika, a social network analysis startup that uses AI to understand online communities. “We don’t know how to measure the impact of foreign interference for many complicated reasons,” said Francois. “The aim of a foreign interference campaign is not necessarily to impact a vote. It’s to divide, it’s to confuse, and it’s to create chaos. How do you measure chaos?”

In addition to the discussion on misinformation, WiDS Cambridge featured a wide variety of insights from industry and academia. Asu Ozdaglar, deputy dean of academics, head of the Department of Electrical Engineering and Computer Science, and faculty member in IDSS and the Laboratory for Information and Decision Systems (LIDS), highlighted robustness in machine learning. Citing the common example of image classification system errors, she explored how ‘perturbed’ data can, with small variations, disrupt otherwise accurate models, and offered a ‘minmax’ approach using generative adversarial networks (GANs) to increase robustness.

For an industry perspective, Jess Stauth, managing director at Fidelity Labs, provided ways to apply basic research principles to modern data science business problems. Data science is a collection of tools from statistics to computing, she says, and businesses require infrastructure to use them to create tangible business value. “A data scientist alone in a room with a laptop is probably not going to be all that successful,” she muses.

The conference provided opportunities for participants to network and job search, with sponsor companies hosting recruiting tables and answering questions. WiDS also empowered newer practitioners with a student and postdoc poster session and lightning talks. Over 30 poster presenters participated, showcasing work in fields as diverse as demographic bias in natural language processing, crime prediction, neurodegenerative disease, and sustainable buildings.

“WiDS is a wonderful event where you can interact with your peers, present your research, and build confidence,” says Marie Charpignon, a graduate student in MIT’s SES PhD program who presented a poster on using causal inference on electronic health records to explore repurposing diabetes medication to treat dementia. “The conference brings together students, professors, industry researchers, and even venture capitalists in search of promising ideas. WiDS gives you a sense of the myriad paths you could take after graduation.”"
83;news.mit.edu;http://news.mit.edu/2020/3-questions-ron-rivest-trusting-electronic-voting-systems-0226;2020-03-19;3 Questions: Ron Rivest on trusting electronic voting systems;"Ron Rivest is an MIT Institute Professor in the Department of Electrical Engineering and Computer Science. He’s an authority on algorithms and an inventor of the RSA public-key cryptosystem, one of the most widely used algorithms to securely transmit data. Since the 1980s, he’s taught students how to use cryptography to help secure voting systems. Then, in 2000, an historic recount in Florida determined the outcome of the U.S. presidential election, and the Caltech / MIT Voting Technology Project was founded with the mission to secure future elections, pulling in Rivest, who has been involved since, as well as other MIT faculty from the Department of Political Science and the MIT Sloan School of Management.

For five years, Rivest advised the U.S. Election Assistance Commission, where he helped set standards for voting system certification. In that time, he became an advocate for keeping paper ballots and auditing election outcomes based on a statistical analysis of a random sample of ballots, recommended steps to verify the reported outcome. In his research, he’s also developed technologies to use cryptography for voting, helping to secure elections dependent on electronic records.

As election security becomes a top concern in the United States, Rivest continues applying his cryptography expertise to help improve voting systems. Here, he discusses the major issues with securing all-electronic voting systems and explains why he prefers keeping paper ballots as backup to verify voter intentions have been recorded — and that the election outcome isn’t based on a computer bug.

Q: If an electronic voting system has been certified, does that mean it’s secure?

A: You have to be careful with what you expect from the certification process. You could go into it thinking, well, I’m going to have these systems certified, and because they’re certified, they’re secure, and therefore, because the voting systems are secure, I can trust the election outcomes are right. And that turns out not to be a terribly good mode of thought. For one thing, you can’t really show that something is secure by testing.

Security relates to the absence of ways an adversary can affect the election outcome. Testing a voting system may show that certain adversarial attacks don’t work, but it doesn’t reveal that there are no attacks that work. Furthermore, commercial software is well-known to have several bugs per thousand lines of code, any of which could be a security hole. Finding all bugs is well-nigh impossible, even with a vigorous testing, so certification will never provide a guarantee that a voting system is secure.

Q: Can an electronic voting system ever be secure?

A: One major problem is that you never know that the system that is running is actually the system that was tested. There are procedures in place that are supposed to ensure that, but a common way to attack a system is to attack the supply chain, so that the voting system somebody installs is not what they think it is.

You never want to be in a position where you have to say, “I trust the election outcomes because I trust the computer.” Because computers, in the end, aren’t that trustworthy. They can be manipulated. They can have their programming changed. Every day new breaches of major computer systems are reported. Computer systems just are very difficult to make secure, especially for something that's very important, like elections.

All-electronic voting systems are therefore next-to-impossible to secure. A voting system founded instead on voter-verifiable paper ballots (preferably hand-marked paper ballots) provides a basis for checking that election outcomes are correctly derived from expressed voter intentions, instead of from some computer bug.

Q: What’s your new philosophy when it comes to securing U.S. elections?

A: The new philosophy and the change of perspective I’ve adopted is not to believe an election outcome is right because you believe that machinery is doing the right thing, but rather to check that the outcome is right for each election.

You look at a sample of the paper ballots, you use some statistics, and you confirm with high confidence that the reported election outcome is consistent with that sample. A number of us have been working on that technology; Philip Stark at University of California at Berkeley is the leading statistician involved with this and the inventor of such “risk-limiting audits.”

There was a panel I was on recently for the National Academy of Sciences that produced a report called “Securing the Vote” (September 2018). That report recommended two things strongly: using paper ballots, and performing statistical post-election audits to check the tabulation of the paper ballots. In conjunction with other procedures (that, for example, ensure that the paper ballots checked are those that are cast and tabulated), one can develop confidence that our election outcomes are indeed correct."
84;news.mit.edu;http://news.mit.edu/2019/emily-richmond-pollock-opera-zero-hour-1002;;A new act for opera;"In November 1953, the Nationaltheater in Mannheim, Germany, staged a new opera, the composer Boris Blacher’s “Abstrakte Oper Nr. 1,” which had debuted just months previously. As it ran, music fans were treated to both a performance and a raging controversy about the work, which one critic called “a monstrosity of musical progress,” and another termed “a stillbirth.”

Some of this vitriol stemmed from Blacher’s experimental composition, which had jazz and pop sensibilities, few words in the libretto (but some nonsense syllables), and no traditional storyline. The controversy was heightened by the Mannheim production, which projected images of postwar ruins and other related tropes onto the backdrop.

“The staging was very political,” says MIT music scholar Emily Richmond Pollock, author of a new book about postwar German opera. “Putting these very concrete images behind [the stage], that people had just lived through, produced a very uncomfortable feeling.”

It wasn’t just critics who were dubious: One audience member wrote to the Mannheim morning newspaper to say that Blacher’s “cacophonous concoction is actually approaching absolute zero and is not even original in doing so.”

In short, “Abstrakte Oper Nr. 1” hardly fit its genre’s traditions. Blacher’s work was introduced soon after the supposed “Zero Hour” in German society — the years after World War Two ended in 1945. Germany had instigated the deadliest war in history, and the country was supposed to be building itself entirely anew on political, civic, and cultural fronts. But the reaction to “Abstrakte Oper Nr. 1” shows the limits of that concept; Germans also craved continuity.

“There is this mythology of the Zero Hour, that Germans had to start all over again,” says Pollock, an associate professor in MIT’s Music and Theater Arts Section.

Pollock’s new book, “Opera after the Zero Hour,” just published by Oxford University Press, explores these tensions in rich detail. In the work, Pollock closely scrutinizes five postwar German operas while examining the varied reactions they produced. Rather than participating in a total cultural teardown, she concludes, many Germans were attempting to construct a useable past and build a future connected to it.

“Opera in general is a conservative art form,” Pollock says. “It has often been identified very closely with whomever is in power.” For that reason, she adds, “Opera is a really good place to examine why tradition was a problem [after 1945], and how different artists chose to approach that problem.”

The politics of cultural nationalism

Rebuilding Germany after 1945 was a monumental task, even beyond creating a new political state. A significant part of Germany lay in rubble; for that matter, most large opera houses had been bombed.

Nonetheless, opera soon bloomed again in Germany. There were 170 new operas staged in Germany from 1945 to 1965. Operationally, as Pollock notes in the book, this inevitably meant including former Nazis in the opera business — efforts at “denazification” of society, she thinks, were of limited effectiveness. Substantively, meanwhile, the genre’s sense of tradition set audience expectations that could be difficult to alter.

“There’s a lot of investment in opera, but it’s not [usually] going to be avant-garde,” Pollock says, noting there were “hundreds of years of opera tradition pressing down” on composers, as well as “a bourgeois restored German culture that doesn’t want to do anything too radical.” However, she notes, after 1945, “There are a lot of traditions of music-making as part of the culture of being German that feel newly problematic [to socially-aware observers].”

Thus a substantial portion of those 170 new operas — besides “Abstrakte Oper Nr. 1” — contained distinctive blends of innovation and tradition. Consider Carl Orff’s “Oedipus der Tyrann,” a 1958 work of musical innovation with a traditional theme. Orff was one of Germany’s best-known composers (he wrote “Carmina Burana” in 1937) and had professional room to experiment. “Oedipus der Tyrann” strips away operatic musical form, with scant melody or symphonic expression, though Pollock’s close reading of the score shows some remaining links to mainstream operatic tradition. But the subject of the opera is classical: Orff uses the German poet Friedrich Holderlin’s 1804 translation of Sophocles’ “Oedipus” as his content. As Pollock notes, in 1958, this could be a problematic theme.

“When Germans claim special ownership of Greek culture, they’re saying they’re better than other countries — it’s cultural nationalism,” Pollock observes. “So what does it mean that a German composer is taking Greek tropes and reinterpreting them for a postwar context? Only recently, [there had been] events like the Berlin Olympics, where the Third Reich was specifically mobilizing an identification between Germans and the Greeks.”

In this case, Pollock says, “I think Orff was not able to think clearly about the potential political implications of what he was doing. He would have thought of music as largely apolitical. We can now look back more critically and see the continuities there.” Even if Orff’s subject matter was not intentionally political, though, it was certainly not an expression of a cultural “Zero Hour,” either.

Opera is the key

“Opera after the Zero Hour” continually illustrates how complex music creation can be. In the composer Bernd Alois Zimmerman’s 1960s opera “Die Soldaten,” Pollock notes a variety of influences, chiefly Richard Wagner’s idea of the “totalizing work of art” and the composer Alban Berg’s musical idioms — but without Wagner’s nationalistic impulses.

Even as it details the nuances of specific operas, Pollock’s book is also part of a larger dialogue about which types of music are most worth studying. If operas had limited overlap with the most radical forms of musical composition of the time, then opera’s popularity, as well as the intriguing forms of innovation and experiment that did occur within the form, make it a vital area of study, in Pollock’s view.

“History is always very selective,” Pollock says. “A canon of postwar music will include a very narrow slice of pieces that did really cool, new stuff, that no one had ever heard before.” But focusing on such self-consciously radical music only yields a limited understanding of the age and its cultural tastes, Pollock adds, because “there is a lot of music written for the opera house that people who loved music, and loved opera, were invested in.”

Other music scholars say “Opera after the Zero Hour” is a significant contribution to its field. Brigid Cohen, an associate professor of music at New York University, has stated that the book makes “a powerful case for taking seriously long-neglected operatic works that speak to a vexed cultural history still relevant in the present.”

Pollock, for her part, writes in the book that, given all the nuances and tensions and wrinkles in the evolution of the art form, “opera is the key” to understanding the relationship between postwar German composers and the country’s newly fraught cultural tradition, in a fully complicated and historical mode.

“If you look at [cultural] conservatism as interesting, you find a lot of interesting things,” Pollock says. “And if you assume things that are less innovative are less interesting, then you’re ignoring a lot of things that people cared about.”"
85;machinelearningmastery.com;https://machinelearningmastery.com/neural-networks-tricks-of-the-trade-review/;2019-02-19;Neural Networks: Tricks of the Trade Review;"Tweet Share Share

Last Updated on August 6, 2019

Deep learning neural networks are challenging to configure and train.

There are decades of tips and tricks spread across hundreds of research papers, source code, and in the heads of academics and practitioners.

The book “Neural Networks: Tricks of the Trade” originally published in 1998 and updated in 2012 at the cusp of the deep learning renaissance ties together the disparate tips and tricks into a single volume. It includes advice that is required reading for all deep learning neural network practitioners.

In this post, you will discover the book “Neural Networks: Tricks of the Trade” that provides advice by neural network academics and practitioners on how to get the most out of your models.

After reading this post, you will know:

The motivation for why the book was written.

A breakdown of the chapters and topics in the first and second editions.

A list and summary of the must-read chapters for every neural network practitioner.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

Neural Networks: Tricks of the Trade is a collection of papers on techniques to get better performance from neural network models.

The first edition was published in 1998 comprised of five parts and 17 chapters. The second edition was published right on the cusp of the new deep learning renaissance in 2012 and includes three more parts and 13 new chapters.

If you are a deep learning practitioner, then it is a must read book.

I own and reference both editions.

Motivation

The motivation for the book was to collate the empirical and theoretically grounded tips, tricks, and best practices used to get the best performance from neural network models in practice.

The author’s concern is that many of the useful tips and tricks are tacit knowledge in the field, trapped in peoples heads, code bases, or at the end of conference papers and that beginners to the field should be aware of them.

It is our belief that researchers and practitioners acquire, through experience and word-of-mouth, techniques and heuristics that help them successfully apply neural networks to difficult real-world problems. […] they are usually hidden in people’s heads or in the back pages of space-constrained conference papers.

The book is an effort to try to group the tricks together, after the success of a workshop at the 1996 NIPS conference with the same name.

This book is an outgrowth of a 1996 NIPS workshop called Tricks of the Trade whose goal was to begin the process of gathering and documenting these tricks. The interest that the workshop generated motivated us to expand our collection and compile it into this book.

— Page 1, Neural Networks: Tricks of the Trade, Second Edition, 2012.

Breakdown of First Edition

The first edition of the book was put together (edited) by Genevieve Orr and Klaus-Robert Muller comprised of five parts and 17 chapters and was published 20 years ago in 1998.

Each part includes a useful preface that summarizes what to expect in the upcoming chapters, and each chapter written by one or more academics in the field.

The breakdown of this first edition was as follows:

Part 1: Speeding Learning

Chapter 1: Efficient BackProp

Part 2: Regularization Techniques to Improve Generalization

Chapter 2: Early Stopping – But When?

Chapter 3: A Simple Trick for Estimating the Weight Decay Parameter

Chapter 4: Controlling the Hyperparameter Search on MacKay’s Bayesian Neural Network Framework

Chapter 5: Adaptive Regularization in Neural Network Modeling

Chapter 6: Large Ensemble Averaging

Part 3: Improving Network Models and Algorithmic Tricks

Chapter 7: Square Unit Augmented, Radically Extended, Multilayer Perceptrons

Chapter 8: A Dozen Tricks with Multitask Learning

Chapter 9: Solving the Ill-Conditioning on Neural Network Learning

Chapter 10: Centering Neural Network Gradient Factors

Chapter 11: Avoiding Roundoff Error in Backpropagating Derivatives

Part 4: Representation and Incorporating PRior Knowledge in Neural Network Training

Chapter 12: Transformation Invariance in Pattern Recognition – Tangent Distance and Tangent Propagation

Chapter 13: Combining Neural Networks and Context-Driven Search for On-Line Printed Handwriting Recognition in the Newton

Chapter 14: Neural Network Classification and Prior Class Probabilities

Chapter 15: Applying Divide and Conquer to Large Scale Pattern Recognition Tasks

Part 5: Tricks for Time Series

Chapter 16: Forecasting the Economy with Neural Nets: A Survey of Challenges and Solutions

Chapter 17: How to Train Neural Networks

It is an expensive book, and if you can pick-up a cheap second-hand copy of this first edition, then I highly recommend it.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Additions in the Second Edition

The second edition of the book was released in 2012, seemingly right at the beginning of the large push that became “deep learning.” As such, the book captures the new techniques at the time such as layer-wise pretraining and restricted Boltzmann machines.

It was too early to focus on the ReLU, ImageNet with CNNs, and use of large LSTMs.

Nevertheless, the second edition included three new parts and 13 new chapters.

The breakdown of the additions in the second edition are as follows:

Part 6: Big Learning in Deep Neural Networks

Chapter 18: Stochastic Gradient Descent Tricks

Chapter 19: Practical Recommendations for Gradient-Based Training of Deep Architectures

Chapter 20: Training Deep and Recurrent Networks with Hessian-Free Optimization

Chapter 21: Implementing Neural Networks Efficiently

Part 7: Better Representations: Invariant, Disentangled and Reusable

Chapter 22: Learning Feature Representations with K-Means

Chapter 23: Deep Big Multilayer Perceptrons for Digit Recognition

Chapter 24: A Practical Guide to Training Restricted Boltzmann Machines

Chapter 25: Deep Boltzmann Machines and the Centering Trick

Chapter 26: Deep Learning via Semi-supervised Embedding

Part 8: Identifying Dynamical Systems for Forecasting and Control

Chapter 27: A Practical Guide to Applying Echo State Networks

Chapter 28: Forecasting with Recurrent Neural Networks: 12 Tricks

Chapter 29: Solving Partially Observable Reinforcement Learning Problems with Recurrent Neural Networks

Chapter 30: 10 Steps and Some Tricks to Set up Neural Reinforcement Controllers

Must-Read Chapters

The whole book is a good read, although I don’t recommend reading all of it if you are looking for quick and useful tips that you can use immediately.

This is because many of the chapters focus on the writers’ pet projects, or on highly specialized methods. Instead, I recommend reading four specific chapters, two from the first edition and two from the second.

The second edition of the book is worth purchasing for these four chapters alone, and I highly recommend picking up a copy for yourself, your team, or your office.

Fortunately, there are pre-print PDFs of these chapters available for free online.

The recommended chapters are:

Let’s take a closer look at each of these chapters in turn.

Efficient BackProp

This chapter focuses on providing very specific tips to get the most out of the stochastic gradient descent optimization algorithm and the backpropagation weight update algorithm.

Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.

— Page 9, Neural Networks: Tricks of the Trade, First Edition, 1998.

The chapter proceeds to provide a dense and theoretically supported list of tips for configuring the algorithm, preparing input data, and more.

The chapter is so dense that it is hard to summarize, although a good list of recommendations is provided in the “Discussion and Conclusion” section at the end, quoted from the book below:

– shuffle the examples

– center the input variables by subtracting the mean

– normalize the input variable to a standard deviation of 1

– if possible, decorrelate the input variables.

– pick a network with the sigmoid function shown in figure 1.4

– set the target values within the range of the sigmoid, typically +1 and -1.

– initialize the weights to random values as prescribed by 1.16. The preferred method for training the network should be picked as follows:

– if the training set is large (more than a few hundred samples) and redundant, and if the task is classification, use stochastic gradient with careful tuning, or use the stochastic diagonal Levenberg Marquardt method.

– if the training set is not too large, or if the task is regression, use conjugate gradient.

— Pages 47-48, Neural Networks: Tricks of the Trade, First Edition, 1998.

The field of applied neural networks has come a long way in the twenty years since this was published (e.g. the comments on sigmoid activation functions are no longer relevant), yet the basics have not changed.

This chapter is required reading for all deep learning practitioners.

Early Stopping – But When?

This chapter describes the simple yet powerful regularization method called early stopping that will halt the training of a neural network when the performance of the model begins to degrade on a hold-out validation dataset.

Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (“early stopping”)

— Page 55, Neural Networks: Tricks of the Trade, First Edition, 1998.

The challenge of early stopping is the choice and configuration of the trigger used to stop the training process, and the systematic configuration of early stopping is the focus of the chapter.

The general early stopping criteria are described as:

GL : stop as soon as the generalization loss exceeds a specified threshold.

: stop as soon as the generalization loss exceeds a specified threshold. PQ : stop as soon as the quotient of generalization loss and progress exceeds a threshold.

: stop as soon as the quotient of generalization loss and progress exceeds a threshold. UP: stop when the generalization error increases in strips.

Three recommendations are provided, e.g. “the trick“:

1. Use fast stopping criteria unless small improvements of network performance (e.g. 4%) are worth large increases of training time (e.g. factor 4).

2. To maximize the probability of finding a “good” solution (as opposed to maximizing the average quality of solutions), use a GL criterion.

3. To maximize the average quality of solutions, use a PQ criterion if the net- work overfits only very little or an UP criterion otherwise.

— Page 60, Neural Networks: Tricks of the Trade, First Edition, 1998.

The rules are analyzed empirically over a large number of training runs and test problems. The crux of the finding is that being more patient with the early stopping criteria results in better hold-out performance at the cost of additional computational complexity.

I conclude slower stopping criteria allow for small improvements in generalization (here: about 4% on average), but cost much more training time (here: about factor 4 longer on average).

— Page 55, Neural Networks: Tricks of the Trade, First Edition, 1998.

Stochastic Gradient Descent Tricks

This chapter focuses on a detailed review of the stochastic gradient descent optimization algorithm and tips to help get the most out of it.

This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.

— Page 421, Neural Networks: Tricks of the Trade, Second Edition, 2012.

There is a lot of overlap with Chapter 1: Efficient BackProp, and although the chapter calls out tips along the way with boxes, a useful list of tips is not summarized at the end of the chapter.

Nevertheless, it is a compulsory read for all neural network practitioners.

Below is my own summary of the tips called out in boxes throughout the chapter, mostly quoting directly from the second edition:

Use stochastic gradient descent (batch=1) when training time is the bottleneck.

Randomly shuffle the training examples.

Use preconditioning techniques.

Monitor both the training cost and the validation error.

Check the gradients using finite differences.

Experiment with the learning rates [with] a small sample of the training set.

Leverage the sparsity of the training examples.

Use a decaying learning rate.

Try averaged stochastic gradient (i.e. a specific variant of the algorithm).

Some of these tips are pithy without context; I recommend reading the chapter.

Practical Recommendations for Gradient-Based Training of Deep Architectures

This chapter focuses on the effective training of neural networks and early deep learning models.

It ties together the classical advice from Chapters 1 and 29 but adds comments on (at the time) recent deep learning developments like greedy layer-wise pretraining, modern hardware like GPUs, modern efficient code libraries like BLAS, and advice from real projects tuning the training of models, like the order to train hyperparameters.

This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on backpropagated gradient and gradient-based optimization.

— Page 437, Neural Networks: Tricks of the Trade, Second Edition, 2012.

It’s also long, divided into six main sections:

Deep Learning Innovations . Including greedy layer-wise pretraining, denoising autoencoders, and online learning.

. Including greedy layer-wise pretraining, denoising autoencoders, and online learning. Gradients . Including mini-batch gradient descent and automatic differentiation.

. Including mini-batch gradient descent and automatic differentiation. Hyperparameters . Including learning rate, mini-batch size, epochs, momentum, nodes, weight regularization, activity regularization, hyperparameter search, and recommendations.

. Including learning rate, mini-batch size, epochs, momentum, nodes, weight regularization, activity regularization, hyperparameter search, and recommendations. Debugging and Analysis. Including monitoring loss for overfitting, visualization, and statistics.

and Analysis. Including monitoring loss for overfitting, visualization, and statistics. Other Recommendations . Including GPU hardware and use of efficient linear algebra libraries such as BLAS.

. Including GPU hardware and use of efficient linear algebra libraries such as BLAS. Open Questions. Including the difficulty of training deep models and adaptive learning rates.

There’s far too much for me to summarize; the chapter is dense with useful advice for configuring and tuning neural network models.

Without a doubt, this is required reading and provided the seeds for the recommendations later described in the 2016 book Deep Learning, of which Yoshua Bengio was one of three authors.

The chapter finishes on a strong, optimistic note.

The practice summarized here, coupled with the increase in available computing power, now allows researchers to train neural networks on a scale that is far beyond what was possible at the time of the first edition of this book, helping to move us closer to artificial intelligence.

— Page 473, Neural Networks: Tricks of the Trade, Second Edition, 2012.

Further Reading

Get the Book on Amazon

Other Book Pages

Pre-Prints of Recommended Chapters

Summary

In this post, you discovered the book “Neural Networks: Tricks of the Trade” that provides advice from neural network academics and practitioners on how to get the most out of your models.

Have you read some or all of this book? What do you think of it?

Let me know in the comments below.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
86;news.mit.edu;http://news.mit.edu/2020/remote-control-hormone-release-nanoparticles-0410;;Researchers achieve remote control of hormone release;"Abnormal levels of stress hormones such as adrenaline and cortisol are linked to a variety of mental health disorders, including depression and posttraumatic stress disorder (PTSD). MIT researchers have now devised a way to remotely control the release of these hormones from the adrenal gland, using magnetic nanoparticles.

This approach could help scientists to learn more about how hormone release influences mental health, and could eventually offer a new way to treat hormone-linked disorders, the researchers say.

“We’re looking how can we study and eventually treat stress disorders by modulating peripheral organ function, rather than doing something highly invasive in the central nervous system,” says Polina Anikeeva, an MIT professor of materials science and engineering and of brain and cognitive sciences.

To achieve control over hormone release, Dekel Rosenfeld, an MIT-Technion postdoc in Anikeeva’s group, has developed specialized magnetic nanoparticles that can be injected into the adrenal gland. When exposed to a weak magnetic field, the particles heat up slightly, activating heat-responsive channels that trigger hormone release. This technique can be used to stimulate an organ deep in the body with minimal invasiveness.

Anikeeva and Alik Widge, an assistant professor of psychiatry at the University of Minnesota and a former research fellow at MIT’s Picower Institute for Learning and Memory, are the senior authors of the study. Rosenfeld is the lead author of the paper, which appears today in Science Advances.

Controlling hormones

Anikeeva’s lab has previously devised several novel magnetic nanomaterials, including particles that can release drugs at precise times in specific locations in the body.

In the new study, the research team wanted to explore the idea of treating disorders of the brain by manipulating organs that are outside the central nervous system but influence it through hormone release. One well-known example is the hypothalamic-pituitary-adrenal (HPA) axis, which regulates stress response in mammals. Hormones secreted by the adrenal gland, including cortisol and adrenaline, play important roles in depression, stress, and anxiety.

“Some disorders that we consider neurological may be treatable from the periphery, if we can learn to modulate those local circuits rather than going back to the global circuits in the central nervous system,” says Anikeeva, who is a member of MIT’s Research Laboratory of Electronics and McGovern Institute for Brain Research.

As a target to stimulate hormone release, the researchers decided on ion channels that control the flow of calcium into adrenal cells. Those ion channels can be activated by a variety of stimuli, including heat. When calcium flows through the open channels into adrenal cells, the cells begin pumping out hormones. “If we want to modulate the release of those hormones, we need to be able to essentially modulate the influx of calcium into adrenal cells,” Rosenfeld says.

Unlike previous research in Anikeeva’s group, in this study magnetothermal stimulation was applied to modulate the function of cells without artificially introducing any genes.

To stimulate these heat-sensitive channels, which naturally occur in adrenal cells, the researchers designed nanoparticles made of magnetite, a type of iron oxide that forms tiny magnetic crystals about 1/5000 the thickness of a human hair. In rats, they found these particles could be injected directly into the adrenal glands and remain there for at least six months. When the rats were exposed to a weak magnetic field — about 50 millitesla, 100 times weaker than the fields used for magnetic resonance imaging (MRI) — the particles heated up by about 6 degrees Celsius, enough to trigger the calcium channels to open without damaging any surrounding tissue.

The heat-sensitive channel that they targeted, known as TRPV1, is found in many sensory neurons throughout the body, including pain receptors. TRPV1 channels can be activated by capsaicin, the organic compound that gives chili peppers their heat, as well as by temperature. They are found across mammalian species, and belong to a family of many other channels that are also sensitive to heat.

This stimulation triggered a hormone rush — doubling cortisol production and boosting noradrenaline by about 25 percent. That led to a measurable increase in the animals’ heart rates.

Treating stress and pain

The researchers now plan to use this approach to study how hormone release affects PTSD and other disorders, and they say that eventually it could be adapted for treating such disorders. This method would offer a much less invasive alternative to potential treatments that involve implanting a medical device to electrically stimulate hormone release, which is not feasible in organs such as the adrenal glands that are soft and highly vascularized, the researchers say.

Another area where this strategy could hold promise is in the treatment of pain, because heat-sensitive ion channels are often found in pain receptors.

“Being able to modulate pain receptors with this technique potentially will allow us to study pain, control pain, and have some clinical applications in the future, which hopefully may offer an alternative to medications or implants for chronic pain,” Anikeeva says. With further investigation of the existence of TRPV1 in other organs, the technique can potentially be extended to other peripheral organs such as the digestive system and the pancreas.

The research was funded by the U.S. Defense Advance Research Projects Agency ElectRx Program, a Bose Research Grant, the National Institutes of Health BRAIN Initiative, and a MIT-Technion fellowship."
87;machinelearningmastery.com;https://machinelearningmastery.com/load-machine-learning-data-scratch-python/;2016-10-11;How to Load Machine Learning Data From Scratch In Python;"from csv import reader

# Load a CSV file

def load_csv ( filename ) :

file = open ( filename , ""rb"" )

lines = reader ( file )

dataset = list ( lines )

return dataset

# Convert string column to float

def str_column_to_float ( dataset , column ) :

for row in dataset :

row [ column ] = float ( row [ column ] . strip ( ) )

# Load pima-indians-diabetes dataset

filename = 'pima-indians-diabetes.csv'

dataset = load_csv ( filename )

print ( 'Loaded data file {0} with {1} rows and {2} columns' ) . format ( filename , len ( dataset ) , len ( dataset [ 0 ] ) )

print ( dataset [ 0 ] )

# convert string columns to float

for i in range ( len ( dataset [ 0 ] ) ) :

str_column_to_float ( dataset , i )"
88;news.mit.edu;http://news.mit.edu/2020/deep-learning-mechanical-property-metallic-0316;;Deep learning for mechanical property evaluation;"A standard method for testing some of the mechanical properties of materials is to poke them with a sharp point. This “indentation technique” can provide detailed measurements of how the material responds to the point’s force, as a function of its penetration depth.

With advances in nanotechnology during the past two decades, the indentation force can be measured to a resolution on the order of one-billionth of a Newton (a measure of the force approximately equivalent to the force you feel when you hold a medium-sized apple in your hand), and the sharp tip’s penetration depth can be captured to a resolution as small as a nanometer, or about 1/100,000 the diameter of a human hair. Such instrumented nanoindentation tools have provided new opportunities for probing physical properties in a wide variety of materials, including metals and alloys, plastics, ceramics, and semiconductors.

But while indentation techniques, including nanoindentation, work well for measuring some properties, they exhibit large errors when probing plastic properties of materials — the kind of permanent deformation that happens, for example, if you press your thumb into a piece of silly putty and leave a dent, or when you permanently bend a paper clip using your fingers. Such tests can be important in a wide variety of industrial applications, including conventional and digital manufacturing (3-D printing) of metallic structures, material quality assurance of engineering parts, and optimization of performance and cost. However, conventional indentation tests and existing methods to extract critical properties can be highly inaccurate.

Now, an international research team comprising researchers from MIT, Brown University, and Nanyang Technological University (NTU) in Singapore has developed a new analytical technique that can improve the estimation of mechanical properties of metallic materials from instrumented indention, with as much as 20 times greater accuracy than existing methods. Their findings are described today in the Proceedings of the National Academy of Sciences, in a paper combining indentation experiments with computational modeling of materials using the latest machine learning tools.

The team includes co-lead and senior author Ming Dao, a principal research scientist at MIT, and senior author Subra Suresh, MIT Vannevar Bush Professor Emeritus who is president and distinguished university professor at NTU Singapore. Their co-authors are doctoral student Lu Lu and Professor George Em Karniadakis of Brown University and research fellow Punit Kumar and Professor Upadrasta Ramamurty of NTU Singapore.

Animation showing schematically the process of extracting mechanical properties from indentation tests. It is a challenging task to accurately obtain the yield strength and nonlinear mechanical behavior from indention tests. Courtesy of the researchers.

“Small” challenges beyond elasticity

“Indentation is a very good method for testing mechanical properties,” Dao says, especially in cases where only small samples are available for testing. “When you try to develop new materials, you often have only a small quantity, and you can use indentation or nanoindentation to test really small quantities of materials,” he says.

Such testing can be quite accurate for elastic properties — that is, situations where the material bounces back to its original shape after having been poked. But when the applied force goes beyond the material’s “yield strength” — the point at which the poking leaves a lasting mark on the surface — this is called plastic deformation, and traditional indentation testing becomes much less accurate. “In fact, there's no widely available method that's being used” that can produce reliable information in such cases, Dao says.

Indentation can be used to determine hardness, but Dao explains that “hardness is only a combination of a material’s elastic and plastic properties. It's not a ‘clean’ parameter that can be used directly for design purposes. … But properties at or beyond yield strength, the strength denoting the point at which the material begins to deform irreversibly, are important to access the material’s suitability for engineering applications.”

Technique demands smaller amounts of high-quality data

The new method does not require any changes to experimental equipment or operation, but rather provides a way to work with the data to improve the accuracy of its predictions. By using an advanced neural network machine-learning system, the team found that a carefully planned integration of both real experimental data and computer-generated “synthetic” data of different levels of accuracy (a so-called multifidelity approach to deep learning) can produce the kind of quick and simple yet highly accurate data that industrial applications require for testing materials.

Traditional machine learning approaches require large amounts of high-quality data. However, detailed experiments on actual material samples are time-consuming and expensive to conduct. But the team found that doing the neural network training with lots of low-cost synthetic data and then incorporating a relatively small number of real experimental data points — somewhere between three and 20, as compared with 1,000 or more accurate, albeit high-cost, datasets — can substantially improve the accuracy of the outcome. In addition, they utilize established scaling laws to further reduce the number of training datasets needed in covering the parameter space for all engineering metals and alloys.

What’s more, the authors found that the majority of the time-consuming training process can be done ahead of time, so that for evaluating the actual tests a small number of real experimental results can be added for “calibration” training just when they’re needed, and give highly accurate results.

Animation illustrating the key features and advantages of the novel “multi-fidelity” deep learning method. Courtesy of the researchers.

Applications for digital manufacturing and more

These multifidelity deep-learning approaches have been validated using conventionally manufactured aluminum alloys as well as 3-D-printed titanium alloys.

Professor Javier Llorca, scientific director of IMDEA Materials Institute in Madrid, who was not connected with this research, says, “The new approach takes advantage of novel machine learning strategies to improve the accuracy of the predictions and has a large potential for fast screening of the mechanical properties of components manufactured by 3-D printing. It will allow one to discriminate the differences in the mechanical properties in different regions of the 3-D-printed components, leading to more accurate designs.”

Professor Ares Rosakis at Caltech, who also was not connected with this work, says this approach “results in remarkable computational efficiency and in unprecedented predictive accuracy of the mechanical properties. ... Most importantly, it provides a previously unavailable, fresh pair of eyes for ensuring mechanical property uniformity as well as manufacturing reproducibility of 3D-printed components of complex geometry for which classical testing is impossible.”

In principle, the basic process they use could be extended and applied to many other kinds of problems involving machine-learning, Dao says. “This idea, I think, can be generalized to solve other challenging engineering problems.” The use of the real experimental data helps to compensate for the idealized conditions assumed in the synthetic data, where the shape of the indenter tip is perfectly sharp, the motion of the indenter is perfectly smooth, and so on. By using “hybrid” data that includes both the idealized and the real-world situations, “the end result is a drastically reduced error,” he says.

The work was supported by the Army Research Laboratory, the U.S. Department of Energy, and the Nanyang Technical University Distinguished University Professorship."
89;towardsdatascience.com;https://towardsdatascience.com/data-augmentation-for-end-to-end-speech-translation-be4a1bd9ffbf?source=collection_home---4------4-----------------------;2020-04-19;Data Augmentation for End-to-End Speech Translation;"Data Augmentation for End-to-End Speech Translation

How to solve data scarcity with audio and text augmentation techniques

Photo by Alexander Sinn on Unsplash

End-to-end (or direct) speech translation is an approach to speech translation (ST) that is gaining high interest from the research world in the last few years. It consists in using a single deep learning model that learns to generate translated text of the input audio in an end-to-end fashion. Its surge in popularity is due to the scientific interest of achieving such a difficult task, but also to the expected effectiveness in practical applications.

The use of a single model is appealing for many reasons:

Decoding with a single model is faster than decoding with a pipeline of (at least) an automatic speech recognition (ASR) and a machine translation (MT) system. The absence of a transcription step can prevent the propagation of early bad decisions (error propagation), possibly resulting in superior quality. A single system is easier to manage and use than a cascaded system, and could save memory and GPU resources when deployed.

There is one big problem with this. End-to-end speech translation requires different data from ASR and MT, and while these two tasks have been studied for decades and have plenty of data at their disposal (at least for some languages), very little is publicly available for the new approach. In ST, we only have some hundreds of thousands segment pairs for the best-resourced languages, while a neural machine translation system (an easier task because the input is textual) is usually trained on tens or hundreds of millions of sentence pairs.

Then, how can we train a system with such small data? The answer so far is data augmentation: creating synthetic data through transformation of the existing data.

Data augmentation is a commonly used technique in deep learning, as this approach is famous to be data hungry and models can improve significantly their quality by adding data. The classic approach is to alter the input sample while keeping fixed its class label. One example for computer vision is to rotate images: the input image will be different as it is viewed by a different angle, but the content will be the same. A dog stays a dog and a cat stays a cat, though rotated. A practical example can be found here:

In this post, I want to focus on text and audio augmentation techniques that have been proposed for speech translation but can also be used for other tasks involving these types of data. Obviously, rotating and shifting pixels are two techniques that do not apply to text and audio, so we need more sophisticated techniques, sometimes involving the use of other machine learning systems.

Audio Augmentation

The first approach is similar to what happens with images: alter the input but keep the labels. A broadly used approach also by the community of ASR is speed perturbation, which consists in using a tool like SoX for perturbing the audio speed while keeping the translation fixed. It was used, for example, in the winning submission at IWSLT 2019 [1], an international workshop on speech translation that organizes a competition every year. One common practice is to multiply the speed (or the time duration, which is equivalent for our purpose) by a random factor in the range [0.9–1.1], which usually produces an audio that is still human-like but sounds different from the original. This kind of transformation is usually applied offline when the dataset is being built.

Another type of audio transformation happens online. Only some types of transformations can be applied online because current speech translation systems receive as input spectrograms, not wave forms. In order to occupy less disk space and save computational time during training, the datasets are usually stored as spectrograms, which require power to be computed and are more compact than the waveforms.

Wave form (above) and spectrogram (below) for the same utterance. The wave form is a time series, while the spectrogram is like a matrix: the x-axis is for time, the y-axis is for frequencies.

SpecAugment [2] is a popular spectrogram augmentation technique that was proposed last year and gathered a great success. It consists of three steps:

i) time warp is a complex and computational expensive algorithm that shifts a portion of the spectrogram along the time axis; ii) frequency masking applies a horizontal mask that covers some frequencies for the whole temporal dimension; iii) time masking applies a vertical mask that covers all the frequencies for some adjacent time steps. Time and frequency masking are the two most effective components of SpecAugment and basically force the model to predict the target sequence while being deaf to some frequencies or portions of the audio. The two types of masks have different width and positions at every iteration, so that the model learns to use them all.

SpecAugment applied to the original input (topmost image). From the SpecAugment paper [2]

A similar idea has been proposed more recently by Nguyen and colleagues [3] to perturb the audio speed in an online fashion. Their method, called time stretch divides the input spectrogram in fixed-width windows and shrinks or stretches each window with a different random factor picked from (0.8–1.25). With this technique every spectrogram will have windows perturbed in time in different and also contrasting ways, and according to the authors it can replace offline speed perturbation.

Transfer Learning

The most direct way to transfer knowledge between tasks, usually when one of the two has more data available, is the so-called pre-training. First, train a model on the large-resourced task, then use the same deep learning architecture for the second task, and initialize the weights with the ones learned from the first task. This is exactly one of the first approaches proposed for transferring knowledge from MT and ASR systems to direct ST systems [4,5,6]. Train an ASR system using the same architecture (at least for the encoder side) that will be used for direct ST. Train an MT system with the same decoder architecture that will be used in direct ST. Finally, initialize the direct ST system with the encoder weights learned in ASR and the decoder weights learned in MT. This approach leads to faster convergence and better translation quality, however, the decoder pre-training appears to be less effective than encoder pre-training. To overcome this problem, Bahar et al. [7] showed that decoder pre-training is more effective if one additional “adapter” encoder layer is put on top of the pre-trained encoder.

Two-Stage Decoding

Kano et al. [8] proposed two-stage decoding as a way to better use pre-trained components. It is a network consisting of three components, one encoder and two cascaded decoders. The first decoder generates sentences in the source language, similarly to an ASR system, while the second decoder generated text in the target language. The second decoder computes its attention with the states of the first decoder, right before they are used to select the output symbols. In this sense, the two-stage decoding is similar to a cascaded systems, but it does not discretize the source sequence in order to prevent errors. The two-stage decoding model is initialized from pre-trained ASR and MT models for an increased effectiveness. Sperber et al. [9] proposed an “attention-passing” model, which improves the two-stage decoding model for a more effective pre-training. They identify a problem in the two-stage decoding as it passes the first decoder outputs as attention input to the second decoder. The decoder states already contain the information to select the next word to generate, then a form of error propagation appears again in the model. In order to overcome it, they propose to use the context vectors produced by the attention between the first decoder and the encoder as input to the second decoder attention. This way, they suppose that the second decoder is connected directly to the relevant audio portion and no error is introduced by some early decisions made by the first decoder. This model seems to be more data-efficient through a clever scheme of multitask learning involving ASR, ST, and MT in the same training.

Schematic comparison between cascade, direct ST, attention-passing model and two-stage decoding model. Credit: Sperber et al. [9]

Knowledge Distillation

If your data is organized in triplets audio-transcript-translation, the knowledge of an MT model can be transferred to the direct ST model through knowledge distillation. For each triplet, the audio is given as input to the direct ST model, the transcript as input to the MT model, then the direct ST model is trained against the distribution generated by the MT model and not the ground truth. This kind of training is useful in some ways: 1) the soft distribution generated by the MT model is easier to learn than the one-hot ground truth; and 2) the MT distribution also contains some relations between symbols that the MT model knows but maybe are not present in the ST training data. This way, the direct ST model can learn a larger vocabulary than what is present in the training data [11].

Weak Supervision

Jia et al. [12] proposed to create synthetic parallel audio-translation data using existing systems for other tasks: 1) when only audio-transcript data is available, use an MT system to translate the transcripts in the target language; 2) when only parallel source-target texts are available, use a text-to-speech (TTS) system to generate the source audio. With this approach, the training data for direct ST can be augmented by orders of magnitude, and the quality of systems for those tasks ensures that the data is high quality. They also showed that the use of MT is more effective than TTS, maybe because the synthesized data is not very similar to human voice in real conditions. Similar results have been confirmed by Pino et al. [13], who also showed that this kind of data augmentation outperforms all other proposed methods and in some cases also makes pre-training useless.

Multilingual ST

A final technique for data augmentation is to go multilingual. As multilingual MT increases the translation quality for the low-resourced languages, Inaguma et al. [14] and us [15] proposed in two parallel works to leverage data from different languages. The objective is twofold: 1) more data to train the audio encoder, and 2) a positive transfer between similar languages that can have similar grammar or identical words, like English and French. The results in both studies are positive, but it is yet to see if the gain from multilingual training is kept also when using other data augmentation techniques.

Conclusion

Many techniques have been proposed for direct ST to leverage more data than the few available for this task. My personal recommendation to build a state of the art system is to always use an encoder pre-trained with a strong ASR model, augment the training set by generating synthetic translations with a strong MT system, and use SpecAugment during training. In our experiments, this combination was always effective, or at least not hurting. The other techniques are also useful, but their effectiveness can be reduced when used in combination with other methods, which may be a problem if they increase the training time.

I hope that this roundup of the existing data augmentation methods for direct ST is useful if you are approaching this field now, but also if you work in related fields based on audio and/or text. In particular, great improvements can come out by generating data with other components trained on more data for other tasks, or with online augmentation techniques that distort the original data.

If you are interested in direct ST but do not know where to start to work with it, check out my tutorial:

References

[1] Potapczyk, Tomasz, et al. (2019). Samsung’s System for the IWSLT 2019 End-to-End Speech Translation Task. Proceeding of IWSLT 2019.

[2] Park, Daniel S., et al. “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition.” Proc. Interspeech 2019 (2019): 2613–2617.

[3] Nguyen, Thai-Son, et al. “Improving sequence-to-sequence speech recognition training with on-the-fly data augmentation.” arXiv preprint arXiv:1910.13296 (2019).

[4] Weiss, Ron J., et al. “Sequence-to-Sequence Models Can Directly Translate Foreign Speech.” Proc. Interspeech 2017 (2017): 2625–2629.

[5] Bérard, Alexandre, et al. “End-to-end automatic speech translation of audiobooks.” 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.

[6] Bansal, Sameer, et al. “Pre-training on High-Resource Speech Recognition Improves Low-Resource Speech-to-Text Translation.” Proceedings of NAACL-HLT. 2019.

[7] Bahar, Parnia, Tobias Bieschke, and Hermann Ney. “A comparative study on end-to-end speech to text translation.” Proceedings of ASRU. (2019a).

[8] Kano, Takatomo, Sakriani Sakti, and Satoshi Nakamura. “Structured-based Curriculum Learning for End-to-end English-Japanese Speech Translation.” (2017).

[9] Sperber, Matthias, et al. “Attention-passing models for robust and data-efficient end-to-end speech translation.” Transactions of the Association for Computational Linguistics 7 (2019): 313–325.

[10] Liu, Yuchen, et al. “End-to-End Speech Translation with Knowledge Distillation.” Proc. Interspeech 2019 (2019): 1128–1132.

[11] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).

[12] Jia, Ye, et al. “Leveraging weakly supervised data to improve end-to-end speech-to-text translation.” ICASSP 2019–2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.

[13] Pino, Juan, et al. “Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade.” Proceedings of the 16th International Workshop on Spoken Language Translation (IWSLT). 2019.

[14] Inaguma, Hirofumi, et al. “Multilingual end-to-end speech translation.” Proceedings of ASRU (2019).

[15] Di Gangi, Mattia Antonino, Matteo Negri, and Marco Turchi. “One-to-many multilingual end-to-end speech translation.” Proceedings of ASRU (2019)."
90;machinelearningmastery.com;https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/;2017-10-29;How to Develop a Deep Convolutional Neural Network for Sentiment Analysis (Text Classification);"from string import punctuation

from os import listdir

from numpy import array

from numpy import asarray

from numpy import zeros

from keras . preprocessing . text import Tokenizer

from keras . preprocessing . sequence import pad_sequences

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import Embedding

from keras . layers . convolutional import Conv1D

from keras . layers . convolutional import MaxPooling1D

# load doc into memory

def load_doc ( filename ) :

# open the file as read only

file = open ( filename , 'r' )

# read all text

text = file . read ( )

# close the file

file . close ( )

return text

# turn a doc into clean tokens

def clean_doc ( doc , vocab ) :

# split into tokens by white space

tokens = doc . split ( )

# remove punctuation from each token

table = str . maketrans ( '' , '' , punctuation )

tokens = [ w . translate ( table ) for w in tokens ]

# filter out tokens not in vocab

tokens = [ w for w in tokens if w in vocab ]

tokens = ' ' . join ( tokens )

return tokens

# load all docs in a directory

def process_docs ( directory , vocab , is_trian ) :

documents = list ( )

# walk through all files in the folder

for filename in listdir ( directory ) :

# skip any reviews in the test set

if is_trian and filename . startswith ( 'cv9' ) :

continue

if not is_trian and not filename . startswith ( 'cv9' ) :

continue

# create the full path of the file to open

path = directory + '/' + filename

# load the doc

doc = load_doc ( path )

# clean doc

tokens = clean_doc ( doc , vocab )

# add to list

documents . append ( tokens )

return documents

# load embedding as a dict

def load_embedding ( filename ) :

# load embedding into memory, skip first line

file = open ( filename , 'r' )

lines = file . readlines ( )

file . close ( )

# create a map of words to vectors

embedding = dict ( )

for line in lines :

parts = line . split ( )

# key is string word, value is numpy array for vector

embedding [ parts [ 0 ] ] = asarray ( parts [ 1 : ] , dtype = 'float32' )

return embedding

# create a weight matrix for the Embedding layer from a loaded embedding

def get_weight_matrix ( embedding , vocab ) :

# total vocabulary size plus 0 for unknown words

vocab_size = len ( vocab ) + 1

# define weight matrix dimensions with all 0

weight_matrix = zeros ( ( vocab_size , 100 ) )

# step vocab, store vectors using the Tokenizer's integer mapping

for word , i in vocab . items ( ) :

vector = embedding . get ( word )

if vector is not None :

weight_matrix [ i ] = vector

return weight_matrix

# load the vocabulary

vocab_filename = 'vocab.txt'

vocab = load_doc ( vocab_filename )

vocab = vocab . split ( )

vocab = set ( vocab )

# load all training reviews

positive_docs = process_docs ( 'txt_sentoken/pos' , vocab , True )

negative_docs = process_docs ( 'txt_sentoken/neg' , vocab , True )

train_docs = negative_docs + positive_docs

# create the tokenizer

tokenizer = Tokenizer ( )

# fit the tokenizer on the documents

tokenizer . fit_on_texts ( train_docs )

# sequence encode

encoded_docs = tokenizer . texts_to_sequences ( train_docs )

# pad sequences

max_length = max ( [ len ( s . split ( ) ) for s in train_docs ] )

Xtrain = pad_sequences ( encoded_docs , maxlen = max_length , padding = 'post' )

# define training labels

ytrain = array ( [ 0 for _ in range ( 900 ) ] + [ 1 for _ in range ( 900 ) ] )

# load all test reviews

positive_docs = process_docs ( 'txt_sentoken/pos' , vocab , False )

negative_docs = process_docs ( 'txt_sentoken/neg' , vocab , False )

test_docs = negative_docs + positive_docs

# sequence encode

encoded_docs = tokenizer . texts_to_sequences ( test_docs )

# pad sequences

Xtest = pad_sequences ( encoded_docs , maxlen = max_length , padding = 'post' )

# define test labels

ytest = array ( [ 0 for _ in range ( 100 ) ] + [ 1 for _ in range ( 100 ) ] )

# define vocabulary size (largest integer value)

vocab_size = len ( tokenizer . word_index ) + 1

# load embedding from file

raw_embedding = load_embedding ( 'glove.6B.100d.txt' )

# get vectors in the right order

embedding_vectors = get_weight_matrix ( raw_embedding , tokenizer . word_index )

# create the embedding layer

embedding_layer = Embedding ( vocab_size , 100 , weights = [ embedding_vectors ] , input_length = max_length , trainable = False )

# define model

model = Sequential ( )

model . add ( embedding_layer )

model . add ( Conv1D ( filters = 128 , kernel_size = 5 , activation = 'relu' ) )

model . add ( MaxPooling1D ( pool_size = 2 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

print ( model . summary ( ) )

# compile network

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( Xtrain , ytrain , epochs = 10 , verbose = 2 )

# evaluate

loss , acc = model . evaluate ( Xtest , ytest , verbose = 0 )"
91;machinelearningmastery.com;https://machinelearningmastery.com/multi-output-regression-models-with-python/;2020-03-26;How to Develop Multi-Output Regression Models with Python;"# linear regression for multioutput regression

from sklearn . datasets import make_regression

from sklearn . linear_model import LinearRegression

# create datasets

X , y = make_regression ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_targets = 2 , random_state = 1 )

# define model

model = LinearRegression ( )

# fit model

model . fit ( X , y )

# make a prediction

data_in = [ [ - 2.02220122 , 0.31563495 , 0.82797464 , - 0.30620401 , 0.16003707 , - 1.44411381 , 0.87616892 , - 0.50446586 , 0.23009474 , 0.76201118 ] ]

yhat = model . predict ( data_in )

# summarize prediction"
92;towardsdatascience.com;https://towardsdatascience.com/a-must-have-algorithm-for-your-machine-learning-toolbox-xgboost-3e295cf8d69b?source=collection_home---4------1-----------------------;2020-04-19;A Must-have Algorithm for Your Machine Learning Toolbox: XGBoost;"Image by WikiImages from Pixabay

One of the most performant machine learning algorithms

XGBoost is a supervised learning algorithm that can be used for both regression & classification. Like all algorithms, it has its virtues & draws, of which we’ll be sure to walk through.

For this post, we’ll just be learning about XGBoost from the context of classification problems. For the regression portion, be sure to keep an eye on my blog at datasciencelessons.com.

Supervised learning catch up

I won't dive in too deep here, but for those who need a quick refresher on supervised learning; supervised learning is when you have a specific thing in mind you’d like to predict. For instance, you want to predict future home prices; so you have what you want to predict, the next step would be labeling historic data as a means to predict the future. To dive deeper into this example; let's say you wanted to sell your home but wanted to know what price you should pay, you could potentially accumulate data points about the homes as well as what their sale price was during that same time period. From this point, you would train a model to which you would pass the datapoints about your own home to generate a prediction about the value of your home. For the classification example where your predicting class; let's say your Gmail and wanting to predict spam… this requires a model to train on many emails that were ‘labeled’ as spam as well as a corresponding amount that were not.

What exactly is it?

XGBoost uses what is called an ensemble method. Without going into too much detail on ensemble methods, what makes XGBoost unique is how it leverages the outputs of many models to generate its prediction! XGBoost makes use of what are known as many ‘weak learners’ to produce a ‘strong learner’.

The XGBoost process looks something like this:

It iteratively trains many weak models

Weights each prediction according to performance

Combines the many weighted predictions to come up with a final output.

What makes XGBoost so popular?

Accuracy

Speed

This algorithm makes good use of modern computation allows itself to be parallelized

Consistent outperformance of other algorithms

Your First XGBoost Model

Let's break down the steps!

You’ll start by importing the XGBoost package in python

Break out your dependent & independent variables using y & X respectively

Break out train test split

Instantiate your classifier

Train your classifier

Predict y for your test set

Assess accuracy!

For this example, we’ll be classifying survival on the titanic.

import xgboost as xgb from sklearn.model_selection

import train_test_split X, y = titanic.iloc[:,:-1], titanic.iloc[:,-1] X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=.2, random_state=44) xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators= 7, seed=44) xgb_model.fit(X_train, y_train) pred = xgb_model.predict(X_test) accuracy = float(np.sum(pred == y_test)) / y_test.shape[0]

Well done! That was a great first pass! Let's learn a bit more about how to assess the quality of our model.

Assessing Performance

We’ve already seen accuracy in the code snippet above, but there are also the other aspects of a confusion matrix, precision & recall. I won't talk about those two here, but if you’d like more info, jump over to this post on the random forest algorithm: https://datasciencelessons.com/2019/08/13/random-forest-for-classification-in-r/

Apart from those, I’d like to talk about AUC.

To put it simply; if you chose one positive datapoint & one negative datapoint, AUC is the probability that the positive datapoint would be more highly ranked than the negative datapoint.

XGBoost allows you to run cross validation testing & to specify metrics you care about in the core algorithm call itself. This is partly done by creating a data structure called a dmatrix which consists of your X & y values.

The core difference this time is that we’ll create a dmatrix, specify parameters of our model, then generate an output where we specify the metric as AUC.

titanic_dm = xgb.DMatrix(data=X, label=y) params = {""objective"":""reg:logistic"", ""max_depth"":3} output = xgb.cv(dtrain=titanic_dm, params=params, nfold=3, num_boost_round=5, metrics=""auc"", as_pandas=True, seed=123) print((output[""test-auc-mean""]).iloc[-1])

How often should you use it?

XGBoost is not to be used every time you need to predict something. It can, however, be very useful in the right scenarios:

You have a lot of training data

You don’t only have categorical data, rather a good mix of numeric & categorical variables or just numeric variables.

You’ll definitely want to keep XGBoost away from computer vision & nlp related tasks… or if you have some very limited data.

As always, I hope this proves useful in your data science endeavors! Be sure to check out my other posts at datasciencelessons.com!

Happy Data Science-ing!"
93;machinelearningmastery.com;https://machinelearningmastery.com/multi-output-regression-models-with-python/#comments;2020-03-26;How to Develop Multi-Output Regression Models with Python;"# linear regression for multioutput regression

from sklearn . datasets import make_regression

from sklearn . linear_model import LinearRegression

# create datasets

X , y = make_regression ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_targets = 2 , random_state = 1 )

# define model

model = LinearRegression ( )

# fit model

model . fit ( X , y )

# make a prediction

data_in = [ [ - 2.02220122 , 0.31563495 , 0.82797464 , - 0.30620401 , 0.16003707 , - 1.44411381 , 0.87616892 , - 0.50446586 , 0.23009474 , 0.76201118 ] ]

yhat = model . predict ( data_in )

# summarize prediction"
94;news.mit.edu;http://news.mit.edu/2020/enzyme-square-dance-helps-generate-dna-building-blocks-0330;;Newly discovered enzyme “square dance” helps generate DNA building blocks;"How do you capture a cellular process that transpires in the blink of an eye? Biochemists at MIT have devised a way to trap and visualize a vital enzyme at the moment it becomes active — informing drug development and revealing how biological systems store and transfer energy.

The enzyme, ribonucleotide reductase (RNR), is responsible for converting RNA building blocks into DNA building blocks, in order to build new DNA strands and repair old ones. RNR is a target for anti-cancer therapies, as well as drugs that treat viral diseases like HIV/AIDS. But for decades, scientists struggled to determine how the enzyme is activated because it happens so quickly. Now, for the first time, researchers have trapped the enzyme in its active state and observed how the enzyme changes shape, bringing its two subunits closer together and transferring the energy needed to produce the building blocks for DNA assembly.

Before this study, many believed RNR’s two subunits came together and fit with perfect symmetry, like a key into a lock. “For 30 years, that’s what we thought,” says Catherine Drennan, an MIT professor of chemistry and biology and a Howard Hughes Medical Institute investigator. “But now, we can see the movement is much more elegant. The enzyme is actually performing a ‘molecular square dance,’ where different parts of the protein hook onto and swing around other parts. It’s really quite beautiful.”

Drennan and JoAnne Stubbe, professor emerita of chemistry and biology at MIT, are the senior authors on the study, which appeared in the journal Science on March 26. Former graduate student Gyunghoon ""Kenny"" Kang PhD ’19 is the lead author.

All proteins, including RNR, are composed of fundamental units known as amino acids. For over a decade, Stubbe’s lab has been experimenting with substituting RNR’s natural amino acids for synthetic ones. In doing so, the lab realized they could trap the enzyme in its active state and slow down its return to normal. However, it wasn’t until the Drennan lab gained access to a key technological advancement — cryo-electron microscopy — that they could snap high-resolution images of these “trapped” enzymes from the Stubbe lab and get a closer look.

“We really hadn’t done any cryo-electron microscopy at the point that we actively started trying to do the impossible: get the structure of RNR in its active state,” Drennan says. “I can’t believe it worked; I’m still pinching myself.”

The combination of these techniques allowed the team to visualize the complex molecular dance that allows the enzyme to transport the catalytic “firepower” from one subunit to the next, in order to generate DNA building blocks. This firepower is derived from a highly reactive unpaired electron (a radical), which must be carefully controlled to prevent damage to the enzyme.

According to Drennan, the team “wanted to see how RNR does the equivalent of playing with fire without getting burned.”

First author Kang says slowing down the radical transfer allowed them to observe parts of the enzyme no one had been able to see before in full. “Before this study, we knew this molecular dance was happening, but we’d never seen the dance in action,” he says. “But now that we have a structure for RNR in its active state, we have a much better idea about how the different components of the enzyme are moving and interacting in order to transfer the radical across long distances.”

Although this molecular dance brings the subunits together, there is still considerable distance between them: The radical must travel 35-40 angstroms from the first subunit to the second. This journey is roughly 10 times farther than the average radical transfer, according to Drennan. The radical must then travel back to its starting place and be stored safely, all within a fraction of a second before the enzyme returns to its normal conformation.

Because RNR is a target for drugs treating cancer and certain viruses, knowing its active-state structure could help researchers devise more effective treatments. Understanding the enzyme’s active state could also provide insight into biological electron transport for applications like biofuels. Drennan and Kang hope their study will encourage others to capture fleeting cellular events that have been difficult to observe in the past.

“We may need to reassess decades of past results,” Drennan says. “This study could open more questions than it answers; it’s more of a beginning than an end.”

This research was funded by the National Institutes of Health, a David H. Koch Graduate Fellowship, and the Howard Hughes Medical Institute."
95;news.mit.edu;http://news.mit.edu/2019/dr-martin-luther-king-jr-visiting-professors-and-scholars-program-1008;;Meet the 2019-20 MLK Visiting Professors and Scholars;"Founded in 1990, the Martin Luther King Jr. (MLK) Visiting Professors and Scholars Program honors the life and legacy of Martin Luther King by increasing the presence of, and recognizing the contributions of, underrepresented minority scholars at MIT. MLK Visiting Professors and Scholars enhance their scholarship through intellectual engagement with the MIT community and enrich the cultural, academic, and professional experience of students. The program hosts between four and eight scholars each year with financial and institutional support from the Office of the Provost and oversight from the Institute Community and Equity Office. Six scholars are visiting MIT this academic year as part of the program.

Kasso Okoudjou is returning for a second year as an MLK Visiting Professor in the Department of Mathematics. Originally from Benin, he moved to the United States in 1998 and earned a PhD in mathematics from Georgia Tech. Okoudjou joins MIT from the University of Maryland College Park, where he is a professor. His research interests include applied and pure harmonic analysis, especially time-frequency and time-scale analysis; frame theory; and analysis and differential equations on fractals. He is interested in broadening the participation of underrepresented minorities in (undergraduate) research in the mathematical sciences.

Matthew Schumaker joins MIT for another year in the Music and Theater Arts Section within the School of Humanities, Arts, and Social Sciences. Schumaker received his doctorate in music composition from the University of California at Berkeley. At MIT, he teaches a new course, 21M.380 (Composing for Solo Instrument and Live Electronics), a hands-on music technology composition seminar combining instrumental writing with real-time computer music. Additionally, The Radius Ensemble in Cambridge, Massachusetts has commissioned Schumaker to write a new piece of music that seeks to translate into music the vibrant, curved gestures and slashed markings in the abstract landscapes of celebrated Ethiopian-born painter Julie Mehretu.

Jamie Macbeth is visiting from Smith College, where he is an assistant professor in computer science. He received his PhD in computer science from University of California at Los Angeles. Although this is his first year as an MLK Visiting Scholar, he is not new to MIT, since he has been a visiting scientist since 2017. He is hosted by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). Macbeth’s research is focused on building and studying intelligent computing systems that demonstrate a human-like capability for in-depth understanding and production of natural language, and thus can achieve richer interactions with human users. He is especially keen on building systems that decompose the meaning of language into complex conceptual structures that reflect humans’ embodied cognition, memory, imagery and knowledge about social situations.

Ben McDonald has been a postdoc in the Department of Chemistry since 2018 and is now an MLK Visiting Scholar. McDonald received his PhD in synthetic organic chemistry from Northwestern University. His research focused on the total synthesis of flavonolignan natural products and the development of reverse-polarity carbon-carbon bond forming reactions. As a member of the department’s Chemistry Alliance for Inclusion and Diversity, he is focused on advancing diversity, equity and inclusion efforts. One of the initiatives he seeks to establish is a summer research program, which recruits talented future scientists from underrepresented backgrounds.

Tina Opie is an associate professor in the Management Division at Babson College. Opie obtained her PhD in management (with a concentration in organizational behavior) from New York University’s Stern School of Business. As an MLK Visiting Scholar in MIT Sloan School of Management, along with access to MIT’s Behavioral Research Lab, she is conducting research to develop the construct of Shared Sisterhood. “Shared Sisterhood examines how high-quality relationships (e.g., relationships characterized by trust, emotional vulnerability) between black, white, and Latinx women at work facilitate workplace inclusion and equity.” Though her work has a specific focus, people of all genders and racioethnic backgrounds can be “sisters” and can contribute to fostering a more inclusive work environment. Opie established Opie Consulting Group, a diversity-and-inclusion consultancy that incorporates Shared Sisterhood in creating inclusive workplaces.

Rhonda Williams, an MLK Visiting Professor hosted by the Department of History, joins MIT from Vanderbilt University, where she was recently appointed the John L. Seigenthaler Chair in American History. She is the founder of the Social Justice Institute at Case Western Reserve University. Her essay titled “Black Women Who Educate for Justice and Put Their Time, Lives, and Spirits on the Line"" was recently published in ""Black Women And Social Justice Education: Legacies and Lessons"" (2019, SUNY Press). On Oct. 25, Williams will deliver a social justice-related performance-lecture called “The Things That Divide Us: Meditations” at MIT. In spring 2020, she will facilitate a social justice workshop for students, faculty and staff.

For more information about our scholars and the program, visit mlkscholars.mit.edu."
96;machinelearningmastery.com;http://machinelearningmastery.com/how-to-use-machine-learning-results/;2013-12-31;How to Use Machine Learning Results;"Tweet Share Share

Last Updated on June 7, 2016

Once you have found and tuned a viable model of your problem it is time to make use of that model. You may need to revisit your why and remind yourself what form you need a solution for the problem you are solving.

The problem is not addressed until you do something with the results. In this post you will learn tactics for presenting your results in answer to a question and considerations when turning your prototype model into a production system.

Depending on the type of problem you are trying to solve, the presentation of results will be very different. There are two main facets to making use of the results of your machine learning endeavor:

Report the results

Operationalize the system

Report Results

Once you have discovered a good model and a good enough result (or not, as the case may be), you will want to summarize what was learned and present it to stakeholders. This may be yourself, a client or the company for which you work.

Use a powerpoint template and address the sections listed below. You might like to write up a one-pager and use part section as a section header. Try to follow this process even on small experimental projects you do for yourself such as tutorials and competitions. It is easy to spend an inordinate number of hours on a project and you want to make sure to capture all the great things you learn along the way.

Below are the sections you can complete when reporting the results for a project.

Context (Why): Define the environment in which the problem exists and set up the motivation for the research question.

(Why): Define the environment in which the problem exists and set up the motivation for research question. Problem (Question): Concisely describe the problem as a question that you went out and answered.

(Question): Concisely describe the problem as a question that you went out and answered. Solution (Answer): Concisely describe the solution as an answer to the question you posed in the previous section. Be specific.

(Answer): Concisely describe the solution as an answer to the question you posed in the previous section. Be specific. Findings : Bulleted lists of discoveries you made along the way that interest the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey.

: Bulleted lists of discoveries you made along the way that interest the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey. Limitations : Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel.

: Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel. Conclusions (Why+Question+Answer): Revisit the why, research question and the answer you discovered in a tight little package that is easy to remember and repeat for yourself and others.

The type of audience you are presenting to will define the amount of detail you go into. Having the discipline to complete your projects with a report on results, even on small side projects will accelerate your learning in field. On such small side projects, I highly recommend sharing results of side projects on blogs or with communities and elicit feedback that you can capture and take with you into the start of your next project.

Operationalize

You have found a model that is good enough at addressing the problem you face that you would like to put it into production. This may be an operational installation on your workstation in the case of a fun side project, all the way up to integrating the model into an existing enterprise application. The scope is enormous. In this section you will learn three key aspects to operationalizing a model that you could consider carefully before putting a system into production.

Three areas that you should think carefully about are the algorithm implementation, the automated testing of your model and the tracking of the models performance through time. These three issues will very likely influence the type of model you choose.

Algorithm Implementation

It is likely that you were using a research library to discover the best performing method. The algorithm implementations in research libraries can be excellent, but they can also be written for the general case of the problem rather than the specific case with which you are working.

Think very hard about the dependencies and technical debt you may be creating by putting such an implementation directly into production. Consider locating a production-level library that supports the method you wish to use. You may have to repeat the process of algorithm tuning if you switch to a production level library at this point.

You may also consider implementing the algorithm yourself. This option may introduce risk depending on the complexity of the algorithm you have chosen and the implementation tricks it uses. Even with open source code, there may be a number of complex operations that may be very difficult to internalize and reproduce confidently.

Model Tests

Write automated tests that verify that the model can be constructed and achieve a minimum level of performance repeatedly. Also write tests for any data preparation steps. You may wish to control the randomness used by the algorithm (random number seeds) for each unit test run so that tests are 100% reproducible.

Tracking

Add infrastructure to monitor the performance of the model over time and raise alarms if accuracy drops below a minimum level. Tracking may occur in real-time or with samples of live data on a re-created model in a separate environment. A raised alarm may be an indication that that structure learned by the model in the data have changed (concept drift) and that the model may need to be updated or tuned.

There are some model types that can perform online learning and update themselves. Think carefully in allowing models to update themselves in a production environment. In some circumstances, it can be wiser to manage the model update process and switch out models (their internal configuration) as they are verified to be more performant.

Summary

In this post you learned that a project is not considered complete until you deliver the results. Results may be presented to yourself or to your clients and there is a minimum structure to follow when presenting results.

You also learned three concerns when using a learned model in a production environment, specifically the nature of the algorithm implementation, model tests and on going tracking."
97;machinelearningmastery.com;https://machinelearningmastery.com/youre-wrong-machine-learning-not-hard/;2018-03-27;Why Machine Learning Does Not Have to Be So Hard;"Tweet Share Share

Last Updated on July 13, 2019

Technical topics like mathematics, physics, and even computer science are taught using a bottom-up approach.

This approach involves laying out the topics in an area of study in a logical way with a natural progression in complexity and capability.

The problem is, humans are not robots executing a learning program. We require motivation, excitement, and most importantly, a connection of the topic to tangible results.

Useful skills we use every day like reading, driving, and programming were not learned this way and were in fact learned using an inverted top-down approach. This top-down approach can be used to learn technical subjects directly such as machine learning, which can make you a lot more productive a lot sooner, and be a lot of fun.

In this post, you will discover the concrete difference between the top-down and bottom-up approaches to learning technical material and why this is the approach that practitioners should use to learn machine learning and even related mathematics.

After reading this post, you will know:

The bottom-up approach used in universities to teach technical subjects and the problems with it.

How people learn to read, drive, and program in a top-down manner and how the top-down approach works.

The frame of machine learning and even mathematics using the top-down approach to learning and how to start to make rapid progress as a practitioner.

Let’s get started.

Overview

This is an important blog post, because I think it can really help to shake you out of the bottom-up, university-style way of learning machine learning.

This post is divided into seven parts; they are:

Bottom-Up Learning Learning to Read Learning to Drive Learning to Code Top-Down Learning Learn Machine Learning Learning Mathematics

Bottom-Up Learning

Take a field of study, such as mathematics.

There is a logical way to lay out the topics in mathematics that build on each other and lead through a natural progression in skills, capability, and understanding.

The problem is, this logical progression might only make sense to those who are already on the other side and can intuit the relationships between the topics.

Most of school is built around this bottom-up natural progression through material. A host of technical and scientific fields of study are taught this way.

Think back to high-school or undergraduate studies and the fundamental fields you may have worked through: examples such as:

Mathematics, as mentioned.

Biology.

Chemistry.

Physics.

Computer Science.

Think about how the material was laid out, week-by-week, semester-by-semester, year-by-year. Bottom-up, logical progression.

The problem is, the logical progression through the material may not be the best way to learn the material in order to be productive.

We are not robots executing a learning program. We are emotional humans that need motivation, interest, attention, encouragement, and results.

You can learn technical subjects from the bottom-up, and a small percentage of people do prefer things this way, but it is not the only way.

Now, if you have completed a technical subject, think back to how to you actually learned it. I bet it was not bottom-up.

Learning to Read

Think back; how did you learn to read?

My son is starting to read. Without thinking too much, here are the general techniques he’s using (really the school and us as parents):

Start by being read to in order to generate interest and show benefits.

Get the alphabet down and making the right sounds.

Memorize the most frequent words, their sounds, and how to spell them.

Learn the “spell-out-the-word” heuristic to deal with unknown words.

Read through books with supervision.

Read through books without supervision.

It is important that he continually knows why reading is important, connected to very tangible things he wants to do, like:

Read captions on TV shows.

Read stories on topics he loves, like Star Wars.

Read signs and menus when we are out and about.

So on…

It is also important that he gets results that he can track and in which he can see improvement.

Larger vocabulary.

Smoother reading style

Books of increasing complexity.

Here’s how he did not learn to read:

Definitions of word types (verbs, nouns, adverbs, etc.)

Rules of grammar.

Rules of punctuation.

Theory of human languages.

Learning to Drive

Do you drive?

It’s cool if you don’t, but most adults do out of necessity. Society and city design is built around personal mobility.

How did you learn to drive?

I remember some written tests and maybe a test on a computer. I have no memory of studying for them, though I very likely did. Here’s what I do remember.

I remember hiring a driving instructor and doing driving lessons. Every single lesson was practical, in the car, practicing the skill I was required to master, driving the vehicle in traffic.

Here’s what I did not study or discuss with my driving instructor:

The history of the automobile.

The theory of combustion engines.

The common mechanical faults in cars.

The electrical system of the car.

The theory of traffic flows.

To this day, I still manage to drive safely without any knowledge on these topics.

In fact, I never expect to learn these topics. I have zero need or interest and they will not help me realize the thing I want and need, which is safe and easy personal mobility.

If the car breaks, I’ll call an expert.

Learning to Code

I started programming without any idea of what coding or software engineering meant.

At home, I messed around with commands in Basic. I messed around with commands in Excel. I modified computer games. And so on. It was fun.

When I started to learn programming and software engineering, it was in university and it was bottom up.

We started with:

Language theory

Data types

Control flow structures

Data structures

etc.

When we did get to write code, it was on the command line and plagued with compiler problems, path problems, and a whole host of problems unrelated to actually learning programming.

I hated programming.

Flash-forward a few years. Somehow, I eventually starting working as a professional software engineer on some complex systems that were valued by their users. I was really good at it and I loved it.

Eventually, I did a course that showed how to create graphical user interfaces. And another that showed how to get computers to talk to each other using socket programming. And another on how to get multiple things to run at the same time using threads.

I connected the boring stuff with the thing I really liked: making software that could solve problems, that others could use. I connected it to something that mattered. It was no longer abstract and esoteric.

At least for me, and many developers like me, they taught it wrong. They really did. And it wasted years of time, effort, and results/outcomes that enthusiastic and time-free students like me could dedicate to something they are truly passionate about.

Top-Down Learning

The bottom-up approach is not just a common way for teaching technical topics; it looks like the only way.

At least until you think about how you actually learn.

The designers of university courses, masters of their subject area, are trying to help. They are laying everything out to give you the logical progression through the material that they think will get you to the skills and capabilities that you require (hopefully).

And as I mentioned, it can work for some people.

It does not work for me, and I expect it does not work for you. In fact, very few programmers I’ve met that are really good at their craft came through computer science programs, or if they did, they learned at home, alone, hacking on side projects.

An alternative is the top-down approach.

Flip the conventional approach on its head.

Don’t start with definitions and theory. Instead, start by connecting the subject with the results you want and show how to get results immediately.

Lay out a program that focuses on practicing this process of getting results, going deeper into some areas as needed, but always in the context of the result they require.

It Is Different

It is not the traditional path.

Be careful not to use traditional ways of thinking or comparison if you take this path.

The onus is on you. There is no system to blame. You only fail when you stop.

It is iterative . Topics are revisited many times with deeper understanding.

. Topics are revisited many times with deeper understanding. It is imperfect . Results may be poor in the beginning, but improve with practice.

. Results may be poor in the beginning, but improve with practice. It requires discovery . The learner must be open to continual learning and discoverery.

. The learner must be open to continual learning and discoverery. It requires ownership . The learner is responsible for improvement.

. The learner is responsible for improvement. It requires curiosity. The learner must pay attention to what interests them and follow it.

It Is Dangerous

Seriously, I’ve heard “experts” say this many times, saying things like:

You have to know the theory first before you can use this technique, otherwise you cannot use it properly.

I agree that results will be imperfect in the beginning, but improvement and even expertise does not only have to come from theory and fundamentals.

If you believe that a beginner programmer should not be pushing changes to production and deploying them, then surely you must believe that a beginner machine learning practitioner would suffer the same constraints.

Skill must be demonstrated.

Trust must be earned.

This is true regardless of how a skill is acquired.

You’re a Technician

Really!?

This is another “criticism” I’ve seen leveled at this approach to learning.

Exactly. We want to be technicians, using the tools in practice to help people and not be researchers..

You do not need to cover all of the same ground because you have a different learning objective. Although you can circle back and learn anything you like later once you have a context in which to integrate the abstract knowledge.

Developers in industry are not computer scientists; they are engineers. They are proud technicians of the craft.

Efficient, Effective, and a Fun Way to Learn

The benefits vastly outweigh the challenge of learning this way:

You go straight to the thing you want and start practicing it.

You have a context for connecting deeper knowledge and even theory.

You can efficiently sift and filter topics based on your goals in the subject.

It’s faster.

It’s more fun.

And, I bet it makes you much better.

How could you be better?

Because the subject is connected to you emotionally. You have connected it to an outcome or result that matters to you. You are invested. You have demonstrable competence. We all love things we are good at (even if we are a little color blind to how good we are), which drives motivation, enthusiasm, and passion.

An enthusiastic learner will blow straight past the fundamentalist.

Learn Machine Learning

So, how have you approached the subject of machine learning?

Seriously, tell me your approach in the comments below.

Are you taking a bottom-up university course?

Are you modeling your learning on such a course?

Or worse:

Are you following a top-down type approach but are riddled with guilt, math envy, and insecurities?

You are not alone; I see this every single day in helping beginners on this website.

To connect the dots for you, I strongly encourage you to study machine learning using the top-down approach.

Don’t start with precursor math.

Don’t start with machine learning theory.

Don’t code every algorithm from scratch.

This can all come later to refine and deepen your understanding once you have connections for this abstract knowledge.

Start by learning how to work through very simple predictive modeling problems using a fixed framework with free and easy-to-use open source tools. Practice on many small projects and slowly increase their complexity. Show your work by building a public portfolio.

I have written about this approach many times; see the “Further Reading” section at the end of the post for some solid posts on how to get started with the top-down approach to machine learning.

“Experts” entrenched in universities will say it’s dangerous. Ignore them.

World-class practitioners will tell you it’s the way they learned and continue to learn. Model them.

Remember:

You learned to read by practicing reading, not by studying language theory.

You learned to drive by practicing driving, not by studying combustion engines.

You learned to code by practicing coding, not by studying computability theory.

You can learn machine learning by practicing predictive modeling, not by studying math and theory.

Not only is this the way I learned and continue to practice machine learning, but it has helped tens of thousands of my students (and the many millions of readers of this blog).

Learning Mathematics

Don’t stop there.

A time may come when you want or need to pull back the curtain on the mathematical pillars of machine learning such as linear algebra, calculus, statistics, probability, and so on.

You can use the exact same top-down approach.

Pick a goal or result that matters to you, and use that as a lens, filter, or sift on the topics to study and learn to the depth you need to get that result.

For example, let’s say you pick linear algebra.

A goal might be to grok SVD or PCA. These are methods used in machine learning for data projection, data reduction, and feature selection type tasks.

A top-down approach might be to:

Implement the method in a high-level library such as scikit-learn and get a result. Implement the method in a lower-level library such as NumPy/SciPy and reproduce the result. Implement the method directly using matrices and matrix operations in NumPy or Octave. Study and explore the matrix arithmetic operations involved. Study and explore the matrix decomposition operations involved. Study methods for approximating the eigendecomposition of a matrix. And so on…

The goal provides the context and you can let your curiosity define the depth of study.

Painted this way, studying math is no different to studying any other topic in programming, machine learning, or other technical subjects.

It’s highly productive, and it’s a lot of fun!

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this post, you discovered the concrete difference between the top-down and bottom-up approaches to learning technical material and why this is the approach that practitioners should and do use to learn machine learning and even related mathematics.

Specifically, you learned:

The bottom-up approach used in universities to teach technical subjects and the problems with it.

How people learn to read, drive, and program in a top-down manner and how the top-down approach works.

The frame of machine learning and even mathematics using the top-down approach to learning and how to start to make rapid progress as a practitioner.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer."
98;machinelearningmastery.com;http://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/;2016-09-08;A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

Gradient boosting is one of the most powerful techniques for building predictive models.

In this post you will discover the gradient boosting machine learning algorithm and get a gentle introduction into where it came from and how it works.

After reading this post, you will know:

The origin of boosting from learning theory and AdaBoost.

How gradient boosting works including the loss function, weak learners and the additive model.

How to improve performance over the base algorithm with various regularization schemes.

Discover how to configure, fit, tune and evaluation gradient boosting models with XGBoost in my new book, with 15 step-by-step tutorial lessons, and full python code.

Let’s get started.

Need help with XGBoost in Python? Take my free 7-day email course and discover configuration, tuning and more (with sample code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

The Origin of Boosting

The idea of boosting came out of the idea of whether a weak learner can be modified to become better.

Michael Kearns articulated the goal as the “Hypothesis Boosting Problem” stating the goal from a practical standpoint as:

… an efficient algorithm for converting relatively poor hypotheses into very good hypotheses

— Thoughts on Hypothesis Boosting [PDF], 1988

A weak hypothesis or weak learner is defined as one whose performance is at least slightly better than random chance.

These ideas built upon Leslie Valiant’s work on distribution free or Probability Approximately Correct (PAC) learning, a framework for investigating the complexity of machine learning problems.

Hypothesis boosting was the idea of filtering observations, leaving those observations that the weak learner can handle and focusing on developing new weak learns to handle the remaining difficult observations.

The idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified. … Note, however, it is not obvious at all how this can be done

— Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World, page 152, 2013

AdaBoost the First Boosting Algorithm

The first realization of boosting that saw great success in application was Adaptive Boosting or AdaBoost for short.

Boosting refers to this general problem of producing a very accurate prediction rule by combining rough and moderately inaccurate rules-of-thumb.

— A decision-theoretic generalization of on-line learning and an application to boosting [PDF], 1995

The weak learners in AdaBoost are decision trees with a single split, called decision stumps for their shortness.

AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns.

This means that samples that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies these samples

— Applied Predictive Modeling, 2013

Predictions are made by majority vote of the weak learners’ predictions, weighted by their individual accuracy. The most successful form of the AdaBoost algorithm was for binary classification problems and was called AdaBoost.M1.

You can learn more about the AdaBoost algorithm in the post:

Generalization of AdaBoost as Gradient Boosting

AdaBoost and related algorithms were recast in a statistical framework first by Breiman calling them ARCing algorithms.

Arcing is an acronym for Adaptive Reweighting and Combining. Each step in an arcing algorithm consists of a weighted minimization followed by a recomputation of [the classifiers] and [weighted input].

— Prediction Games and Arching Algorithms [PDF], 1997

This framework was further developed by Friedman and called Gradient Boosting Machines. Later called just gradient boosting or gradient tree boosting.

The statistical framework cast boosting as a numerical optimization problem where the objective is to minimize the loss of the model by adding weak learners using a gradient descent like procedure.

This class of algorithms were described as a stage-wise additive model. This is because one new weak learner is added at a time and existing weak learners in the model are frozen and left unchanged.

Note that this stagewise strategy is different from stepwise approaches that readjust previously entered terms when new ones are added.

— Greedy Function Approximation: A Gradient Boosting Machine [PDF], 1999

The generalization allowed arbitrary differentiable loss functions to be used, expanding the technique beyond binary classification problems to support regression, multi-class classification and more.

How Gradient Boosting Works

Gradient boosting involves three elements:

A loss function to be optimized. A weak learner to make predictions. An additive model to add weak learners to minimize the loss function.

1. Loss Function

The loss function used depends on the type of problem being solved.

It must be differentiable, but many standard loss functions are supported and you can define your own.

For example, regression may use a squared error and classification may use logarithmic loss.

A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used.

2. Weak Learner

Decision trees are used as the weak learner in gradient boosting.

Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions.

Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss.

Initially, such as in the case of AdaBoost, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels.

It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.

This is to ensure that the learners remain weak, but can still be constructed in a greedy manner.

3. Additive Model

Trees are added one at a time, and existing trees in the model are not changed.

A gradient descent procedure is used to minimize the loss when adding trees.

Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.

Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss.

Generally this approach is called functional gradient descent or gradient descent with functions.

One way to produce a weighted combination of classifiers which optimizes [the cost] is by gradient descent in function space

— Boosting Algorithms as Gradient Descent in Function Space [PDF], 1999

The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or improve the final output of the model.

A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset.

Improvements to Basic Gradient Boosting

Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.

It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.

In this this section we will look at 4 enhancements to basic gradient boosting:

Tree Constraints Shrinkage Random sampling Penalized Learning

1. Tree Constraints

It is important that the weak learners have skill but remain weak.

There are a number of ways that the trees can be constrained.

A good general heuristic is that the more constrained tree creation is, the more trees you will need in the model, and the reverse, where less constrained individual trees, the fewer trees that will be required.

Below are some constraints that can be imposed on the construction of decision trees:

Number of trees , generally adding more trees to the model can be very slow to overfit. The advice is to keep adding trees until no further improvement is observed.

, generally adding more trees to the model can be very slow to overfit. The advice is to keep adding trees until no further improvement is observed. Tree depth , deeper trees are more complex trees and shorter trees are preferred. Generally, better results are seen with 4-8 levels.

, deeper trees are more complex trees and shorter trees are preferred. Generally, better results are seen with 4-8 levels. Number of nodes or number of leaves , like depth, this can constrain the size of the tree, but is not constrained to a symmetrical structure if other constraints are used.

, like depth, this can constrain the size of the tree, but is not constrained to a symmetrical structure if other constraints are used. Number of observations per split imposes a minimum constraint on the amount of training data at a training node before a split can be considered

imposes a minimum constraint on the amount of training data at a training node before a split can be considered Minimim improvement to loss is a constraint on the improvement of any split added to a tree.

2. Weighted Updates

The predictions of each tree are added together sequentially.

The contribution of each tree to this sum can be weighted to slow down the learning by the algorithm. This weighting is called a shrinkage or a learning rate.

Each update is simply scaled by the value of the “learning rate parameter v”

— Greedy Function Approximation: A Gradient Boosting Machine [PDF], 1999

The effect is that learning is slowed down, in turn require more trees to be added to the model, in turn taking longer to train, providing a configuration trade-off between the number of trees and learning rate.

Decreasing the value of v [the learning rate] increases the best value for M [the number of trees].

— Greedy Function Approximation: A Gradient Boosting Machine [PDF], 1999

It is common to have small values in the range of 0.1 to 0.3, as well as values less than 0.1.

Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model.

— Stochastic Gradient Boosting [PDF], 1999

3. Stochastic Gradient Boosting

A big insight into bagging ensembles and random forest was allowing trees to be greedily created from subsamples of the training dataset.

This same benefit can be used to reduce the correlation between the trees in the sequence in gradient boosting models.

This variation of boosting is called stochastic gradient boosting.

at each iteration a subsample of the training data is drawn at random (without replacement) from the full training dataset. The randomly selected subsample is then used, instead of the full sample, to fit the base learner.

— Stochastic Gradient Boosting [PDF], 1999

A few variants of stochastic boosting that can be used:

Subsample rows before creating each tree.

Subsample columns before creating each tree

Subsample columns before considering each split.

Generally, aggressive sub-sampling such as selecting only 50% of the data has shown to be beneficial.

According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling

— XGBoost: A Scalable Tree Boosting System, 2016

4. Penalized Gradient Boosting

Additional constraints can be imposed on the parameterized trees in addition to their structure.

Classical decision trees like CART are not used as weak learners, instead a modified form called a regression tree is used that has numeric values in the leaf nodes (also called terminal nodes). The values in the leaves of the trees can be called weights in some literature.

As such, the leaf weight values of the trees can be regularized using popular regularization functions, such as:

L1 regularization of weights.

L2 regularization of weights.

The additional regularization term helps to smooth the final learnt weights to avoid over-fitting. Intuitively, the regularized objective will tend to select a model employing simple and predictive functions.

— XGBoost: A Scalable Tree Boosting System, 2016

Gradient Boosting Resources

Gradient boosting is a fascinating algorithm and I am sure you want to go deeper.

This section lists various resources that you can use to learn more about the gradient boosting algorithm.

Gradient Boosting Videos

Gradient Boosting in Textbooks

Gradient Boosting Papers

Gradient Boosting Slides

Gradient Boosting Web Pages

Summary

In this post you discovered the gradient boosting algorithm for predictive modeling in machine learning.

Specifically, you learned:

The history of boosting in learning theory and AdaBoost.

How the gradient boosting algorithm works with a loss function, weak learners and an additive model.

How to improve the performance of gradient boosting with regularization.

Do you have any questions about the gradient boosting algorithm or about this post? Ask your questions in the comments and I will do my best to answer.

Discover The Algorithm Winning Competitions! Develop Your Own XGBoost Models in Minutes ...with just a few lines of Python Discover how in my new Ebook:

XGBoost With Python It covers self-study tutorials like:

Algorithm Fundamentals, Scaling, Hyperparameters, and much more... Bring The Power of XGBoost To Your Own Projects Skip the Academics. Just Results. See What's Inside"
99;news.mit.edu;http://news.mit.edu/2020/solve-pandemic-challenge-covid-0413;;MIT Solve rises to meet health security and pandemic challenge;"In the face of the Covid-19 pandemic, MIT Solve launched a new Global Challenge: How can communities around the world prepare for, detect, and respond to emerging pandemics and health security threats? Solve’s mission is to foster innovation, seeking out tech-based social entrepreneurs and helping them scale up their ideas.

The new Solve Health Security and Pandemics Challenge is designed to produce both short-term solutions to the impact of the current pandemic as well as longer-term strategies for future crises. “The reason that this pandemic is costing so many lives is that we were unprepared,” says Pooja Wagh ’06, Solve’s director of health community and results measurement. “We need stronger health care supply chains and better disease surveillance. This will happen again, and we need to be better positioned to mitigate the impact on human lives.”

MIT Solve hopes to leverage its extensive community to identify tech innovations that will make a difference. “We had a role to play because of our massive network of innovators, member organizations, MIT students and faculty, and all the people we reach through our communications,” says Wagh.

“Solve was built on the ethos that great ideas can come from anywhere,” she explains. “The idea was to democratize access to the resources we have at MIT, since a lack of resources keeps many great solutions from coming to fruition.”

There are several rounds of review in the Solve selection process, with 50 to 60 experts from both inside and outside MIT reading every application. “We draw on the experience of our community,” says Wagh. “Decision makers are a panel of judges with deep expertise in the relevant areas. For this challenge, judges will include representatives from public health agencies and corporations.”

In addition to the new health challenge, Solve issued its 2020 Global Challenges earlier this year related to jobs and entrepreneurship among marginalized populations, sustainable food systems, maternal and newborn health, and access to education for marginalized girls and women.

“We solicit applications from people all over the world,” says Wagh. “Our network of more than 150,000 people includes centers at MIT as well as 130 Solver teams and more than 120 Solve Members with a wide diversity of interests. What binds them together is their motivation to help our innovators succeed.” A small number of Solver teams are chosen and then matched with partners, including funders, who can help them make their ideas a reality.

“Given the immediacy and urgency of this crisis, we want to build a pathway for scale for all the Solver teams we select,” Wagh says. “We’ve been talking to potential partners and funders, and dozens of people have reached out to offer support for the future Solver teams that are selected. Others want to help us choose the teams. People should know that we’re looking for both applicants and supporters — there is a role for anyone who wants to engage.”

There has been an enthusiastic response to the new challenge, with applications pouring in. Individuals and groups with a tech-based solution to health security threats are invited to submit a proposal by June 18, while potential supporters of the teams chosen can investigate options for partnership with Solve."
100;machinelearningmastery.com;http://machinelearningmastery.com/what-is-holding-you-back-from-your-machine-learning-goals/;2014-06-03;What Is Holding You Back From Your Machine Learning Goals?;"Tweet Share Share

Last Updated on December 24, 2016

Identify and Tackle Your Self-Limiting Beliefs and

Finally Make Progress

I get a lot of email from developers and students looking to get started in machine learning.

The first question I ask them is what is stopping them from getting started?

I try to get to the heart of what they are struggling with, and almost always it is a self-limiting belief that has halted their progress.

In this post, I want to touch on some self-limiting beliefs I see crop up in my email exchanges and discussions with coaching students.

Maybe you will see yourself in one or more of these beliefs. If so, I urge you to challenge your assumptions.

Self-Limiting Belief

A self-limiting belief is something that you assume to be true that is limiting your progress. You presuppose something about yourself or about the thing you want to achieve. The problem is you hold that belief to be true and you don’t question it.

Steve Pavlina lists 3 types of self-limiting beliefs in is post: Dissolving Limiting Beliefs:

If-then Beliefs : e.g. If I get started in machine learning, I will fail because I am not good enough.

: e.g. If I get started in machine learning, I will fail because I am not good enough. Universal Beliefs : e.g. All Data Scientists have a Ph.D. and are mathematics rock gods.

: e.g. All Data Scientists have a Ph.D. and are mathematics rock gods. Personal and Self-Esteem Beliefs: e.g. I’m not good enough to be a machine learner.

You’re probably a logical and rational thinker. Apply those skills to your own beliefs about your goals and aspirations in machine learning and challenge them.

Waiting To Get Started

I think the biggest class of limiting belief I see is the belief that you cannot get started until you have some specific prior knowledge. The problem is that the prior knowledge you think you need is either not required or is so vast in scope that even experts in that subject don’t know it all.

For example: “I need to KNOW statistics“. See how ambiguous that belief is. How much statistics, what areas of statistics and why do you need to know them before you can start your investigation into machine learning?

Below are some of the more common self-limiting beliefs of skills or prior knowledge that must be obtained before you can get started in machine learning.

I can’t get into machine learning until…

…I get a degree or higher degree

…I complete a course

…I am good at linear algebra

…I know statistics and probability theory

…I have mastered the R programming language

You can get started in machine learning today, right now. Run your first classifier in 5 minutes. You’re in. Now, start blocking out what it is from machine learning that you really want?

I have written about some of these before, for example:

Awaiting Perfect Conditions

Another class of self-limiting belief is where you are waiting for the perfect environment or conditions before taking the leap. Things will never be perfect, leap and make a mess, then leap again.

I can’t get started in machine learning because…

…I don’t have the time right now

…I don’t have a fast CPU, GPU or a bazillion MB of RAM

…I am just a student right now

…I am not a good programmer at the moment

…I am very busy at work right now

It does take a lot of time and effort to get good at machine learning, but not all at once and not all at the beginning.

You can make good progress with a few hours a week, or tens of minutes per day. There are plenty of small snack-sized tasks you could take on to get started in machine learning. You can get started, it is just going to take some sacrifice, like all good things in life.

Struggling or Tried and Failed

The third class of limiting belief is that where you have made a small start but you are struggling or have failed to achieve your goal.

This is a tough one. Machine learning is hard but no harder than other technical skills like programming. It takes persistence and dedication. It’s applied and empirical and demands trial and error.

I can’t get into machine learning because…

…I feel overwhelmed

…I don’t understand x

…I will never be as good as y

…I don’t know what to do next

…I can’t get my program to work

My advice is to cut scope or change direction. I advocate small projects as often as I can because the methodology has been so successful for me.

What is your self-limiting belief?

Do you have a self-limiting belief? Think about it. What are your goals and why do you think you are not there yet?

Do you have a goal to get into machine learning, to become a data scientist or a machine learning engineer but have not taken the first step?

Are you waiting to acquire some perfect set of skills before getting started?

Are you waiting for the perfect conditions before getting started?

Have you taken a first step and abandoned the trail?

Where do you want to be and what are you struggling with?"
101;machinelearningmastery.com;http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/;2015-08-18;8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset;"Tweet Share Share

Last Updated on January 14, 2020

Has this happened to you?

You are working on your dataset. You create a classification model and get 90% accuracy immediately. “Fantastic” you think. You dive a little deeper and discover that 90% of the data belongs to one class. Damn!

This is an example of an imbalanced dataset and the frustrating results it can cause.

In this post you will discover the tactics that you can use to deliver great results on machine learning datasets with imbalanced data.

Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more in my new book, with 30 step-by-step tutorials and full Python source code.

Let’s get started.

Coming To Grips With Imbalanced Data

I get emails about class imbalance all the time, for example:

I have a binary classification problem and one class is present with 60:1 ratio in my training set. I used the logistic regression and the result seems to just ignores one class.

And this:

I am working on a classification model. In my dataset I have three different labels to be classified, let them be A, B and C. But in the training dataset I have A dataset with 70% volume, B with 25% and C with 5%. Most of time my results are overfit to A. Can you please suggest how can I solve this problem?

I write long lists of techniques to try and think about the best ways to get past this problem. I finally took the advice of one of my students:

Perhaps one of your upcoming blog posts could address the problem of training a model to perform against highly imbalanced data, and outline some techniques and expectations.

Frustration!

Imbalanced data can cause you a lot of frustration.

You feel very frustrated when you discovered that your data has imbalanced classes and that all of the great results you thought you were getting turn out to be a lie.

The next wave of frustration hits when the books, articles and blog posts don’t seem to give you good advice about handling the imbalance in your data.

Relax, there are many options and we’re going to go through them all. It is possible, you can build predictive models for imbalanced data.

Want to Get Started With Imbalance Classification? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

What is Imbalanced Data?

Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally.

For example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.

This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.

You can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either.

The remaining discussions will assume a two-class classification problem because it is easier to think about and describe.

Imbalance is Common

Most classification data sets do not have exactly equal number of instances in each class, but a small difference often does not matter.

There are problems where a class imbalance is not just common, it is expected. For example, in datasets like those that characterize fraudulent transactions are imbalanced. The vast majority of the transactions will be in the “Not-Fraud” class and a very small minority will be in the “Fraud” class.

Another example is customer churn datasets, where the vast majority of customers stay with the service (the “No-Churn” class) and a small minority cancel their subscription (the “Churn” class).

When there is a modest class imbalance like 4:1 in the example above it can cause problems.

Accuracy Paradox

The accuracy paradox is the name for the exact situation in the introduction to this post.

It is the case where your accuracy measures tell the story that you have excellent accuracy (such as 90%), but the accuracy is only reflecting the underlying class distribution.

It is very common, because classification accuracy is often the first measure we use when evaluating models on our classification problems.

Put it All On Red!

What is going on in our models when we train on an imbalanced dataset?

As you might have guessed, the reason we get 90% accuracy on an imbalanced data (with 90% of the instances in Class-1) is because our models look at the data and cleverly decide that the best thing to do is to always predict “Class-1” and achieve high accuracy.

This is best seen when using a simple rule based algorithm. If you print out the rule in the final model you will see that it is very likely predicting one class regardless of the data it is asked to predict.

8 Tactics To Combat Imbalanced Training Data

We now understand what class imbalance is and why it provides misleading classification accuracy.

So what are our options?

1) Can You Collect More Data?

You might think it’s silly, but collecting more data is almost always overlooked.

Can you collect more data? Take a second and think about whether you are able to gather more data on your problem.

A larger dataset might expose a different and perhaps more balanced perspective on the classes.

More examples of minor classes may be useful later when we look at resampling your dataset.

2) Try Changing Your Performance Metric

Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.

There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes.

I give more advice on selecting different performance measures in my post “Classification Accuracy is Not Enough: More Performance Measures You Can Use“.

In that post I look at an imbalanced dataset that characterizes the recurrence of breast cancer in patients.

From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:

Confusion Matrix : A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).

: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned). Precision : A measure of a classifiers exactness.

: A measure of a classifiers exactness. Recall : A measure of a classifiers completeness

: A measure of a classifiers completeness F1 Score (or F-score): A weighted average of precision and recall.

I would also advice you to take a look at the following:

Kappa (or Cohen’s kappa) : Classification accuracy normalized by the imbalance of the classes in the data.

: Classification accuracy normalized by the imbalance of the classes in the data. ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.

You can learn a lot more about using ROC Curves to compare classification accuracy in our post “Assessing and Comparing Classifier Performance with ROC Curves“.

Still not sure? Start with kappa, it will give you a better idea of what is going on than classification accuracy.

3) Try Resampling Your Dataset

You can change the dataset that you use to build your predictive model to have more balanced data.

This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:

You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or You can delete instances from the over-represented class, called under-sampling.

These approaches are often very easy to implement and fast to run. They are an excellent starting point.

In fact, I would advise you to always try both approaches on all of your imbalanced datasets, just to see if it gives you a boost in your preferred accuracy measures.

You can learn a little more in the the Wikipedia article titled “Oversampling and undersampling in data analysis“.

Some Rules of Thumb

Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)

Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less)

Consider testing random and non-random (e.g. stratified) sampling schemes.

Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)

4) Try Generate Synthetic Samples

A simple way to generate synthetic samples is to randomly sample the attributes from instances in the minority class.

You could sample them empirically within your dataset or you could use a method like Naive Bayes that can sample each attribute independently when run in reverse. You will have more and different data, but the non-linear relationships between the attributes may not be preserved.

There are systematic algorithms that you can use to generate synthetic samples. The most popular of such algorithms is called SMOTE or the Synthetic Minority Over-sampling Technique.

As its name suggests, SMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances.

Learn more about SMOTE, see the original 2002 paper titled “SMOTE: Synthetic Minority Over-sampling Technique“.

There are a number of implementations of the SMOTE algorithm, for example:

In Python, take a look at the “UnbalancedDataset” module. It provides a number of implementations of SMOTE as well as various other resampling techniques that you could try.

In R, the DMwR package provides an implementation of SMOTE.

In Weka, you can use the SMOTE supervised filter.

5) Try Different Algorithms

As always, I strongly advice you to not use your favorite algorithm on every problem. You should at least be spot-checking a variety of different types of algorithms on a given problem.

For more on spot-checking algorithms, see my post “Why you should be Spot-Checking Algorithms on your Machine Learning Problems”.

That being said, decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed.

If in doubt, try a few popular decision tree algorithms like C4.5, C5.0, CART, and Random Forest.

For some example R code using decision trees, see my post titled “Non-Linear Classification in R with Decision Trees“.

For an example of using CART in Python and scikit-learn, see my post titled “Get Your Hands Dirty With Scikit-Learn Now“.

6) Try Penalized Models

You can use the same algorithms but give them a different perspective on the problem.

Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.

Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA.

It is also possible to have generic frameworks for penalized models. For example, Weka has a CostSensitiveClassifier that can wrap any classifier and apply a custom penalty matrix for miss classification.

Using penalization is desirable if you are locked into a specific algorithm and are unable to resample or you’re getting poor results. It provides yet another way to “balance” the classes. Setting up the penalty matrix can be complex. You will very likely have to try a variety of penalty schemes and see what works best for your problem.

7) Try a Different Perspective

There are fields of study dedicated to imbalanced datasets. They have their own algorithms, measures and terminology.

Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas.

Two you might like to consider are anomaly detection and change detection.

Anomaly detection is the detection of rare events. This might be a machine malfunction indicated through its vibrations or a malicious activity by a program indicated by it’s sequence of system calls. The events are rare and when compared to normal operation.

This shift in thinking considers the minor class as the outliers class which might help you think of new ways to separate and classify samples.

Change detection is similar to anomaly detection except rather than looking for an anomaly it is looking for a change or difference. This might be a change in behavior of a user as observed by usage patterns or bank transactions.

Both of these shifts take a more real-time stance to the classification problem that might give you some new ways of thinking about your problem and maybe some more techniques to try.

8) Try Getting Creative

Really climb inside your problem and think about how to break it down into smaller problems that are more tractable.

For inspiration, take a look at the very creative answers on Quora in response to the question “In classification, how do you handle an unbalanced training set?”

For example:

Decompose your larger class into smaller number of other classes… …use a One Class Classifier… (e.g. treat like outlier detection) …resampling the unbalanced training set into not one balanced set, but several. Running an ensemble of classifiers on these sets could produce a much better result than one classifier alone

These are just a few of some interesting and creative ideas you could try.

For more ideas, check out these comments on the reddit post “Classification when 80% of my training set is of one class“.

Pick a Method and Take Action

You do not need to be an algorithm wizard or a statistician to build accurate and reliable models from imbalanced datasets.

We have covered a number of techniques that you can use to model an imbalanced dataset.

Hopefully there are one or two that you can take off the shelf and apply immediately, for example changing your accuracy metric and resampling your dataset. Both are fast and will have an impact straight away.

Which method are you going to try?

A Final Word, Start Small

Remember that we cannot know which approach is going to best serve you and the dataset you are working on.

You can use some expert heuristics to pick this method or that, but in the end, the best advice I can give you is to “become the scientist” and empirically test each method and select the one that gives you the best results.

Start small and build upon what you learn.

Want More? Further Reading…

There are resources on class imbalance if you know where to look, but they are few and far between.

I’ve looked and the following are what I think are the cream of the crop. If you’d like to dive deeper into some of the academic literature on dealing with class imbalance, check out some of the links below.

Books

Papers

Did you find this post useful? Still have questions?

Leave a comment and let me know about your problem and any questions you still have about handling imbalanced classes.

Get a Handle on Imbalanced Classification! Develop Imbalanced Learning Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Imbalanced Classification with Python It provides self-study tutorials and end-to-end projects on:

Performance Metrics, Undersampling Methods, SMOTE, Threshold Moving, Probability Calibration, Cost-Sensitive Algorithms

and much more... Bring Imbalanced Classification Methods to Your Machine Learning Projects See What's Inside"
102;machinelearningmastery.com;https://machinelearningmastery.com/how-to-get-started-with-generative-adversarial-networks-7-day-mini-course/;2019-07-10;How to Get Started With Generative Adversarial Networks (7-Day Mini-Course);". . .

# define the discriminator model

model = Sequential ( )

# downsample to 14x14

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 28 , 28 , 3 ) ) )

model . add ( BatchNormalization ( ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample to 7x7

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# classify

model . add ( Flatten ( ) )"
103;machinelearningmastery.com;http://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/;2016-12-13;Basic Feature Engineering With Time Series Data in Python;"# create date time features of a dataset

from pandas import read_csv

from pandas import DataFrame

series = read_csv ( 'daily-minimum-temperatures.csv' , header = 0 , index_col = 0 , parse_dates = True , squeeze = True )

dataframe = DataFrame ( )

dataframe [ 'month' ] = [ series . index [ i ] . month for i in range ( len ( series ) ) ]

dataframe [ 'day' ] = [ series . index [ i ] . day for i in range ( len ( series ) ) ]

dataframe [ 'temperature' ] = [ series [ i ] for i in range ( len ( series ) ) ]"
104;machinelearningmastery.com;https://machinelearningmastery.com/truncated-backpropagation-through-time-in-keras/;2017-06-27;How to Prepare Sequence Prediction for Truncated BPTT in Keras;"Tweet Share Share

Last Updated on August 14, 2019

Recurrent neural networks are able to learn the temporal dependence across multiple timesteps in sequence prediction problems.

Modern recurrent neural networks like the Long Short-Term Memory, or LSTM, network are trained with a variation of the Backpropagation algorithm called Backpropagation Through Time. This algorithm has been modified further for efficiency on sequence prediction problems with very long sequences and is called Truncated Backpropagation Through Time.

An important configuration parameter when training recurrent neural networks like LSTMs using Truncated Backpropagation Through Time is deciding how many timesteps to use as input. That is, how exactly to split up your very long input sequences into subsequences in order to get the best performance.

In this post, you will discover 6 different ways you can split up very long input sequences to effectively train recurrent neural networks using Truncated Backpropagation Through Time in Python with Keras.

After reading this post, you will know:

What Truncated Backpropagation Through Time is and how it has been implemented in the Python deep learning library Keras.

How exactly the choice of the number of input timesteps affects learning within recurrent neural networks.

6 different techniques you can use to split up your very long sequence prediction problems to make best use of the Truncated Backpropagation Through Time training algorithm.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Truncated Backpropagation Through Time

Backpropagation is the training algorithm used to update the weights in a neural network in order to minimize the error between the expected output and the predicted output for a given input.

For sequence prediction problems where there is an order dependence between observations, recurrent neural networks are used instead of classical feed-forward neural networks. Recurrent neural networks are trained using a variation of the Backpropagation algorithm called Backpropagation Through Time, or BPTT for short.

In effect, BPTT unrolls the recurrent neural network and propagates the error backward over the entire input sequence, one timestep at a time. The weights are then updated with the accumulated gradients.

BPTT can be slow to train recurrent neural networks on problems with very long input sequences. In addition to speed, the accumulation of gradients over so many timesteps can result in a shrinking of values to zero, or a growth of values that eventually overflow, or explode.

A modification of BPTT is to limit the number of timesteps used on the backward pass and in effect estimate the gradient used to update the weights rather than calculate it fully.

This variation is called Truncated Backpropagation Through Time, or TBPTT.

The TBPTT training algorithm has two parameters:

k1 : Defines the number of timesteps shown to the network on the forward pass.

: Defines the number of timesteps shown to the network on the forward pass. k2: Defines the number of timesteps to look at when estimating the gradient on the backward pass.

As such, we can use the notation TBPTT(k1, k2) when considering how to configure the training algorithm, where k1 = k2 = n, where n is the input sequence length for classical non-truncated BPTT.

Impact of TBPTT Configuration on the RNN Sequence Model

Modern recurrent neural networks like LSTMs can use their internal state to remember over very long input sequences. Such as over thousands of timesteps.

This means that the configuration of TBPTT does not necessarily define the memory of the network that you are optimizing with the choice of the number of timesteps. You can choose when the internal state of the network is reset separately from the regime used to update network weights.

Instead, the choice of TBPTT parameters influences how the network estimates the error gradient used to update the weights. More generally, the configuration defines the number of timesteps from which the network may be considered to model your sequence problem.

We can state this formally as something like:

yhat(t) = f(X(t), X(t-1), X(t-2), ... X(t-n)) 1 yhat(t) = f(X(t), X(t-1), X(t-2), ... X(t-n))

Where yhat is the output for a specific timestep, f(…) is the relationship that the recurrent neural network is approximating, and X(t) are observations at specific timesteps.

It is conceptually similar (but quite different in practice) to the window size on Multilayer Perceptrons trained on time series problems or to the p and q parameters of linear time series models like ARIMA. The TBPTT defines the scope of the input sequence for the model during training.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Keras Implementation of TBPTT

The Keras deep learning library provides an implementation of TBPTT for training recurrent neural networks.

The implementation is more restricted than the general version listed above.

Specifically, the k1 and k2 values are equal to each other and fixed.

TBPTT(k1, k2), where k1 = k2

This is realized by the fixed sized three-dimensional input required to train recurrent neural networks like the Long Short-Term Memory network, or LSTM.

The LSTM expects input data to have the dimensions: samples, timesteps, and features.

It is the second dimension of this input format, the timesteps that defines the number of timesteps used for forward and backward passes on your sequence prediction problem.

Therefore, careful choice must be given to the number of timesteps specified when preparing your input data for sequence prediction problems in Keras.

The choice of timesteps will influence both:

The internal state accumulated during the forward pass.

The gradient estimate used to update weights on the backward pass.

Note that by default, the internal state of the network is reset after each batch, but more explicit control over when the internal state is reset can be achieved by using a so-called stateful LSTM and calling the reset operation manually.

For more on stateful LSTMs in Keras, see the posts:

Prepare Sequence Data for TBPTT in Keras

The way that you break up your sequence data will define the number of timesteps used in the forward and backward passes of BPTT.

As such, you must put careful thought into how you will prepare your training data.

This section lists 6 techniques you may consider.

1. Use Data As-Is

You may use your input sequences as-is if the number of timesteps in each sequence is modest, such as tens or a few hundred timesteps.

Practical limits have been suggested for TBPTT of about 200-to-400 timesteps.

If your sequence data is less than or equal to this range, you may reshape the sequence observations as timesteps for the input data.

For example, if you had a collection of 100 univariate sequences of 25 timesteps, this could be reshaped as 100 samples, 25 timesteps, and 1 feature or [100, 25, 1].

2. Naive Data Split

If you have long input sequences, such as thousands of timesteps, you may need to break the long input sequences into multiple contiguous subsequences.

This will require the use of a stateful LSTM in Keras so that internal state is preserved across the input of the sub-sequences and only reset at the end of a true fuller input sequence.

For example, if you had 100 input sequences of 50,000 timesteps, then each input sequence could be divided into 100 subsequences of 500 timesteps. One input sequence would become 100 samples, therefore the 100 original samples would become 10,000. The dimensionality of the input for Keras would be 10,000 samples, 500 timesteps, and 1 feature or [10000, 500, 1]. Care would be needed to preserve state across each 100 subsequences and reset the internal state after each 100 samples either explicitly or by using a batch size of 100.

A split that neatly divides the full sequence into fixed-sized subsequences is preferred. The choice of the factor of the full sequence (subsequence length) is arbitrary, hence the name “naive data split”.

The splitting of the sequence into subsequences does not take into account domain information about a suitable number of timesteps to estimate the error gradient used to update weights.

3. Domain-Specific Data Split

It can be hard to know the correct number of timesteps required to provide a useful estimate of the error gradient.

We can use the naive approach (above) to get a model quickly, but the model may be far from optimized.

Alternately, we can use domain specific information to estimate the number of timesteps that will be relevant to the model while learning the problem.

For example, if the sequence problem is a regression time series, perhaps a review of the autocorrelation and partial autocorrelation plots can inform the choice of the number of the timesteps.

If the sequence problem is a natural language processing problem, perhaps the input sequence can be divided by sentence and then padded to a fixed length, or split according to the average sentence length in the domain.

Think broadly and consider what knowledge specific to your domain that you can use to split up the sequence into meaningful chunks.

4. Systematic Data Split (e.g. grid search)

Rather than guessing at a suitable number of timesteps, you can systematically evaluate a suite of different subsequence lengths for your sequence prediction problem.

You could perform a grid search over each sub-sequence length and adopt the configuration that results in the best performing model on average.

Some notes of caution if you are considering this approach:

Start with subsequence lengths that are a factor of the full sequence length.

Use padding and perhaps masking if exploring subsequence lengths that are not a factor of the full sequence length.

Consider using a slightly over-prescribed network (more memory cells and more training epochs) than is required to address the problem to help rule out network capacity as a limitation on your experiment.

Take the average performance over multiple runs (e.g. 30) of each different configuration.

If compute resources are not a limitation, then a systematic investigation of different numbers of timesteps is recommended.

5. Lean Heavily On Internal State With TBPTT(1, 1)

You can reformulate your sequence prediction problem as having one input and one output each timestep.

For example, if you had 100 sequences of 50 timesteps, each timestep would become a new sample. The 100 samples would become 5,000. The three-dimensional input would become 5,000 samples, 1 timestep, and 1 feature, or [5000, 1, 1].

Again, this would require the internal state to be preserved across each timestep of the sequence and reset at the end of each actual sequence (50 samples).

This would put the burden of learning the sequence prediction problem on the internal state of the recurrent neural network. Depending on the type of problem, it may be more than the network can handle and the prediction problem may not be learnable.

Personal experience suggests that this formulation may work well for prediction problems that require memory over the sequence, but perform poorly when the outcome is a complex function of past observations.

6. Decouple Forward and Backward Sequence Length

The Keras deep learning library used to support a decoupled number of timesteps for the forward and backward pass of Truncated Backpropagation Through Time.

In essence, the k1 parameter could be specified by the number of timesteps on input sequences and the k2 parameter could be specified by a “truncate_gradient” argument on the LSTM layer.

This is no longer supported, but there is some desire to have this feature re-added to the library. It is unclear exactly why it was removed, although evidence suggests it was done for efficiency reasons.

You could explore this approach in Keras. Some ideas include:

Install and use an older version of the Keras library that supports the “truncate_gradient” argument (circa 2015).

Extend the LSTM layer implementation in Keras to support a “truncate_gradient” type behavior.

Perhaps there are third-party extensions available for Keras that support this behavior.

If you find any, let me know in the comments below.

Summary

In this post, you discovered how you can prepare your sequence prediction problem data to make effective use of the Truncated Backpropagation Through Time training algorithm in the Python deep learning library Keras.

Specifically, you learned:

How Truncated Backpropagation Through Time works and how this is implemented in Keras.

How to reformulate or split your data with very long input sequences in the context of TBPTT.

How to systematically investigate different TBPTT configurations in Keras.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside"
105;news.mit.edu;http://news.mit.edu/2020/why-workers-smaller-share-GDP-0311;;Why are workers getting smaller pieces of the pie?;"It’s one of the biggest economic changes in recent decades: Workers get a smaller slice of company revenue, while a larger share is paid to capital owners and distributed as profits. Or, as economists like to say, there has been a fall in labor’s share of gross domestic product, or GDP.

A new study co-authored by MIT economists uncovers a major reason for this trend: Big companies that spend more on capital and less on workers are gaining market share, while smaller firms that spend more on workers and less on capital are losing market share. That change, the researchers say, is a key reason why the labor share of GDP in the U.S. has dropped from around 67 percent in 1980 to 59 percent today, following decades of stability.

“To understand this phenomenon, you need to understand the reallocation of economic activity across firms,” says MIT economist David Autor, co-author of the paper. “That’s our key point.”

To be sure, many economists have suggested other hypotheses, including new generations of software and machines that substitute directly for workers, the effects of international trade and outsourcing, and the decline of labor union power. The current study does not entirely rule out all of those explanations, but it does highlight the importance of what the researchers term “superstar firms” as a primary factor.

“We feel this is an incredibly important and robust fact pattern that you have to grapple with,” adds Autor, the Ford Professor of Economics in MIT’s Department of Economics.

The paper, “The Fall of the Labor Share and the Rise of Superstar Firms,” appears in advance online form in the Quarterly Journal of Economics. In addition to Autor, the other authors are David Dorn, a professor of economics at the University of Zurich; Lawrence Katz, a professor of economics at Harvard University; Christina Patterson, PhD ’19, a postdoc at Northwestern University who will join the faculty at the University of Chicago’s Booth School of Business in July; and John Van Reenen, the Gordon Y. Billard Professor of Management and Economics at MIT.

An economic “miracle” vanishes

For much of the 20th century, labor’s share of GDP was notably consistent. As the authors note, John Maynard Keynes once called it “something of a miracle” in the face of economic changes, and the British economist Nicholas Kaldor included labor’s steady portion of GDP as one of his often-cited six “stylized facts” of growth.

To conduct the study, the researchers scrutinized data for the U.S. and other countries in the Organization of Economic Cooperation and Development (OECD). The scholars used U.S. Economic Census data from 1982 to 2012 to study six economic sectors that account for about 80 percent of employment and GDP: manufacturing, retail trade, wholesale trade, services, utilities and transportation, and finance. The data includes payroll, total output, and total employment.

The researchers also used information from the EU KLEMS database, housed at the Vienna Institute for International Economic Studies, to examine the other OECD countries.

The increase in market dominance for highly competitive top firms in many of those sectors is evident in the data. In the retail trade, for instance, the top four firms accounted for just under 15 percent of sales in 1981, but that grew to around 30 percent of sales in 2011. In utilities and transportation, those figures moved from 29 percent to 41 percent in the same time frame. In manufacturing, this top-four sales concentration grew from 39 percent in 1981 to almost 44 percent in 2011.

At the same time, the average payroll-to-sales ratio declined in five of those sectors — with finance being the one exception. In manufacturing, the payroll-to-sales ratio decreased from roughly 18 percent in 1981 to about 12 percent in 2011. On aggregate, the labor share of GDP declined at most times except the period from 1997 to 2002, the final years of an economic expansion with high employment.

But surprisingly, labor’s share is not falling at the typical firm. Rather, reallocation of market share between firms is the key. In general, says Autor, the picture is of a “winner-take-most setting, where a smaller number of firms are accounting for a larger amount of economic activity, and those are firms where workers historically got a smaller share of the pie.”

A key insight provided by the study is that the dynamics within industry sectors has powered the drop in the labor share of GDP. The overall change is not just the result of, say, an increase in the deployment of technology in manufacturing, which some economists have suggested. While manufacturing is important to the big picture, the same phenomenon is unfolding across and within many sectors of the economy.

As far as testing the remaining alternate hypotheses, the study found no special pattern within industries linked to changes in trade policy — a subject Autor has studied extensively in the past. And while the decline in union power cannot be ruled out as a cause, the drop in labor share of GDP occurs even in countries where unions remain relatively stronger than they do in the U.S.

Deserved market power, or not?

As Autor notes, there are nuances within the findings. Many “superstar” firms pay above-average wages to their employees; it is not that these firms are increasingly “squeezing” their workers, as he puts it. Rather, labor’s share of the economic value added across the industrial sectors in the study is falling because market-leading “superstar” firms are now a bigger piece of all economic activity.

On a related note, Autor suggests that the growth in market power is related to technological investment by firms in many sectors.

“We shouldn’t presume that just because a market is concentrated — with a few leading firms accounting for a large fraction of sales — it’s a market with low productivity and high prices,” Autor says. “It might be a market where you have some very productive leading firms.” Today, he adds, “more competition is platform-based competition, as opposed to simple price competition. Walmart is a platform business. Amazon is a platform business. Many tech companies are platform businesses. Many financial services companies are platform businesses. You have to make some huge investment to create a sophisticated service or set of offerings. Once that’s in place, it’s hard for your competitors to replicate.”

With this in mind, Autor says we may want to distinguish whether market concentration is “the bad kind, where lazy monopolists are jacking up prices, or the good kind, where the more competitive firms are getting a larger share. To the best we can distinguish, the rise of superstar firms appears more the latter than the former. These firms are in more innovative industries — their productivity growth has developed faster, they make more investment, they patent more. It looks like this is happening more in the frontier sectors than the laggard sectors.”

Still Autor adds, the paper does contain policy implications for regulators.

“Once a firm is that far ahead, there’s potential for abuse,” he notes. “Maybe Facebook shouldn’t be allowed to buy all its competitors. Maybe Amazon shouldn’t be both the host of a market and a competitor in that market. This potentially creates regulatory issues we should be looking at. There’s nothing in this paper that says everyone should just take a few years off and not worry about the issue.”

“We don’t think our paper is in any sense the last word on the topic,” Autor notes. “We think it adds useful paragraphs to the conversation for everybody to listen to and grapple with. We’ve had too few facts chased by too many theories. We need more facts to allow us to adjudicate among theories.”

Support for the research project was provided by Accenture LLC, the Economic and Social Research Council, the European Research Council, IBM Global Universities Programs, the MIT Initiative on the Digital Economy, the National Science Foundation, Schmidt Futures, the Sloan Foundation, the Smith Richardson Foundation, and the Swiss National Science Foundation."
106;machinelearningmastery.com;https://machinelearningmastery.com/promise-of-deep-learning-for-computer-vision/;2019-03-25;A Gentle Introduction to the Promise of Deep Learning for Computer Vision;"Tweet Share Share

Last Updated on July 5, 2019

The promise of deep learning in the field of computer vision is better performance by models that may require more data but less digital signal processing expertise to train and operate.

There is a lot of hype and large claims around deep learning methods, but beyond the hype, deep learning methods are achieving state-of-the-art results on challenging problems. Notably, on computer vision tasks such as image classification, object recognition, and face detection.

In this post, you will discover the specific promises that deep learning methods have for tackling computer vision problems.

After reading this post, you will know:

The promises of deep learning for computer vision.

Examples of where deep learning has or is delivering on its promises.

Key deep learning methods and applications for computer vision.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Promises of Deep Learning Types of Deep Learning Networks Models Types of Computer Vision Problems

Promises of Deep Learning

Deep learning methods are popular, primarily because they are delivering on their promise.

That is not to say that there is no hype around the technology, but that the hype is based on very real results that are being demonstrated across a suite of very challenging artificial intelligence problems from computer vision and natural language processing.

Some of the first large demonstrations of the power of deep learning were in computer vision, specifically image recognition. More recently in object detection and face recognition.

In this post, we will look at five specific promises of deep learning methods in the field of computer vision.

In summary, they are:

The Promise of Automatic Feature Extraction . Features can be automatically learned and extracted from raw image data.

. Features can be automatically learned and extracted from raw image data. The Promise of End-to-End Models . Single end-to-end models can replace pipelines of specialized models.

. Single end-to-end models can replace pipelines of specialized models. The Promise of Model Reuse . Learned features and even entire models can be reused across tasks.

. Learned features and even entire models can be reused across tasks. The Promise of Superior Performance . Techniques demonstrate better skill on challenging tasks.

. Techniques demonstrate better skill on challenging tasks. The Promise of General Method. A single general method can be used on a range of related tasks.

We will now take a closer look at each.

There are other promises of deep learning for computer vision; these were just the five that I chose to highlight.

What do you think the promise of deep learning is for computer vision?

Let me know in the comments below.

Promise 1: Automatic Feature Extraction

A major focus of study in the field of computer vision is on techniques to detect and extract features from digital images.

Extracted features provide the context for inference about an image, and often the richer the features, the better the inference.

Sophisticated hand-designed features such as scale-invariant feature transform (SIFT), Gabor filters, and histogram of oriented gradients (HOG) have been the focus of computer vision for feature extraction for some time, and have seen good success.

The promise of deep learning is that complex and useful features can be automatically learned directly from large image datasets. More specifically, that a deep hierarchy of rich features can be learned and automatically extracted from images, provided by the multiple deep layers of neural network models.

They have deeper architectures with the capacity to learn more complex features than the shallow ones. Also the expressivity and robust training algorithms allow to learn informative object representations without the need to design features manually.

— Object Detection with Deep Learning: A Review, 2018.

Deep neural network models are delivering on this promise, most notably demonstrated by the transition away from sophisticated hand-crafted feature detection methods such as SIFT toward deep convolutional neural networks on standard computer vision benchmark datasets and competitions, such as the ImageNet Large Scale Visual Recognition Competition (ILSVRC).

ILSVRC over the past five years has paved the way for several breakthroughs in computer vision. The field of categorical object recognition has dramatically evolved […] starting from coded SIFT features and evolving to large-scale convolutional neural networks dominating at all three tasks of image classification, single-object localization, and object detection.

— ImageNet Large Scale Visual Recognition Challenge, 2015.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Promise 2: End-to-End Models

Addressing computer vision tasks traditionally involved using a system of modular models.

Each model was designed for a specific task, such as feature extraction, image alignment, or classification. The models are used in a pipeline with a raw image at one end and an outcome, such as a prediction, at the other end.

This pipeline approach can and is still used with deep learning models, where a feature detector model can be replaced with a deep neural network.

Alternately, deep neural networks allow a single model to subsume two or more traditional models, such as feature extraction and classification. It is common to use a single model trained directly on raw pixel values for image classification, and there has been a trend toward replacing pipelines that use a deep neural network model where a single model is trained end-to-end directly.

With the availability of so much training data (along with an efficient algorithmic implementation and GPU computing resources) it became possible to learn neural networks directly from the image data, without needing to create multi-stage hand-tuned pipelines of extracted features and discriminative classifiers.

— ImageNet Large Scale Visual Recognition Challenge, 2015.

A good example of this is in object detection and face recognition where initially superior performance was achieved using a deep convolutional neural network for feature extraction only, where more recently, end-to-end models are trained directly using multiple-output models (e.g. class and bounding boxes) and/or new loss functions (e.g. contrastive or triplet loss functions).

Promise 3: Model Reuse

Typically, the feature detectors prepared for a dataset are highly specific to that dataset.

This makes sense, as the more domain information that you can use in the model, the better the model is likely to perform in the domain.

Deep neural networks are typically trained on datasets that are much larger than traditional datasets, e.g. millions or billions of images. This allows the models to learn features and hierarchies of features that are general across photographs, which is itself remarkable.

If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems may involve completely different classes than those of the original task.

— Page 143, Deep Learning with Python, 2017.

For example, it is common to use deep models trained in the large ImageNet dataset, or a subset of this dataset, directly or as a starting point on a range of computer vision tasks.

… it is common to use the features from a convolutional network trained on ImageNet to solve other computer vision tasks

— Page 426, Deep Learning, 2016.

This is called transfer learning, and the use of pretrained models that can take days and sometimes weeks to train has become standard practice.

The pretrained models can be used to extract useful general features from digital images and can also be fine-tuned, tailored to the specifics of the new task. This can save a lot of time and resources and result in very good results almost immediately.

A common and highly effective approach to deep learning on small image datasets is to use a pretrained network.

— Page 143, Deep Learning with Python, 2017.

Promise 4: Superior Performance

An important promise of deep neural networks in computer vision is better performance.

It is the dramatically better performance with deep neural networks that has been a catalyst for the growth and interest in the field of deep learning. Although the techniques have been around for decades, the spark was the outstanding performance by Alex Krizhevsky, et al. in 2012 for image classification.

The current intensity of commercial interest in deep learning began when Krizhevsky et al. (2012) won the ImageNet object recognition challenge …

— Page 371, Deep Learning, 2016.

Their deep convolutional neural network model, at the time called SuperVision, and later referred to as AlexNet, resulted in a leap in classification accuracy.

We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.

— ImageNet Classification with Deep Convolutional Neural Networks, 2012.

The technique was then adopted for a range of very challenging computer vision tasks, including object detection, which also saw a large leap in model performance over then state-of-the-art traditional methods.

The first breakthrough in object detection was the RCNN which resulted in an improvement of nearly 30% over the previous state of the art.

— A Survey of Modern Object Detection Literature using Deep Learning, 2018.

This trend of improvement has continued year-over-year on a range of computer vision tasks.

Performance has been so dramatic that tasks previously thought not easily addressable by computers and used as CAPTCHA to prevent spam (such as predicting whether a photo is of a dog or cat) are effectively “solved” and models on problems such as face recognition achieve better-than-human performance.

We can observe significant performance (mean average precision) improvement since deep learning entered the scene in 2012. The performance of the best detector has been steadily increasing by a significant amount on a yearly basis.

— Deep Learning for Generic Object Detection: A Survey, 2018.

Promise 5: General Method

Perhaps the most important promise of deep learning is that the top-performing models are all developed from the same basic components.

The impressive results have come from one type of network, called the convolutional neural network, comprised of convolutional and pooling layers. It was specifically designed for image data and can be trained on pixel data directly (with some minor scaling).

Convolutional networks provide a way to specialize neural networks to work with data that has a clear grid-structured topology and to scale such models to very large size. This approach has been the most successful on a two-dimensional, image topology.

— Page 372, Deep Learning, 2016.

This is different from the broader field that may have required specialized feature detection methods developed for handwriting recognition, character recognition, face recognition, object detection, and so on. Instead, a single general class of model can be configured and used across each computer vision task directly.

This is the promise of machine learning in general; it is impressive that such a versatile technique has been discovered and demonstrated for computer vision.

Further, the model is relatively straightforward to understand and to train, although may require modern GPU hardware to train efficiently on a large dataset, and may require model hyperparameter tuning to achieve bleeding-edge performance.

Types of Deep Learning Networks Models

Deep Learning is a large field of study, and not all of it is relevant to computer vision.

It is easy to get bogged down in specific optimization methods or extensions to model types intended to lift performance.

From a high-level, there is one method from deep learning that deserves the most attention for application in computer vision. It is:

Convolutional Neural Networks (CNNs).

The reason that CNNs are the focus of attention for deep learning models is that they were specifically designed for image data.

Additionally, both of the following network types may be useful for interpreting or developing inference models from the features learned and extracted by CNNs; they are:

Multilayer Perceptrons (MLP).

Recurrent Neural Networks (RNNs).

The MLP or fully-connected type neural network layers are useful for developing models that make predictions given the learned features extracted by CNNs. RNNs, such as LSTMs, may be helpful when working with sequences of images over time, such as with video.

Types of Computer Vision Problems

Deep learning will not solve computer vision or artificial intelligence.

To date, deep learning methods have been evaluated on a broader suite of problems from computer vision and achieved success on a small set, where success suggests performance or capability at or above what was previously possible with other methods.

Importantly, those areas where deep learning methods are showing the greatest success are some of the more end-user facing, challenging, and perhaps more interesting problems.

Five examples include:

Optical Character Recognition.

Image Classification.

Object Detection.

Face Detection.

Face Recognition.

All five tasks are related under the umbrella of “object recognition,” which refers to tasks that involve identifying, localizing, and/or extracting specific content from digital photographs.

Most deep learning for computer vision is used for object recognition or detection of some form, whether this means reporting which object is present in an image, annotating an image with bounding boxes around each object, transcribing a sequence of symbols from an image, or labeling each pixel in an image with the identity of the object it belongs to.

— Page 453, Deep Learning, 2016.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Papers

Summary

In this post, you discovered the specific promises that deep learning methods have for tackling computer vision problems.

Specifically, you learned:

The promises of deep learning for computer vision.

Examples of where deep learning has or is delivering on its promises.

Key deep learning methods and applications for computer vision.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning Models for Vision Today! Develop Your Own Vision Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Computer Vision It provides self-study tutorials on topics like:

classification, object detection (yolo and rcnn), face recognition (vggface and facenet), data preparation and much more... Finally Bring Deep Learning to your Vision Projects Skip the Academics. Just Results. See What's Inside"
107;machinelearningmastery.com;http://machinelearningmastery.com/learning-vector-quantization-for-machine-learning/;2016-04-17;Learning Vector Quantization for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

A downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset.

The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that lets you choose how many training instances to hang onto and learns exactly what those instances should look like.

In this post you will discover the Learning Vector Quantization algorithm. After reading this post you will know:

The representation used by the LVQ algorithm that you actually save to a file.

The procedure that you can use to make predictions with a learned LVQ model.

How to learn an LVQ model from training data.

The data preparation to use to get the best performance from the LVQ algorithm.

Where to look for more information on LVQ.

This post was written for developers and assumes no background in statistics or mathematics. The post focuses on how the algorithm works and how to use it for predictive modeling problems.

If you have any questions on LVQ, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

LVQ Model Representation

The representation for LVQ is a collection of codebook vectors.

LVQ was developed and is best understood as a classification algorithm. It supports both binary (two-class) and multi-class classification problems.

A codebook vector is a list of numbers that have the same input and output attributes as your training data. For example, if your problem is a binary classification with classes 0 and 1, and the inputs width, length height, then a codebook vector would be comprised of all four attributes: width, length, height and class.

The model representation is a fixed pool of codebook vectors, learned from the training data. They look like training instances, but the values of each attribute have been adapted based on the learning procedure.

In the language of neural networks, each codebook vector may be called a neuron, each attribute on a codebook vector is called a weight and the collection of codebook vectors is called a network.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Making Predictions with an LVQ Model

Predictions are made using the LVQ codebook vectors in the same way as K-Nearest Neighbors.

Predictions are made for a new instance (x) by searching through all codebook vectors for the K most similar instances and summarizing the output variable for those K instances. For classification this is the mode (or most common) class value.

Typically predictions are made with K=1, and the codebook vector that matches is called the Best Matching Unit (BMU).

To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance. Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) for each attribute j.

EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )

Learning an LVQ Model From Data

The LVQ algorithm learns the codebook vectors from the training data.

You must choose the number of codebook vectors to use, such as 20 or 40. You can find the best number of codebook vectors to use by testing different configurations on your training dataset.

The learning algorithm starts with a pool of random codebook vectors. These could be randomly selected instances from the training data, or randomly generated vectors with the same scale as the training data. Codebook vectors have the same number of input attributes as the training data. They also have an output class variable.

The instances in the training dataset are processed one at a time. For a given training instance, the most similar codebook vector is selected from the pool.

If the codebook vector has the same output as the training instance, the codebook vector is moved closer to the training instance. If it does not match, it is moved further away. The amount that the vector is moved is controlled by an algorithm parameter called the learning_rate.

For example, the input variable (x) of a codebook vector is moved closer to the training input value (t) by the amount in the learning_rate if the classes match as follows:

x = x + learning_rate * (t – x)

The opposite case of moving the input variables of a codebook variable away from a training instance is calculated as:

x = x – learning_rate * (t – x)

This would be repeated for each input variable.

Because one codebook vector is selected for modification for each training instance the algorithm is referred to as a winner-take-all algorithm or a type of competitive learning.

This process is repeated for each instance in the training dataset. One iteration of the training dataset is called an epoch. The process is completed for a number of epochs that you must choose (max_epoch), such as 200.

You must also choose an initial learning rate (such as alpha=0.3). The learning rate is decreased with the epoch, starting at the large value you specify at epoch 1 which makes the most change to the codebook vectors and finishing with a small value near zero on the last epoch, making very minor changes to the codebook vectors.

The learning rate for each epoch is calculated as:

learning_rate = alpha * (1 – (epoch/max_epoch))

Where learning_rate is the learning rate for the current epoch (0 to max_epoch-1), alpha is the learning rate specified to the algorithm at the start of the training run and max_epoch is the total number of epochs to run the algorithm also specified at the start of the run.

The intuition for the learning process is that the pool of codebook vectors is a compression of the training dataset to the points that best characterize the separation of the classes.

Data Preparation for LVQ

Generally, it is a good idea to prepare data for LVQ in the same way as you would prepare it for K-Nearest Neighbors.

Classification : LVQ is a classification algorithm that works for both binary (two-class) and multi-class classification algorithms. The technique has been adapted for regression.

: LVQ is a classification algorithm that works for both binary (two-class) and multi-class classification algorithms. The technique has been adapted for regression. Multiple-Passes : Good technique with LVQ involves performing multiple passes of the training dataset over the codebook vectors (e.g. multiple learning runs). The first with a higher learning rate to settle the pool codebook vectors and the second run with a small learning rate to fine tune the vectors.

: Good technique with LVQ involves performing multiple passes of the training dataset over the codebook vectors (e.g. multiple learning runs). The first with a higher learning rate to settle the pool codebook vectors and the second run with a small learning rate to fine tune the vectors. Multiple Best Matches : Extensions of LVQ select multiple best matching units to modify during learning, such as one of the same class and one of a different class which are drawn toward and away from a training sample respectively. Other extensions use a custom learning rate for each codebook vector. These extensions can improve the learning process.

: Extensions of LVQ select multiple best matching units to modify during learning, such as one of the same class and one of a different class which are drawn toward and away from a training sample respectively. Other extensions use a custom learning rate for each codebook vector. These extensions can improve the learning process. Normalize Inputs : Traditionally, inputs are normalized (rescaled) to values between 0 and 1. This is to avoid one attribute from dominating the distance measure. If the input data is normalized, then the initial values for the codebook vectors can be selected as random values between 0 and 1.

: Traditionally, inputs are normalized (rescaled) to values between 0 and 1. This is to avoid one attribute from dominating the distance measure. If the input data is normalized, then the initial values for the codebook vectors can be selected as random values between 0 and 1. Feature Selection: Feature selection that can reduce the dimensionality of the input variables can improve the accuracy of the method. LVQ suffers from the same curse of dimensionality in making predictions as K-Nearest Neighbors.

LVQ Tutorial in Python

For a step-by-step tutorial on implementing LVQ from scratch in Python, see the post;

Further Reading

The technique was developed by Kohonen who wrote the seminal book on LVQ and the sister method Self-Organizing Maps called: Self-Organizing Maps.

I highly recommend this book if you are interested in LVQ.

Summary

In this post you discovered the LVQ algorithm. You learned:

The representation for LVQ is a small pool of codebook vectors, smaller than the training dataset.

The codebook vectors are used to make predictions using the same technique as K-Nearest Neighbors.

The codebook vectors are learned from the training dataset by moving them closer when they are good match and further away when they are a bad match.

The codebook vectors are a compression of the training data to best separate the classes.

Data preparation traditionally involves normalizing the input values to the range between 0 and 1.

Do you have any questions about this post or the LVQ algorithm? Leave a comment and ask your question and I will do my best to answer it.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
108;machinelearningmastery.com;http://machinelearningmastery.com/ladder-approach-to-becoming-a-machine-learning-consultant/;2015-03-02;Get Paid To Apply Machine Learning;"Tweet Share Share

Last Updated on September 27, 2016

The Ladder Approach That You Can Use To Become a

Machine Learning Consultant

Do you want to do machine learning and get paid for it?

Be careful what you wish for.

In this post I outline a blueprint that you can use to learn enough machine learning to help small businesses and start-ups with their general data needs.

It’s not easy, you will have to work hard outside of your comfort zone. You will have to talk to real people in the real world!

Blueprint

The blueprint presented in this post will take you from a passionate interest in machine learning and the dedication to learn through to being capable and confident to work through the general data problems in a small to medium business or start-up and deliver a solution.

The blueprint for this path is as follows:

Build a foundation Build a portfolio Deliver solutions

Given your background and interests, you can tailor the roadmap to your needs.

To be clear, we are only interested in applied machine learning. We are only interested in theory and tools as much as they allow you to better understand your problem and achieve better results on the problem you are working.

This is a counter-intuitive but very productive view. Learn what you need just-in-time and focus on delivering results. It is about reliably achieving good results, not perfection.

1. Build a Foundation

You need to learn enough applied machine learning to have the confidence to work a problem from start to finish. To define it accurately and deliver a model or report required as an outcome for the project.

Pick and learn a process. Learn a step-by-step process that you can follow that will take you from problem definition through to delivering a result. Some examples include KDD, Crisp-DM, OSEMN, and others. Pick and learn a tool. Learn a tool or libraries that you can use to complete your selected process. I recommend one of Weka, scikit-learn, R depending on your interests and preference. Practice on small datasets. Download small datasets on which you can practice. Spend a lot of time on the UCI ML repository.

You are ready to move on when you are confident and capable enough to pick an arbitrary in-memory problem and use your tool to work it from start to finish.

2. Build a Portfolio

Once you have a foundation capability to work problems you need objective indicators that others can use to evaluate your capability. You need completed projects that demonstrate your ability to deliver.

You can do this by building a portfolio of completed machine learning projects.

Interlude on Mindset

Pause for a moment and take on the mindset of a manager or small business owner with a data problem.

As such a person, you are hiring programmers based on their ability to deliver results on project at other companies and in open source. You are hiring marketers based on their ability to lift conversions to attack the bottomline. If such a manager needed a “data guy” to deliver a report or a model, what would they look at to evaluate that a candidate could deliver a result?

Me in that position, I would want to see evidence of completed projects. More than that, I would want to see evidence of completed projects that are very close to the result I am looking for.

Your Portfolio

Pick a theme. This is the type of projects that you want to work on. A no-brainer would be reports on customer data (high-value customers, predictions of prospects that convert, etc.). Find open datasets. You need to locate datasets that you can practice on that are close to or on your theme. Look on competition websites like Kaggle and KDDCup as a starting point. There are a lot of public access datasets these days that you can practice on! Complete projects. Treat each dataset like a project with a client and apply your process to it in order to deliver a result. This may require you to assume the role of the client and take an educated guess as to the outcome they are looking for (model or report on a specific question, etc.) Write-up. Write-up your findings as a semi-formal work product and host it publicly online.

This last point is key and I will elaborate it.

Ideally, make each part of your process scripted so that you can re-execute it any time as you find bugs or gain insight. Consider uploading all of your code and scripts to a public github account for the project.

Write up the result of each project as a technical report or a power point. Consider recording a short video presenting your findings. Host the report on github, your blog, or somewhere. Write up the project on your public LinkedIn profile.

Your goal is to have a place that you can point someone and they can see all of the projects you have completed at a glance, and dive down into one and see what you did and what you delivered.

You are ready to move on when you can objectively convince someone that you are able to deliver results on your theme. I think 3-5 modest sized completed projects would be reasonable.

Learn more about building a portfolio of machine learning projects in the post “Build a Machine Learning Portfolio: Complete Small Focused Projects and Demonstrate Your Skills“.

3. Deliver Solutions

Now that you have the capability to deliver and evidence to that fact, it is time to seek out projects in the wild for you to complete.

You are going to have to get out there and talk to people. This step will be the great filter. This step may be a little scary and a little difficult and it will be your true test.

Find someone that you can help. Use your social network. Attend meet-ups, get introductions, etc. Look for a small company or start-up that you can meet with face to face (ideally) find out about their problems and get access to their data. Be honest. Tell the truth. Explain where you have come from, what you have done and what you can do for them. Consider doing the first piece of work for free or cheap to get your first project under your belt. Your path is an advantage, it shows you are hungry, eager to deliver and driven. We all want to work people that present this way. Deliver. Do the work. Specify the project accurately, keep the scope small and clear and deliver what you say you will deliver. Again, don’t promise something you have not done before or don’t know how to do. Repeat.

Keep projects small in scope and short in time. Ideally, deliver in 1-2 weeks. You need momentum, fast results and fast learnings for your client.

As you complete real projects, add them to your portfolio (in a muted form respecting the privacy of your clients).

Summary

In this post you discovered a roadmap that you can use to take your passionate interest in machine learning and turn it into a consulting gig.

There is a not a lot of hand-holding in this approach. This makes it exciting and empowering. You can execute this approach to your level of comfort and take on some moonlighting work or a whole new career.

If you have followed this path or know someone that has, leave a comment and share your experiences."
109;machinelearningmastery.com;http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/;2016-08-18;How to Develop Your First XGBoost Model in Python with scikit-learn;"# First XGBoost model for Pima Indians dataset

from numpy import loadtxt

from xgboost import XGBClassifier

from sklearn . model_selection import train_test_split

from sklearn . metrics import accuracy_score

# load data

dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" )

# split data into X and y

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# split data into train and test sets

seed = 7

test_size = 0.33

X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = test_size , random_state = seed )

# fit model no training data

model = XGBClassifier ( )

model . fit ( X_train , y_train )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )"
110;news.mit.edu;http://news.mit.edu/2020/psfc-receives-arpa-e-funding-to-explore-practical-fusion-paths-0415;;Plasma Science and Fusion Center receives $1.25M from ARPA-E to explore practical paths to fusion;"The Plasma Science and Fusion Center (PSFC) will receive $1.25 million in funding from the U.S. Department of Energy’s Advanced Research Projects Agency-Energy (ARPA-E). The award is part of the $32 million Breakthrough Enabling Thermonuclear-fusion Energy program established to explore lower-cost approaches to creating energy from nuclear fusion.

Fusion requires confining plasmas at extraordinarily high temperatures, up to 200 million degrees Celsius. One of the most promising ways to heat plasmas to these temperatures is with electromagnetic waves. Fusion power plants will need tens of thousands of kilowatts to be launched and absorbed by the plasma — with each kilowatt roughly the power of a conventional microwave oven. Complex analytic theory and computer simulations are required to design effective and efficient plasma heating scenarios. MIT’s project seeks to apply established state-of-the-art theoretical and simulation tools, developed and tested by the fusion community on more traditional concepts like tokamaks and stellarators, to explore the potential of novel, lower-cost fusion concepts.

The principal investigator for the project, PSFC Principal Research Scientist John Wright, along with PSFC Principal Research Scientist Abhay Ram, comprise the MIT team that will be working with colleagues from Oak Ridge National Laboratory, Lawrence Livermore National Laboratory, and CompX, a private company. They are excited about leveraging MIT radio-frequency (RF) heating expertise in the service of new, or renewed, approaches to fusion.

“The main fusion program pursues the tokamak and the stellarator,” says Wright, “but there are other magnetic geometries considered by the program in the past that have been set aside due to difficulties at the time. They may deserve a second look, to see if any of these have a shorter or cheaper path to fusion. These include the mirror concept, for which there are two ARPA-E experimental awards.”

While a tokamak confines hot plasma in a toroidal chamber, using high magnetic fields to steer it away from the interior walls, a mirror contains the plasma in a linear device that features higher-strength magnets at each end. The PSFC will help explore the mirror’s potential both as a fusion concept and as a source of neutrons, which are needed to study the effect of neutrons on materials used in long-lived fusion or fission devices.

The PSFC will work closely with two other projects funded in this ARPA-E funding round — one on magnetic mirrors led by the University of Wisconsin at Madison, and another led by Commonwealth Fusion Systems (CFS) on fast-ramping high-temperature superconducting solenoids. The PSFC is a significant collaborator on the CFS-led project, responsible for testing the performance of the novel magnets built by CFS. If successfully demonstrated, these magnets will reduce the cost and complexity of commercial tokamak power plants, further accelerating the advent of commercial fusion. An immediate application for these magnets would be for the SPARC project, now under design by a joint team from MIT and CFS. The collaboration with the University of Wisconsin grew out of discussions about applications of high-temperature superconducting magnets to other fusion experimental configurations.

“The tools we have developed in the tokamak community can help other fusion concepts, including the mirror, to do as well as possible,” says Wright. “We are excited to take our knowledge of RF in tokamak and stellarator geometries and see what surprises there are as we travel other paths to fusion.”"
111;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/;2019-05-16;How to Classify Photos of Dogs and Cats (with 97% accuracy);"Tweet Share Share

Last Updated on October 3, 2019

Develop a Deep Convolutional Neural Network Step-by-Step to Classify Photographs of Dogs and Cats

The Dogs vs. Cats dataset is a standard computer vision dataset that involves classifying photos as either containing a dog or cat.

Although the problem sounds simple, it was only effectively addressed in the last few years using deep learning convolutional neural networks. While the dataset is effectively solved, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch.

This includes how to develop a robust test harness for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.

In this tutorial, you will discover how to develop a convolutional neural network to classify photos of dogs and cats.

After completing this tutorial, you will know:

How to load and prepare photos of dogs and cats for modeling.

How to develop a convolutional neural network for photo classification from scratch and improve model performance.

How to develop a model for photo classification using transfer learning.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Updated Oct/2019: Updated for Keras 2.3 and TensorFlow 2.0.

Tutorial Overview

This tutorial is divided into six parts; they are:

Dogs vs. Cats Prediction Problem Dogs vs. Cats Dataset Preparation Develop a Baseline CNN Model Develop Model Improvements Explore Transfer Learning How to Finalize the Model and Make Predictions

Dogs vs. Cats Prediction Problem

The dogs vs cats dataset refers to a dataset used for a Kaggle machine learning competition held in 2013.

The dataset is comprised of photos of dogs and cats provided as a subset of photos from a much larger dataset of 3 million manually annotated photos. The dataset was developed as a partnership between Petfinder.com and Microsoft.

The dataset was originally used as a CAPTCHA (or Completely Automated Public Turing test to tell Computers and Humans Apart), that is, a task that it is believed a human finds trivial, but cannot be solved by a machine, used on websites to distinguish between human users and bots. Specifically, the task was referred to as “Asirra” or Animal Species Image Recognition for Restricting Access, a type of CAPTCHA. The task was described in the 2007 paper titled “Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization“.

We present Asirra, a CAPTCHA that asks users to identify cats out of a set of 12 photographs of both cats and dogs. Asirra is easy for users; user studies indicate it can be solved by humans 99.6% of the time in under 30 seconds. Barring a major advance in machine vision, we expect computers will have no better than a 1/54,000 chance of solving it.

— Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization, 2007.

At the time that the competition was posted, the state-of-the-art result was achieved with an SVM and described in a 2007 paper with the title “Machine Learning Attacks Against the Asirra CAPTCHA” (PDF) that achieved 80% classification accuracy. It was this paper that demonstrated that the task was no longer a suitable task for a CAPTCHA soon after the task was proposed.

… we describe a classifier which is 82.7% accurate in telling apart the images of cats and dogs used in Asirra. This classifier is a combination of support-vector machine classifiers trained on color and texture features extracted from images. […] Our results suggest caution against deploying Asirra without safeguards.

— Machine Learning Attacks Against the Asirra CAPTCHA, 2007.

The Kaggle competition provided 25,000 labeled photos: 12,500 dogs and the same number of cats. Predictions were then required on a test dataset of 12,500 unlabeled photographs. The competition was won by Pierre Sermanet (currently a research scientist at Google Brain) who achieved a classification accuracy of about 98.914% on a 70% subsample of the test dataset. His method was later described as part of the 2013 paper titled “OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.”

The dataset is straightforward to understand and small enough to fit into memory. As such, it has become a good “hello world” or “getting started” computer vision dataset for beginners when getting started with convolutional neural networks.

As such, it is routine to achieve approximately 80% accuracy with a manually designed convolutional neural network and 90%+ accuracy using transfer learning on this task.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Dogs vs. Cats Dataset Preparation

The dataset can be downloaded for free from the Kaggle website, although I believe you must have a Kaggle account.

If you do not have a Kaggle account, sign-up first.

Download the dataset by visiting the Dogs vs. Cats Data page and click the “Download All” button.

This will download the 850-megabyte file “dogs-vs-cats.zip” to your workstation.

Unzip the file and you will see train.zip, train1.zip and a .csv file. Unzip the train.zip file, as we will be focusing only on this dataset.

You will now have a folder called ‘train/‘ that contains 25,000 .jpg files of dogs and cats. The photos are labeled by their filename, with the word “dog” or “cat“. The file naming convention is as follows:

cat.0.jpg ... cat.124999.jpg dog.0.jpg dog.124999.jpg 1 2 3 4 5 cat.0.jpg ... cat.124999.jpg dog.0.jpg dog.124999.jpg

Plot Dog and Cat Photos

Looking at a few random photos in the directory, you can see that the photos are color and have different shapes and sizes.

For example, let’s load and plot the first nine photos of dogs in a single figure.

The complete example is listed below.

# plot dog photos from the dogs vs cats dataset from matplotlib import pyplot from matplotlib.image import imread # define location of dataset folder = 'train/' # plot first few images for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # define filename filename = folder + 'dog.' + str(i) + '.jpg' # load image pixels image = imread(filename) # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # plot dog photos from the dogs vs cats dataset from matplotlib import pyplot from matplotlib . image import imread # define location of dataset folder = 'train/' # plot first few images for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # define filename filename = folder + 'dog.' + str ( i ) + '.jpg' # load image pixels image = imread ( filename ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example creates a figure showing the first nine photos of dogs in the dataset.

We can see that some photos are landscape format, some are portrait format, and some are square.

We can update the example and change it to plot cat photos instead; the complete example is listed below.

# plot cat photos from the dogs vs cats dataset from matplotlib import pyplot from matplotlib.image import imread # define location of dataset folder = 'train/' # plot first few images for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # define filename filename = folder + 'cat.' + str(i) + '.jpg' # load image pixels image = imread(filename) # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # plot cat photos from the dogs vs cats dataset from matplotlib import pyplot from matplotlib . image import imread # define location of dataset folder = 'train/' # plot first few images for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # define filename filename = folder + 'cat.' + str ( i ) + '.jpg' # load image pixels image = imread ( filename ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Again, we can see that the photos are all different sizes.

We can also see a photo where the cat is barely visible (bottom left corner) and another that has two cats (lower right corner). This suggests that any classifier fit on this problem will have to be robust.

Select Standardized Photo Size

The photos will have to be reshaped prior to modeling so that all images have the same shape. This is often a small square image.

There are many ways to achieve this, although the most common is a simple resize operation that will stretch and deform the aspect ratio of each image and force it into the new shape.

We could load all photos and look at the distribution of the photo widths and heights, then design a new photo size that best reflects what we are most likely to see in practice.

Smaller inputs mean a model that is faster to train, and typically this concern dominates the choice of image size. In this case, we will follow this approach and choose a fixed size of 200×200 pixels.

Pre-Process Photo Sizes (Optional)

If we want to load all of the images into memory, we can estimate that it would require about 12 gigabytes of RAM.

That is 25,000 images with 200x200x3 pixels each, or 3,000,000,000 32-bit pixel values.

We could load all of the images, reshape them, and store them as a single NumPy array. This could fit into RAM on many modern machines, but not all, especially if you only have 8 gigabytes to work with.

We can write custom code to load the images into memory and resize them as part of the loading process, then save them ready for modeling.

The example below uses the Keras image processing API to load all 25,000 photos in the training dataset and reshapes them to 200×200 square photos. The label is also determined for each photo based on the filenames. A tuple of photos and labels is then saved.

# load dogs vs cats dataset, reshape and save to a new file from os import listdir from numpy import asarray from numpy import save from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array # define location of dataset folder = 'train/' photos, labels = list(), list() # enumerate files in the directory for file in listdir(folder): # determine class output = 0.0 if file.startswith('cat'): output = 1.0 # load image photo = load_img(folder + file, target_size=(200, 200)) # convert to numpy array photo = img_to_array(photo) # store photos.append(photo) labels.append(output) # convert to a numpy arrays photos = asarray(photos) labels = asarray(labels) print(photos.shape, labels.shape) # save the reshaped photos save('dogs_vs_cats_photos.npy', photos) save('dogs_vs_cats_labels.npy', labels) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # load dogs vs cats dataset, reshape and save to a new file from os import listdir from numpy import asarray from numpy import save from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array # define location of dataset folder = 'train/' photos , labels = list ( ) , list ( ) # enumerate files in the directory for file in listdir ( folder ) : # determine class output = 0.0 if file . startswith ( 'cat' ) : output = 1.0 # load image photo = load_img ( folder + file , target_size = ( 200 , 200 ) ) # convert to numpy array photo = img_to_array ( photo ) # store photos . append ( photo ) labels . append ( output ) # convert to a numpy arrays photos = asarray ( photos ) labels = asarray ( labels ) print ( photos . shape , labels . shape ) # save the reshaped photos save ( 'dogs_vs_cats_photos.npy' , photos ) save ( 'dogs_vs_cats_labels.npy' , labels )

Running the example may take about one minute to load all of the images into memory and prints the shape of the loaded data to confirm it was loaded correctly.

Note: running this example assumes you have more than 12 gigabytes of RAM. You can skip this example if you do not have sufficient RAM; it is only provided as a demonstration.

(25000, 200, 200, 3) (25000,) 1 (25000, 200, 200, 3) (25000,)

At the end of the run, two files with the names ‘dogs_vs_cats_photos.npy‘ and ‘dogs_vs_cats_labels.npy‘ are created that contain all of the resized images and their associated class labels. The files are only about 12 gigabytes in size together and are significantly faster to load than the individual images.

The prepared data can be loaded directly; for example:

# load and confirm the shape from numpy import load photos = load('dogs_vs_cats_photos.npy') labels = load('dogs_vs_cats_labels.npy') print(photos.shape, labels.shape) 1 2 3 4 5 # load and confirm the shape from numpy import load photos = load ( 'dogs_vs_cats_photos.npy' ) labels = load ( 'dogs_vs_cats_labels.npy' ) print ( photos . shape , labels . shape )

Pre-Process Photos into Standard Directories

Alternately, we can load the images progressively using the Keras ImageDataGenerator class and flow_from_directory() API. This will be slower to execute but will run on more machines.

This API prefers data to be divided into separate train/ and test/ directories, and under each directory to have a subdirectory for each class, e.g. a train/dog/ and a train/cat/ subdirectories and the same for test. Images are then organized under the subdirectories.

We can write a script to create a copy of the dataset with this preferred structure. We will randomly select 25% of the images (or 6,250) to be used in a test dataset.

First, we need to create the directory structure as follows:

dataset_dogs_vs_cats ├── test │ ├── cats │ └── dogs └── train ├── cats └── dogs 1 2 3 4 5 6 7 dataset_dogs_vs_cats ├── test │ ├── cats │ └── dogs └── train ├── cats └── dogs

We can create directories in Python using the makedirs() function and use a loop to create the dog/ and cat/ subdirectories for both the train/ and test/ directories.

# create directories dataset_home = 'dataset_dogs_vs_cats/' subdirs = ['train/', 'test/'] for subdir in subdirs: # create label subdirectories labeldirs = ['dogs/', 'cats/'] for labldir in labeldirs: newdir = dataset_home + subdir + labldir makedirs(newdir, exist_ok=True) 1 2 3 4 5 6 7 8 9 # create directories dataset_home = 'dataset_dogs_vs_cats/' subdirs = [ 'train/' , 'test/' ] for subdir in subdirs : # create label subdirectories labeldirs = [ 'dogs/' , 'cats/' ] for labldir in labeldirs : newdir = dataset_home + subdir + labldir makedirs ( newdir , exist_ok = True )

Next, we can enumerate all image files in the dataset and copy them into the dogs/ or cats/ subdirectory based on their filename.

Additionally, we can randomly decide to hold back 25% of the images into the test dataset. This is done consistently by fixing the seed for the pseudorandom number generator so that we get the same split of data each time the code is run.

# seed random number generator seed(1) # define ratio of pictures to use for validation val_ratio = 0.25 # copy training dataset images into subdirectories src_directory = 'train/' for file in listdir(src_directory): src = src_directory + '/' + file dst_dir = 'train/' if random() < val_ratio: dst_dir = 'test/' if file.startswith('cat'): dst = dataset_home + dst_dir + 'cats/' + file copyfile(src, dst) elif file.startswith('dog'): dst = dataset_home + dst_dir + 'dogs/' + file copyfile(src, dst) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # seed random number generator seed ( 1 ) # define ratio of pictures to use for validation val_ratio = 0.25 # copy training dataset images into subdirectories src_directory = 'train/' for file in listdir ( src_directory ) : src = src_directory + '/' + file dst_dir = 'train/' if random ( ) < val_ratio : dst_dir = 'test/' if file . startswith ( 'cat' ) : dst = dataset_home + dst_dir + 'cats/' + file copyfile ( src , dst ) elif file . startswith ( 'dog' ) : dst = dataset_home + dst_dir + 'dogs/' + file copyfile ( src , dst )

The complete code example is listed below and assumes that you have the images in the downloaded train.zip unzipped in the current working directory in train/.

# organize dataset into a useful structure from os import makedirs from os import listdir from shutil import copyfile from random import seed from random import random # create directories dataset_home = 'dataset_dogs_vs_cats/' subdirs = ['train/', 'test/'] for subdir in subdirs: # create label subdirectories labeldirs = ['dogs/', 'cats/'] for labldir in labeldirs: newdir = dataset_home + subdir + labldir makedirs(newdir, exist_ok=True) # seed random number generator seed(1) # define ratio of pictures to use for validation val_ratio = 0.25 # copy training dataset images into subdirectories src_directory = 'train/' for file in listdir(src_directory): src = src_directory + '/' + file dst_dir = 'train/' if random() < val_ratio: dst_dir = 'test/' if file.startswith('cat'): dst = dataset_home + dst_dir + 'cats/' + file copyfile(src, dst) elif file.startswith('dog'): dst = dataset_home + dst_dir + 'dogs/' + file copyfile(src, dst) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # organize dataset into a useful structure from os import makedirs from os import listdir from shutil import copyfile from random import seed from random import random # create directories dataset_home = 'dataset_dogs_vs_cats/' subdirs = [ 'train/' , 'test/' ] for subdir in subdirs : # create label subdirectories labeldirs = [ 'dogs/' , 'cats/' ] for labldir in labeldirs : newdir = dataset_home + subdir + labldir makedirs ( newdir , exist_ok = True ) # seed random number generator seed ( 1 ) # define ratio of pictures to use for validation val_ratio = 0.25 # copy training dataset images into subdirectories src_directory = 'train/' for file in listdir ( src_directory ) : src = src_directory + '/' + file dst_dir = 'train/' if random ( ) < val_ratio : dst_dir = 'test/' if file . startswith ( 'cat' ) : dst = dataset_home + dst_dir + 'cats/' + file copyfile ( src , dst ) elif file . startswith ( 'dog' ) : dst = dataset_home + dst_dir + 'dogs/' + file copyfile ( src , dst )

After running the example, you will now have a new dataset_dogs_vs_cats/ directory with a train/ and val/ subfolders and further dogs/ can cats/ subdirectories, exactly as designed.

Develop a Baseline CNN Model

In this section, we can develop a baseline convolutional neural network model for the dogs vs. cats dataset.

A baseline model will establish a minimum model performance to which all of our other models can be compared, as well as a model architecture that we can use as the basis of study and improvement.

A good starting point is the general architectural principles of the VGG models. These are a good starting point because they achieved top performance in the ILSVRC 2014 competition and because the modular structure of the architecture is easy to understand and implement. For more details on the VGG model, see the 2015 paper “Very Deep Convolutional Networks for Large-Scale Image Recognition.”

The architecture involves stacking convolutional layers with small 3×3 filters followed by a max pooling layer. Together, these layers form a block, and these blocks can be repeated where the number of filters in each block is increased with the depth of the network such as 32, 64, 128, 256 for the first four blocks of the model. Padding is used on the convolutional layers to ensure the height and width shapes of the output feature maps matches the inputs.

We can explore this architecture on the dogs vs cats problem and compare a model with this architecture with 1, 2, and 3 blocks.

Each layer will use the ReLU activation function and the He weight initialization, which are generally best practices. For example, a 3-block VGG-style architecture where each block has a single convolutional and pooling layer can be defined in Keras as follows:

# block 1 model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) # block 2 model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) # block 3 model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) 1 2 3 4 5 6 7 8 9 # block 1 model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) # block 2 model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) # block 3 model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

We can create a function named define_model() that will define a model and return it ready to be fit on the dataset. This function can then be customized to define different baseline models, e.g. versions of the model with 1, 2, or 3 VGG style blocks.

The model will be fit with stochastic gradient descent and we will start with a conservative learning rate of 0.001 and a momentum of 0.9.

The problem is a binary classification task, requiring the prediction of one value of either 0 or 1. An output layer with 1 node and a sigmoid activation will be used and the model will be optimized using the binary cross-entropy loss function.

Below is an example of the define_model() function for defining a convolutional neural network model for the dogs vs. cats problem with one vgg-style block.

# define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model

It can be called to prepare a model as needed, for example:

# define model model = define_model() 1 2 # define model model = define_model ( )

Next, we need to prepare the data.

This involves first defining an instance of the ImageDataGenerator that will scale the pixel values to the range of 0-1.

# create data generator datagen = ImageDataGenerator(rescale=1.0/255.0) 1 2 # create data generator datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 )

Next, iterators need to be prepared for both the train and test datasets.

We can use the flow_from_directory() function on the data generator and create one iterator for each of the train/ and test/ directories. We must specify that the problem is a binary classification problem via the “class_mode” argument, and to load the images with the size of 200×200 pixels via the “target_size” argument. We will fix the batch size at 64.

# prepare iterators train_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode='binary', batch_size=64, target_size=(200, 200)) test_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode='binary', batch_size=64, target_size=(200, 200)) 1 2 3 4 5 # prepare iterators train_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/train/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) test_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/test/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) )

We can then fit the model using the train iterator (train_it) and use the test iterator (test_it) as a validation dataset during training.

The number of steps for the train and test iterators must be specified. This is the number of batches that will comprise one epoch. This can be specified via the length of each iterator, and will be the total number of images in the train and test directories divided by the batch size (64).

The model will be fit for 20 epochs, a small number to check if the model can learn the problem.

# fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0) 1 2 3 # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 20 , verbose = 0 )

Once fit, the final model can be evaluated on the test dataset directly and the classification accuracy reported.

# evaluate model _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> %.3f' % (acc * 100.0)) 1 2 3 # evaluate model _ , acc = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> %.3f' % ( acc * 100.0 ) )

Finally, we can create a plot of the history collected during training stored in the “history” directory returned from the call to fit_generator().

The History contains the model accuracy and loss on the test and training dataset at the end of each epoch. Line plots of these measures over training epochs provide learning curves that we can use to get an idea of whether the model is overfitting, underfitting, or has a good fit.

The summarize_diagnostics() function below takes the history directory and creates a single figure with a line plot of the loss and another for the accuracy. The figure is then saved to file with a filename based on the name of the script. This is helpful if we wish to evaluate many variations of the model in different files and create line plots automatically for each.

# plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Classification Accuracy') pyplot.plot(history.history['accuracy'], color='blue', label='train') pyplot.plot(history.history['val_accuracy'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Classification Accuracy' ) pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( )

We can tie all of this together into a simple test harness for testing a model configuration.

The complete example of evaluating a one-block baseline model on the dogs and cats dataset is listed below.

# baseline model for the dogs vs cats dataset import sys from matplotlib import pyplot from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Conv2D from keras.layers import MaxPooling2D from keras.layers import Dense from keras.layers import Flatten from keras.optimizers import SGD from keras.preprocessing.image import ImageDataGenerator # define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model # plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Classification Accuracy') pyplot.plot(history.history['accuracy'], color='blue', label='train') pyplot.plot(history.history['val_accuracy'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() # run the test harness for evaluating a model def run_test_harness(): # define model model = define_model() # create data generator datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode='binary', batch_size=64, target_size=(200, 200)) test_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode='binary', batch_size=64, target_size=(200, 200)) # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0) # evaluate model _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> %.3f' % (acc * 100.0)) # learning curves summarize_diagnostics(history) # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # baseline model for the dogs vs cats dataset import sys from matplotlib import pyplot from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . optimizers import SGD from keras . preprocessing . image import ImageDataGenerator # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Classification Accuracy' ) pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( ) # run the test harness for evaluating a model def run_test_harness ( ) : # define model model = define_model ( ) # create data generator datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/train/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) test_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/test/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 20 , verbose = 0 ) # evaluate model _ , acc = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> %.3f' % ( acc * 100.0 ) ) # learning curves summarize_diagnostics ( history ) # entry point, run the test harness run_test_harness ( )

Now that we have a test harness, let’s look at the evaluation of three simple baseline models.

One Block VGG Model

The one-block VGG model has a single convolutional layer with 32 filters followed by a max pooling layer.

The define_model() function for this model was defined in the previous section but is provided again below for completeness.

# define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model

Running this example first prints the size of the train and test datasets, confirming that the dataset was loaded correctly.

The model is then fit and evaluated, which takes approximately 20 minutes on modern GPU hardware.

Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 72.331 1 2 3 Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 72.331

Your specific results may differ given the stochastic nature of the learning algorithm.

In this case, we can see that the model achieved an accuracy of about 72% on the test dataset.

A figure is also created showing a line plot for the loss and another for the accuracy of the model on both the train (blue) and test (orange) datasets.

Reviewing this plot, we can see that the model has overfit the training dataset at about 12 epochs.

Two Block VGG Model

The two-block VGG model extends the one block model and adds a second block with 64 filters.

The define_model() function for this model is provided below for completeness.

# define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model

Running this example again prints the size of the train and test datasets, confirming that the dataset was loaded correctly.

The model is fit and evaluated and the performance on the test dataset is reported.

Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 76.646 1 2 3 Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 76.646

Your specific results may differ given the stochastic nature of the learning algorithm.

In this case, we can see that the model achieved a small improvement in performance from about 72% with one block to about 76% accuracy with two blocks

Reviewing the plot of the learning curves, we can see that again the model appears to have overfit the training dataset, perhaps sooner, in this case at around eight training epochs.

This is likely the result of the increased capacity of the model, and we might expect this trend of sooner overfitting to continue with the next model.

Three Block VGG Model

The three-block VGG model extends the two block model and adds a third block with 128 filters.

The define_model() function for this model was defined in the previous section but is provided again below for completeness.

# define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model

Running this example prints the size of the train and test datasets, confirming that the dataset was loaded correctly.

The model is fit and evaluated and the performance on the test dataset is reported.

Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 80.184 1 2 3 Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 80.184

Your specific results may differ given the stochastic nature of the learning algorithm.

In this case, we can see that we achieved a further lift in performance from about 76% with two blocks to about 80% accuracy with three blocks. This result is good, as it is close to the prior state-of-the-art reported in the paper using an SVM at about 82% accuracy.

Reviewing the plot of the learning curves, we can see a similar trend of overfitting, in this case perhaps pushed back as far as to epoch five or six.

Discussion

We have explored three different models with a VGG-based architecture.

The results can be summarized below, although we must assume some variance in these results given the stochastic nature of the algorithm:

VGG 1 : 72.331%

: 72.331% VGG 2 : 76.646%

: 76.646% VGG 3: 80.184%

We see a trend of improved performance with the increase in capacity, but also a similar case of overfitting occurring earlier and earlier in the run.

The results suggest that the model will likely benefit from regularization techniques. This may include techniques such as dropout, weight decay, and data augmentation. The latter can also boost performance by encouraging the model to learn features that are further invariant to position by expanding the training dataset.

Develop Model Improvements

In the previous section, we developed a baseline model using VGG-style blocks and discovered a trend of improved performance with increased model capacity.

In this section, we will start with the baseline model with three VGG blocks (i.e. VGG 3) and explore some simple improvements to the model.

From reviewing the learning curves for the model during training, the model showed strong signs of overfitting. We can explore two approaches to attempt to address this overfitting: dropout regularization and data augmentation.

Both of these approaches are expected to slow the rate of improvement during training and hopefully counter the overfitting of the training dataset. As such, we will increase the number of training epochs from 20 to 50 to give the model more space for refinement.

Dropout Regularization

Dropout regularization is a computationally cheap way to regularize a deep neural network.

Dropout works by probabilistically removing, or “dropping out,” inputs to a layer, which may be input variables in the data sample or activations from a previous layer. It has the effect of simulating a large number of networks with very different network structures and, in turn, making nodes in the network generally more robust to the inputs.

For more information on dropout, see the post:

Typically, a small amount of dropout can be applied after each VGG block, with more dropout applied to the fully connected layers near the output layer of the model.

Below is the define_model() function for an updated version of the baseline model with the addition of Dropout. In this case, a dropout of 20% is applied after each VGG block, with a larger dropout rate of 50% applied after the fully connected layer in the classifier part of the model.

# define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dropout ( 0.5 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model

The full code listing of the baseline model with the addition of dropout on the dogs vs. cats dataset is listed below for completeness.

# baseline model with dropout for the dogs vs cats dataset import sys from matplotlib import pyplot from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Conv2D from keras.layers import MaxPooling2D from keras.layers import Dense from keras.layers import Flatten from keras.layers import Dropout from keras.optimizers import SGD from keras.preprocessing.image import ImageDataGenerator # define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model # plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Classification Accuracy') pyplot.plot(history.history['accuracy'], color='blue', label='train') pyplot.plot(history.history['val_accuracy'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() # run the test harness for evaluating a model def run_test_harness(): # define model model = define_model() # create data generator datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterator train_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode='binary', batch_size=64, target_size=(200, 200)) test_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode='binary', batch_size=64, target_size=(200, 200)) # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0) # evaluate model _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> %.3f' % (acc * 100.0)) # learning curves summarize_diagnostics(history) # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # baseline model with dropout for the dogs vs cats dataset import sys from matplotlib import pyplot from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . layers import Dropout from keras . optimizers import SGD from keras . preprocessing . image import ImageDataGenerator # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dropout ( 0.5 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Classification Accuracy' ) pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( ) # run the test harness for evaluating a model def run_test_harness ( ) : # define model model = define_model ( ) # create data generator datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterator train_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/train/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) test_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/test/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 50 , verbose = 0 ) # evaluate model _ , acc = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> %.3f' % ( acc * 100.0 ) ) # learning curves summarize_diagnostics ( history ) # entry point, run the test harness run_test_harness ( )

Running the example first fits the model, then reports the model performance on the hold out test dataset.

Your specific results may vary given the stochastic nature of the learning algorithm.

In this case, we can see a small lift in model performance from about 80% accuracy for the baseline model to about 81% with the addition of dropout.

Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 81.279 1 2 3 Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 81.279

Reviewing the learning curves, we can see that dropout has had an effect on the rate of improvement of the model on both the train and test sets.

Overfitting has been reduced or delayed, although performance may begin to stall towards the end of the run.

The results suggest that further training epochs may result in further improvement of the model. It may also be interesting to explore perhaps a slightly higher dropout rate after the VGG blocks in addition to the increase in training epochs.

Image Data Augmentation

Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.

Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.

Data augmentation can also act as a regularization technique, adding noise to the training data, and encouraging the model to learn the same features, invariant to their position in the input.

Small changes to the input photos of dogs and cats might be useful for this problem, such as small shifts and horizontal flips. These augmentations can be specified as arguments to the ImageDataGenerator used for the training dataset. The augmentations should not be used for the test dataset, as we wish to evaluate the performance of the model on the unmodified photographs.

This requires that we have a separate ImageDataGenerator instance for the train and test dataset, then iterators for the train and test sets created from the respective data generators. For example:

# create data generators train_datagen = ImageDataGenerator(rescale=1.0/255.0, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = train_datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode='binary', batch_size=64, target_size=(200, 200)) test_it = test_datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode='binary', batch_size=64, target_size=(200, 200)) 1 2 3 4 5 6 7 8 9 # create data generators train_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 , width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True ) test_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = train_datagen . flow_from_directory ( 'dataset_dogs_vs_cats/train/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) test_it = test_datagen . flow_from_directory ( 'dataset_dogs_vs_cats/test/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) )

In this case, photos in the training dataset will be augmented with small (10%) random horizontal and vertical shifts and random horizontal flips that create a mirror image of a photo. Photos in both the train and test steps will have their pixel values scaled in the same way.

The full code listing of the baseline model with training data augmentation for the dogs and cats dataset is listed below for completeness.

# baseline model with data augmentation for the dogs vs cats dataset import sys from matplotlib import pyplot from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Conv2D from keras.layers import MaxPooling2D from keras.layers import Dense from keras.layers import Flatten from keras.optimizers import SGD from keras.preprocessing.image import ImageDataGenerator # define cnn model def define_model(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3))) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(1, activation='sigmoid')) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model # plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Classification Accuracy') pyplot.plot(history.history['accuracy'], color='blue', label='train') pyplot.plot(history.history['val_accuracy'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() # run the test harness for evaluating a model def run_test_harness(): # define model model = define_model() # create data generators train_datagen = ImageDataGenerator(rescale=1.0/255.0, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True) test_datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = train_datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode='binary', batch_size=64, target_size=(200, 200)) test_it = test_datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode='binary', batch_size=64, target_size=(200, 200)) # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0) # evaluate model _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> %.3f' % (acc * 100.0)) # learning curves summarize_diagnostics(history) # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 # baseline model with data augmentation for the dogs vs cats dataset import sys from matplotlib import pyplot from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . optimizers import SGD from keras . preprocessing . image import ImageDataGenerator # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 200 , 200 , 3 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Classification Accuracy' ) pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( ) # run the test harness for evaluating a model def run_test_harness ( ) : # define model model = define_model ( ) # create data generators train_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 , width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True ) test_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = train_datagen . flow_from_directory ( 'dataset_dogs_vs_cats/train/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) test_it = test_datagen . flow_from_directory ( 'dataset_dogs_vs_cats/test/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 200 , 200 ) ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 50 , verbose = 0 ) # evaluate model _ , acc = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> %.3f' % ( acc * 100.0 ) ) # learning curves summarize_diagnostics ( history ) # entry point, run the test harness run_test_harness ( )

Running the example first fits the model, then reports the model performance on the hold out test dataset.

Your specific results may vary given the stochastic nature of the learning algorithm.

In this case, we can see a lift in performance of about 5% from about 80% for the baseline model to about 85% for the baseline model with simple data augmentation.

> 85.816 1 > 85.816

Reviewing the learning curves, we can see that it appears the model is capable of further learning with both the loss on the train and test dataset still decreasing even at the end of the run. Repeating the experiment with 100 or more epochs will very likely result in a better performing model.

It may be interesting to explore other augmentations that may further encourage the learning of features invariant to their position in the input, such as minor rotations and zooms.

Discussion

We have explored three different improvements to the baseline model.

The results can be summarized below, although we must assume some variance in these results given the stochastic nature of the algorithm:

Baseline VGG3 + Dropout: 81.279%

+ Dropout: 81.279% Baseline VGG3 + Data Augmentation: 85.816

As suspected, the addition of regularization techniques slows the progression of the learning algorithms and reduces overfitting, resulting in improved performance on the holdout dataset. It is likely that the combination of both approaches with further increase in the number of training epochs will result in further improvements.

This is just the beginning of the types of improvements that can be explored on this dataset. In addition to tweaks to the regularization methods described, other regularization methods could be explored such as weight decay and early stopping.

It may be worth exploring changes to the learning algorithm such as changes to the learning rate, use of a learning rate schedule, or an adaptive learning rate such as Adam.

Alternate model architectures may also be worth exploring. The chosen baseline model is expected to offer more capacity than may be required for this problem and a smaller model may faster to train and in turn could result in better performance.

Explore Transfer Learning

Transfer learning involves using all or parts of a model trained on a related task.

Keras provides a range of pre-trained models that can be loaded and used wholly or partially via the Keras Applications API.

A useful model for transfer learning is one of the VGG models, such as VGG-16 with 16 layers that at the time it was developed, achieved top results on the ImageNet photo classification challenge.

The model is comprised of two main parts, the feature extractor part of the model that is made up of VGG blocks, and the classifier part of the model that is made up of fully connected layers and the output layer.

We can use the feature extraction part of the model and add a new classifier part of the model that is tailored to the dogs and cats dataset. Specifically, we can hold the weights of all of the convolutional layers fixed during training, and only train new fully connected layers that will learn to interpret the features extracted from the model and make a binary classification.

This can be achieved by loading the VGG-16 model, removing the fully connected layers from the output-end of the model, then adding the new fully connected layers to interpret the model output and make a prediction. The classifier part of the model can be removed automatically by setting the “include_top” argument to “False“, which also requires that the shape of the input also be specified for the model, in this case (224, 224, 3). This means that the loaded model ends at the last max pooling layer, after which we can manually add a Flatten layer and the new clasifier layers.

The define_model() function below implements this and returns a new model ready for training.

# define cnn model def define_model(): # load model model = VGG16(include_top=False, input_shape=(224, 224, 3)) # mark loaded layers as not trainable for layer in model.layers: layer.trainable = False # add new classifier layers flat1 = Flatten()(model.layers[-1].output) class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1) output = Dense(1, activation='sigmoid')(class1) # define new model model = Model(inputs=model.inputs, outputs=output) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # define cnn model def define_model ( ) : # load model model = VGG16 ( include_top = False , input_shape = ( 224 , 224 , 3 ) ) # mark loaded layers as not trainable for layer in model . layers : layer . trainable = False # add new classifier layers flat1 = Flatten ( ) ( model . layers [ - 1 ] . output ) class1 = Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ( flat1 ) output = Dense ( 1 , activation = 'sigmoid' ) ( class1 ) # define new model model = Model ( inputs = model . inputs , outputs = output ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model

Once created, we can train the model as before on the training dataset.

Not a lot of training will be required in this case, as only the new fully connected and output layer have trainable weights. As such, we will fix the number of training epochs at 10.

The VGG16 model was trained on a specific ImageNet challenge dataset. As such, it is configured to expected input images to have the shape 224×224 pixels. We will use this as the target size when loading photos from the dogs and cats dataset.

The model also expects images to be centered. That is, to have the mean pixel values from each channel (red, green, and blue) as calculated on the ImageNet training dataset subtracted from the input. Keras provides a function to perform this preparation for individual photos via the preprocess_input() function. Nevertheless, we can achieve the same effect with the ImageDataGenerator by setting the “featurewise_center” argument to “True” and manually specifying the mean pixel values to use when centering as the mean values from the ImageNet training dataset: [123.68, 116.779, 103.939].

The full code listing of the VGG model for transfer learning on the dogs vs. cats dataset is listed below.

# vgg16 model used for transfer learning on the dogs and cats dataset import sys from matplotlib import pyplot from keras.utils import to_categorical from keras.applications.vgg16 import VGG16 from keras.models import Model from keras.layers import Dense from keras.layers import Flatten from keras.optimizers import SGD from keras.preprocessing.image import ImageDataGenerator # define cnn model def define_model(): # load model model = VGG16(include_top=False, input_shape=(224, 224, 3)) # mark loaded layers as not trainable for layer in model.layers: layer.trainable = False # add new classifier layers flat1 = Flatten()(model.layers[-1].output) class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1) output = Dense(1, activation='sigmoid')(class1) # define new model model = Model(inputs=model.inputs, outputs=output) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model # plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Classification Accuracy') pyplot.plot(history.history['accuracy'], color='blue', label='train') pyplot.plot(history.history['val_accuracy'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() # run the test harness for evaluating a model def run_test_harness(): # define model model = define_model() # create data generator datagen = ImageDataGenerator(featurewise_center=True) # specify imagenet mean values for centering datagen.mean = [123.68, 116.779, 103.939] # prepare iterator train_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode='binary', batch_size=64, target_size=(224, 224)) test_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode='binary', batch_size=64, target_size=(224, 224)) # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=10, verbose=1) # evaluate model _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> %.3f' % (acc * 100.0)) # learning curves summarize_diagnostics(history) # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 # vgg16 model used for transfer learning on the dogs and cats dataset import sys from matplotlib import pyplot from keras . utils import to_categorical from keras . applications . vgg16 import VGG16 from keras . models import Model from keras . layers import Dense from keras . layers import Flatten from keras . optimizers import SGD from keras . preprocessing . image import ImageDataGenerator # define cnn model def define_model ( ) : # load model model = VGG16 ( include_top = False , input_shape = ( 224 , 224 , 3 ) ) # mark loaded layers as not trainable for layer in model . layers : layer . trainable = False # add new classifier layers flat1 = Flatten ( ) ( model . layers [ - 1 ] . output ) class1 = Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ( flat1 ) output = Dense ( 1 , activation = 'sigmoid' ) ( class1 ) # define new model model = Model ( inputs = model . inputs , outputs = output ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Classification Accuracy' ) pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( ) # run the test harness for evaluating a model def run_test_harness ( ) : # define model model = define_model ( ) # create data generator datagen = ImageDataGenerator ( featurewise_center = True ) # specify imagenet mean values for centering datagen . mean = [ 123.68 , 116.779 , 103.939 ] # prepare iterator train_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/train/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 224 , 224 ) ) test_it = datagen . flow_from_directory ( 'dataset_dogs_vs_cats/test/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 224 , 224 ) ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 10 , verbose = 1 ) # evaluate model _ , acc = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> %.3f' % ( acc * 100.0 ) ) # learning curves summarize_diagnostics ( history ) # entry point, run the test harness run_test_harness ( )

Running the example first fits the model, then reports the model performance on the hold out test dataset.

Your specific results may vary given the stochastic nature of the learning algorithm.

In this case, we can see that the model achieved very impressive results with a classification accuracy of about 97% on the holdout test dataset.

Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 97.636 1 2 3 Found 18697 images belonging to 2 classes. Found 6303 images belonging to 2 classes. > 97.636

Reviewing the learning curves, we can see that the model fits the dataset quickly. It does not show strong overfitting, although the results suggest that perhaps additional capacity in the classifier and/or the use of regularization might be helpful.

There are many improvements that could be made to this approach, including adding dropout regularization to the classifier part of the model and perhaps even fine-tuning the weights of some or all of the layers in the feature detector part of the model.

How to Finalize the Model and Make Predictions

The process of model improvement may continue for as long as we have ideas and the time and resources to test them out.

At some point, a final model configuration must be chosen and adopted. In this case, we will keep things simple and use the VGG-16 transfer learning approach as the final model.

First, we will finalize our model by fitting a model on the entire training dataset and saving the model to file for later use. We will then load the saved model and use it to make a prediction on a single image.

Prepare Final Dataset

A final model is typically fit on all available data, such as the combination of all train and test datasets.

In this tutorial, we will demonstrate the final model fit only on the training dataset as we only have labels for the training dataset.

The first step is to prepare the training dataset so that it can be loaded by the ImageDataGenerator class via flow_from_directory() function. Specifically, we need to create a new directory with all training images organized into dogs/ and cats/ subdirectories without any separation into train/ or test/ directories.

This can be achieved by updating the script we developed at the beginning of the tutorial. In this case, we will create a new finalize_dogs_vs_cats/ folder with dogs/ and cats/ subfolders for the entire training dataset.

The structure will look as follows:

finalize_dogs_vs_cats ├── cats └── dogs 1 2 3 finalize_dogs_vs_cats ├── cats └── dogs

The updated script is listed below for completeness.

# organize dataset into a useful structure from os import makedirs from os import listdir from shutil import copyfile # create directories dataset_home = 'finalize_dogs_vs_cats/' # create label subdirectories labeldirs = ['dogs/', 'cats/'] for labldir in labeldirs: newdir = dataset_home + labldir makedirs(newdir, exist_ok=True) # copy training dataset images into subdirectories src_directory = 'dogs-vs-cats/train/' for file in listdir(src_directory): src = src_directory + '/' + file if file.startswith('cat'): dst = dataset_home + 'cats/' + file copyfile(src, dst) elif file.startswith('dog'): dst = dataset_home + 'dogs/' + file copyfile(src, dst) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # organize dataset into a useful structure from os import makedirs from os import listdir from shutil import copyfile # create directories dataset_home = 'finalize_dogs_vs_cats/' # create label subdirectories labeldirs = [ 'dogs/' , 'cats/' ] for labldir in labeldirs : newdir = dataset_home + labldir makedirs ( newdir , exist_ok = True ) # copy training dataset images into subdirectories src_directory = 'dogs-vs-cats/train/' for file in listdir ( src_directory ) : src = src_directory + '/' + file if file . startswith ( 'cat' ) : dst = dataset_home + 'cats/' + file copyfile ( src , dst ) elif file . startswith ( 'dog' ) : dst = dataset_home + 'dogs/' + file copyfile ( src , dst )

Save Final Model

We are now ready to fit a final model on the entire training dataset.

The flow_from_directory() must be updated to load all of the images from the new finalize_dogs_vs_cats/ directory.

# prepare iterator train_it = datagen.flow_from_directory('finalize_dogs_vs_cats/', class_mode='binary', batch_size=64, target_size=(224, 224)) 1 2 3 # prepare iterator train_it = datagen . flow_from_directory ( 'finalize_dogs_vs_cats/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 224 , 224 ) )

Additionally, the call to fit_generator() no longer needs to specify a validation dataset.

# fit model model.fit_generator(train_it, steps_per_epoch=len(train_it), epochs=10, verbose=0) 1 2 # fit model model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , epochs = 10 , verbose = 0 )

Once fit, we can save the final model to an H5 file by calling the save() function on the model and pass in the chosen filename.

# save model model.save('final_model.h5') 1 2 # save model model . save ( 'final_model.h5' )

Note, saving and loading a Keras model requires that the h5py library is installed on your workstation.

The complete example of fitting the final model on the training dataset and saving it to file is listed below.

# save the final model to file from keras.applications.vgg16 import VGG16 from keras.models import Model from keras.layers import Dense from keras.layers import Flatten from keras.optimizers import SGD from keras.preprocessing.image import ImageDataGenerator # define cnn model def define_model(): # load model model = VGG16(include_top=False, input_shape=(224, 224, 3)) # mark loaded layers as not trainable for layer in model.layers: layer.trainable = False # add new classifier layers flat1 = Flatten()(model.layers[-1].output) class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1) output = Dense(1, activation='sigmoid')(class1) # define new model model = Model(inputs=model.inputs, outputs=output) # compile model opt = SGD(lr=0.001, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy']) return model # run the test harness for evaluating a model def run_test_harness(): # define model model = define_model() # create data generator datagen = ImageDataGenerator(featurewise_center=True) # specify imagenet mean values for centering datagen.mean = [123.68, 116.779, 103.939] # prepare iterator train_it = datagen.flow_from_directory('finalize_dogs_vs_cats/', class_mode='binary', batch_size=64, target_size=(224, 224)) # fit model model.fit_generator(train_it, steps_per_epoch=len(train_it), epochs=10, verbose=0) # save model model.save('final_model.h5') # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # save the final model to file from keras . applications . vgg16 import VGG16 from keras . models import Model from keras . layers import Dense from keras . layers import Flatten from keras . optimizers import SGD from keras . preprocessing . image import ImageDataGenerator # define cnn model def define_model ( ) : # load model model = VGG16 ( include_top = False , input_shape = ( 224 , 224 , 3 ) ) # mark loaded layers as not trainable for layer in model . layers : layer . trainable = False # add new classifier layers flat1 = Flatten ( ) ( model . layers [ - 1 ] . output ) class1 = Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ( flat1 ) output = Dense ( 1 , activation = 'sigmoid' ) ( class1 ) # define new model model = Model ( inputs = model . inputs , outputs = output ) # compile model opt = SGD ( lr = 0.001 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) return model # run the test harness for evaluating a model def run_test_harness ( ) : # define model model = define_model ( ) # create data generator datagen = ImageDataGenerator ( featurewise_center = True ) # specify imagenet mean values for centering datagen . mean = [ 123.68 , 116.779 , 103.939 ] # prepare iterator train_it = datagen . flow_from_directory ( 'finalize_dogs_vs_cats/' , class_mode = 'binary' , batch_size = 64 , target_size = ( 224 , 224 ) ) # fit model model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , epochs = 10 , verbose = 0 ) # save model model . save ( 'final_model.h5' ) # entry point, run the test harness run_test_harness ( )

After running this example, you will now have a large 81-megabyte file with the name ‘final_model.h5‘ in your current working directory.

Make Prediction

We can use our saved model to make a prediction on new images.

The model assumes that new images are color and they have been segmented so that one image contains at least one dog or cat.

Below is an image extracted from the test dataset for the dogs and cats competition. It has no label, but we can clearly tell it is a photo of a dog. You can save it in your current working directory with the filename ‘sample_image.jpg‘.

We will pretend this is an entirely new and unseen image, prepared in the required way, and see how we might use our saved model to predict the integer that the image represents. For this example, we expect class “1” for “Dog“.

Note: the subdirectories of images, one for each class, are loaded by the flow_from_directory() function in alphabetical order and assigned an integer for each class. The subdirectory “cat” comes before “dog“, therefore the class labels are assigned the integers: cat=0, dog=1. This can be changed via the “classes” argument in calling flow_from_directory() when training the model.

First, we can load the image and force it to the size to be 224×224 pixels. The loaded image can then be resized to have a single sample in a dataset. The pixel values must also be centered to match the way that the data was prepared during the training of the model. The load_image() function implements this and will return the loaded image ready for classification.

# load and prepare the image def load_image(filename): # load the image img = load_img(filename, target_size=(224, 224)) # convert to array img = img_to_array(img) # reshape into a single sample with 3 channels img = img.reshape(1, 224, 224, 3) # center pixel data img = img.astype('float32') img = img - [123.68, 116.779, 103.939] return img 1 2 3 4 5 6 7 8 9 10 11 12 # load and prepare the image def load_image ( filename ) : # load the image img = load_img ( filename , target_size = ( 224 , 224 ) ) # convert to array img = img_to_array ( img ) # reshape into a single sample with 3 channels img = img . reshape ( 1 , 224 , 224 , 3 ) # center pixel data img = img . astype ( 'float32' ) img = img - [ 123.68 , 116.779 , 103.939 ] return img

Next, we can load the model as in the previous section and call the predict() function to predict the content in the image as a number between “0” and “1” for “cat” and “dog” respectively.

# predict the class result = model.predict(img) 1 2 # predict the class result = model . predict ( img )

The complete example is listed below.

# make a prediction for a new image. from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array from keras.models import load_model # load and prepare the image def load_image(filename): # load the image img = load_img(filename, target_size=(224, 224)) # convert to array img = img_to_array(img) # reshape into a single sample with 3 channels img = img.reshape(1, 224, 224, 3) # center pixel data img = img.astype('float32') img = img - [123.68, 116.779, 103.939] return img # load an image and predict the class def run_example(): # load the image img = load_image('sample_image.jpg') # load model model = load_model('final_model.h5') # predict the class result = model.predict(img) print(result[0]) # entry point, run the example run_example() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # make a prediction for a new image. from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from keras . models import load_model # load and prepare the image def load_image ( filename ) : # load the image img = load_img ( filename , target_size = ( 224 , 224 ) ) # convert to array img = img_to_array ( img ) # reshape into a single sample with 3 channels img = img . reshape ( 1 , 224 , 224 , 3 ) # center pixel data img = img . astype ( 'float32' ) img = img - [ 123.68 , 116.779 , 103.939 ] return img # load an image and predict the class def run_example ( ) : # load the image img = load_image ( 'sample_image.jpg' ) # load model model = load_model ( 'final_model.h5' ) # predict the class result = model . predict ( img ) print ( result [ 0 ] ) # entry point, run the example run_example ( )

Running the example first loads and prepares the image, loads the model, and then correctly predicts that the loaded image represents a ‘dog‘ or class ‘1‘.

1 1 1

Extensions

This section lists some ideas for extending the tutorial that you may wish to explore.

Tune Regularization . Explore minor changes to the regularization techniques used on the baseline model, such as different dropout rates and different image augmentation.

. Explore minor changes to the regularization techniques used on the baseline model, such as different dropout rates and different image augmentation. Tune Learning Rate . Explore changes to the learning algorithm used to train the baseline model, such as alternate learning rate, a learning rate schedule, or an adaptive learning rate algorithm such as Adam.

. Explore changes to the learning algorithm used to train the baseline model, such as alternate learning rate, a learning rate schedule, or an adaptive learning rate algorithm such as Adam. Alternate Pre-Trained Model. Explore an alternate pre-trained model for transfer learning on the problem, such as Inception or ResNet.

If you explore any of these extensions, I’d love to know.

Post your findings in the comments below.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

API

Articles

Summary

In this tutorial, you discovered how to develop a convolutional neural network to classify photos of dogs and cats.

Specifically, you learned:

How to load and prepare photos of dogs and cats for modeling.

How to develop a convolutional neural network for photo classification from scratch and improve model performance.

How to develop a model for photo classification using transfer learning.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning Models for Vision Today! Develop Your Own Vision Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Computer Vision It provides self-study tutorials on topics like:

classification, object detection (yolo and rcnn), face recognition (vggface and facenet), data preparation and much more... Finally Bring Deep Learning to your Vision Projects Skip the Academics. Just Results. See What's Inside"
112;machinelearningmastery.com;https://machinelearningmastery.com/types-of-classification-in-machine-learning/#comments;2020-04-07;4 Types of Classification Tasks in Machine Learning;"# example of binary classification task

from numpy import where

from collections import Counter

from sklearn . datasets import make_blobs

from matplotlib import pyplot

# define dataset

X , y = make_blobs ( n_samples = 1000 , centers = 2 , random_state = 1 )

# summarize dataset shape

print ( X . shape , y . shape )

# summarize observations by class label

counter = Counter ( y )

print ( counter )

# summarize first few examples

for i in range ( 10 ) :

print ( X [ i ] , y [ i ] )

# plot the dataset and color the by class label

for label , _ in counter . items ( ) :

row_ix = where ( y == label ) [ 0 ]

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( label ) )

pyplot . legend ( )"
113;machinelearningmastery.com;http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/;2016-06-26;Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras;"Tweet Share Share

Last Updated on September 13, 2019

A popular demonstration of the capability of deep learning techniques is object recognition in image data.

The “hello world” of object recognition for machine learning and deep learning is the MNIST dataset for handwritten digit recognition.

In this post you will discover how to develop a deep learning model to achieve near state of the art performance on the MNIST handwritten digit recognition task in Python using the Keras deep learning library.

After completing this tutorial, you will know:

How to load the MNIST dataset in Keras.

How to develop and evaluate a baseline neural network model for the MNIST problem.

How to implement and evaluate a simple Convolutional Neural Network for MNIST.

How to implement a close to state-of-the-art deep learning model for MNIST.

Discover how to develop deep learning models for a range of predictive modeling problems with just a few lines of code in my new book, with 18 step-by-step tutorials and 9 projects.

Let’s get started.

Update Oct/2016 : Updated for Keras 1.1.0, TensorFlow 0.10.0 and scikit-learn v0.18.

: Updated for Keras 1.1.0, TensorFlow 0.10.0 and scikit-learn v0.18. Update Mar/2017 : Updated for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0.

: Updated for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0. Update Sep/2019: Updated for Keras 2.2.5 API.

Note, for an extended version of this tutorial see:

Description of the MNIST Handwritten Digit Recognition Problem

The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes and Christopher Burges for evaluating machine learning models on the handwritten digit classification problem.

The dataset was constructed from a number of scanned document dataset available from the National Institute of Standards and Technology (NIST). This is where the name for the dataset comes from, as the Modified NIST or MNIST dataset.

Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required.

Each image is a 28 by 28 pixel square (784 pixels total). A standard split of the dataset is used to evaluate and compare models, where 60,000 images are used to train a model and a separate set of 10,000 images are used to test it.

It is a digit recognition task. As such there are 10 digits (0 to 9) or 10 classes to predict. Results are reported using prediction error, which is nothing more than the inverted classification accuracy.

Excellent results achieve a prediction error of less than 1%. State-of-the-art prediction error of approximately 0.2% can be achieved with large Convolutional Neural Networks. There is a listing of the state-of-the-art results and links to the relevant papers on the MNIST and other datasets on Rodrigo Benenson’s webpage.

Need help with Deep Learning in Python? Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Loading the MNIST dataset in Keras

The Keras deep learning library provides a convenience method for loading the MNIST dataset.

The dataset is downloaded automatically the first time this function is called and is stored in your home directory in ~/.keras/datasets/mnist.pkl.gz as a 15MB file.

This is very handy for developing and testing deep learning models.

To demonstrate how easy it is to load the MNIST dataset, we will first write a little script to download and visualize the first 4 images in the training dataset.

# Plot ad hoc mnist instances from keras.datasets import mnist import matplotlib.pyplot as plt # load (downloaded if needed) the MNIST dataset (X_train, y_train), (X_test, y_test) = mnist.load_data() # plot 4 images as gray scale plt.subplot(221) plt.imshow(X_train[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(X_train[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(X_train[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(X_train[3], cmap=plt.get_cmap('gray')) # show the plot plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Plot ad hoc mnist instances from keras . datasets import mnist import matplotlib . pyplot as plt # load (downloaded if needed) the MNIST dataset ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( ) # plot 4 images as gray scale plt . subplot ( 221 ) plt . imshow ( X_train [ 0 ] , cmap = plt . get_cmap ( 'gray' ) ) plt . subplot ( 222 ) plt . imshow ( X_train [ 1 ] , cmap = plt . get_cmap ( 'gray' ) ) plt . subplot ( 223 ) plt . imshow ( X_train [ 2 ] , cmap = plt . get_cmap ( 'gray' ) ) plt . subplot ( 224 ) plt . imshow ( X_train [ 3 ] , cmap = plt . get_cmap ( 'gray' ) ) # show the plot plt . show ( )

You can see that downloading and loading the MNIST dataset is as easy as calling the mnist.load_data() function. Running the above example, you should see the image below.

Baseline Model with Multi-Layer Perceptrons

Do we really need a complex model like a convolutional neural network to get the best results with MNIST?

You can get very good results using a very simple neural network model with a single hidden layer. In this section we will create a simple multi-layer perceptron model that achieves an error rate of 1.74%. We will use this as a baseline for comparing more complex convolutional neural network models.

Let’s start off by importing the classes and functions we will need.

from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.utils import np_utils ... 1 2 3 4 5 6 from keras . datasets import mnist from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from keras . utils import np _ utils . . .

Now we can load the MNIST dataset using the Keras helper function.

... # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() 1 2 3 . . . # load data ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( )

The training dataset is structured as a 3-dimensional array of instance, image width and image height. For a multi-layer perceptron model we must reduce the images down into a vector of pixels. In this case the 28×28 sized images will be 784 pixel input values.

We can do this transform easily using the reshape() function on the NumPy array. We can also reduce our memory requirements by forcing the precision of the pixel values to be 32 bit, the default precision used by Keras anyway.

... # flatten 28*28 images to a 784 vector for each image num_pixels = X_train.shape[1] * X_train.shape[2] X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32') X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32') 1 2 3 4 5 . . . # flatten 28*28 images to a 784 vector for each image num_pixels = X_train . shape [ 1 ] * X_train . shape [ 2 ] X_train = X_train . reshape ( ( X_train . shape [ 0 ] , num_pixels ) ) . astype ( 'float32' ) X_test = X_test . reshape ( ( X_test . shape [ 0 ] , num_pixels ) ) . astype ( 'float32' )

The pixel values are gray scale between 0 and 255. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255.

... # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 1 2 3 4 . . . # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255

Finally, the output variable is an integer from 0 to 9. This is a multi-class classification problem. As such, it is good practice to use a one hot encoding of the class values, transforming the vector of class integers into a binary matrix.

We can easily do this using the built-in np_utils.to_categorical() helper function in Keras.

... # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] 1 2 3 4 5 . . . # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ]

We are now ready to create our simple neural network model. We will define our model in a function. This is handy if you want to extend the example later and try and get a better score.

... # define baseline model def baseline_model(): # create model model = Sequential() model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu')) model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 . . . # define baseline model def baseline_model ( ) : # create model model = Sequential ( ) model . add ( Dense ( num_pixels , input_dim = num_pixels , kernel_initializer = 'normal' , activation = 'relu' ) ) model . add ( Dense ( num_classes , kernel_initializer = 'normal' , activation = 'softmax' ) ) # Compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) return model

The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). A rectifier activation function is used for the neurons in the hidden layer.

A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the model’s output prediction. Logarithmic loss is used as the loss function (called categorical_crossentropy in Keras) and the efficient ADAM gradient descent algorithm is used to learn the weights.

We can now fit and evaluate the model. The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch.

Finally, the test dataset is used to evaluate the model and a classification error rate is printed.

... # build the model model = baseline_model() # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(""Baseline Error: %.2f%%"" % (100-scores[1]*100)) 1 2 3 4 5 6 7 8 . . . # build the model model = baseline_model ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 10 , batch_size = 200 , verbose = 2 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Baseline Error: %.2f%%"" % ( 100 - scores [ 1 ] * 100 ) )

Tying this all together, the complete code listing is provided below.

# Baseline MLP for MNIST dataset from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.utils import np_utils # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # flatten 28*28 images to a 784 vector for each image num_pixels = X_train.shape[1] * X_train.shape[2] X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32') X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32') # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] # define baseline model def baseline_model(): # create model model = Sequential() model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu')) model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model # build the model model = baseline_model() # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(""Baseline Error: %.2f%%"" % (100-scores[1]*100)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Baseline MLP for MNIST dataset from keras . datasets import mnist from keras . models import Sequential from keras . layers import Dense from keras . utils import np_utils # load data ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( ) # flatten 28*28 images to a 784 vector for each image num_pixels = X_train . shape [ 1 ] * X_train . shape [ 2 ] X_train = X_train . reshape ( ( X_train . shape [ 0 ] , num_pixels ) ) . astype ( 'float32' ) X_test = X_test . reshape ( ( X_test . shape [ 0 ] , num_pixels ) ) . astype ( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ] # define baseline model def baseline_model ( ) : # create model model = Sequential ( ) model . add ( Dense ( num_pixels , input_dim = num_pixels , kernel_initializer = 'normal' , activation = 'relu' ) ) model . add ( Dense ( num_classes , kernel_initializer = 'normal' , activation = 'softmax' ) ) # Compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) return model # build the model model = baseline_model ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 10 , batch_size = 200 , verbose = 2 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Baseline Error: %.2f%%"" % ( 100 - scores [ 1 ] * 100 ) )

Running the example might take a few minutes when run on a CPU.

Note: Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times.

You should see the output below. This very simple network defined in very few lines of code achieves a respectable error rate of 1.71%.

Train on 60000 samples, validate on 10000 samples Epoch 1/10 - 2s - loss: 0.2754 - acc: 0.9231 - val_loss: 0.1339 - val_acc: 0.9600 Epoch 2/10 - 2s - loss: 0.1089 - acc: 0.9684 - val_loss: 0.0935 - val_acc: 0.9717 Epoch 3/10 - 2s - loss: 0.0710 - acc: 0.9794 - val_loss: 0.0866 - val_acc: 0.9743 Epoch 4/10 - 2s - loss: 0.0496 - acc: 0.9854 - val_loss: 0.0732 - val_acc: 0.9766 Epoch 5/10 - 2s - loss: 0.0358 - acc: 0.9900 - val_loss: 0.0634 - val_acc: 0.9798 Epoch 6/10 - 2s - loss: 0.0257 - acc: 0.9933 - val_loss: 0.0597 - val_acc: 0.9826 Epoch 7/10 - 2s - loss: 0.0192 - acc: 0.9955 - val_loss: 0.0626 - val_acc: 0.9802 Epoch 8/10 - 2s - loss: 0.0142 - acc: 0.9969 - val_loss: 0.0612 - val_acc: 0.9817 Epoch 9/10 - 2s - loss: 0.0107 - acc: 0.9981 - val_loss: 0.0573 - val_acc: 0.9825 Epoch 10/10 - 2s - loss: 0.0081 - acc: 0.9985 - val_loss: 0.0558 - val_acc: 0.9829 Baseline Error: 1.71% 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Train on 60000 samples, validate on 10000 samples Epoch 1/10 - 2s - loss: 0.2754 - acc: 0.9231 - val_loss: 0.1339 - val_acc: 0.9600 Epoch 2/10 - 2s - loss: 0.1089 - acc: 0.9684 - val_loss: 0.0935 - val_acc: 0.9717 Epoch 3/10 - 2s - loss: 0.0710 - acc: 0.9794 - val_loss: 0.0866 - val_acc: 0.9743 Epoch 4/10 - 2s - loss: 0.0496 - acc: 0.9854 - val_loss: 0.0732 - val_acc: 0.9766 Epoch 5/10 - 2s - loss: 0.0358 - acc: 0.9900 - val_loss: 0.0634 - val_acc: 0.9798 Epoch 6/10 - 2s - loss: 0.0257 - acc: 0.9933 - val_loss: 0.0597 - val_acc: 0.9826 Epoch 7/10 - 2s - loss: 0.0192 - acc: 0.9955 - val_loss: 0.0626 - val_acc: 0.9802 Epoch 8/10 - 2s - loss: 0.0142 - acc: 0.9969 - val_loss: 0.0612 - val_acc: 0.9817 Epoch 9/10 - 2s - loss: 0.0107 - acc: 0.9981 - val_loss: 0.0573 - val_acc: 0.9825 Epoch 10/10 - 2s - loss: 0.0081 - acc: 0.9985 - val_loss: 0.0558 - val_acc: 0.9829 Baseline Error: 1.71%

Simple Convolutional Neural Network for MNIST

Now that we have seen how to load the MNIST dataset and train a simple multi-layer perceptron model on it, it is time to develop a more sophisticated convolutional neural network or CNN model.

Keras does provide a lot of capability for creating convolutional neural networks.

In this section we will create a simple CNN for MNIST that demonstrates how to use all of the aspects of a modern CNN implementation, including Convolutional layers, Pooling layers and Dropout layers.

The first step is to import the classes and functions needed.

from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils ... 1 2 3 4 5 6 7 8 9 from keras . datasets import mnist from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from keras . layers import Flatten from keras . layers . convolutional import Conv2D from keras . layers . convolutional import MaxPooling2D from keras . utils import np _ utils . . .

Next we need to load the MNIST dataset and reshape it so that it is suitable for use training a CNN. In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [pixels][width][height][channels].

Note, we are forcing so-called channels-last ordering for consistency in this example.

In the case of RGB, the last dimension pixels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In the case of MNIST where the pixel values are gray scale, the pixel dimension is set to 1.

... # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # reshape to be [samples][width][height][channels] X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') 1 2 3 4 5 6 . . . # load data ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( ) # reshape to be [samples][width][height][channels] X_train = X_train . reshape ( X_train . shape [ 0 ] , 28 , 28 , 1 ) . astype ( 'float32' ) X_test = X_test . reshape ( X_test . shape [ 0 ] , 28 , 28 , 1 ) . astype ( 'float32' )

As before, it is a good idea to normalize the pixel values to the range 0 and 1 and one hot encode the output variables.

... # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] 1 2 3 4 5 6 7 8 . . . # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ]

Next we define our neural network model.

Convolutional neural networks are more complex than standard multi-layer perceptrons, so we will start by using a simple structure to begin with that uses all of the elements for state of the art results. Below summarizes the network architecture.

The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5×5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above [pixels][width][height]. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2×2. The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting. Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers. Next a fully connected layer with 128 neurons and rectifier activation function. Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.

As before, the model is trained using logarithmic loss and the ADAM gradient descent algorithm.

... def baseline_model(): # create model model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(1, 28, 28), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 . . . def baseline_model ( ) : # create model model = Sequential ( ) model . add ( Conv2D ( 32 , ( 5 , 5 ) , input_shape = ( 1 , 28 , 28 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' ) ) model . add ( Dense ( num_classes , activation = 'softmax' ) ) # Compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) return model

We evaluate the model the same way as before with the multi-layer perceptron. The CNN is fit over 10 epochs with a batch size of 200.

... # build the model model = baseline_model() # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(""CNN Error: %.2f%%"" % (100-scores[1]*100)) 1 2 3 4 5 6 7 8 . . . # build the model model = baseline_model ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 10 , batch_size = 200 , verbose = 2 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""CNN Error: %.2f%%"" % ( 100 - scores [ 1 ] * 100 ) )

Tying this all together, the complete example is listed below.

# Simple CNN for the MNIST Dataset from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # reshape to be [samples][width][height][channels] X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] # define a simple CNN model def baseline_model(): # create model model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu')) model.add(MaxPooling2D()) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model # build the model model = baseline_model() # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(""CNN Error: %.2f%%"" % (100-scores[1]*100)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # Simple CNN for the MNIST Dataset from keras . datasets import mnist from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from keras . layers import Flatten from keras . layers . convolutional import Conv2D from keras . layers . convolutional import MaxPooling2D from keras . utils import np_utils # load data ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( ) # reshape to be [samples][width][height][channels] X_train = X_train . reshape ( ( X_train . shape [ 0 ] , 28 , 28 , 1 ) ) . astype ( 'float32' ) X_test = X_test . reshape ( ( X_test . shape [ 0 ] , 28 , 28 , 1 ) ) . astype ( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ] # define a simple CNN model def baseline_model ( ) : # create model model = Sequential ( ) model . add ( Conv2D ( 32 , ( 5 , 5 ) , input_shape = ( 28 , 28 , 1 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' ) ) model . add ( Dense ( num_classes , activation = 'softmax' ) ) # Compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) return model # build the model model = baseline_model ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 10 , batch_size = 200 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""CNN Error: %.2f%%"" % ( 100 - scores [ 1 ] * 100 ) )

Running the example, the accuracy on the training and validation test is printed each epoch and at the end of the classification error rate is printed.

Note: Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times.

Epochs may take about 45 seconds to run on the GPU (e.g. on AWS). You can see that the network achieves an error rate of 0.95%, which is better than our simple multi-layer perceptron model above.

Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 7s 120us/step - loss: 0.2495 - acc: 0.9275 - val_loss: 0.0854 - val_acc: 0.9737 Epoch 2/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0763 - acc: 0.9772 - val_loss: 0.0513 - val_acc: 0.9840 Epoch 3/10 60000/60000 [==============================] - 7s 119us/step - loss: 0.0548 - acc: 0.9836 - val_loss: 0.0431 - val_acc: 0.9857 Epoch 4/10 60000/60000 [==============================] - 7s 116us/step - loss: 0.0424 - acc: 0.9869 - val_loss: 0.0347 - val_acc: 0.9887 Epoch 5/10 60000/60000 [==============================] - 7s 116us/step - loss: 0.0350 - acc: 0.9894 - val_loss: 0.0411 - val_acc: 0.9873 Epoch 6/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0274 - acc: 0.9911 - val_loss: 0.0370 - val_acc: 0.9869 Epoch 7/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0238 - acc: 0.9919 - val_loss: 0.0339 - val_acc: 0.9888 Epoch 8/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0209 - acc: 0.9931 - val_loss: 0.0355 - val_acc: 0.9895 Epoch 9/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0300 - val_acc: 0.9906 Epoch 10/10 60000/60000 [==============================] - 7s 119us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0297 - val_acc: 0.9905 CNN Error: 0.95% 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 7s 120us/step - loss: 0.2495 - acc: 0.9275 - val_loss: 0.0854 - val_acc: 0.9737 Epoch 2/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0763 - acc: 0.9772 - val_loss: 0.0513 - val_acc: 0.9840 Epoch 3/10 60000/60000 [==============================] - 7s 119us/step - loss: 0.0548 - acc: 0.9836 - val_loss: 0.0431 - val_acc: 0.9857 Epoch 4/10 60000/60000 [==============================] - 7s 116us/step - loss: 0.0424 - acc: 0.9869 - val_loss: 0.0347 - val_acc: 0.9887 Epoch 5/10 60000/60000 [==============================] - 7s 116us/step - loss: 0.0350 - acc: 0.9894 - val_loss: 0.0411 - val_acc: 0.9873 Epoch 6/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0274 - acc: 0.9911 - val_loss: 0.0370 - val_acc: 0.9869 Epoch 7/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0238 - acc: 0.9919 - val_loss: 0.0339 - val_acc: 0.9888 Epoch 8/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0209 - acc: 0.9931 - val_loss: 0.0355 - val_acc: 0.9895 Epoch 9/10 60000/60000 [==============================] - 7s 117us/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0300 - val_acc: 0.9906 Epoch 10/10 60000/60000 [==============================] - 7s 119us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0297 - val_acc: 0.9905 CNN Error: 0.95%

Larger Convolutional Neural Network for MNIST

Now that we have seen how to create a simple CNN, let’s take a look at a model capable of close to state of the art results.

We import classes and function then load and prepare the data the same as in the previous CNN example.

# Larger CNN for the MNIST Dataset from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # reshape to be [samples][width][height][channels] X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] ... 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Larger CNN for the MNIST Dataset from keras . datasets import mnist from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from keras . layers import Flatten from keras . layers . convolutional import Conv2D from keras . layers . convolutional import MaxPooling2D from keras . utils import np_utils # load data ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( ) # reshape to be [samples][width][height][channels] X_train = X_train . reshape ( ( X_train . shape [ 0 ] , 28 , 28 , 1 ) ) . astype ( 'float32' ) X_test = X_test . reshape ( ( X_test . shape [ 0 ] , 28 , 28 , 1 ) ) . astype ( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ] . . .

This time we define a large CNN architecture with additional convolutional, max pooling layers and fully connected layers. The network topology can be summarized as follows.

Convolutional layer with 30 feature maps of size 5×5. Pooling layer taking the max over 2*2 patches. Convolutional layer with 15 feature maps of size 3×3. Pooling layer taking the max over 2*2 patches. Dropout layer with a probability of 20%. Flatten layer. Fully connected layer with 128 neurons and rectifier activation. Fully connected layer with 50 neurons and rectifier activation. Output layer.

... # define the larger model def larger_model(): # create model model = Sequential() model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(15, (3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(50, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . . . # define the larger model def larger_model ( ) : # create model model = Sequential ( ) model . add ( Conv2D ( 30 , ( 5 , 5 ) , input_shape = ( 1 , 28 , 28 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ) ) ) model . add ( Conv2D ( 15 , ( 3 , 3 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( num_classes , activation = 'softmax' ) ) # Compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) return model

Like the previous two experiments, the model is fit over 10 epochs with a batch size of 200.

... # build the model model = larger_model() # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(""Large CNN Error: %.2f%%"" % (100-scores[1]*100)) 1 2 3 4 5 6 7 8 . . . # build the model model = larger_model ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 10 , batch_size = 200 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Large CNN Error: %.2f%%"" % ( 100 - scores [ 1 ] * 100 ) )

Tying this all together, the complete example is listed below.

# Larger CNN for the MNIST Dataset from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # reshape to be [samples][width][height][channels] X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] # define the larger model def larger_model(): # create model model = Sequential() model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu')) model.add(MaxPooling2D()) model.add(Conv2D(15, (3, 3), activation='relu')) model.add(MaxPooling2D()) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(50, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model # build the model model = larger_model() # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(""Large CNN Error: %.2f%%"" % (100-scores[1]*100)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # Larger CNN for the MNIST Dataset from keras . datasets import mnist from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from keras . layers import Flatten from keras . layers . convolutional import Conv2D from keras . layers . convolutional import MaxPooling2D from keras . utils import np_utils # load data ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( ) # reshape to be [samples][width][height][channels] X_train = X_train . reshape ( ( X_train . shape [ 0 ] , 28 , 28 , 1 ) ) . astype ( 'float32' ) X_test = X_test . reshape ( ( X_test . shape [ 0 ] , 28 , 28 , 1 ) ) . astype ( 'float32' ) # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ] # define the larger model def larger_model ( ) : # create model model = Sequential ( ) model . add ( Conv2D ( 30 , ( 5 , 5 ) , input_shape = ( 28 , 28 , 1 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Conv2D ( 15 , ( 3 , 3 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' ) ) model . add ( Dense ( 50 , activation = 'relu' ) ) model . add ( Dense ( num_classes , activation = 'softmax' ) ) # Compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) return model # build the model model = larger_model ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 10 , batch_size = 200 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Large CNN Error: %.2f%%"" % ( 100 - scores [ 1 ] * 100 ) )

Running the example prints accuracy on the training and validation datasets each epoch and a final classification error rate.

Note: Your specific results may vary given the stochastic nature of the learning algorithm. Consider running the example a few times.

The model takes about 100 seconds to run per epoch. This slightly larger model achieves the respectable classification error rate of 0.83%.

Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 9s 157us/step - loss: 0.3871 - acc: 0.8776 - val_loss: 0.0884 - val_acc: 0.9715 Epoch 2/10 60000/60000 [==============================] - 9s 154us/step - loss: 0.1028 - acc: 0.9681 - val_loss: 0.0571 - val_acc: 0.9829 Epoch 3/10 60000/60000 [==============================] - 9s 153us/step - loss: 0.0740 - acc: 0.9781 - val_loss: 0.0468 - val_acc: 0.9851 Epoch 4/10 60000/60000 [==============================] - 9s 154us/step - loss: 0.0624 - acc: 0.9804 - val_loss: 0.0339 - val_acc: 0.9886 Epoch 5/10 60000/60000 [==============================] - 10s 161us/step - loss: 0.0496 - acc: 0.9845 - val_loss: 0.0383 - val_acc: 0.9878 Epoch 6/10 60000/60000 [==============================] - 9s 153us/step - loss: 0.0466 - acc: 0.9849 - val_loss: 0.0284 - val_acc: 0.9906 Epoch 7/10 60000/60000 [==============================] - 8s 141us/step - loss: 0.0375 - acc: 0.9884 - val_loss: 0.0282 - val_acc: 0.9909 Epoch 8/10 60000/60000 [==============================] - 8s 141us/step - loss: 0.0348 - acc: 0.9887 - val_loss: 0.0276 - val_acc: 0.9903 Epoch 9/10 60000/60000 [==============================] - 9s 144us/step - loss: 0.0317 - acc: 0.9900 - val_loss: 0.0254 - val_acc: 0.9922 Epoch 10/10 60000/60000 [==============================] - 8s 139us/step - loss: 0.0295 - acc: 0.9907 - val_loss: 0.0265 - val_acc: 0.9917 Large CNN Error: 0.83% 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Train on 60000 samples, validate on 10000 samples Epoch 1/10 60000/60000 [==============================] - 9s 157us/step - loss: 0.3871 - acc: 0.8776 - val_loss: 0.0884 - val_acc: 0.9715 Epoch 2/10 60000/60000 [==============================] - 9s 154us/step - loss: 0.1028 - acc: 0.9681 - val_loss: 0.0571 - val_acc: 0.9829 Epoch 3/10 60000/60000 [==============================] - 9s 153us/step - loss: 0.0740 - acc: 0.9781 - val_loss: 0.0468 - val_acc: 0.9851 Epoch 4/10 60000/60000 [==============================] - 9s 154us/step - loss: 0.0624 - acc: 0.9804 - val_loss: 0.0339 - val_acc: 0.9886 Epoch 5/10 60000/60000 [==============================] - 10s 161us/step - loss: 0.0496 - acc: 0.9845 - val_loss: 0.0383 - val_acc: 0.9878 Epoch 6/10 60000/60000 [==============================] - 9s 153us/step - loss: 0.0466 - acc: 0.9849 - val_loss: 0.0284 - val_acc: 0.9906 Epoch 7/10 60000/60000 [==============================] - 8s 141us/step - loss: 0.0375 - acc: 0.9884 - val_loss: 0.0282 - val_acc: 0.9909 Epoch 8/10 60000/60000 [==============================] - 8s 141us/step - loss: 0.0348 - acc: 0.9887 - val_loss: 0.0276 - val_acc: 0.9903 Epoch 9/10 60000/60000 [==============================] - 9s 144us/step - loss: 0.0317 - acc: 0.9900 - val_loss: 0.0254 - val_acc: 0.9922 Epoch 10/10 60000/60000 [==============================] - 8s 139us/step - loss: 0.0295 - acc: 0.9907 - val_loss: 0.0265 - val_acc: 0.9917 Large CNN Error: 0.83%

This is not an optimized network topology. Nor is a reproduction of a network topology from a recent paper. There is a lot of opportunity for you to tune and improve upon this model.

What is the best error rate score you can achieve?

Post your configuration and best score in the comments.

Resources on MNIST

The MNIST dataset is very well studied. Below are some additional resources you might like to look into.

Summary

In this post you discovered the MNIST handwritten digit recognition problem and deep learning models developed in Python using the Keras library that are capable of achieving excellent results.

Working through this tutorial you learned:

How to load the MNIST dataset in Keras and generate plots of the dataset.

How to reshape the MNIST dataset and develop a simple but well performing multi-layer perceptron model on the problem.

How to use Keras to create convolutional neural network models for MNIST.

How to develop and evaluate larger CNN models for MNIST capable of near world class results.

Do you have any questions about handwriting recognition with deep learning or this post? Ask your question in the comments and I will do my best to answer.

Develop Deep Learning Projects with Python! What If You Could Develop A Network in Minutes ...with just a few lines of Python Discover how in my new Ebook:

Deep Learning With Python It covers end-to-end projects on topics like:

Multilayer Perceptrons, Convolutional Nets and Recurrent Neural Nets, and more... Finally Bring Deep Learning To

Your Own Projects Skip the Academics. Just Results. See What's Inside"
114;machinelearningmastery.com;https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/;2020-04-09;Stacking Ensemble Machine Learning With Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65

# compare ensemble to each baseline classifier from numpy import mean from numpy import std from sklearn . datasets import make_classification from sklearn . model_selection import cross_val_score from sklearn . model_selection import RepeatedStratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . neighbors import KNeighborsClassifier from sklearn . tree import DecisionTreeClassifier from sklearn . svm import SVC from sklearn . naive_bayes import GaussianNB from sklearn . ensemble import StackingClassifier from matplotlib import pyplot # get the dataset def get_dataset ( ) : X , y = make_classification ( n_samples = 1000 , n_features = 20 , n_informative = 15 , n_redundant = 5 , random_state = 1 ) return X , y # get a stacking ensemble of models def get_stacking ( ) : # define the base models level0 = list ( ) level0 . append ( ( 'lr' , LogisticRegression ( ) ) ) level0 . append ( ( 'knn' , KNeighborsClassifier ( ) ) ) level0 . append ( ( 'cart' , DecisionTreeClassifier ( ) ) ) level0 . append ( ( 'svm' , SVC ( ) ) ) level0 . append ( ( 'bayes' , GaussianNB ( ) ) ) # define meta learner model level1 = LogisticRegression ( ) # define the stacking ensemble model = StackingClassifier ( estimators = level0 , final_estimator = level1 , cv = 5 ) return model # get a list of models to evaluate def get_models ( ) : models = dict ( ) models [ 'lr' ] = LogisticRegression ( ) models [ 'knn' ] = KNeighborsClassifier ( ) models [ 'cart' ] = DecisionTreeClassifier ( ) models [ 'svm' ] = SVC ( ) models [ 'bayes' ] = GaussianNB ( ) models [ 'stacking' ] = get_stacking ( ) return models # evaluate a give model using cross-validation def evaluate_model ( model ) : cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 ) scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' ) return scores # define dataset X , y = get_dataset ( ) # get the models to evaluate models = get_models ( ) # evaluate the models and store results results , names = list ( ) , list ( ) for name , model in models . items ( ) : scores = evaluate_model ( model ) results . append ( scores ) names . append ( name ) print ( '>%s %.3f (%.3f)' % ( name , mean ( scores ) , std ( scores ) ) ) # plot model performance for comparison pyplot . boxplot ( results , labels = names , showmeans = True ) pyplot . show ( )"
115;machinelearningmastery.com;http://machinelearningmastery.com/better-understand-machine-learning-data-weka/;2016-06-29;How to Better Understand Your Machine Learning Data in Weka;"Tweet Share Share

Last Updated on August 22, 2019

It is important to take your time to learn about your data when starting on a new machine learning problem.

There are key things that you can look at to very quickly learn more about your dataset, such as descriptive statistics and data visualizations.

In this post you will discover how you can learn more about your data in the Weka machine learning workbench my reviewing descriptive statistics and visualizations of your data.

After reading this post you will know about:

The distribution of attributes from reviewing statistical summaries.

The distribution of attributes from reviewing univariate plots.

The relationship between attributes from reviewing multivariate plots.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started

Better Understand Your Data With Descriptive Statistics

The Weka explorer will automatically calculate descriptives statistics for numerical attributes.

Open The Weka GUI Chooser. Click “Explorer” to open the Weka Explorer. Load the Pima Indians datasets from data/diabetes.arff

The Pima Indians dataset contains numeric input variables that we can use to demonstrate the calculation of descriptive statistics.

Firstly, note that the dataset summary in the “Current Relation” section. This panel summarizes the following details about the loaded datasets:

Dataset name (relation).

The number of rows (instances).

The number of columns (attributes).

Click on the first attribute in the dataset in the “Attributes” panel.

Take note of the details in the “Selected attribute” panel. It lists a lot of information about the selected attribute, such as:

The name of the attribute.

The number of missing values and the ratio of missing values across the whole dataset.

The number of distinct values.

The data type.

The table below lists a number of descriptive statistics and their values. A useful four number summary is provided for numeric attributes including:

Minimum value.

Maximum value.

Mean value.

Standard deviation.

You can learn a lot from this information. For example:

The presence and ratio of missing data can give you an indication of whether or not you need to remove or impute values.

The mean and standard deviation give you a quantified idea of the spread of data for each attribute.

The number of distinct values can give you an idea of the granularity of the attribute distribution.

Click the class attribute. This attribute has a nominal type. Review the “Selected attribute panel”.

We can now see that for nominal attributes that we are provided with a list of each category and the count of instances that belong to each category. There is also mention of weightings, which we can ignore for now. This is used if we want to assign more or less weight to specific attribute values or instances in the dataset.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Univariate Attribute Distributions

The distribution of each attribute can be plotted to give a visual qualitative understanding of the distribution.

Weka provides these plots automatically when you select an attribute in the “Preprocess” tab.

We can follow on from the previous section where we already have the Pima Indians dataset loaded.

Click on the “preg” attribute in the “Attributes panel” and note the plot below the “Selected attribute” panel. You will see the distribution of preg values between 0 and 17 along the x-axis. The y-axis shows the count or frequency of values with each preg value.

Note the red and blue colors referring to the positive and negative classes respectively. The colors are assigned automatically to each categorical value. If there were three categories for the class value, we would see the breakdown of the preg distribution by three colors rather than two.

This is useful to get a quick idea of whether the problem is easily separable for a given attribute, e.g. all the red and blue are cleanly separated for a single attribute. Clicking through each attribute in the list of Attributes and reviewing the plots, we can see that there is no such easy separation of the classes.

We can quickly get an overview of the distribution of all attributes in the dataset and the breakdown of distributions by class by clicking the “Visualize All” button above the univariate plot.

Looking at these plots we can see a few interesting things about this dataset.

It looks like the plas, pres and mass attributes have a nearly Gaussian distribution.

It looks likes pres, skin, insu and mass have values at 0 that look out of place.

Looking at plots like this and jotting down things that come to mind can give you an idea of further data preparation operations that could be applied (like marking 0 values as corrupt) and even techniques that might be useful (like linear discriminant analysis and logistic regression that assume a Gaussian distribution in input variables).

Visualize Attribute Interactions

So far we have only been looking at the properties of individual features, next we will look at patterns in combinations of attributes.

When attributes are numeric we can create a scatter plot of one attribute against another. This is useful as it can highlight any patterns in the relationship between the attributes, such as positive or negative correlations.

We can create scatter plots for all pairs of input attributes. This is called a scatter plot matrix and reviewing it before modeling your data can shed more light on further preprocessing techniques that you could investigate.

Weka provides a scatter plot matrix for review by default in the “Visualise” tab.

Continuing on from the previous section with the Pima Indians dataset loaded, click the “Visualize” tab, and make the window large enough to review all of the individual scatter plots.

You can see that all combinations of attributes are plotted in a systematic way. You can also see that each plot appears twice, first in the top left triangle and again in the bottom right triangle with the axes flipped. You can also see a series of plots starting in the bottom left and continuing to the top right where each attribute is plotted against itself. These can be ignored.

Finally, notice that the dots in the scatter plots are colored by their class value. It is good to look for trends or patterns in the dots, such as clear separation of the colors.

Clicking on a plot will give you a new window with the plot that you can further play with.

Note the controls at the bottom of the screen. They let you increase the size of the plots, increase the size of the dots and add jitter.

This last point about jitter is useful when you have a lot of dots overlaying each other and it is hard to see what is going on. Jitter will add some random noise to the data in the plots, spread out the points a bit and help you see what is going on.

When you make a change to these controls, click the “Update” button to apply the changes.

For example, below are the same plots with a larger dot size that makes it easier to see any trends in the data.

Summary

In this post you discovered how you can learn more about your machine learning data by reviewing descriptive statistics and data visualizations.

Specifically, you learned:

That Weka automatically calculates descriptive statistics for each attribute.

That Weka allows you to review the distribution of each attribute easily.

That Weka provides a scatter plot visualization to review the pairwise relationships between attributes.

Do you have any questions about descriptive statistics and data visualization in Weka or about this post? Ask your questions in the comments below and I will do my best to answer them.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
116;machinelearningmastery.com;https://machinelearningmastery.com/probability-for-machine-learning-7-day-mini-course/;2019-10-02;Probability for Machine Learning (7-Day Mini-Course);"Tweet Share Share

Last Updated on January 10, 2020

Probability for Machine Learning Crash Course.

Get on top of the probability used in machine learning in 7 days.

Probability is a field of mathematics that is universally agreed to be the bedrock for machine learning.

Although probability is a large field with many esoteric theories and findings, the nuts and bolts, tools and notations taken from the field are required for machine learning practitioners. With a solid foundation of what probability is, it is possible to focus on just the good or relevant parts.

In this crash course, you will discover how you can get started and confidently understand and implement probabilistic methods used in machine learning with Python in seven days.

This is a big and important post. You might want to bookmark it.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Update Jan/2020: Updated for changes in scikit-learn v0.22 API.

Who Is This Crash-Course For?

Before we get started, let’s make sure you are in the right place.

This course is for developers that may know some applied machine learning. Maybe you know how to work through a predictive modeling problem end-to-end, or at least most of the main steps, with popular tools.

The lessons in this course do assume a few things about you, such as:

You know your way around basic Python for programming.

You may know some basic NumPy for array manipulation.

You want to learn probability to deepen your understanding and application of machine learning.

You do NOT need to be:

A math wiz!

A machine learning expert!

This crash course will take you from a developer that knows a little machine learning to a developer who can navigate the basics of probabilistic methods.

Note: This crash course assumes you have a working Python3 SciPy environment with at least NumPy installed. If you need help with your environment, you can follow the step-by-step tutorial here:

Crash-Course Overview

This crash course is broken down into seven lessons.

You could complete one lesson per day (recommended) or complete all of the lessons in one day (hardcore). It really depends on the time you have available and your level of enthusiasm.

Below is a list of the seven lessons that will get you started and productive with probability for machine learning in Python:

Lesson 01 : Probability and Machine Learning

: Probability and Machine Learning Lesson 02 : Three Types of Probability

: Three Types of Probability Lesson 03 : Probability Distributions

: Probability Distributions Lesson 04 : Naive Bayes Classifier

: Naive Bayes Classifier Lesson 05 : Entropy and Cross-Entropy

: Entropy and Cross-Entropy Lesson 06 : Naive Classifiers

: Naive Classifiers Lesson 07: Probability Scores

Each lesson could take you 60 seconds or up to 30 minutes. Take your time and complete the lessons at your own pace. Ask questions and even post results in the comments below.

The lessons expect you to go off and find out how to do things. I will give you hints, but part of the point of each lesson is to force you to learn where to go to look for help on and about the statistical methods and the NumPy API and the best-of-breed tools in Python. (Hint: I have all of the answers directly on this blog; use the search box.)

Post your results in the comments; I’ll cheer you on!

Hang in there; don’t give up.

Note: This is just a crash course. For a lot more detail and fleshed-out tutorials, see my book on the topic titled “Probability for Machine Learning.”

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Lesson 01: Probability and Machine Learning

In this lesson, you will discover why machine learning practitioners should study probability to improve their skills and capabilities.

Probability is a field of mathematics that quantifies uncertainty.

Machine learning is about developing predictive modeling from uncertain data. Uncertainty means working with imperfect or incomplete information.

Uncertainty is fundamental to the field of machine learning, yet it is one of the aspects that causes the most difficulty for beginners, especially those coming from a developer background.

There are three main sources of uncertainty in machine learning; they are:

Noise in observations , e.g. measurement errors and random noise.

, e.g. measurement errors and random noise. Incomplete coverage of the domain , e.g. you can never observe all data.

, e.g. you can never observe all data. Imperfect model of the problem, e.g. all models have errors, some are useful.

Uncertainty in applied machine learning is managed using probability.

Probability and statistics help us to understand and quantify the expected value and variability of variables in our observations from the domain.

Probability helps to understand and quantify the expected distribution and density of observations in the domain.

Probability helps to understand and quantify the expected capability and variance in performance of our predictive models when applied to new data.

This is the bedrock of machine learning. On top of that, we may need models to predict a probability, we may use probability to develop predictive models (e.g. Naive Bayes), and we may use probabilistic frameworks to train predictive models (e.g. maximum likelihood estimation).

Your Task

For this lesson, you must list three reasons why you want to learn probability in the context of machine learning.

These may be related to some of the reasons above, or they may be your own personal motivations.

Post your answer in the comments below. I would love to see what you come up with.

In the next lesson, you will discover the three different types of probability and how to calculate them.

Lesson 02: Three Types of Probability

In this lesson, you will discover a gentle introduction to joint, marginal, and conditional probability between random variables.

Probability quantifies the likelihood of an event.

Specifically, it quantifies how likely a specific outcome is for a random variable, such as the flip of a coin, the roll of a die, or drawing a playing card from a deck.

We can discuss the probability of just two events: the probability of event A for variable X and event B for variable Y, which in shorthand is X=A and Y=B, and that the two variables are related or dependent in some way.

As such, there are three main types of probability we might want to consider.

Joint Probability

We may be interested in the probability of two simultaneous events, like the outcomes of two different random variables.

For example, the joint probability of event A and event B is written formally as:

P(A and B)

The joint probability for events A and B is calculated as the probability of event A given event B multiplied by the probability of event B.

This can be stated formally as follows:

P(A and B) = P(A given B) * P(B)

Marginal Probability

We may be interested in the probability of an event for one random variable, irrespective of the outcome of another random variable.

There is no special notation for marginal probability; it is just the sum or union over all the probabilities of all events for the second variable for a given fixed event for the first variable.

P(X=A) = sum P(X=A, Y=yi) for all y

Conditional Probability

We may be interested in the probability of an event given the occurrence of another event.

For example, the conditional probability of event A given event B is written formally as:

P(A given B)

The conditional probability for events A given event B can be calculated using the joint probability of the events as follows:

P(A given B) = P(A and B) / P(B)

Your Task

For this lesson, you must practice calculating joint, marginal, and conditional probabilities.

For example, if a family has two children and the oldest is a boy, what is the probability of this family having two sons? This is called the “Boy or Girl Problem” and is one of many common toy problems for practicing probability.

Post your answer in the comments below. I would love to see what you come up with.

In the next lesson, you will discover probability distributions for random variables.

Lesson 03: Probability Distributions

In this lesson, you will discover a gentle introduction to probability distributions.

In probability, a random variable can take on one of many possible values, e.g. events from the state space. A specific value or set of values for a random variable can be assigned a probability.

There are two main classes of random variables.

Discrete Random Variable . Values are drawn from a finite set of states.

. Values are drawn from a finite set of states. Continuous Random Variable. Values are drawn from a range of real-valued numerical values.

A discrete random variable has a finite set of states; for example, the colors of a car. A continuous random variable has a range of numerical values; for example, the height of humans.

A probability distribution is a summary of probabilities for the values of a random variable.

Discrete Probability Distributions

A discrete probability distribution summarizes the probabilities for a discrete random variable.

Some examples of well-known discrete probability distributions include:

Poisson distribution.

Bernoulli and binomial distributions.

Multinoulli and multinomial distributions.

Continuous Probability Distributions

A continuous probability distribution summarizes the probability for a continuous random variable.

Some examples of well-known continuous probability distributions include:

Normal or Gaussian distribution.

Exponential distribution.

Pareto distribution.

Randomly Sample Gaussian Distribution

We can define a distribution with a mean of 50 and a standard deviation of 5 and sample random numbers from this distribution. We can achieve this using the normal() NumPy function.

The example below samples and prints 10 numbers from this distribution.

# sample a normal distribution from numpy.random import normal # define the distribution mu = 50 sigma = 5 n = 10 # generate the sample sample = normal(mu, sigma, n) print(sample) 1 2 3 4 5 6 7 8 9 # sample a normal distribution from numpy . random import normal # define the distribution mu = 50 sigma = 5 n = 10 # generate the sample sample = normal ( mu , sigma , n ) print ( sample )

Running the example prints 10 numbers randomly sampled from the defined normal distribution.

Your Task

For this lesson, you must develop an example to sample from a different continuous or discrete probability distribution function.

For a bonus, you can plot the values on the x-axis and the probability on the y-axis for a given distribution to show the density of your chosen probability distribution function.

Post your answer in the comments below. I would love to see what you come up with.

In the next lesson, you will discover the Naive Bayes classifier.

Lesson 04: Naive Bayes Classifier

In this lesson, you will discover the Naive Bayes algorithm for classification predictive modeling.

In machine learning, we are often interested in a predictive modeling problem where we want to predict a class label for a given observation.

One approach to solving this problem is to develop a probabilistic model. From a probabilistic perspective, we are interested in estimating the conditional probability of the class label given the observation, or the probability of class y given input data X.

P(y | X)

Bayes Theorem provides an alternate and principled way for calculating the conditional probability using the reverse of the desired conditional probability, which is often simpler to calculate.

The simple form of the calculation for Bayes Theorem is as follows:

P(A|B) = P(B|A) * P(A) / P(B)

Where the probability that we are interested in calculating P(A|B) is called the posterior probability and the marginal probability of the event P(A) is called the prior.

The direct application of Bayes Theorem for classification becomes intractable, especially as the number of variables or features (n) increases. Instead, we can simplify the calculation and assume that each input variable is independent. Although dramatic, this simpler calculation often gives very good performance, even when the input variables are highly dependent.

We can implement this from scratch by assuming a probability distribution for each separate input variable and calculating the probability of each specific input value belonging to each class and multiply the results together to give a score used to select the most likely class.

P(yi | x1, x2, …, xn) = P(x1|y1) * P(x2|y1) * … P(xn|y1) * P(yi)

The scikit-learn library provides an efficient implementation of the algorithm if we assume a Gaussian distribution for each input variable.

To use a scikit-learn Naive Bayes model, first the model is defined, then it is fit on the training dataset. Once fit, probabilities can be predicted via the predict_proba() function and class labels can be predicted directly via the predict() function.

The complete example of fitting a Gaussian Naive Bayes model (GaussianNB) to a test dataset is listed below.

# example of gaussian naive bayes from sklearn.datasets import make_blobs from sklearn.naive_bayes import GaussianNB # generate 2d classification dataset X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1) # define the model model = GaussianNB() # fit the model model.fit(X, y) # select a single sample Xsample, ysample = [X[0]], y[0] # make a probabilistic prediction yhat_prob = model.predict_proba(Xsample) print('Predicted Probabilities: ', yhat_prob) # make a classification prediction yhat_class = model.predict(Xsample) print('Predicted Class: ', yhat_class) print('Truth: y=%d' % ysample) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # example of gaussian naive bayes from sklearn . datasets import make_blobs from sklearn . naive_bayes import GaussianNB # generate 2d classification dataset X , y = make_blobs ( n_samples = 100 , centers = 2 , n_features = 2 , random_state = 1 ) # define the model model = GaussianNB ( ) # fit the model model . fit ( X , y ) # select a single sample Xsample , ysample = [ X [ 0 ] ] , y [ 0 ] # make a probabilistic prediction yhat_prob = model . predict_proba ( Xsample ) print ( 'Predicted Probabilities: ' , yhat_prob ) # make a classification prediction yhat_class = model . predict ( Xsample ) print ( 'Predicted Class: ' , yhat_class ) print ( 'Truth: y=%d' % ysample )

Running the example fits the model on the training dataset, then makes predictions for the same first example that we used in the prior example.

Your Task

For this lesson, you must run the example and report the result.

For a bonus, try the algorithm on a real classification dataset, such as the popular toy classification problem of classifying iris flower species based on flower measurements.

Post your answer in the comments below. I would love to see what you come up with.

In the next lesson, you will discover entropy and the cross-entropy scores.

Lesson 05: Entropy and Cross-Entropy

In this lesson, you will discover cross-entropy for machine learning.

Information theory is a field of study concerned with quantifying information for communication.

The intuition behind quantifying information is the idea of measuring how much surprise there is in an event. Those events that are rare (low probability) are more surprising and therefore have more information than those events that are common (high probability).

Low Probability Event : High Information (surprising).

: High Information (surprising). High Probability Event: Low Information (unsurprising).

We can calculate the amount of information there is in an event using the probability of the event.

Information(x) = -log( p(x) )

We can also quantify how much information there is in a random variable.

This is called entropy and summarizes the amount of information required on average to represent events.

Entropy can be calculated for a random variable X with K discrete states as follows:

Entropy(X) = -sum(i=1 to K p(K) * log(p(K)))

Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. It is widely used as a loss function when optimizing classification models.

It builds upon the idea of entropy and calculates the average number of bits required to represent or transmit an event from one distribution compared to the other distribution.

CrossEntropy(P, Q) = – sum x in X P(x) * log(Q(x))

We can make the calculation of cross-entropy concrete with a small example.

Consider a random variable with three events as different colors. We may have two different probability distributions for this variable. We can calculate the cross-entropy between these two distributions.

The complete example is listed below.

# example of calculating cross entropy from math import log2 # calculate cross entropy def cross_entropy(p, q): return -sum([p[i]*log2(q[i]) for i in range(len(p))]) # define data p = [0.10, 0.40, 0.50] q = [0.80, 0.15, 0.05] # calculate cross entropy H(P, Q) ce_pq = cross_entropy(p, q) print('H(P, Q): %.3f bits' % ce_pq) # calculate cross entropy H(Q, P) ce_qp = cross_entropy(q, p) print('H(Q, P): %.3f bits' % ce_qp) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # example of calculating cross entropy from math import log2 # calculate cross entropy def cross_entropy ( p , q ) : return - sum ( [ p [ i ] * log2 ( q [ i ] ) for i in range ( len ( p ) ) ] ) # define data p = [ 0.10 , 0.40 , 0.50 ] q = [ 0.80 , 0.15 , 0.05 ] # calculate cross entropy H(P, Q) ce_pq = cross_entropy ( p , q ) print ( 'H(P, Q): %.3f bits' % ce_pq ) # calculate cross entropy H(Q, P) ce_qp = cross_entropy ( q , p ) print ( 'H(Q, P): %.3f bits' % ce_qp )

Running the example first calculates the cross-entropy of Q from P, then P from Q.

Your Task

For this lesson, you must run the example and describe the results and what they mean. For example, is the calculation of cross-entropy symmetrical?

Post your answer in the comments below. I would love to see what you come up with.

In the next lesson, you will discover how to develop and evaluate a naive classifier model.

Lesson 06: Naive Classifiers

In this lesson, you will discover how to develop and evaluate naive classification strategies for machine learning.

Classification predictive modeling problems involve predicting a class label given an input to the model.

Given a classification model, how do you know if the model has skill or not?

This is a common question on every classification predictive modeling project. The answer is to compare the results of a given classifier model to a baseline or naive classifier model.

Consider a simple two-class classification problem where the number of observations is not equal for each class (e.g. it is imbalanced) with 25 examples for class-0 and 75 examples for class-1. This problem can be used to consider different naive classifier models.

For example, consider a model that randomly predicts class-0 or class-1 with equal probability. How would it perform?

We can calculate the expected performance using a simple probability model.

P(yhat = y) = P(yhat = 0) * P(y = 0) + P(yhat = 1) * P(y = 1)

We can plug in the occurrence of each class (0.25 and 0.75) and the predicted probability for each class (0.5 and 0.5) and estimate the performance of the model.

P(yhat = y) = 0.5 * 0.25 + 0.5 * 0.75

P(yhat = y) = 0.5

It turns out that this classifier is pretty poor.

Now, what if we consider predicting the majority class (class-1) every time? Again, we can plug in the predicted probabilities (0.0 and 1.0) and estimate the performance of the model.

P(yhat = y) = 0.0 * 0.25 + 1.0 * 0.75

P(yhat = y) = 0.75

It turns out that this simple change results in a better naive classification model, and is perhaps the best naive classifier to use when classes are imbalanced.

The scikit-learn machine learning library provides an implementation of the majority class naive classification algorithm called the DummyClassifier that you can use on your next classification predictive modeling project.

The complete example is listed below.

# example of the majority class naive classifier in scikit-learn from numpy import asarray from sklearn.dummy import DummyClassifier from sklearn.metrics import accuracy_score # define dataset X = asarray([0 for _ in range(100)]) class0 = [0 for _ in range(25)] class1 = [1 for _ in range(75)] y = asarray(class0 + class1) # reshape data for sklearn X = X.reshape((len(X), 1)) # define model model = DummyClassifier(strategy='most_frequent') # fit model model.fit(X, y) # make predictions yhat = model.predict(X) # calculate accuracy accuracy = accuracy_score(y, yhat) print('Accuracy: %.3f' % accuracy) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # example of the majority class naive classifier in scikit-learn from numpy import asarray from sklearn . dummy import DummyClassifier from sklearn . metrics import accuracy_score # define dataset X = asarray ( [ 0 for _ in range ( 100 ) ] ) class0 = [ 0 for _ in range ( 25 ) ] class1 = [ 1 for _ in range ( 75 ) ] y = asarray ( class0 + class1 ) # reshape data for sklearn X = X . reshape ( ( len ( X ) , 1 ) ) # define model model = DummyClassifier ( strategy = 'most_frequent' ) # fit model model . fit ( X , y ) # make predictions yhat = model . predict ( X ) # calculate accuracy accuracy = accuracy_score ( y , yhat ) print ( 'Accuracy: %.3f' % accuracy )

Running the example prepares the dataset, then defines and fits the DummyClassifier on the dataset using the majority class strategy.

Your Task

For this lesson, you must run the example and report the result, confirming whether the model performs as we expected from our calculation.

As a bonus, calculate the expected probability of a naive classifier model that randomly chooses a class label from the training dataset each time a prediction is made.

Post your answer in the comments below. I would love to see what you come up with.

In the next lesson, you will discover metrics for scoring models that predict probabilities.

Lesson 07: Probability Scores

In this lesson, you will discover two scoring methods that you can use to evaluate the predicted probabilities on your classification predictive modeling problem.

Predicting probabilities instead of class labels for a classification problem can provide additional nuance and uncertainty for the predictions.

The added nuance allows more sophisticated metrics to be used to interpret and evaluate the predicted probabilities.

Let’s take a closer look at the two popular scoring methods for evaluating predicted probabilities.

Log Loss Score

Logistic loss, or log loss for short, calculates the log likelihood between the predicted probabilities and the observed probabilities.

Although developed for training binary classification models like logistic regression, it can be used to evaluate multi-class problems and is functionally equivalent to calculating the cross-entropy derived from information theory.

A model with perfect skill has a log loss score of 0.0. The log loss can be implemented in Python using the log_loss() function in scikit-learn.

For example:

# example of log loss from numpy import asarray from sklearn.metrics import log_loss # define data y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3] # define data as expected, e.g. probability for each event {0, 1} y_true = asarray([[v, 1-v] for v in y_true]) y_pred = asarray([[v, 1-v] for v in y_pred]) # calculate log loss loss = log_loss(y_true, y_pred) print(loss) 1 2 3 4 5 6 7 8 9 10 11 12 # example of log loss from numpy import asarray from sklearn . metrics import log_loss # define data y_true = [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] y_pred = [ 0.8 , 0.9 , 0.9 , 0.6 , 0.8 , 0.1 , 0.4 , 0.2 , 0.1 , 0.3 ] # define data as expected, e.g. probability for each event {0, 1} y_true = asarray ( [ [ v , 1 - v ] for v in y_true ] ) y_pred = asarray ( [ [ v , 1 - v ] for v in y_pred ] ) # calculate log loss loss = log_loss ( y_true , y_pred ) print ( loss )

Brier Score

The Brier score, named for Glenn Brier, calculates the mean squared error between predicted probabilities and the expected values.

The score summarizes the magnitude of the error in the probability forecasts.

The error score is always between 0.0 and 1.0, where a model with perfect skill has a score of 0.0.

The Brier score can be calculated in Python using the brier_score_loss() function in scikit-learn.

For example:

# example of brier loss from sklearn.metrics import brier_score_loss # define data y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0] y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3] # calculate brier score score = brier_score_loss(y_true, y_pred, pos_label=1) print(score) 1 2 3 4 5 6 7 8 # example of brier loss from sklearn . metrics import brier_score_loss # define data y_true = [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] y_pred = [ 0.8 , 0.9 , 0.9 , 0.6 , 0.8 , 0.1 , 0.4 , 0.2 , 0.1 , 0.3 ] # calculate brier score score = brier_score_loss ( y_true , y_pred , pos_label = 1 ) print ( score )

Your Task

For this lesson, you must run each example and report the results.

As a bonus, change the mock predictions to make them better or worse and compare the resulting scores.

Post your answer in the comments below. I would love to see what you come up with.

This was the final lesson.

The End!

(Look How Far You Have Come)

You made it. Well done!

Take a moment and look back at how far you have come.

You discovered:

The importance of probability in applied machine learning.

The three main types of probability and how to calculate them.

Probability distributions for random variables and how to draw random samples from them.

How Bayes theorem can be used to calculate conditional probability and how it can be used in a classification model.

How to calculate information, entropy, and cross-entropy scores and what they mean.

How to develop and evaluate the expected performance for naive classification models.

How to evaluate the skill of a model that predicts probability values for a classification problem.

Take the next step and check out my book on Probability for Machine Learning.

Summary

How did you do with the mini-course?

Did you enjoy this crash course?

Do you have any questions? Were there any sticking points?

Let me know. Leave a comment below.

Get a Handle on Probability for Machine Learning! Develop Your Understanding of Probability ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Probability for Machine Learning It provides self-study tutorials and end-to-end projects on:

Bayes Theorem, Bayesian Optimization, Distributions, Maximum Likelihood, Cross-Entropy, Calibrating Models

and much more... Finally Harness Uncertainty in Your Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
117;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/;2018-08-09;How to Develop a Skillful Machine Learning Time Series Forecasting Model;"Tweet Share Share

Last Updated on August 5, 2019

You are handed data and told to develop a forecast model.

What do you do?

This is a common situation; far more common than most people think.

Perhaps you are sent a CSV file.

Perhaps you are given access to a database.

Perhaps you are starting a competition.

The problem can be reasonably well defined:

You have or can access historical time series data.

You know or can find out what needs to be forecasted.

You know or can find out how what is most important in evaluating a candidate model.

So how do you tackle this problem?

Unless you have been through this trial by fire, you may struggle.

You may struggle because you are new to the fields of machine learning and time series.

You may struggle even if you have machine learning experience because time series data is different.

You may struggle even if you have a background in time series forecasting because machine learning methods may outperform the classical approaches on your data.

In all of these cases, you will benefit from working through the problem carefully and systematically.

In this post, I want to give you a specific and actionable procedure that you can use to work through your time series forecasting problem.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Process Overview

The goal of this process is to get a “good enough” forecast model as fast as possible.

This process may or may not deliver the best possible model, but it will deliver a good model: a model that is better than a baseline prediction, if such a model exists.

Typically, this process will deliver a model that is 80% to 90% of what can be achieved on the problem.

The process is fast. As such, it focuses on automation. Hyperparameters are searched rather than specified based on careful analysis. You are encouraged to test suites of models in parallel, rapidly getting an idea of what works and what doesn’t.

Nevertheless, the process is flexible, allowing you to circle back or go as deep as you like on a given step if you have the time and resources.

This process is divided into four parts; they are:

Define Problem Design Test Harness Test Models Finalize Model

You will notice that the process is different from a classical linear work-through of a predictive modeling problem. This is because it is designed to get a working forecast model fast and then slow down and see if you can get a better model.

What is your process for working through a new time series forecasting problem?

Share it below in the comments.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Use This Process

The biggest mistake is skipping steps.

For example, the mistake that almost all beginners make is going straight to modeling without a strong idea of what problem is being solved or how to robustly evaluate candidate solutions. This almost always results in a lot of wasted time.

Slow down, follow the process, and complete each step.

I recommend having separate code for each experiment that can be re-run at any time.

This is important so that you can circle back when you discover a bug, fix the code, and re-run an experiment. You are running experiments and iterating quickly, but if you are sloppy, then you cannot trust any of your results. This is especially important when it comes to the design of your test harness for evaluating candidate models.

Let’s take a closer look at each step of the process.

1. Define Problem

Define your time series problem.

Some topics to consider and motivating questions within each topic are as follows:

Inputs vs. Outputs What are the inputs and outputs for a forecast? Endogenous vs. Exogenous What are the endogenous and exogenous variables? Unstructured vs. Structured Are the time series variables unstructured or structured? Regression vs. Classification Are you working on a regression or classification predictive modeling problem? What are some alternate ways to frame your time series forecasting problem? Univariate vs. Multivariate Are you working on a univariate or multivariate time series problem? Single-step vs. Multi-step Do you require a single-step or a multi-step forecast? Static vs. Dynamic Do you require a static or a dynamically updated model?

Answer each question even if you have to estimate or guess.

Some useful tools to help get answers include:

Data visualizations (e.g. line plots, etc.).

Statistical analysis (e.g. ACF/PACF plots, etc.).

Domain experts.

Project stakeholders.

Update your answers to these questions as you learn more.

2. Design Test Harness

Design a test harness that you can use to evaluate candidate models.

This includes both the method used to estimate model skill and the metric used to evaluate predictions.

Below is a common time series forecasting model evaluation scheme if you are looking for ideas:

Split the dataset into a train and test set. Fit a candidate approach on the training dataset. Make predictions on the test set directly or using walk-forward validation. Calculate a metric that compares the predictions to the expected values.

The test harness must be robust and you must have complete trust in the results it provides.

An important consideration is to ensure that any coefficients used for data preparation are estimated from the training dataset only and then applied on the test set. This might include mean and standard deviation in the case of data standardization.

3. Test Models

Test many models using your test harness.

I recommend carefully designing experiments to test a suite of configurations for standard models and letting them run. Each experiment can record results to a file, to allow you to quickly discover the top three to five most skilful configurations from each run.

Some common classes of methods that you can design experiments around include the following:

Baseline. Persistence (grid search the lag observation that is persisted) Rolling moving average. …

Autoregression. ARMA for stationary data. ARIMA for data with a trend. SARIMA for data with seasonality. …

Exponential Smoothing. Simple Smoothing Holt Winters Smoothing …

Linear Machine Learning. Linear Regression Ridge Regression Lasso Regression Elastic Net Regression ….

Nonlinear Machine Learning. k-Nearest Neighbors Classification and Regression Trees Support Vector Regression …

Ensemble Machine Learning. Bagging Boosting Random Forest Gradient Boosting …

Deep Learning. MLP CNN LSTM Hybrids …



This list is based on a univariate time series forecasting problem, but you can adapt it for the specifics of your problem, e.g. use VAR/VARMA/etc. in the case of multivariate time series forecasting.

Slot in more of your favorite classical time series forecasting methods and machine learning methods as you see fit.

Order here is important and is structured in increasing complexity from classical to modern methods. Early approaches are simple and give good results fast; later approaches are slower and more complex, but also have a higher bar to clear to be skillful.

The resulting model skill can be used in a ratchet. For example, the skill of the best persistence configuration provide a baseline skill that all other models must outperform. If an autoregression model does better than persistence, it becomes the new level to outperform in order for a method to be considered skilful.

Ideally, you want to exhaust each level before moving on to the next. E.g. get the most out of Autoregression methods and use the results as a new baseline to define “skilful” before moving on to Exponential Smoothing methods.

I put deep learning at the end as generally neural networks are poor at time series forecasting, but there is still a lot of room for improvement and experimentation in this area.

The more time and resources that you have, the more configurations that you can evaluate.

For example, with more time and resources, you could:

Search model configurations at a finer resolution around a configuration known to already perform well.

Search more model hyperparameter configurations.

Use analysis to set better bounds on model hyperparameters to be searched.

Use domain knowledge to better prepare data or engineer input features.

Explore different potentially more complex methods.

Explore ensembles of well performing base models.

I also encourage you to include data preparation schemes as hyperparameters for model runs.

Some methods will perform some basic data preparation, such as differencing in ARIMA, nevertheless, it is often unclear exactly what data preparation schemes or combinations of schemes are required to best present a dataset to a modeling algorithm. Rather than guess, grid search and decide based on real results.

Some data preparation schemes to consider include:

Differencing to remove a trend.

Seasonal differencing to remove seasonality.

Standardize to center.

Normalize to rescale.

Power Transform to make normal.

So much searching can be slow.

Some ideas to speed up the evaluation of models include:

Use multiple machines in parallel via cloud hardware (such as Amazon EC2).

Reduce the size of the train or test dataset to make the evaluation process faster.

Use a more coarse grid of hyperparameters and circle back if you have time later.

Perhaps do not refit a model for each step in walk-forward validation.

4. Finalize Model

At the end of the previous time step, you know whether your time series is predictable.

If it is predictable, you will have a list of the top 5 to 10 candidate models that are skillful on the problem.

You can pick one or multiple models and finalize them. This involves training a new final model on all available historical data (train and test).

The model is ready for use; for example:

Make a prediction for the future.

Save the model to file for later use in making predictions.

Incorporate the model into software for making predictions.

If you have time, you can always circle back to the previous step and see if you can further improve upon the final model.

This may be required periodically if the data changes significantly over time.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this post, you discovered a simple four-step process that you can use to quickly discover a skilful predictive model for your time series forecasting problem.

Did you find this process useful?

Let me know below.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
118;news.mit.edu;http://news.mit.edu/2020/3-questions-kenda-mutongi-africa-women-power-human-decency-0114;2020-03-19;3 Questions: Professor Kenda Mutongi on Africa, women, power — and human decency;"MIT Professor Kenda Mutongi teaches courses in African history, world history, and gender history, and serves on the MIT Africa Working Group. She is the author of two award-winning books: “Matatu: A History of Popular Transportation in Nairobi” (University of Chicago Press, 2017) and “Worries of the Heart: Widows, Family, and Community in Kenya” (University of Chicago Press, 2007). The latter book explores how widows, a marginalized group in Kenya, weathered the country’s transition to a post-colonial society and found novel ways to address their collective social, economic, and political problems.

Mutongi, born and raised in rural Kenya, recently spoke with SHASS Communications on the interplay of African history and current gender issues within the United States; the importance of multidisciplinary thinking in solving the world's concerns; and the wider implications of her research.

Q: In “Worries of the Heart” you examine how Kenyan widows have navigated the country’s changing power structure. What lessons can be drawn from this history, two years into the #MeToo movement?

A: In that book, I argue that the patriarchal structure of society in rural western Kenya established the expectation that women would be dependent on men. Widows were generally considered weak and helpless, and yet they found ways to get their needs met.

To ensure that men supported them, widows presented their grievances to the men in public so that they could be witnessed by the whole community. By airing their grievances publicly, widows were able to put the men in a vulnerable position. Men who refused to help widows were emasculated in the eyes of others, because strong, respectable men were expected to take good care of women — especially widows.

In essence, the widows turned the very gender norms that were designed to oppress them to their own advantage. While the Kenyan widows did not overturn the patriarchal structure, they managed to refashion it to their advantage and garner the resources they needed to raise their children.

Today, I see something similar taking place in the #MeToo movement, which encourages women who have survived sexual assault and harassment to go public and expose those who have assaulted them. The assumption is that this going public will empower the women, but also send a strong message to the men about the serious consequences of their behavior. By going public, the #MeToo women are doing exactly what widows in Kenya did: They are — at least partly — trying to modify men’s behavior by shaming them, and in the process they are empowering themselves.

Reflecting on this history is important because it underscores a central truth of power dynamics: There are many ways in which populations that are not in power can make gains, even when existing societal structures are heavily stacked against them.



Q: Your latest book, “Matatu: A History of Popular Transportation in Nairobi,” provides a window into African entrepreneurship that serves as a counterpoint to the idea that African businesses need help from outsiders. What other myths do you think most need to be dispelled about Africa’s countries, cultures, and peoples?

A: One myth is that Africans are not capable of mechanical or scientific innovation. Africans are just as innovative as anybody; they just lack the capital to turn their discoveries into something bigger.

This is similar to the idea that Africans lack the entrepreneurial spirit, which can be debunked by examples such as that of Nairobi’s matatu industry. Matatus are minibuses used as shared private taxis. The success of the matatu drivers reflects wholly African entrepreneurship: The industry is thriving without any governmental or outside development assistance.

Unfortunately, Africans are often underestimated. In terms of scientific innovation, I think of my aunt, who was an herbalist when I was growing up in rural western Kenya. She searched the fields and found many herbs that could cure family members of stomach aches, flu, skin rashes, coughs, and colds. Rich drug companies such as Pfizer and Unilever use some of these same herbs — yellow dye root, rosy periwinkle, and Asiatic pennywort — in their pharmaceuticals.

My aunt certainly knew the medical significance of the herbs. What Africans like my aunt too often lack are the resources to turn their specialized knowledge into what is accepted as science in the global context. This is an important topic and one that has thoroughly been addressed in the work of my colleagues [MIT Associate Professor] Clapperton Chakanetsa Mavhunga and [MIT Program in Science, Technology and Science graduate student] Jia-Hui Lee.

We need to start taking all forms of African knowledge seriously, because until we do, we will continue to lose out on the contributions of a large swath of humanity.

Q: MIT President L. Rafael Reif has said that solving the great challenges of our time will require multidisciplinary problem-solving and “bilingual thinkers” — approaches that bring together insights and expertise from the sci/tech and humanistic fields. Can you share some ways you think knowledge from your field is important to such global problem-solving?

A: Lately I have been trying to think of African history from the perspective of goodness and basic human decency. Many of us in the humanistic disciplines have become so dedicated to examining conflict that we have reached a point where every action — arguably, even positive action — is treated as if it simply magnifies the webs of power in which we live.

And yet, I suspect that we cannot expect to change our societies for the better when no good deed is left uncriticized. Of course, conflicts exist, and do a great deal of damage in our lives, and we must confront them — but we must also allow ourselves to appreciate basic goodness and kindness when we see it.

In my own work I plan to examine more how people are working together and helping each other build stronger communities rather than focusing only on corruption and civil wars. Goodness is just as complex as evil, and yet, we tend to privilege analyses of evil. As the late Toni Morrison has noted, “Evil has a blockbuster audience; Goodness lurks backstage. Evil has vivid speech; Goodness bites its tongue.”

Story prepared by MIT SHASS Communications

Editorial team: Emily Hiestand and Kathryn O’Neill"
119;machinelearningmastery.com;https://machinelearningmastery.com/techniques-to-understand-machine-learning-algorithms-without-the-background-in-mathematics/;2015-08-23;5 Ways To Understand Machine Learning Algorithms (without math);"Tweet Share Share

Last Updated on August 12, 2019

Where does theory fit into a top-down approach to studying machine learning?

In the traditional approach to teaching machine learning, theory comes first requiring an extensive background in mathematics to be able to understand it. In my approach to teaching machine learning, I start with teaching you how to work problems end-to-end and deliver results.

So where does the theory fit?

In this post you will discover what we really mean when we talk about “theory” in machine learning. Hint: it’s all about the algorithms.

You will discover that once you get skilled at working through problems and delivering results, you will develop a compulsion to dive deeper in order to better understanding and results. Nobody will be able to hold you back.

Finally, you will discover 5 techniques that you can use when you are practicing machine learning on standard datasets to incrementally build up your understanding of machine learning algorithms.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Learn Theory Last, Not First

The way machine learning is taught to developers is crap.

It is taught bottom-up. This is crap if you are a developer who is primarily interested in using machine learning as a tool to solve problems rather than being a researcher in the field.

The traditional approach requires that you learn all of the prerequisite mathematics like linear algebra, probability and statistics before learning the theory of algorithms. You’re lucky if you ever go near a working implementation of an algorithm or discuss how to work a problem end-to-end and deliver a working, reliable and accurate predictive model.

I teach a top-down approach to learning machine learning. In this approach we start with 1) learning a systematic process for working through problems end-to-end, 2) map the process onto “best of breed” machine learning tools and platforms then 3) complete targeted practice on test datasets.

You can learn more about my approach to teaching top-down machine learning in the post “Machine Learning for Programmers: Leap from developer to machine learning practitioner“.

So where does theory fit into this process?

If the model is flipped, then theory is taught later. But what theory are we talking about and how exactly do you learn that theory when you are practicing on test datasets?

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

The Theory is Really All About Algorithms

The field of machine learning is theory-dense.

It’s dense because there is a tradition to describe and explain concepts mathematically.

This is useful because mathematical descriptions can be very concise, cutting down on the ambiguity. They also lend themselves to analysis by leveraging the techniques from the context in which they are described (e.g. a probabilistic understanding of a process).

A lot of these tangential mathematical techniques are often bundled in with the description of machine learning algorithms. For someone who just wants to build a superficial understanding of a method to be able to configure and apply it, this feels overwhelming. Frustratingly so.

It is frustrating if you do not have the grounding to be able to parse and understand the description of an algorithm. It’s frustrating because coming from a field like computer science, algorithms are described all the time, but the difference is the descriptions are intended for fast comprehension (e.g. for desk checking) and implementation.

We know that for example when learning what a hash table is and how to use it, that we almost never need to know the specifics of the hashing function in our day-to-day. But we also know what a hashing function is and where to go to learn more about hashing function specifics and how to write your own. Why can’t machine learning work like that?

The bulk of the “theory” one encounters in machine learning is related to machine learning algorithms. If you ask any beginner about why they are frustrated with the theory, you will learn that it is in relation to learning how to understand or use a specific machine learning algorithm.

Here, algorithms is more broad than a process for creating a predictive model. It also refers to algorithms for selecting features, engineering new features, transforming data and estimating the accuracy of a model on unseen data (e.g. cross validation).

So, learning theory last, really means learning about machine learning algorithms.

A Compulsion To Dive Into Theory

I generally advise targeted practice on well known machine learning datasets.

This is because well known machine learning dataset, like those on the UCI Machine Learning Repository are easy to work with. They are small so they fit into memory and can be processed on your workstation. They are also well studied and understood so you have a baseline for comparison.

You can learn more about targeted practice of machine learning datasets in the post “Practice Machine Learning with Small In-Memory Datasets from the UCI Machine Learning Repository“.

Understanding machine learning algorithms fits into this process. The reason is in the pursuit of getting results on standard machine learning algorithms you are going to run into limitations. You are going to want to know how to get more out of a given algorithm or to know more about how to best configure it, or how it actually works.

This need to know more and curiosity will drive you into studying the theory of machine learning algorithms. You will be compelled to piece together an understand of the algorithms in order to achieve better results.

We see this same effect in young developers from varied backgrounds that end up eventually studying the code of open source projects, textbooks and even research papers in order to hone their craft. The need to being a better more capable programmer drives them to it.

If you are curious and motivated to succeed, you cannot resist studying the theory.

5 Techniques To Understand Machine Learning Algorithms

The time will come to dive into machine learning algorithms as part of your targeted practice

When that time comes, there are a number of techniques and template that you can use to short cut the process.

In this section you will discover 5 techniques that you can use to understand the theory of machine learning algorithms, fast.

1) Create Lists of Machine Learning Algorithms

When you are just starting out you may feel overwhelmed by the larger number of algorithms available.

Even when spot testing algorithms, you may be unsure of which algorithms to include in your mix (hint, be diverse).

An excellent trick you can use when starting out is to keep track of the algorithms you read about. These lists can be as simple as the name of the algorithm, and can increase in complexity as you interest and curiosity build.

Capture details like the problem type to which they are suited (classification or regression), related algorithms, and taxonomic class (decision tree, kernel, etc.). When you see the name of an algorithm that is new to you, add it to your list. When you start a new problem, try some algorithms you have never used before. Mark a check next to algorithms you have used before. And so on.

Controlling the names of algorithms in lists gives you power. This ridiculously simple tactic can help you get on top of the overwhelm. Examples of where your simple algorithm lists can save you a lot of time and frustration are:

Ideas of algorithms to try on new and different problem types (time series, rating systems, etc.)

Algorithms that you can investigate to learn more about how to apply.

Get a handle on algorithm types by category (trees, kernels, etc.).

Avoid the problem of fixating on a favorite algorithm.

Start by creating lists of algorithms, open a spreadsheet and get started.

See the post “Take Control By Creating Targeted Lists of Machine Learning Algorithms” for more information on this tactic.

2) Research Machine Learning Algorithms

When you want to know more about a machine learning algorithm you need to research it.

The main reasons you will be interested to research an algorithm is to learn how to configure it and to learn how it works.

Research is not just for academics. A few simple tips can take you a long way in gathering information on a given machine learning algorithm.

The key is diversity of information sources. The following is a short list of the types of sources you can consult for information on an algorithm you are researching.

Authoritative sources like textbooks, lecture notes, slide and overview papers. Seminal sources like the papers and articles in which the algorithm was first described. Leading-edge sources that describe state-of-the-art extensions and experiments on the algorithm. Heuristic sources like those that come out of machine learning competitions, posts on Q&A websites and conference papers. Implementation sources such as open source code for tools and libraries, blog posts and technical reports.

You do not need to be a PhD researcher nor a machine learning algorithm expert.

Take your time and pick over many sources collecting facts on a machine learning algorithm you are trying to figure out. Focus on the practical details you can apply or understand and leave the rest.

For more information on researching machine learning algorithms see the post “How to Research a Machine Learning Algorithm“.

3) Create Your Own Algorithm Descriptions

Machine learning algorithm descriptions you will discover in your research will be incomplete and inconsistent.

An approach that you can use is to put together your own mini algorithm descriptions. This is another very simple and very powerful tactic.

You can design a standard algorithm description template with only those details that are useful to you in getting the most from algorithms, like algorithm usage heuristics, pseudo-code listings, parameter ranges and resource lists.

You can then use the same algorithm description template across a number of key algorithms and start to build up your own little algorithm encyclopedia that you can refer to on future projects.

Some questions you might like to use in your own algorithm description template include:

What are the standard abbreviations used for the algorithm?

What is the objective or goal for the algorithm?

What is the pseudo-code or flowchart description of the algorithm?

What are the heuristics or rules of thumb for using the algorithm?

What are useful resources for learning more about the algorithm?

You will be surprised at how useful and practical these descriptions can be. For example, I used this approach to write a book of nature-inspired algorithm descriptions that I still refer back to years later.

For more on how to create effective algorithm description templates, see the post “How to Learn a Machine Learning Algorithm“.

For more information on my book of algorithms described using a standard algorithm description template, see “Clever Algorithms: Nature-Inspired Programming Recipes“.

4) Investigate Algorithm Behavior

Machine learning algorithms are complex systems that are sometimes best understood by their behaviors on actual datasets.

By designing small experiments on machine learning algorithms using small datasets you can learn a lot about how an algorithm works, it’s limitations and how to configure it in ways that may transfer to exceptional results on other problems.

A simple procedure that you can use to investigate a machine learning algorithm is as follows:

Select an algorithm that you would like to know more about (e.g. random forests). Identify a question about that algorithm you would like answered (e.g. the effect of the number of trees). Design an experiment to find an answer to that question (e.g. try different numbers of trees on a few binary classification problems and chart the relationship with classification accuracy). Execute the experiment and write-up your results so that you can make use of them in the future. Repeat the process.

This is one of the truly exciting aspects of applied machine learning, that through your own simple investigations you can achieve surprising and state of the art results.

For more information on how to study algorithms from their behavior, see the post “How To Investigate Machine Learning Algorithm Behavior“.

5) Implement Machine Learning Algorithms

You cannot get more intimate with a machine learning algorithm than by implementing it.

In implementing a machine learning algorithm from scratch you will be confronted with the myriad of micro-decisions that go into a given implementation. You may decide to cover some up with rules of thumb of expose them all as parameters to the user.

Below is a repeatable process that you can use to implement machine learning algorithms from scratch.

Select a programming language, one that you are most familiar with is probably best. Select an algorithm to implement, start with something easy (see below for a list). Select a problem to test your implementation on as you develop, 2D data is good for visualizing (even in Excel). Research the algorithm and leverage many and diverse sources of information (e.g. read tutorials, papers, other implementations, and so on). Unit test the algorithm to confirm your understanding and validate the implementation.

Start small and build confidence.

For example 3 algorithms that you select as your first machine learning algorithm implementation from scratch are:

For more information on how to implement machine learning algorithms, see the post “How to Implement a Machine Learning Algorithm“.

Also see the posts:

Theory is Not Just For the Mathematicians

Machine learning is not just for the mathematical elite. You can learn how machine learning algorithms work and how to get the most from them without diving deep into multivariate statistics.

You do not need to be good at math.

As we saw in the techniques section, you can start with algorithm lists and transition deeper into algorithm research, descriptions and algorithm behavior.

You can go very far with these methods without diving much at all into the math.

You do not need to be an academic researcher.

Research is not just for academics. Anyone can read books and papers and compile their own understanding of a topic like a specific machine learning algorithm.

Your biggest breakthroughs will come when you take on the persona of “the scientist” and start experimenting on machine learning algorithms as though they were complex systems in need of study. You will discover all kinds of interesting quirks in behavior that may not even be documented.

Take Action

Pick one of the techniques listed above and get started.

I mean today, now.

Unsure where to start?

Here’s 5 great ideas of where you could start:

Make a list of 10 machine algorithms for classification (take a look at my tour of algorithms to get some ideas). Find five books that give detailed descriptions of Random Forests. Create a five-slide presentation on Naive Bayes using your own algorithm description template. Open Weka and see how the “k” parameter affects accuracy of k-nearest neighbor on the iris flowers data set. Implement linear regression using stochastic gradient descent.

Did you take action? Enjoy this post? Leave a comment below.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
120;machinelearningmastery.com;http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/;2016-07-20;Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras;"# Stacked LSTM for international airline passengers problem with memory

import numpy

import matplotlib . pyplot as plt

from pandas import read_csv

import math

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from sklearn . preprocessing import MinMaxScaler

from sklearn . metrics import mean_squared_error

# convert an array of values into a dataset matrix

def create_dataset ( dataset , look_back = 1 ) :

dataX , dataY = [ ] , [ ]

for i in range ( len ( dataset ) - look_back - 1 ) :

a = dataset [ i : ( i + look_back ) , 0 ]

dataX . append ( a )

dataY . append ( dataset [ i + look_back , 0 ] )

return numpy . array ( dataX ) , numpy . array ( dataY )

# fix random seed for reproducibility

numpy . random . seed ( 7 )

# load the dataset

dataframe = read_csv ( 'airline-passengers.csv' , usecols = [ 1 ] , engine = 'python' )

dataset = dataframe . values

dataset = dataset . astype ( 'float32' )

# normalize the dataset

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

dataset = scaler . fit_transform ( dataset )

# split into train and test sets

train_size = int ( len ( dataset ) * 0.67 )

test_size = len ( dataset ) - train_size

train , test = dataset [ 0 : train_size , : ] , dataset [ train_size : len ( dataset ) , : ]

# reshape into X=t and Y=t+1

look_back = 3

trainX , trainY = create_dataset ( train , look_back )

testX , testY = create_dataset ( test , look_back )

# reshape input to be [samples, time steps, features]

trainX = numpy . reshape ( trainX , ( trainX . shape [ 0 ] , trainX . shape [ 1 ] , 1 ) )

testX = numpy . reshape ( testX , ( testX . shape [ 0 ] , testX . shape [ 1 ] , 1 ) )

# create and fit the LSTM network

batch_size = 1

model = Sequential ( )

model . add ( LSTM ( 4 , batch_input_shape = ( batch_size , look_back , 1 ) , stateful = True , return_sequences = True ) )

model . add ( LSTM ( 4 , batch_input_shape = ( batch_size , look_back , 1 ) , stateful = True ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

for i in range ( 100 ) :

model . fit ( trainX , trainY , epochs = 1 , batch_size = batch_size , verbose = 2 , shuffle = False )

model . reset_states ( )

# make predictions

trainPredict = model . predict ( trainX , batch_size = batch_size )

model . reset_states ( )

testPredict = model . predict ( testX , batch_size = batch_size )

# invert predictions

trainPredict = scaler . inverse_transform ( trainPredict )

trainY = scaler . inverse_transform ( [ trainY ] )

testPredict = scaler . inverse_transform ( testPredict )

testY = scaler . inverse_transform ( [ testY ] )

# calculate root mean squared error

trainScore = math . sqrt ( mean_squared_error ( trainY [ 0 ] , trainPredict [ : , 0 ] ) )

print ( 'Train Score: %.2f RMSE' % ( trainScore ) )

testScore = math . sqrt ( mean_squared_error ( testY [ 0 ] , testPredict [ : , 0 ] ) )

print ( 'Test Score: %.2f RMSE' % ( testScore ) )

# shift train predictions for plotting

trainPredictPlot = numpy . empty_like ( dataset )

trainPredictPlot [ : , : ] = numpy . nan

trainPredictPlot [ look_back : len ( trainPredict ) + look_back , : ] = trainPredict

# shift test predictions for plotting

testPredictPlot = numpy . empty_like ( dataset )

testPredictPlot [ : , : ] = numpy . nan

testPredictPlot [ len ( trainPredict ) + ( look_back* 2 ) + 1 : len ( dataset ) - 1 , : ] = testPredict

# plot baseline and predictions

plt . plot ( scaler . inverse_transform ( dataset ) )

plt . plot ( trainPredictPlot )

plt . plot ( testPredictPlot )"
121;machinelearningmastery.com;http://machinelearningmastery.com/what-is-the-weka-machine-learning-workbench/;2014-02-04;What is the Weka Machine Learning Workbench;"Tweet Share Share

Last Updated on August 22, 2019

Machine learning is an iterative process rather than a linear process that requires each step to be revisited as more is learned about the problem under investigation. This iterative process can require using many different tools, programs and scripts for each process.

A machine learning workbench is a platform or environment that supports and facilitates a range of machine learning activities reducing or removing the need for multiple tools.

Some statistical and machine learning work benches like R provide very advanced tools but require a lot of manual configuration in the form of scripts and programming. The tools can also be fragile, written by and for academics rather than written to be robust and used in production environments.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

What is Weka

The Weka machine learning workbench is a modern platform for applied machine learning. Weka is an acronym which stands for Waikato Environment for Knowledge Analysis. It is also the name of a New Zealand bird the Weka.

Five features of Weka that I like to promote are:

Open Source : It is released as open source software under the GNU GPL. It is dual licensed and Pentaho Corporation owns the exclusive license to use the platform for business intelligence in their own product.

: It is released as open source software under the GNU GPL. It is dual licensed and Pentaho Corporation owns the exclusive license to use the platform for business intelligence in their own product. Graphical Interface : It has a Graphical User Interface (GUI). This allows you to complete your machine learning projects without programming.

: It has a Graphical User Interface (GUI). This allows you to complete your machine learning projects without programming. Command Line Interface : All features of the software can used from the command line. This can be very useful for scripting large jobs.

: All features of the software can used from the command line. This can be very useful for scripting large jobs. Java API : It is written in Java and provides a API that is well documented and promotes integration into your own applications. Note that the GNU GPL means that in turn your software would also have to be released as GPL.

: It is written in Java and provides a API that is well documented and promotes integration into your own applications. Note that the GNU GPL means that in turn your software would also have to be released as GPL. Documentation: There books, manuals, wikis and MOOC courses that can train you how to use the platform effectively.

The main reason I promote Weka is because a beginner can go through the process of applied machine learning using the graphical interface without having to do any programming. This is a big deal because getting a handle on the process, handling data and experimenting with algorithms is what a beginner should be learning about, not learning yet another scripting language.

Introduction to the Weka GUI

Now I want to show of the graphical user interface a bit and encourage you to download and have a play with Weka. The workbench provides three main ways to work on your problem: The Explorer for playing around and trying things out, the Experimenter for controlled experiments, and the KnowledgeFlow for graphically designing a pipeline for your problem.

Weka Explorer

The explorer is where you play around with your data and think about what transforms to apply to your data, what algorithms you want to run in experiments.

The Explorer interface is divided into 5 different tabs:

Preprocess : Load a dataset and manipulate the data into a form that you want to work with.

: Load a dataset and manipulate the data into a form that you want to work with. Classify : Select and run classification and regression algorithms to operate on your data.

: Select and run classification and regression algorithms to operate on your data. Cluster : Select and run clustering algorithms on your dataset.

: Select and run clustering algorithms on your dataset. Associate : Run association algorithms to extract insights from your data.

: Run association algorithms to extract insights from your data. Select Attributes : Run attribute selection algorithms on your data to select those attributes that are relevant to the feature you want to predict.

: Run attribute selection algorithms on your data to select those attributes that are relevant to the feature you want to predict. Visualize: Visualize the relationship between attributes.

Weka Experimenter

This interface is for designing experiments with your selection of algorithms and datasets, running experiments and analyzing the results.

The tools for analyzing results are very powerful, allowing you to consider and compare results that are statistically significant over multiple runs.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Knowledge Flow

Applied machine learning is a process and the Knowledge Flow interface allows you to graphically design that process and run the designs that you create. This includes the loading and transforming of input data, running of algorithms and the presentation of results.

It’s a powerful interface and metaphor for solving complex problems graphically.

Tips for Getting Started

Here are some tips for getting up and running fast:

Download Weka Right Now

It supports the three main platforms: Windows, OS X and Linux. Find the distribution for your platform, download it, install it and start it up. You might have to install Java first. The installation includes many standard experimental datasets (in the data directory) that you can load and practice on.

Read the Weka Documentation

The download includes a PDF manual (WekaManual.pdf) that can get you up to speed very quickly. It is very details and comprehensive with screenshots. There is plenty of supplemetry documentation online, check out:

Don’t forget the book. If you get into Weka, then buy the book. It provides an introduction to applied machine learning as well as an introduction to the Weka platform itself. Highly recommended.

Extensions and Plugins for Weka

There are a lot of plugin algorithm, extends and even platforms that build on Weka:

Online Courses on Weka

There are two online courses that teach data mining with Weka:

Rushdi Shams has an amazing Channel of YouTube videos showing you how to do lots of specific tasks in Weka. Check out his Weka YouTube channel here.

Have you used Weka? Leave a comment and share your experiences.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
122;news.mit.edu;http://news.mit.edu/2020/bluetooth-covid-19-contact-tracing-0409;;Bluetooth signals from your smartphone could automate Covid-19 contact tracing while preserving privacy;"Imagine you’ve been diagnosed as Covid-19 positive. Health officials begin contact tracing to contain infections, asking you to identify people with whom you’ve been in close contact. The obvious people come to mind — your family, your coworkers. But what about the woman ahead of you in line last week at the pharmacy, or the man bagging your groceries? Or any of the other strangers you may have come close to in the past 14 days?

A team led by MIT researchers and including experts from many institutions is developing a system that augments “manual” contact tracing by public health officials, while preserving the privacy of all individuals. The system relies on short-range Bluetooth signals emitted from people’s smartphones. These signals represent random strings of numbers, likened to “chirps” that other nearby smartphones can remember hearing.

If a person tests positive, they can upload the list of chirps their phone has put out in the past 14 days to a database. Other people can then scan the database to see if any of those chirps match the ones picked up by their phones. If there’s a match, a notification will inform that person that they may have been exposed to the virus, and will include information from public health authorities on next steps to take. Vitally, this entire process is done while maintaining the privacy of those who are Covid-19 positive and those wishing to check if they have been in contact with an infected person.

“I keep track of what I’ve broadcasted, and you keep track of what you’ve heard, and this will allow us to tell if someone was in close proximity to an infected person,” says Ron Rivest, MIT Institute Professor and principal investigator of the project. “But for these broadcasts, we’re using cryptographic techniques to generate random, rotating numbers that are not just anonymous, but pseudonymous, constantly changing their ‘ID,’ and that can’t be traced back to an individual.”

This approach to private, automated contact tracing will be available in a number of ways, including through the privacy-first effort launched at MIT in response to Covid-19 called SafePaths. This broad set of mobile apps is under development by a team led by Ramesh Raskar of the MIT Media Lab. The design of the new Bluetooth-based system has benefited from SafePaths’ early work in this area.

Bluetooth exchanges

Smartphones already have the ability to advertise their presence to other devices via Bluetooth. Apple’s “Find My” feature, for example, uses chirps from a lost iPhone or MacBook to catch the attention of other Apple devices, helping the owner of the lost device to eventually find it.

“Find My inspired this system. If my phone is lost, it can start broadcasting a Bluetooth signal that’s just a random number; it’s like being in the middle of the ocean and waving a light. If someone walks by with Bluetooth enabled, their phone doesn’t know anything about me; it will just tell Apple, ‘Hey, I saw this light,’” says Marc Zissman, the associate head of MIT Lincoln Laboratory’s Cyber Security and Information Science Division and co-principal investigator of the project.

With their system, the team is essentially asking a phone to send out this kind of random signal all the time and to keep a log of these signals. At the same time, the phone detects chirps it has picked up from other phones, and only logs chirps that would be medically significant for contact tracing — those emitted from within an approximate 6-foot radius and picked up for a certain duration of time, say 10 minutes.

Phone owners would get involved by downloading an app that enables this system. After a positive diagnosis, a person would receive a QR code from a health official. By scanning the code through that app, that person can upload their log to the cloud. Anyone with the app could then initiate their phones to scan these logs. A notification, if there’s a match, could tell a user how long they were near an infected person and the approximate distance.

Privacy-preserving technology

Some countries most successful at containing the spread of Covid-19 have been using smartphone-based approaches to conduct contact tracing, yet the researchers note these approaches have not always protected individual’s privacy. South Korea, for example, has implemented apps that notify officials if a diagnosed person has left their home, and can tap into people’s GPS data to pinpoint exactly where they’ve been.

“We’re not tracking location, not using GPS, not attaching your personal ID or phone number to any of these random numbers your phone is emitting,” says Daniel Weitzner, a principal research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-principal investigator of this effort. “What we want is to enable everyone to participate in a shared process of seeing if you might have been in contact, without revealing, or forcing anyone to reveal, anything.”

Choice is key. Weitzner sees the system as a virtual knock on the door that preserves people’s right to not answer it. The hope, though, is that everyone who can opt in would do so to help contain the spread of Covid-19. “We need a large percentage of the population to opt in for this system to really work. We care about every single Bluetooth device out there; it’s really critical to make this a whole ecosystem,” he says.

Public health impact

Throughout the development process, the researchers have worked closely with a medical advisory team to ensure that this system would contribute effectively to contact tracing efforts. This team is led by Louise Ivers, who is an infectious disease expert, associate professor at Harvard Medical School, and executive director of the Massachusetts General Hospital Center for Global Health.

“In order for the U.S. to really contain this epidemic, we need to have a much more proactive approach that allows us to trace more widely contacts for confirmed cases. This automated and privacy-protecting approach could really transform our ability to get the epidemic under control here and could be adapted to have use in other global settings,” Ivers says. “What’s also great is that the technology can be flexible to how public health officials want to manage contacts with exposed cases in their specific region, which may change over time.”

For example, the system could notify someone that they should self-isolate, or it could request that they check in through the app to connect with specialists regarding daily symptoms and well-being. In other circumstances, public health officials could request that this person get tested if they were noticing a cluster of cases.

The ability to conduct contact tracing quickly and at a large scale can be effective not only in flattening the curve of the outbreak, but also for enabling people to safely enter public life once a community is on the downward side of the curve. “We want to be able to let people carefully get back to normal life while also having this ability to carefully quarantine and identify certain vectors of an outbreak,” Rivest says.

Toward implementation

Lincoln Laboratory engineers have led the prototyping of the system. One of the hardest technical challenges has been achieving interoperability, that is, making it possible for a chirp from an iPhone to be picked up by an Android device and vice versa. A test at the laboratory late last week proved that they achieved this capability, and that chirps could be picked up by other phones of various makes and models.

A vital next step toward implementation is engaging with the smartphone manufacturers and software developers — Apple, Google, and Microsoft. “They have a critical role here. The aim of the prototype is to prove to these developers that this is feasible for them to implement,” Rivest says. As those collaborations are forming, the team is also demonstrating its prototype system to state and federal government agencies.

Rivest emphasizes that collaboration has made this project possible. These collaborators include the Massachusetts General Hospital Center for Global Health, CSAIL, MIT Lincoln Laboratory, Boston University, Brown University, MIT Media Lab, The Weizmann Institute of Science, and SRI International.

The team also aims to play a central, coordinating role with other efforts around the country and in Europe to develop similar, privacy-preserving contact-tracing systems.

“This project is being done in true academic style. It’s not a contest; it’s a collective effort on the part of many, many people to get a system working,” Rivest says."
123;machinelearningmastery.com;https://machinelearningmastery.com/a-gentle-introduction-to-the-challenge-of-training-deep-learning-neural-network-models/;2019-02-14;A Gentle Introduction to the Challenge of Training Deep Learning Neural Network Models;"Tweet Share Share

Last Updated on August 6, 2019

Deep learning neural networks learn a mapping function from inputs to outputs.

This is achieved by updating the weights of the network in response to the errors the model makes on the training dataset. Updates are made to continually reduce this error until either a good enough model is found or the learning process gets stuck and stops.

The process of training neural networks is the most challenging part of using the technique in general and is by far the most time consuming, both in terms of effort required to configure the process and computational complexity required to execute the process.

In this post, you will discover the challenge of finding model parameters for deep learning neural networks.

After reading this post, you will know:

Neural networks learn a mapping function from inputs to outputs that can be summarized as solving the problem of function approximation.

Unlike other machine learning algorithms, the parameters of a neural network must be found by solving a non-convex optimization problem with many good solutions and many misleadingly good solutions.

The stochastic gradient descent algorithm is used to solve the optimization problem where model parameters are updated each iteration using the backpropagation algorithm.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

Neural Nets Learn a Mapping Function Learning Network Weights Is Hard Navigating the Error Surface Components of the Learning Algorithm

Neural Nets Learn a Mapping Function

Deep learning neural networks learn a mapping function.

Developing a model requires historical data from the domain that is used as training data. This data is comprised of observations or examples from the domain with input elements that describe the conditions and an output element that captures what the observation means.

For example, a problem where the output is a quantity would be described generally as a regression predictive modeling problem. Whereas a problem where the output is a label would be described generally as a classification predictive modeling problem.

A neural network model uses the examples to learn how to map specific sets of input variables to the output variable. It must do this in such a way that this mapping works well for the training dataset, but also works well on new examples not seen by the model during training. This ability to work well on specific examples and new examples is called the ability of the model to generalize.

A multilayer perceptron is just a mathematical function mapping some set of input values to output values.

— Page 5, Deep Learning, 2016.

We can describe the relationship between the input variables and the output variables as a complex mathematical function. For a given model problem, we must believe that a true mapping function exists to best map input variables to output variables and that a neural network model can do a reasonable job at approximating the true unknown underlying mapping function.

A feedforward network defines a mapping and learns the value of the parameters that result in the best function approximation.

— Page 168, Deep Learning, 2016.

As such, we can describe the broader problem that neural networks solve as “function approximation.” They learn to approximate an unknown underlying mapping function given a training dataset. They do this by learning weights and the model parameters, given a specific network structure that we design.

It is best to think of feedforward networks as function approximation machines that are designed to achieve statistical generalization, occasionally drawing some insights from what we know about the brain, rather than as models of brain function.

— Page 169, Deep Learning, 2016.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Learning Network Weights Is Hard

Finding the parameters for neural networks in general is hard.

For many simpler machine learning algorithms, we can calculate an optimal model given the training dataset.

For example, we can use linear algebra to calculate the specific coefficients of a linear regression model and a training dataset that best minimizes the squared error.

Similarly, we can use optimization algorithms that offer convergence guarantees when finding an optimal set of model parameters for nonlinear algorithms such as logistic regression or support vector machines.

Finding parameters for many machine learning algorithms involves solving a convex optimization problem: that is an error surface that is shaped like a bowl with a single best solution.

This is not the case for deep learning neural networks.

We can neither directly compute the optimal set of weights for a model, nor can we get global convergence guarantees to find an optimal set of weights.

Stochastic gradient descent applied to non-convex loss functions has no […] convergence guarantee, and is sensitive to the values of the initial parameters.

— Page 177, Deep Learning, 2016.

In fact, training a neural network is the most challenging part of using the technique.

It is quite common to invest days to months of time on hundreds of machines in order to solve even a single instance of the neural network training problem.

— Page 274, Deep Learning, 2016.

The use of nonlinear activation functions in the neural network means that the optimization problem that we must solve in order to find model parameters is not convex.

It is not a simple bowl shape with a single best set of weights that we are guaranteed to find. Instead, there is a landscape of peaks and valleys with many good and many misleadingly good sets of parameters that we may discover.

Solving this optimization is challenging, not least because the error surface contains many local optima, flat spots, and cliffs.

An iterative process must be used to navigate the non-convex error surface of the model. A naive algorithm that navigates the error is likely to become misled, lost, and ultimately stuck, resulting in a poorly performing model.

Navigating the Non-Convex Error Surface

Neural network models can be thought to learn by navigating a non-convex error surface.

A model with a specific set of weights can be evaluated on the training dataset and the average error over all training datasets can be thought of as the error of the model. A change to the model weights will result in a change to the model error. Therefore, we seek a set of weights that result in a model with a small error.

This involves repeating the steps of evaluating the model and updating the model parameters in order to step down the error surface. This process is repeated until a set of parameters is found that is good enough or the search process gets stuck.

This is a search or an optimization process and we refer to optimization algorithms that operate in this way as gradient optimization algorithms, as they naively follow along the error gradient. They are computationally expensive, slow, and their empirical behavior means that using them in practice is more art than science.

The algorithm that is most commonly used to navigate the error surface is called stochastic gradient descent, or SGD for short.

Nearly all of deep learning is powered by one very important algorithm: stochastic gradient descent or SGD.

— Page 151, Deep Learning, 2016.

Other global optimization algorithms designed for non-convex optimization problems could be used, such as a genetic algorithm, but stochastic gradient descent is more efficient as it uses the gradient information specifically to update the model weights via an algorithm called backpropagation.

[Backpropagation] describes a method to calculate the derivatives of the network training error with respect to the weights by a clever application of the derivative chain-rule.

— Page 49, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

Backpropagation refers to a technique from calculus to calculate the derivative (e.g. the slope or the gradient) of the model error for specific model parameters, allowing model weights to be updated to move down the gradient. As such, the algorithm used to train neural networks is also often referred to as simply backpropagation.

Actually, back-propagation refers only to the method for computing the gradient, while another algorithm, such as stochastic gradient descent, is used to perform learning using this gradient.

— Page 204, Deep Learning, 2016.

Stochastic gradient descent can be used to find the parameters for other machine learning algorithms, such as linear regression, and it is used when working with very large datasets, although if there are sufficient resources, then convex-based optimization algorithms are significantly more efficient.

Components of the Learning Algorithm

Training a deep learning neural network model using stochastic gradient descent with backpropagation involves choosing a number of components and hyperparameters. In this section, we’ll take a look at each in turn.

An error function must be chosen, often called the objective function, cost function, or the loss function. Typically, a specific probabilistic framework for inference is chosen called Maximum Likelihood. Under this framework, the commonly chosen loss functions are cross entropy for classification problems and mean squared error for regression problems.

Loss Function. The function used to estimate the performance of a model with a specific set of weights on examples from the training dataset.

The search or optimization process requires a starting point from which to begin model updates. The starting point is defined by the initial model parameters or weights. Because the error surface is non-convex, the optimization algorithm is sensitive to the initial starting point. As such, small random values are chosen as the initial model weights, although different techniques can be used to select the scale and distribution of these values. These techniques are referred to as “weight initialization” methods.

Weight Initialization. The procedure by which the initial small random values are assigned to model weights at the beginning of the training process.

When updating the model, a number of examples from the training dataset must be used to calculate the model error, often referred to simply as “loss.” All examples in the training dataset may be used, which may be appropriate for smaller datasets. Alternately, a single example may be used which may be appropriate for problems where examples are streamed or where the data changes often. A hybrid approach may be used where the number of examples from the training dataset may be chosen and used to used to estimate the error gradient. The choice of the number of examples is referred to as the batch size.

Batch Size. The number of examples used to estimate the error gradient before updating the model parameters.

Once an error gradient has been estimated, the derivative of the error can be calculated and used to update each parameter. There may be statistical noise in the training dataset and in the estimate of the error gradient. Also, the depth of the model (number of layers) and the fact that model parameters are updated separately means that it is hard to calculate exactly how much to change each model parameter to best move down the whole model down the error gradient.

Instead, a small portion of the update to the weights is performed each iteration. A hyperparameter called the “learning rate” controls how much to update model weights and, in turn, controls how fast a model learns on the training dataset.

Learning Rate: The amount that each model parameter is updated per cycle of the learning algorithm.

The training process must be repeated many times until a good or good enough set of model parameters is discovered. The total number of iterations of the process is bounded by the number of complete passes through the training dataset after which the training process is terminated. This is referred to as the number of training “epochs.”

Epochs. The number of complete passes through the training dataset before the training process is terminated.

There are many extensions to the learning algorithm, although these five hyperparameters generally control the learning algorithm for deep learning neural networks.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this post, you discovered the challenge of finding model parameters for deep learning neural networks.

Specifically, you learned:

Neural networks learn a mapping function from inputs to outputs that can be summarized as solving the problem of function approximation.

Unlike other machine learning algorithms, the parameters of a neural network must be found by solving a non-convex optimization problem with many good solutions and many misleadingly good solutions.

The stochastic gradient descent algorithm is used to solve the optimization problem where model parameters are updated each iteration using the backpropagation algorithm.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
124;news.mit.edu;http://news.mit.edu/2020/hacking-challenges-that-are-hard-talk-about-0311;;Hacking the challenges that are hard to talk about;"What happens when 300 people from many industries and walks of life come together to try to address diversity and inclusion challenges? Answer: inventive hacks that can be applied to real-world situations. Last month, Hack for Inclusion — a student-led hackathon to combat bias — partnered with corporations and organizations to pose 14 challenges directly related to problems those groups are currently facing.

“The reason that we have corporations proposing the challenges is to increase the likelihood that the solutions will get resourced and implemented,” says Elaine Harris '78, a director on the MIT Alumni Association Board and one of the roughly dozen alumni who offered support in the planning of the hackathon. “We’re really empowering people to take what they’ve learned here back to their organizations, whether they be corporations, nonprofits, or student groups, and to not be afraid to tackle things that are often challenging to talk about.”

The event, which has been operating in its current format since 2018, was directed by a group of more than 45 planning team members organized by Sloan for Inclusion, an MIT Sloan School of Management student group led by Clare Herceg, Komal Patel, Hannah Phillips, and Udi Rosenhand, and Hacking Discrimination, a group of MIT alumni led by Harris.

Past years’ challenges have addressed bias or lack of inclusion in the workplace, specifically relating to women and underrepresented minorities, those with various physical abilities, and members of the LGBTQ+ community. This year, during the event held Feb. 20–21 at the Microsoft New England Research and Development Center, the challenges spanned topics including changing stigmas around mental illness, gender equity in e-sports, and using technology to address homelessness.

Not only is the outcome of the event focused on finding inclusive solutions, the very nature of the event is structured to be inclusive, says Herceg. She observed many kinds of diversity represented in the turnout, with ages and life stages ranging from high school students to older professionals, as well as people of different races and gender identities. “A lot of people voiced at the end of the event that this is the first time they felt like they could bring their whole self to an event like this — felt like they could be accepted.”

The two-day event, which starts with brainstorming and research and ends in a final pitch, was facilitated by Chris Lloyd, Olivia Seow, and Aaron Stinnett, students from MIT’s Integrated Design and Management program, and employs a design thinking process. Integral to that process’s success is to focus both on the dynamics of the group and on the needs of end users, says Jainaba Seckan, project manager at Harvard University’s Office of Diversity, Inclusion, and Belonging, who led a 2020 challenge sponsored by her office after having participated in the 2019 hackathon.

“The facilitators make sure that you are taking care of your team culture by engaging in exercises where you are checking in and setting norms and setting expectations of each other. By doing that, we created a common language. Taking that time to foster inclusion on the teams really did allow for us to do our best work,” Seckan says.

The challenge Seckan’s office sponsored — Campus Culture: Responding to Traumatic Events — had particular resonance in this university setting. It elicited ideas including an app to organize and prioritize campus-wide concerns to make it easier for all university community members to respond, as well as a communication system that would allow people to send videos and share messages of support following a traumatic event.

“The primary takeaway for me from the two challenge teams was that connection and community are primary solutions to address the isolation, shock, and fear that typically arises following traumatic and triggering events,” says Seckan. “Ultimately, these solutions create opportunities for learning and healing.”

Hack for Inclusion 2020’s overall top prize went to a team called Silver Tsunami, which worked on a Harnessing Wisdom in the Future of Work challenge posed by Steel Partners. With the aim of retaining the knowledge of employees above age 60, the group created a detailed offboarding program. Incorporating mentorship opportunities and community-building activities, the program is designed to support company culture, develop younger workers, and provide a thoughtful and cost-effective way to transition older workers into retirement.

Water Cooler, the second-prize winner, addressed a Deloitte-sponsored challenge focused on workplaces of the future by pitching software that could facilitate informal online and in-person meetings among coworkers from fully remote organizations.

Looking back on the weekend, Herceg says she was inspired not only by all of the teams’ collective output in such a short time frame, but that many of the groups had continued the conversation in the days following the event. “It’s really incredible to see a group of strangers — because that’s what they are — come together and be passionate about a specific challenge, come up with a solution, and then want to carry it forward.”"
125;news.mit.edu;http://news.mit.edu/2018/julie-soriero-receives-ncaa-presidents-pat-summitt-award-0122;;Julie Soriero receives NCAA President's Pat Summitt Award;"On Jan. 18 at the NCAA Convention in Indianapolis, MIT Director of Athletics Julie Soriero was presented with the 2018 NCAA President’s Pat Summitt Award by NCAA President Mark Emmert.

Created in 2017, the award recognizes an individual in the association’s membership who has demonstrated devotion to development of student-athletes and has made a positive impact on their lives.

“I really want to thank President Emmert for recognizing the legacy of Pat Summitt and creating this award,” said Soriero. “I’m certainly humbled, I’m certainly proud and I’m certainly honored to be receiving it today.”

The first Division III administrator and the second overall to receive the honor, Soriero is in her 11th year as the director of athletics at MIT. Under her leadership, the athletic program has transformed into one of the top Division III programs in the nation on an annual basis. MIT has placed in the top 10 in the Learfield Directors’ Cup in four of the past five years and currently sits second nationally after the fall season.

“When I was named to receive this honor, I was really taken aback,” said Soriero. “I coached for over 21 years the sport of women’s basketball. I admired and respected Pat Summitt so much, as I know many of you and the women I know who have been in the basketball career and who have transferred into administration did.”

During her tenure, MIT teams have averaged nearly 10 conference championships, 90 All-American and 13 College Sports Information Directors of America (CoSIDA) Academic All-Americans per year. In the fall of 2017, MIT became the first New England Women’s and Men’s Athletic Conference (NEWMAC) institution to claim all of the women’s fall championships as the Engineers won five league titles in total and had three teams advance to the Sweet 16 of the NCAA Tournament.

Soriero has been active in the college sports governance process, serving on multiple NCAA committees, including chairing the Committee on Women’s Athletics, serving on the Division III Management Council, and working with the Women’s Basketball Rules Committee. Soriero also is the current president of Women Leaders in College Sports.

Award recipients are selected annually by the NCAA president and will receive a $10,000 honorarium to donate to the organization of the honoree’s choice that combats or researches neurological diseases of the brain. Soriero plans to donate the honorarium to Bay Cove Human Services, an organization in Boston that provides care to individuals with developmental disabilities, mental illnesses and substance abuse addictions.

The inaugural recipient of the Pat Summitt Award was Joan Cronan, longtime administrator at Tennessee.

For the latest on MIT Athletics, follow the Engineers via social media on Twitter, Facebook, Instagram and YouTube."
126;machinelearningmastery.com;https://machinelearningmastery.com/question-to-understand-any-machine-learning-algorithm/;2016-04-26;6 Questions To Understand Any Machine Learning Algorithm;"Tweet Share Share

Last Updated on August 12, 2019

There are a lot of machine learning algorithms and each algorithm is an island of research.

You have to choose the level of detail that you study machine learning algorithms. There is a sweet spot if you are a developer interested in applied predictive modeling.

This post describes that sweet spot and gives you a template that you can use to quickly understand any machine learning algorithm.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

What You Need To Know About a Machine Learning Algorithm?

What do you need to know about a machine learning algorithm to be able to use it well on a classification or prediction problem?

I won’t argue that the more that you know about how and why a particular algorithm works, the better you can wield it. But I do believe that there is a point of diminishing returns where you can stop, use what you know to be effective and dive deeper into the theory and research on an algorithm if and only if you need to know more in order to get better results.

Let’s take a look at the 6 questions that will reveal how a machine learning algorithms and how to best use it.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

6 Questions To Ask About Any Algorithm

There are 6 questions that you can ask to get to the heart of any machine learning algorithm:

How do you refer to the technique (e.g. what name)? How do you represent a learned model (e.g. what coefficients)? How to you learn a model (e.g. the optimization process from data to the representation)? How do you make predictions from a learned model (e.g. apply the model to new data)? How do you best prepare your data for the modeling with the technique (e.g. assumptions)? How do you get more information on the technique (e.g. where to look)?

You will note that I have phrased all of these questions as How-To. I did this intentionally to separate the practical concerns of how from the more theoretical concerns of why. I think knowing why a technique works is less important than knowing how it works, if you are looking to use it as a tool to get results. More on this in the next section.

Let’s take a closer look at each of these questions in turn.

1. How Do You Refer To The Technique?

This is obvious but important. You need to know the canonical name of the technique.

You need to be able to recognize the classical name or the name of the method from other fields as well and know that it is the same thing. This also includes the acronym for the algorithm, because sometimes they are less than intuitive.

This will help you sort out the base algorithm from extensions and the family tree of where the algorithm fits and relates to similar algorithms.

2. How Do You Represent a Learned Model?

I really like this nuts and bolts question.

This is question often overlooked in textbooks and papers and is perhaps the first question an engineer has when thinking about how a model will actually be used and deployed.

The representation is the numbers and data structure that captures the distinct details learned from data by the learning algorithm to be used by the prediction algorithm. It’s the stuff you save to disk or the database when you finalize your model. It’s the stuff you update when new training data becomes available.

Let’s make this concrete with an example. In the case of linear regression, the representation is the vector of regression coefficients. That’s it. In the case of a decision tree is is the tree itself including the nodes, how they are connected and the variables and cut-off thresholds chosen.

3. How Do You Learn a Model?

Given some training data, the algorithm needs to create the model or fill in the model representation. This question is about exactly how that occurs.

Often learning involves estimating parameters from the training data directly in simpler algorithms.

In most other algorithms it involves using the training data as part of a cost or loss function and an optimization algorithm to minimize the function. Simpler linear techniques may use linear algebra to achieve this result, whereas others may use a numerical optimization.

Often the way a machine learning algorithm learns a model is synonymous with the algorithm itself. This is the challenging and often time consuming part of running a machine learning algorithm.

The learning algorithm may be parameterized and it is often a good idea to list common ranges for parameter values or configuration heuristics that may be used as a starting point.

4. How Do You Make Predictions With A Model?

Once a model is learned, it is intended to be used to make predictions on new data. Note, we re exclusively talking about predictive modeling machine learning algorithms for classification and regression problems.

This is often the fast and even trivial part of using a machine learning algorithm. Often it is so trivial that it is not even mentioned or discussed in the literature.

It may be trivial because prediction may be as simple as filling in the inputs in an equation and calculating a prediction, or traversing a decision tree to see what leaf-node lights up. In other algorithms, like k-nearest neighbors the prediction algorithm may be the main show (k-NN has no training algorithm other than “store the whole training set”).

5. How Do You Best Prepare Data For The Algorithm?

Machine learning algorithms make assumptions.

Even the most relaxed non-parametric methods make assumptions about your training data. It is good or even critical to review these assumptions. Even better is to translate these assumptions into specific data preparation operations that you can perform.

This question flushes out transforms that you could use on your data before modeling, or at the very least gives you pause to think about data transforms to try. What I mean by this is that it is best to treat algorithm requirements and assumptions as suggestions of things to try to get the most out your model rather than hard and fast rules that your data must adhere to.

Just like you cannot know which algorithm will be best for your data before hand, you cannot know the best transforms to apply to your data to get the most from an algorithm. Real data is messy and it is a good idea to try a number of presentations of your data with a number of different algorithms to see what warrants deeper investigation. The requirements and assumptions of machine learning algorithms help to point out presentations of your data to try.

6. How Do You Get More Information About the Algorithm?

Some algorithms will bubble up as generally better than others on your data problem.

When they do, you need to know where to look to get a deeper understanding of the technique. This can help with further customizing the algorithm for your data and with tuning the parameters of the learning and prediction algorithms.

It is a good idea to collect and list resources that you can reference if and when you need to dive deeper. This may include:

Journal Articles

Conference Papers

Books including textbooks and monographs

Webpages

I also think it is a good idea to know of more practical references like example tutorials and open source implementations that you can look inside to get a more concrete idea of what is going on.

For more on researching machine learning algorithms, see the post How to Research a Machine Learning Algorithm.

Summary

In this post you discovered 6 questions that you can ask of a machine learning, that if answered, will give you a very good and practical idea of how it works and how you can use it effectively.

These questions were focused on machine learning algorithms for predictive modeling problems like classification and regression.

These questions, phrased simply are:

What are the common names of the algorithm? What representation is used by the model? How does the algorithm learn from training data? How can you make predictions from the model on new data? How you can best prepare your data for the algorithm? Where you can you look for more information about the algorithm?

For another post along this theme of defining an algorithm description template see How to Learn a Machine Learning Algorithm.

Do you like this approach? Let me know in the comments.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
127;machinelearningmastery.com;https://machinelearningmastery.com/how-to-use-an-encoder-decoder-lstm-to-echo-sequences-of-random-integers/;2017-06-11;How to use an Encoder-Decoder LSTM to Echo Sequences of Random Integers;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76

from random import randint from numpy import array from numpy import argmax from pandas import DataFrame from pandas import concat from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import TimeDistributed from keras . layers import RepeatVector # generate a sequence of random numbers in [0, 99] def generate_sequence ( length = 25 ) : return [ randint ( 0 , 99 ) for _ in range ( length ) ] # one hot encode sequence def one_hot_encode ( sequence , n_unique = 100 ) : encoding = list ( ) for value in sequence : vector = [ 0 for _ in range ( n_unique ) ] vector [ value ] = 1 encoding . append ( vector ) return array ( encoding ) # decode a one hot encoded string def one_hot_decode ( encoded_seq ) : return [ argmax ( vector ) for vector in encoded_seq ] # convert encoded sequence to supervised learning def to_supervised ( sequence , n_in , n_out ) : # create lag copies of the sequence df = DataFrame ( sequence ) df = concat ( [ df . shift ( n_in - i - 1 ) for i in range ( n_in ) ] , axis = 1 ) # drop rows with missing values df . dropna ( inplace = True ) # specify columns for input and output pairs values = df . values width = sequence . shape [ 1 ] X = values . reshape ( len ( values ) , n_in , width ) y = values [ : , 0 : ( n_out* width ) ] . reshape ( len ( values ) , n_out , width ) return X , y # prepare data for the LSTM def get_data ( n_in , n_out ) : # generate random sequence sequence = generate_sequence ( ) # one hot encode encoded = one_hot_encode ( sequence ) # convert to X,y pairs X , y = to_supervised ( encoded , n_in , n_out ) return X , y # define LSTM n_in = 5 n_out = 2 encoded_length = 100 batch_size = 21 model = Sequential ( ) model . add ( LSTM ( 150 , batch_input_shape = ( batch_size , n_in , encoded_length ) , stateful = True ) ) model . add ( RepeatVector ( n_out ) ) model . add ( LSTM ( 150 , return_sequences = True , stateful = True ) ) model . add ( TimeDistributed ( Dense ( encoded_length , activation = 'softmax' ) ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # train LSTM for epoch in range ( 5000 ) : # generate new random sequence X , y = get_data ( n_in , n_out ) # fit model for one epoch on this sequence model . fit ( X , y , epochs = 1 , batch_size = batch_size , verbose = 2 , shuffle = False ) model . reset_states ( ) # evaluate LSTM X , y = get_data ( n_in , n_out ) yhat = model . predict ( X , batch_size = batch_size , verbose = 0 ) # decode all pairs for i in range ( len ( X ) ) : print ( 'Expected:' , one_hot_decode ( y [ i ] ) , 'Predicted' , one_hot_decode ( yhat [ i ] ) )"
128;news.mit.edu;http://news.mit.edu/2020/catherine-dignazio-visualizing-covid-19-data-0414;;3 Questions: Catherine D’Ignazio on visualizing Covid-19 data;"The Covid-19 pandemic is generating waves of data points from around the world, recording the number of tests performed, cases confirmed, patients recovered, and people who have died from the virus. As these data are continuously updated, media outlets, government agencies, academics, and data-packaging firms are racing to make sense of the numbers, using novel design and visualization tools to chart and graph the virus many different contexts.

In general, data visualizations can help people quickly distill an otherwise overwhelming flood of numbers. Catherine D’Ignazio, assistant professor of urban science and planning at MIT, says it is critical that data are visualized responsibly in a pandemic.

D’Ignazio is the director of the Data and Feminism Lab, where she uses data and computational techniques to work toward gender and racial equity. MIT News spoke with her about the current boom in Covid-19 data visualizations, and how data visualizers can help us make sense of the pandemic’s uncertain numbers.

Q: How have you seen data visualization of Covid-19 evolve in the last few months, since the virus began its spread?

A: The first thing I'll note is that there has been an explosion of data visualization. Since the information about the virus comes in numbers — case counts, death counts, testing rates — it lends itself easily to data visualization. Maps, bar charts, and line charts of confirmed cases predominated at first, and I would say they are still the most common forms of visualization that we are seeing in media reporting and on social media. As a person in the field, the proliferation is both exciting, because it shows the relevance of visualization, and scary, because there is definitely some irresponsible use of visualization.

Many high-profile organizations are plotting case counts on graduated color maps, which is a big no-no unless you have normalized your numbers. So California, a big and densely populated state, will always appear to be worse off in absolute raw case counts. Conversely, this way of plotting could cause you to miss small states with a high rate of infection since they will be low in relative case numbers and would always show up in lighter colors on the map.

Second, as the crisis has developed, media outlets are mapping things other than simply case counts or death rates. There have been many versions of the “flatten the curve” chart. This one is interesting because it’s not about plotting specific numbers, but about explaining a public health concept to a broad audience with a hypothetical chart. The best visual explanation I’ve seen of the flatten the curve concept is from The Washington Post and comes with simulations and animations that explain virus transmission. There have also been a number of visualizations of how social distancing has changed people’s mobility behavior, shifting traffic patterns , and even a global satellite map where you can see how Covid-19 has reduced urban pollution over the past three months.

Finally, this crisis is posing some difficult visual communication problems: How do you depict exponential growth in an accessible way? How do you visually explain the uncertainty in numbers like case counts, where we (at least in the U.S. context) have not done nearly enough testing to make them a reliable indicator of actual cases?

Journalists and health communicators have responded to these challenges by developing new visual conventions, as well as making heavy use of explanations and disclaimers in the narratives themselves. For example, the chart below, by Lisa Charlotte Rost for DataWrapper, uses a log scale on the y-axis for showing exponential rates of change. But note the dotted reference lines, labeled “deaths double every day” or “...every 2nd day.” These annotations help to highlight the use of the log scale (which otherwise might go unnoticed by readers) as well as to explain how to interpret the different slopes of the lines. Likewise, Rost is explicit about only making charts of death rates, not case counts, because of the variation in availability of tests and vast underreporting in many countries. Whereas actual cases may or may not be detected and counted, deaths are more likely to be counted.

A screenshot of an interactive chart, from Datawrapper, shows cumulative numbers of confirmed deaths due to the Covid-19 disease. Chart: Lisa Charlotte Rost, Datawrapper. Source: Johns Hopkins CSSE. Created with Datawrapper.

Q: What are some things people should keep in mind when digging into available datasets to make their own visualizations?

A: This is such a great question, because there has been a proliferation of visualizations and models that are not only erroneous but also irresponsible in a public health crisis. Usually these are made by folks who do not have expertise in epidemiology but assume that their skills in data science can just be magically ported into a new realm. I’d like to shout out here to Amanda Makulec’s excellent guidance on undertaking responsible data visualizations in a public health crisis. One of her main points is to consider simply not making another Covid-19 chart. What this points to is the idea that data scientists and visualization designers need to take their civic role very seriously in a pandemic. Following Makulec’s line of reasoning, designers can think of the visualization they are making in the context of decision support: Their visualization has the power to help people decide whether to reject public health guidance and go out, to stay home, to feel the gravity of the problem without being overwhelmed, or to panic and buy up all the toilet paper.

Data visualization carries the aura of objectivity and authority. If designers wield that authority irresponsibly — for example, by depicting case counts with clean, certain-looking lines when we know that there is deep uncertainty in how case counts in different places were collected — it may deplete public trust, lead to rejecting public health guidance like social distancing, or even incite panic.

This carries over into all manner of visual choices that designers make. For example, color. Visualizations of Covid-19 cases and deaths have tended to use red bubbles or red-colored states and provinces. But color has cultural meaning — in Western cultures, it is used to indicate danger and harm. When a whole country is bathed in shades of red, or laden with red bubbles that obscure its boundaries, we need to be very careful about sensationalism. I’m not saying “never use red”; it is warranted in some cases to communicate the gravity of a situation. But our use of charged colors, particularly during a pandemic like this, involves making very careful ethical decisions. How serious is the risk to the individual reader? What do we want them to feel from viewing the visualization? What do we want them to do with the information in the visualization? Are those goals aligned with public health goals?

The complexity of calculating a fatality rate in order to model the spread of Covid-19. From “Why It’s So Freaking Hard To Make A Good COVID-19 Model ” by Maggie Koerth, Laura Bronner, and Jasmine Mithani for fivethirtyeight.com.

Rather than reducing complexity (to generate sensationalist and attention-grabbing clicks), some of the most responsible visualization is working to explain the complexity behind our current crisis. This is the case in the above graphic. The journalists walk us through why even calculating a simple input like the fatality rate depends on many other variables, both known and unknown.

All that said, public health communication really does need good visualization and data science right now. One of the exciting developments on the responsible-vis horizon is a new program from the Data Visualization Society that matches people with visualization skills to organizations working on Covid-19 that need their help. This is a great way to lend a hand, concretely, to the organizations who need help communicating their data during this crisis.

Q: How can we as individuals best make sense of and put into context all the data being reported, almost by the minute, in some cases?

A: One of my students said something wise to me this week. As she was describing her obsession with checking the news every couple minutes, she reflected, “I realized that I’m looking for answers that I cannot find, because nobody knows them.” She's right. At this point, nobody can truly answer our most basic questions: “When will this end? Will I lose my job? When will my kids return to school? Are my loved ones safe? How will my community be changed by this?”

No amount of data science or data visualization can solve these questions for us and give us the peace of mind we are craving. It is an inherently uncertain time, and we are in the middle of it. Rather than obsessively seeking information like case counts and scenario models to give us peace, I have been telling students to practice self-care and community-care as a way to direct their attention to things they have more control over. For example, in our local communities, Covid-19 is already having a disproportionate impact on the elderly, on health care workers, on first responders, on domestic workers, on single parents, on incarcerated people, and more. Below is one effective graphic that highlights these disproportionate impacts.

From “Coronavirus quarantine: only one in four Americans can work from home” by Mona Chalabi for The Guardian.

As the graphic shows, there is a great dimension of privilege in the people who are able to work from home: The vast majority of folks who can earn money from home are in the richest 25 percent of workers. This attention to how power and privilege play out unequally in data is also a throughline in Lauren F. Klein and my recently published book, “Data Feminism.” A feminist approach demands that we use data science and visualization to expose inequality and work toward equity.

So while it is important to (responsibly) track and visualize death rates from Covid-19, how do we also focus our attention on efforts to support the groups who are most directly and unfairly impacted by this crisis, to get them the care, equipment and the economic security that they need? The reality — even amidst this great uncertainty — is that we can all take action now, in our local communities, to support each other."
129;machinelearningmastery.com;http://machinelearningmastery.com/machine-learning-in-r-step-by-step/;2016-02-02;Your First Machine Learning Project in R Step-By-Step;"# a) linear algorithms

set . seed ( 7 )

fit . lda < - train ( Species ~ . , data = dataset , method = ""lda"" , metric = metric , trControl = control )

# b) nonlinear algorithms

# CART

set . seed ( 7 )

fit . cart < - train ( Species ~ . , data = dataset , method = ""rpart"" , metric = metric , trControl = control )

# kNN

set . seed ( 7 )

fit . knn < - train ( Species ~ . , data = dataset , method = ""knn"" , metric = metric , trControl = control )

# c) advanced algorithms

# SVM

set . seed ( 7 )

fit . svm < - train ( Species ~ . , data = dataset , method = ""svmRadial"" , metric = metric , trControl = control )

# Random Forest

set . seed ( 7 )"
130;news.mit.edu;http://news.mit.edu/2020/first-majorana-fermion-metal-quantum-computing-0410;;First sighting of mysterious Majorana fermion on a common metal;"Physicists at MIT and elsewhere have observed evidence of Majorana fermions — particles that are theorized to also be their own antiparticle — on the surface of a common metal: gold. This is the first sighting of Majorana fermions on a platform that can potentially be scaled up. The results, published in the Proceedings of the National Academy of Sciences, are a major step toward isolating the particles as stable, error-proof qubits for quantum computing.

In particle physics, fermions are a class of elementary particles that includes electrons, protons, neutrons, and quarks, all of which make up the building blocks of matter. For the most part, these particles are considered Dirac fermions, after the English physicist Paul Dirac, who first predicted that all fermionic fundamental particles should have a counterpart, somewhere in the universe, in the form of an antiparticle — essentially, an identical twin of opposite charge.

In 1937, the Italian theoretical physicist Ettore Majorana extended Dirac’s theory, predicting that among fermions, there should be some particles, since named Majorana fermions, that are indistinguishable from their antiparticles. Mysteriously, the physicist disappeared during a ferry trip off the Italian coast just a year after making his prediction. Scientists have been looking for Majorana’s enigmatic particle ever since. It has been suggested, but not proven, that the neutrino may be a Majorana particle. On the other hand, theorists have predicted that Majorana fermions may also exist in solids under special conditions.

Now the MIT-led team has observed evidence of Majorana fermions in a material system they designed and fabricated, which consists of nanowires of gold grown atop a superconducting material, vanadium, and dotted with small, ferromagnetic “islands” of europium sulfide. When the researchers scanned the surface near the islands, they saw signature signal spikes near zero energy on the very top surface of gold that, according to theory, should only be generated by pairs of Majorana fermions.

“Majorana ferminons are these exotic things, that have long been a dream to see, and we now see them in a very simple material — gold,” says Jagadeesh Moodera, a senior research scientist in MIT’s Department of Physics, and a member of MIT’s Plasma Science and Fusion Center. “We’ve shown they are there, and stable, and easily scalable.”

“The next push will be to take these objects and make them into qubits, which would be huge progress toward practical quantum computing,” adds co-author Patrick Lee, the William and Emma Rogers Professor of Physics at MIT.

Lee and Moodera’s coauthors include former MIT postdoc and first author Sujit Manna (currently on the faculty at the Indian Institute of Technology at Delhi), and former MIT postdoc Peng Wei of University of California at Riverside, along with Yingming Xie and Kam Tuen Law of the Hong Kong University of Science and Technology.

High risk

If they could be harnessed, Majorana fermions would be ideal as qubits, or individual computational units for quantum computers. The idea is that a qubit would be made of combinations of pairs of Majorana fermions, each of which would be separated from its partner. If noise errors affect one member of the pair, the other should remain unaffected, thereby preserving the integrity of the qubit and enabling it to correctly carry out a computation.

Scientists have looked for Majorana fermions in semiconductors, the materials used in conventional, transistor-based computing. In their experiments, researchers have combined semiconductors with superconductors — materials through which electrons can travel without resistance. This combination imparts superconductive properties to conventional semiconductors, which physicists believe should induce particles in the semiconductor to split , forming the pair of Majorana fermions.

“There are several material platforms where people believe they’ve seen Majorana particles,” Lee says. “The evidence is stronger and stronger, but it’s still not 100 percent proven.”

What’s more, the semiconductor-based setups to date have been difficult to scale up to produce the thousands or millions of qubits needed for a practical quantum computer, because they require growing very precise crystals of semiconducting material and it is very challenging to turn these into high-quality superconductors.

About a decade ago, Lee, working with his graduate student Andrew Potter, had an idea: Perhaps physicists might be able to observe Majorana fermions in metal, a material that readily becomes superconductive in proximity with a superconductor. Scientists routinely make metals, including gold, into superconductors. Lee’s idea was to see if gold’s surface state — its very top layer of atoms — could be made to be superconductive. If this could be achieved, then gold could serve as a clean, atomically precise system in which researchers could observe Majorana fermions.

Lee proposed, based on Moodera’s prior work with ferromagnetic insulators, that if it were placed atop a superconductive surface state of gold, then researchers should have a good chance of clearly seeing signatures of Majorana fermions.

“When we first proposed this, I couldn’t convince a lot of experimentalists to try it, because the technology was daunting,” says Lee who eventually partnered with Moodera’s experimental group to to secure crucial funding from the Templeton Foundation to realize the design. “Jagadeesh and Peng really had to reinvent the wheel. It was extremely courageous to jump into this, because it’s really a high-risk, but we think a high-payoff, thing.”

“Finding Majorana”

Over the last few years, the researchers have characterized gold’s surface state and proved that it could work as a platform for observing Majorana fermions, after which the group began fabricating the setup that Lee envisioned years ago.

They first grew a sheet of superconducting vanadium, on top of which they overlaid nanowires of gold layer, measuring about 4 nanometers thick. They tested the conductivity of gold’s very top layer, and found that it did, in fact, become superconductive in proximity with the vanadium. They then deposited over the gold nanowires “islands” of europium sulfide, a ferromagnetic material that is able to provide the needed internal magnetic fields to create the Majorana fermions.

The team then applied a tiny voltage and used scanning tunneling microscopy, a specialized technique that enabled the researchers to scan the energy spectrum around each island on gold’s surface.

Moodera and his colleagues then looked for a very specific energy signature that only Majorana fermions should produce, if they exist. In any superconducting material, electrons travel through at certain energy ranges. There is however a desert, or “energy gap” where there should be no electrons. If there is a spike inside this gap, it is very likely a signature of Majorana fermions.

Looking through their data, the researchers observed spikes inside this energy gap on opposite ends of several islands along the the direction of the magnetic field, that were clear signatures of pairs of Majorana fermions.

“We only see this spike on opposite sides of the island, as theory predicted,” Moodera says. “Anywhere else, you don’t see it.”

“In my talks, I like to say that we are finding Majorana, on an island in a sea of gold,” Lee adds.

Moodera says the team’s setup, requiring just three layers — gold sandwiched between a ferromagnet and a superconductor — is an “easily achievable, stable system” that should also be economically scalable compared to conventional, semiconductor-based approaches to generate qubits.

“Seeing a pair of Majorana fermions is an important step toward making a qubit,” Wei says. “The next step is to make a qubit from these particles, and we now have some ideas for how to go about doing this.”

This research was funded, in part, by the John Templeton Foundation, the U.S. Office of Naval Research, the National Science Foundation, and the U.S. Department of Energy."
131;machinelearningmastery.com;http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/;2016-06-19;Dropout Regularization in Deep Learning Models With Keras;"# Baseline Model on the Sonar Dataset

from pandas import read_csv

from keras . models import Sequential

from keras . layers import Dense

from keras . wrappers . scikit_learn import KerasClassifier

from keras . optimizers import SGD

from sklearn . model_selection import cross_val_score

from sklearn . preprocessing import LabelEncoder

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import StandardScaler

from sklearn . pipeline import Pipeline

# load dataset

dataframe = read_csv ( ""sonar.csv"" , header = None )

dataset = dataframe . values

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 60 ] . astype ( float )

Y = dataset [ : , 60 ]

# encode class values as integers

encoder = LabelEncoder ( )

encoder . fit ( Y )

encoded_Y = encoder . transform ( Y )

# baseline

def create_baseline ( ) :

# create model

model = Sequential ( )

model . add ( Dense ( 60 , input_dim = 60 , activation = 'relu' ) )

model . add ( Dense ( 30 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# Compile model

sgd = SGD ( lr = 0.01 , momentum = 0.8 )

model . compile ( loss = 'binary_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ] )

return model

estimators = [ ]

estimators . append ( ( 'standardize' , StandardScaler ( ) ) )

estimators . append ( ( 'mlp' , KerasClassifier ( build_fn = create_baseline , epochs = 300 , batch_size = 16 , verbose = 0 ) ) )

pipeline = Pipeline ( estimators )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True )

results = cross_val_score ( pipeline , X , encoded_Y , cv = kfold )"
132;machinelearningmastery.com;https://machinelearningmastery.com/best-advice-for-configuring-backpropagation-for-deep-learning-neural-networks/;2019-02-21;8 Tricks for Configuring Backpropagation to Train Better Neural Networks;"Tweet Share Share

Last Updated on August 6, 2019

Neural network models are trained using stochastic gradient descent and model weights are updated using the backpropagation algorithm.

The optimization solved by training a neural network model is very challenging and although these algorithms are widely used because they perform so well in practice, there are no guarantees that they will converge to a good model in a timely manner.

The challenge of training neural networks really comes down to the challenge of configuring the training algorithms.

In this post, you will discover tips and tricks for getting the most out of the backpropagation algorithm when training neural network models.

After reading this post, you will know:

The challenge of training a neural network is really the balance between learning the training dataset and generalizing to new examples beyond the training dataset.

Eight specific tricks that you can use to train better neural network models, faster.

Second order optimization algorithms that can also be used to train neural networks under certain circumstances.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Post Overview

This tutorial is divided into five parts; they are:

Efficient BackProp Overview Learning and Generalization 8 Practical Tricks for Backpropagation Second Order Optimization Algorithms Discussion and Conclusion

Efficient BackProp Overview

The 1998 book titled “Neural Networks: Tricks of the Trade” provides a collection of chapters by academics and neural network practitioners that describe best practices for configuring and using neural network models.

The book was updated at the cusp of the deep learning renaissance and a second edition was released in 2012 including 13 new chapters.

The first chapter in both editions is titled “Efficient BackProp” written by Yann LeCun, Leon Bottou, (both at Facebook AI), Genevieve Orr, and Klaus-Robert Muller (also co-editors of the book).

The chapter is also available online for free as a pre-print.

The chapter was also summarized in a preface in both editions of the book titled “Speed Learning.”

It is an important chapter and document as it provides a near-exhaustive summary of how to best configure backpropagation under stochastic gradient descent as of 1998, and much of the advice is just as relevant today.

In this post, we will focus on this chapter or paper and attempt to distill the most relevant advice for modern deep learning practitioners.

For reference, the chapter is divided into 10 sections; they are:

1.1: Introduction

1.2: Learning and Generalization

1.3: Standard Backpropagation

1.4: A Few Practical Tricks

1.5: Convergence of Gradient Descent

1.6: Classical Second Order Optimization Methods

1.7: Tricks to Compute the Hessian Information in Multilayer Networks

1.8: Analysis of the Hessian in Multi-layer Networks

1.9: Applying Second Order Methods to Multilayer Networks

1.10: Discussion and Conclusion

We will focus on the tips and tricks for configuring backpropagation and stochastic gradient descent.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Learning and Generalization

The chapter begins with a description of the general problem of the dual challenge of learning and generalization with neural network models.

The authors motivate the article by highlighting that the backpropagation algorithm is the most widely used algorithm to train neural network models because it works and because it is efficient.

Backpropagation is a very popular neural network learning algorithm because it is conceptually simple, computationally efficient, and because it often works. However, getting it to work well, and sometimes to work at all, can seem more of an art than a science.

The authors also remind us that training neural networks with backpropagation is really hard. Although the algorithm is both effective and efficient, it requires the careful configuration of multiple model properties and model hyperparameters, each of which requires deep knowledge of the algorithm and experience to set correctly.

And yet, there are no rules to follow to “best” configure a model and training process.

Designing and training a network using backprop requires making many seemingly arbitrary choices such as the number and types of nodes, layers, learning rates, training and test sets, and so forth. These choices can be critical, yet there is no foolproof recipe for deciding them because they are largely problem and data dependent.

The goal of training a neural network model is most challenging because it requires solving two hard problems at once:

Learning the training dataset in order to best minimize the loss.

the training dataset in order to best minimize the loss. Generalizing the model performance in order to make predictions on unseen examples.

There is a trade-off between these concerns, as a model that learns too well will generalize poorly, and a model that generalizes well may be underfit. The goal of training a neural network well is to find a happy balance between these two concerns.

This chapter is focused on strategies for improving the process of minimizing the cost function. However, these strategies must be used in conjunction with methods for maximizing the network’s ability to generalize, that is, to predict the correct targets for patterns the learning system has not previously seen.

Interestingly, the problem of training a neural network model is cast in terms of the bias-variance trade-off, often used to describe machine learning algorithms in general.

When fitting a neural network model, these terms can be defined as:

Bias : A measure of how the network output averaged across all datasets differs from the desired function.

: A measure of how the network output averaged across all datasets differs from the desired function. Variance: A measure of how much the network output varies across datasets.

This framing casts defining the capacity of the model as a choice of bias, controlling the range of functions that can be learned. It casts variance as a function of the training process and the balance struck between overfitting the training dataset and generalization error.

This framing can also help in understanding the dynamics of model performance during training. That is, from a model with large bias and small variance in the beginning of training to a model with lower bias and higher variance at the end of training.

Early in training, the bias is large because the network output is far from the desired function. The variance is very small because the data has had little influence yet. Late in training, the bias is small because the network has learned the underlying function.

These are the normal dynamics of the model, although when training, we must guard against training the model too much and overfitting the training dataset. This makes the model fragile, pushing the bias down, specializing the model to training examples and, in turn, causing much larger variance.

However, if trained too long, the network will also have learned the noise specific to that dataset. This is referred to as overtraining. In such a case, the variance will be large because the noise varies between datasets.

A focus on the backpropagation algorithm means a focus on “learning” at the expense of temporally ignoring “generalization” that can be addressed later with the introduction of regularization techniques.

A focus on learning means a focus on minimizing loss both quickly (fast learning) and effectively (learning well).

The idea of this chapter, therefore, is to present minimization strategies (given a cost function) and the tricks associated with increasing the speed and quality of the minimization.

8 Practical Tricks for Backpropagation

The focus of the chapter is a sequence of practical tricks for backpropagation to better train neural network models.

There are eight tricks; they are:

1.4.1: Stochastic Versus Batch Learning

1.4.2: Shuffling the Examples

1.4.3: Normalizing the Inputs

1.4.4: The Sigmoid

1.4.5: Choosing Target Values

1.4.6: Initializing the Weights

1.4.7: Choosing Learning Rates

1.4.8: Radial Basis Function vs Sigmoid

The section starts off with a comment that the optimization problem that we are trying to solve with stochastic gradient descent and backpropagation is challenging.

Backpropagation can be very slow particularly for multilayered networks where the cost surface is typically non-quadratic, non-convex, and high dimensional with many local minima and/or flat regions.

The authors go on to highlight that in choosing stochastic gradient descent and the backpropagation algorithms to optimize and update weights, we have no grantees of performance.

There is no formula to guarantee that (1) the network will converge to a good solution, (2) convergence is swift, or (3) convergence even occurs at all.

These comments provide the context for the tricks that also make no guarantees but instead increase the likelihood of finding a better model, faster.

Let’s take a closer look at each trick in turn.

Many of the tricks are focused on sigmoid (s-shaped) activation functions, which are no longer best practice for use in hidden layers, having been replaced by the rectified linear activation function. As such, we will spend less time on sigmoid-related tricks.

Tip #1: Stochastic Versus Batch Learning

This tip highlights the choice between using either stochastic or batch gradient descent when training your model.

Stochastic gradient descent, also called online gradient descent, refers to a version of the algorithm where the error gradient is estimated from a single randomly selected example from the training dataset and the model parameters (weights) are then updated.

It has the effect of training the model fast, although it can result in large, noisy updates to model weights.

Stochastic learning is generally the preferred method for basic backpropagation for the following three reasons: 1. Stochastic learning is usually much faster than batch learning.

2. Stochastic learning also often results in better solutions.

3. Stochastic learning can be used for tracking changes.

Batch gradient descent involves estimating the error gradient using the average from all examples in the training dataset. It is faster to execute and is better understood from a theoretical perspective, but results in slower learning.

Despite the advantages of stochastic learning, there are still reasons why one might consider using batch learning: 1. Conditions of convergence are well understood.

2. Many acceleration techniques (e.g. conjugate gradient) only operate in batch learning.

3. Theoretical analysis of the weight dynamics and convergence rates are simpler.

Generally, the authors recommend using stochastic gradient descent where possible because it offers faster training of the model.

Despite the advantages of batch updates, stochastic learning is still often the preferred method particularly when dealing with very large data sets because it is simply much faster.

They suggest making use of a learning rate decay schedule in order to counter the noisy effect of the weight updates seen during stochastic gradient descent.

… noise, which is so critical for finding better local minima also prevents full convergence to the minimum. […] So in order to reduce the fluctuations we can either decrease (anneal) the learning rate or have an adaptive batch size.

They also suggest using mini-batches of samples to reduce the noise of the weight updates. This is where the error gradient is estimated across a small subset of samples from the training dataset instead of one sample in the case of stochastic gradient descent or all samples in the case of batch gradient descent.

This variation later became known as Mini-Batch Gradient Descent and is the default when training neural networks.

Another method to remove noise is to use “mini-batches”, that is, start with a small batch size and increase the size as training proceeds.

Tip #2: Shuffling the Examples

This tip highlights the importance that the order of examples shown to the model during training has on the training process.

Generally, the authors highlight that the learning algorithm performs better when the next example used to update the model is different from the previous example. Ideally, it is the most different or unfamiliar to the model.

Networks learn the fastest from the most unexpected sample. Therefore, it is advisable to choose a sample at each iteration that is the most unfamiliar to the system.

One simple way to implement this trick is to ensure that successive examples used to update the model parameters are from different classes.

… a very simple trick that crudely implements this idea is to simply choose successive examples that are from different classes since training examples belonging to the same class will most likely contain similar information.

This trick can also be implemented by showing and re-showing examples to the model it gets the most wrong or makes the most error on when making a prediction. This approach can be effective, but can also lead to disaster if the examples that are over-represented during training are outliers.

Choose Examples with Maximum Information Content 1. Shuffle the training set so that successive training examples never (rarely) belong to the same class.

2. Present input examples that produce a large error more frequently than examples that produce a small error

Tip #3: Normalizing the Inputs

This tip highlights the importance of data preparation prior to training a neural network model.

The authors point out that neural networks often learn faster when the examples in the training dataset sum to zero. This can be achieved by subtracting the mean value from each input variable, called centering.

Convergence is usually faster if the average of each input variable over the training set is close to zero.

They also comment that this centering of inputs also improves the convergence of the model when applied to the inputs to hidden layers from prior layers. This is fascinating as it lays the foundation for the Batch Normalization technique developed and made widely popular nearly 15 years later.

Therefore, it is good to shift the inputs so that the average over the training set is close to zero. This heuristic should be applied at all layers which means that we want the average of the outputs of a node to be close to zero because these outputs are the inputs to the next layer

The authors also comment on the need to normalize the spread of the input variables. This can be achieved by dividing the values by their standard deviation. For variables that have a Gaussian distribution, centering and normalizing values in this way means that they will be reduced to a standard Gaussian with a mean of zero and a standard deviation of one.

Scaling speeds learning because it helps to balance out the rate at which the weights connected to the input nodes learn.

Finally, they suggest de-correlating the input variables. This means removing any linear dependence between the input variables and can be achieved using a Principal Component Analysis as a data transform.

Principal component analysis (also known as the Karhunen-Loeve expansion) can be used to remove linear correlations in inputs

This tip on data preparation can be summarized as follows:

Transforming the Inputs 1. The average of each input variable over the training set should be close to zero.

2. Scale input variables so that their covariances are about the same.

3. Input variables should be uncorrelated if possible.

These recommended three steps of data preparation of centering, normalizing, and de-correlating are summarized nicely in a figure, reproduced from the book below:

The centering of input variables may or may not be the best approach when using the more modern ReLU activation functions in the hidden layers of your network, so I’d recommend evaluating both standardization and normalization procedures when preparing data for your model.

Tip #4: The Sigmoid

This tip recommends the use of sigmoid activation functions in the hidden layers of your network.

Nonlinear activation functions are what give neural networks their nonlinear capabilities. One of the most common forms of activation function is the sigmoid …

Specifically, the authors refer to a sigmoid activation function as any S-shaped function, such as the logistic (referred to as sigmoid) or hyperbolic tangent function (referred to as tanh).

Symmetric sigmoids such as hyperbolic tangent often converge faster than the standard logistic function.

The authors recommend modifying the default functions (if needed) so that the midpoint of the function is at zero.

The use of logistic and tanh activation functions for the hidden layers is no longer a sensible default as the performance models that use ReLU converge much faster.

Tip #5: Choosing Target Values

This tip highlights a more careful consideration of the choice of target variables.

In the case of binary classification problems, target variables may be in the set {0, 1} for the limits of the logistic activation function or in the set {-1, 1} for the hyperbolic tangent function when using the cross-entropy or hinge loss functions respectively, even in modern neural networks.

The authors suggest that using values at the extremes of the activation function may make learning the problem more challenging.

Common wisdom might seem to suggest that the target values be set at the value of the sigmoid’s asymptotes. However, this has several drawbacks.

They suggest that achieving values at the point of saturation of the activation function (edges) may require larger and larger weights, which could make the model unstable.

One approach to addressing this is to use target values away from the edge of the output function.

Choose target values at the point of the maximum second derivative on the sigmoid so as to avoid saturating the output units.

I recall that in the 1990s, it was common advice to use target values in the set of {0.1 and 0.9} with the logistic function instead of {0 and 1}.

Tip #6: Initializing the Weights

This tip highlights the importance of the choice of weight initialization scheme and how it is tightly related to the choice of activation function.

In the context of the sigmoid activation function, they suggest that the initial weights for the network should be chosen to activate the function in the linear region (e.g. the line part not the curve part of the S-shape).

The starting values of the weights can have a significant effect on the training process. Weights should be chosen randomly but in such a way that the sigmoid is primarily activated in its linear region.

This advice may also apply to the weight activation for the ReLU where the linear part of the function is positive.

This highlights the important impact that initial weights have on learning, where large weights saturate the activation function, resulting in unstable learning, and small weights result in very small gradients and, in turn, slow learning. Ideally, we seek model weights that are over the linear (non-curvy) part of the activation function.

… weights that range over the sigmoid’s linear region have the advantage that (1) the gradients are large enough that learning can proceed and (2) the network will learn the linear part of the mapping before the more difficult nonlinear part.

The authors suggest a random weight initialization scheme that uses the number of nodes in the previous layer, the so-called fan-in. This is interesting as it is a precursor of what became known as the Xavier weight initialization scheme.

Tip #7: Choosing Learning Rates

This tip highlights the importance of choosing the learning rate.

The learning rate is the amount that the model weights are updated each iteration of the algorithm. A small learning rate can cause slower convergence but perhaps a better result, whereas a larger learning rate can result in faster convergence but perhaps to a less optimal result.

The authors suggest decreasing the learning rate when the weight values begin changing back and forth, e.g. oscillating.

Most of those schemes decrease the learning rate when the weight vector “oscillates”, and increase it when the weight vector follows a relatively steady direction.

They comment that this is a hard strategy when using online gradient descent as, by default, the weights will oscillate a lot.

The authors also recommend using one learning rate for each parameter in the model. The goal is to help each part of the model to converge at the same rate.

… it is clear that picking a different learning rate (eta) for each weight can improve the convergence. […] The main philosophy is to make sure that all the weights in the network converge roughly at the same speed.

They refer to this property as “equalizing the learning speeds” of each model parameter.

Equalize the Learning Speeds – give each weight its own learning rate

– learning rates should be proportional to the square root of the number of inputs to the unit

– weights in lower layers should typically be larger than in the higher layers

In addition to using a learning rate per parameter, the authors also recommend using momentum and using adaptive learning rates.

It’s interesting that these recommendations later became enshrined in methods like AdaGrad and Adam that are now popular defaults.

Tip #8: Radial Basis Functions vs Sigmoid Units

This final tip is perhaps less relevant today, and I recommend trying radial basis functions (RBF) instead of sigmoid activation functions in some cases.

The authors suggest that training RBF units can be faster than training units using a sigmoid activation.

Unlike sigmoidal units which can cover the entire space, a single RBF unit covers only a small local region of the input space. This can be an advantage because learning can be faster.

Theoretical Grounding

After these tips, the authors go on to provide a theoretical grounding for why many of these tips are a good idea and are expected to result in better or faster convergence when training a neural network model.

Specifically, the tips supported by this analysis are:

Subtract the means from the input variables

Normalize the variances of the input variables.

De-correlate the input variables.

Use a separate learning rate for each weight.

Second Order Optimization Algorithms

The remainer of the chapter focuses on the use of second order optimization algorithms for training neural network models.

This may not be everyone’s cup of tea and requires a background and good memory of matrix calculus. You may want to skip it.

You may recall that the first derivative is the slope of a function (how steep it is) and that backpropagation uses the first derivative to update the models in proportion to their output error. These methods are referred to as first order optimization algorithms, e.g. optimization algorithms that use the first derivative of the error in the output of the model.

You may also recall from calculus that the second order derivative is the rate of change in the first order derivative, or in this case, the gradient of the error gradient itself. It gives an idea of how curved the loss function is for the current set of weights. Algorithms that use the second derivative are referred to as second order optimization algorithms.

The authors go on to introduce five second order optimization algorithms, specifically:

Newton

Conjugate Gradient

Gauss-Newton

Levenberg Marquardt

Quasi-Newton (BFGS)

These algorithms require access to the Hessian matrix or an approximation of the Hessian matrix. You may also recall the Hessian matrix if you covered a theoretical introduction to the backpropagation algorithm. In a hand-wavy way, we use the Hessian to describe the second order derivatives for the model weights.

The authors proceed to outline a number of methods that can be used to approximate the Hessian matrix (for use in second order optimization algorithms), such as: finite difference, square Jacobian approximation, the diagonal of the Hessian, and more.

They then go on to analyze the Hessian in multilayer neural networks and the effectiveness of second order optimization algorithms.

In summary, they highlight that perhaps second order methods are more appropriate for smaller neural network models trained using batch gradient descent.

Classical second-order methods are impractical in almost all useful cases.

Discussion and Conclusion

The chapter ends with a very useful summary of tips for getting the most out of backpropagation when training neural network models.

This summary is reproduced below:

– shuffle the examples

– center the input variables by subtracting the mean

– normalize the input variable to a standard deviation of 1

– if possible, de-correlate the input variables.

– pick a network with the sigmoid function shown in figure 1.4

– set the target values within the range of the sigmoid, typically +1 and -1.

– initialize the weights to random values (as prescribed by 1.16).

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this post, you discovered tips and tricks for getting the most out of the backpropagation algorithm when training neural network models.

Have you tried any of these tricks on your projects?

Let me know about your results in the comments below.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
133;machinelearningmastery.com;http://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/;2016-08-21;Data Preparation for Gradient Boosting with XGBoost in Python;"# multiclass classification

import pandas

import xgboost

from sklearn import model_selection

from sklearn . metrics import accuracy_score

from sklearn . preprocessing import LabelEncoder

# load data

data = pandas . read_csv ( 'iris.csv' , header = None )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 4 ]

Y = dataset [ : , 4 ]

# encode string class values as integers

label_encoder = LabelEncoder ( )

label_encoder = label_encoder . fit ( Y )

label_encoded_y = label_encoder . transform ( Y )

seed = 7

test_size = 0.33

X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , label_encoded_y , test_size = test_size , random_state = seed )

# fit model no training data

model = xgboost . XGBClassifier ( )

model . fit ( X_train , y_train )

print ( model )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )"
134;machinelearningmastery.com;https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/;2017-08-27;How to Make Predictions with Long Short-Term Memory Models in Keras;"from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from numpy import array

from keras . models import load_model

# return training data

def get_train ( ) :

seq = [ [ 0.0 , 0.1 ] , [ 0.1 , 0.2 ] , [ 0.2 , 0.3 ] , [ 0.3 , 0.4 ] , [ 0.4 , 0.5 ] ]

seq = array ( seq )

X , y = seq [ : , 0 ] , seq [ : , 1 ]

X = X . reshape ( ( len ( X ) , 1 , 1 ) )

return X , y

# define model

model = Sequential ( )

model . add ( LSTM ( 10 , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 , activation = 'linear' ) )

# compile model

model . compile ( loss = 'mse' , optimizer = 'adam' )

# fit model

X , y = get_train ( )

model . fit ( X , y , epochs = 300 , shuffle = False , verbose = 0 )

# save model to single file

model . save ( 'lstm_model.h5' )

# snip...

# later, perhaps run from another script

# load model from single file

model = load_model ( 'lstm_model.h5' )

# make predictions

yhat = model . predict ( X , verbose = 0 )"
135;machinelearningmastery.com;https://machinelearningmastery.com/multi-step-time-series-forecasting-with-machine-learning-models-for-household-electricity-consumption/;2018-10-04;Multi-step Time Series Forecasting with Machine Learning for Electricity Usage;"# direct multi-step forecast by lead time

from math import sqrt

from numpy import split

from numpy import array

from pandas import read_csv

from sklearn . metrics import mean_squared_error

from matplotlib import pyplot

from sklearn . preprocessing import StandardScaler

from sklearn . preprocessing import MinMaxScaler

from sklearn . pipeline import Pipeline

from sklearn . linear_model import LinearRegression

from sklearn . linear_model import Lasso

from sklearn . linear_model import Ridge

from sklearn . linear_model import ElasticNet

from sklearn . linear_model import HuberRegressor

from sklearn . linear_model import Lars

from sklearn . linear_model import LassoLars

from sklearn . linear_model import PassiveAggressiveRegressor

from sklearn . linear_model import RANSACRegressor

from sklearn . linear_model import SGDRegressor

# split a univariate dataset into train/test sets

def split_dataset ( data ) :

# split into standard weeks

train , test = data [ 1 : - 328 ] , data [ - 328 : - 6 ]

# restructure into windows of weekly data

train = array ( split ( train , len ( train ) / 7 ) )

test = array ( split ( test , len ( test ) / 7 ) )

return train , test

# evaluate one or more weekly forecasts against expected values

def evaluate_forecasts ( actual , predicted ) :

scores = list ( )

# calculate an RMSE score for each day

for i in range ( actual . shape [ 1 ] ) :

# calculate mse

mse = mean_squared_error ( actual [ : , i ] , predicted [ : , i ] )

# calculate rmse

rmse = sqrt ( mse )

# store

scores . append ( rmse )

# calculate overall RMSE

s = 0

for row in range ( actual . shape [ 0 ] ) :

for col in range ( actual . shape [ 1 ] ) :

s += ( actual [ row , col ] - predicted [ row , col ] ) * * 2

score = sqrt ( s / ( actual . shape [ 0 ] * actual . shape [ 1 ] ) )

return score , scores

# summarize scores

def summarize_scores ( name , score , scores ) :

s_scores = ', ' . join ( [ '%.1f' % s for s in scores ] )

print ( '%s: [%.3f] %s' % ( name , score , s_scores ) )

# prepare a list of ml models

def get_models ( models = dict ( ) ) :

# linear models

models [ 'lr' ] = LinearRegression ( )

models [ 'lasso' ] = Lasso ( )

models [ 'ridge' ] = Ridge ( )

models [ 'en' ] = ElasticNet ( )

models [ 'huber' ] = HuberRegressor ( )

models [ 'lars' ] = Lars ( )

models [ 'llars' ] = LassoLars ( )

models [ 'pa' ] = PassiveAggressiveRegressor ( max_iter = 1000 , tol = 1e - 3 )

models [ 'ranscac' ] = RANSACRegressor ( )

models [ 'sgd' ] = SGDRegressor ( max_iter = 1000 , tol = 1e - 3 )

print ( 'Defined %d models' % len ( models ) )

return models

# create a feature preparation pipeline for a model

def make_pipeline ( model ) :

steps = list ( )

# standardization

steps . append ( ( 'standardize' , StandardScaler ( ) ) )

# normalization

steps . append ( ( 'normalize' , MinMaxScaler ( ) ) )

# the model

steps . append ( ( 'model' , model ) )

# create pipeline

pipeline = Pipeline ( steps = steps )

return pipeline

# # convert windows of weekly multivariate data into a series of total power

def to_series ( data ) :

# extract just the total power from each week

series = [ week [ : , 0 ] for week in data ]

# flatten into a single series

series = array ( series ) . flatten ( )

return series

# convert history into inputs and outputs

def to_supervised ( history , n_input , output_ix ) :

# convert history to a univariate series

data = to_series ( history )

X , y = list ( ) , list ( )

ix_start = 0

# step over the entire history one time step at a time

for i in range ( len ( data ) ) :

# define the end of the input sequence

ix_end = ix_start + n_input

ix_output = ix_end + output_ix

# ensure we have enough data for this instance

if ix_output < len ( data ) :

X . append ( data [ ix_start : ix_end ] )

y . append ( data [ ix_output ] )

# move along one time step

ix_start += 1

return array ( X ) , array ( y )

# fit a model and make a forecast

def sklearn_predict ( model , history , n_input ) :

yhat_sequence = list ( )

# fit a model for each forecast day

for i in range ( 7 ) :

# prepare data

train_x , train_y = to_supervised ( history , n_input , i )

# make pipeline

pipeline = make_pipeline ( model )

# fit the model

pipeline . fit ( train_x , train_y )

# forecast

x_input = array ( train_x [ - 1 , : ] ) . reshape ( 1 , n_input )

yhat = pipeline . predict ( x_input ) [ 0 ]

# store

yhat_sequence . append ( yhat )

return yhat_sequence

# evaluate a single model

def evaluate_model ( model , train , test , n_input ) :

# history is a list of weekly data

history = [ x for x in train ]

# walk-forward validation over each week

predictions = list ( )

for i in range ( len ( test ) ) :

# predict the week

yhat_sequence = sklearn_predict ( model , history , n_input )

# store the predictions

predictions . append ( yhat_sequence )

# get real observation and add to history for predicting the next week

history . append ( test [ i , : ] )

predictions = array ( predictions )

# evaluate predictions days for each week

score , scores = evaluate_forecasts ( test [ : , : , 0 ] , predictions )

return score , scores

# load the new file

dataset = read_csv ( 'household_power_consumption_days.csv' , header = 0 , infer_datetime_format = True , parse_dates = [ 'datetime' ] , index_col = [ 'datetime' ] )

# split into train and test

train , test = split_dataset ( dataset . values )

# prepare the models to evaluate

models = get_models ( )

n_input = 7

# evaluate each model

days = [ 'sun' , 'mon' , 'tue' , 'wed' , 'thr' , 'fri' , 'sat' ]

for name , model in models . items ( ) :

# evaluate and get scores

score , scores = evaluate_model ( model , train , test , n_input )

# summarize scores

summarize_scores ( name , score , scores )

# plot scores

pyplot . plot ( days , scores , marker = 'o' , label = name )

# show plot

pyplot . legend ( )"
136;machinelearningmastery.com;http://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/;2016-03-17;Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning;"Tweet Share Share

Last Updated on October 25, 2019

Supervised machine learning algorithms can best be understood through the lens of the bias-variance trade-off.

In this post, you will discover the Bias-Variance Trade-Off and how to use it to better understand machine learning algorithms and get better performance on your data.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Update Oct/2019: Removed discussion of parametric/nonparametric models (thanks Alex).

Overview of Bias and Variance

In supervised machine learning an algorithm learns a model from training data.

The goal of any supervised machine learning algorithm is to best estimate the mapping function (f) for the output variable (Y) given the input data (X). The mapping function is often called the target function because it is the function that a given supervised machine learning algorithm aims to approximate.

The prediction error for any machine learning algorithm can be broken down into three parts:

Bias Error

Variance Error

Irreducible Error

The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.

In this post, we will focus on the two parts we can influence with our machine learning algorithms. The bias error and the variance error.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Bias Error

Bias are the simplifying assumptions made by a model to make the target function easier to learn.

Generally, linear algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.

Low Bias : Suggests less assumptions about the form of the target function.

: Suggests less assumptions about the form of the target function. High-Bias: Suggests more assumptions about the form of the target function.

Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.

Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.

Variance Error

Variance is the amount that the estimate of the target function will change if different training data was used.

The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.

Machine learning algorithms that have a high variance are strongly influenced by the specifics of the training data. This means that the specifics of the training have influences the number and types of parameters used to characterize the mapping function.

Low Variance : Suggests small changes to the estimate of the target function with changes to the training dataset.

: Suggests small changes to the estimate of the target function with changes to the training dataset. High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.

Generally, nonlinear machine learning algorithms that have a lot of flexibility have a high variance. For example, decision trees have a high variance, that is even higher if the trees are not pruned before use.

Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.

Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.

Bias-Variance Trade-Off

The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance.

You can see a general trend in the examples above:

Linear machine learning algorithms often have a high bias but a low variance.

machine learning algorithms often have a high bias but a low variance. Nonlinear machine learning algorithms often have a low bias but a high variance.

The parameterization of machine learning algorithms is often a battle to balance out bias and variance.

Below are two examples of configuring the bias-variance trade-off for specific algorithms:

The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbors that contribute t the prediction and in turn increases the bias of the model.

The support vector machine algorithm has low bias and high variance, but the trade-off can be changed by increasing the C parameter that influences the number of violations of the margin allowed in the training data which increases the bias but decreases the variance.

There is no escaping the relationship between bias and variance in machine learning.

Increasing the bias will decrease the variance.

Increasing the variance will decrease the bias.

There is a trade-off at play between these two concerns and the algorithms you choose and the way you choose to configure them are finding different balances in this trade-off for your problem

In reality, we cannot calculate the real bias and variance error terms because we do not know the actual underlying target function. Nevertheless, as a framework, bias and variance provide the tools to understand the behavior of machine learning algorithms in the pursuit of predictive performance.

Further Reading

This section lists some recommend resources if you are looking to learn more about bias, variance and the bias-variance trade-off.

Summary

In this post, you discovered bias, variance and the bias-variance trade-off for machine learning algorithms.

You now know that:

Bias is the simplifying assumptions made by the model to make the target function easier to approximate.

Variance is the amount that the estimate of the target function will change given different training data.

Trade-off is tension between the error introduced by the bias and the variance.

Do you have any questions about bias, variance or the bias-variance trade-off. Leave a comment and ask your question and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
137;machinelearningmastery.com;http://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/;2016-08-25;How to Evaluate Gradient Boosting Models with XGBoost in Python;"# train-test split evaluation of xgboost model

from numpy import loadtxt

from xgboost import XGBClassifier

from sklearn . model_selection import train_test_split

from sklearn . metrics import accuracy_score

# load data

dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" )

# split data into X and y

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# split data into train and test sets

X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.33 , random_state = 7 )

# fit model no training data

model = XGBClassifier ( )

model . fit ( X_train , y_train )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )"
138;towardsdatascience.com;https://towardsdatascience.com/how-to-master-python-command-line-arguments-5d5ad4bcf985?source=collection_home---4------0-----------------------;2020-04-19;How to Master Python Command Line Arguments;"I believe most of us have run this command line to execute your python script.

$ python main.py

Can we do a little bit more like defining our own argument in this script? The answer is definitely yes!

$ python main.py arg1 arg2

We are going to use Python argparse module to configure command line arguments and options. The argparse module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.

Getting Started with Argparse

Installing Argparse

As usual, the first thing that we need to do is install this Python module.

conda install argparse

Defining Positional and Optional Arguments

Create a parser object with ArgumentParser with the description of this script. Positional Arguments and Optional Arguments are defined with add_argument function. A brief description of what the argument does is added with help .

Positional Arguments are arguments that need to be included in the proper position or order.

Optional Arguments are keyword arguments that input with a keyword and equals sign and they are optional.

Let’s try to run this script with help argument -h .

$ python employee.py -h

usage: employee.py [-h] [--address ADDRESS] name title This script is going to create an employee profile. positional arguments:

name Name of Employee

title Job Title of Employee optional arguments:

-h, --help show this help message and exit

--address ADDRESS Address of Employee

-h and --help are defined by default in argparse . It will show the descriptions that we have defined in the script to assist our user when they use the script.

Let’s try to input name and title .

$ python employee.py Alex Manager

Name : Alex

Job Title : Manager

Address : None

Due to the absence of address argument, NoneType is passed to Address in this script. We would have to convert it to string in order to print it.

Let’s try with name only

$ python employee.py Alex

usage: employee.py [-h] [--address ADDRESS] name title

employee.py: error: the following arguments are required: title

Because title is also Positional Argument , it is required in this script.

This time let’s try with name , title and address .

$ python employee.py Alex Manager --address 123 Baker Street

usage: employee.py [-h] [--address ADDRESS] name title

employee.py: error: unrecognized arguments: Baker Street

Because 123 Baker Street contains space, the script will treat Baker Street as other arguments. We would need double quotes at this case.

$ python employee.py Alex Manager --address ""123 Baker Street""

Name : Alex

Job Title : Manager

Address : 123 Baker Street

name and title would need double quotes if the name or title are more than one words.

Defining Boolean Arguments

Let’s add the above codes into the existing script. We are going to define an optional argument with default=True . It means that even we never input anything into this argument, it is equal to True by default.

type=strtobool is used here to ensure the input is converted to boolean data type. Else, it will be string data type when the script passed in the input. We also can define it as type=int if we need an integer argument.

%(default)s) in the help is to retrieve the default value in the argument. This is to make sure the description is not hard coded and flexible with the change of default value.

Let’s try again with name , title and address

$ python employee.py Alex Manager --address ""123 Baker Street""

Name : Alex

Job Title : Manager

Address : 123 Baker Street

Alex is a full time employee.

By default, isFullTime is True , so Alex is a full time employee if we never input anything argument to isFullTime .

Let’s try with a False input.

$ python employee.py Alex Manager --address ""123 Baker Street"" --isFullTime False

Name : Alex

Job Title : Manager

Address : 123 Baker Street

Alex is not a full time employee.

Ops! Alex is not a full time employee now.

Defining Choices in the Arguments

We also can limit the possible values for an input argument with choices argument. This is useful to prevent your user from inputing invalid value. For instances, we can restrict the country value to Singapore, United States and Malaysia only with this choices=[“Singapore”, “United States”, “Malaysia”] .

Let’s see what happen if we input the country value not in the choices.

$ python employee.py Alex Manager --country Japan

usage: employee.py [-h] [--address ADDRESS]

[--country {Singapore,United States,Malaysia}]

[--isFullTime ISFULLTIME]

name title

employee.py: error: argument --country: invalid choice: 'Japan' (choose from 'Singapore', 'United States', 'Malaysia')"
139;machinelearningmastery.com;http://machinelearningmastery.com/evaluate-performance-machine-learning-algorithms-python-using-resampling/;2016-05-22;Evaluate the Performance of Machine Learning Algorithms in Python using Resampling;"# Evaluate using a train and a test set

import pandas

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

test_size = 0.33

seed = 7

X_train , X_test , Y_train , Y_test = model_selection . train_test_split ( X , Y , test_size = test_size , random_state = seed )

model = LogisticRegression ( )

model . fit ( X_train , Y_train )

result = model . score ( X_test , Y_test )"
140;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-an-intuition-skewed-class-distributions/;2019-12-26;Develop an Intuition for Severely Skewed Class Distributions;"# create and plot synthetic dataset with a given class distribution

from numpy import unique

from numpy import hstack

from numpy import vstack

from numpy import where

from matplotlib import pyplot

from sklearn . datasets import make_blobs

# create a dataset with a given class distribution

def get_dataset ( proportions ) :

# determine the number of classes

n_classes = len ( proportions )

# determine the number of examples to generate for each class

largest = max ( [ v for k , v in proportions . items ( ) ] )

n_samples = largest * n_classes

# create dataset

X , y = make_blobs ( n_samples = n_samples , centers = n_classes , n_features = 2 , random_state = 1 , cluster_std = 3 )

# collect the examples

X_list , y_list = list ( ) , list ( )

for k , v in proportions . items ( ) :

row_ix = where ( y == k ) [ 0 ]

selected = row_ix [ : v ]

X_list . append ( X [ selected , : ] )

y_list . append ( y [ selected ] )

return vstack ( X_list ) , hstack ( y_list )

# scatter plot of dataset, different color for each class

def plot_dataset ( X , y ) :

# create scatter plot for samples from each class

n_classes = len ( unique ( y ) )

for class_value in range ( n_classes ) :

# get row indexes for samples with this class

row_ix = where ( y == class_value ) [ 0 ]

# create scatter of these samples

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( class_value ) )

# show a legend

pyplot . legend ( )

# show the plot

pyplot . show ( )

# define the class distribution

proportions = { 0 : 10000 , 1 : 10 }

# generate dataset

X , y = get_dataset ( proportions )

# plot dataset"
141;news.mit.edu;http://news.mit.edu/2020/qa-markus-buehler-setting-coronavirus-and-ai-inspired-proteins-to-music-0402;;Q&A: Markus Buehler on setting coronavirus and AI-inspired proteins to music;"The proteins that make up all living things are alive with music. Just ask Markus Buehler: The musician and MIT professor develops artificial intelligence models to design new proteins, sometimes by translating them into sound. His goal is to create new biological materials for sustainable, non-toxic applications. In a project with the MIT-IBM Watson AI Lab, Buehler is searching for a protein to extend the shelf-life of perishable food. In a new study in Extreme Mechanics Letters, he and his colleagues offer a promising candidate: a silk protein made by honeybees for use in hive building.

In another recent study, in APL Bioengineering, he went a step further and used AI discover an entirely new protein. As both studies went to print, the Covid-19 outbreak was surging in the United States, and Buehler turned his attention to the spike protein of SARS-CoV-2, the appendage that makes the novel coronavirus so contagious. He and his colleagues are trying to unpack its vibrational properties through molecular-based sound spectra, which could hold one key to stopping the virus. Buehler recently sat down to discuss the art and science of his work.

Q: Your work focuses on the alpha helix proteins found in skin and hair. Why makes this protein so intriguing?

A: Proteins are the bricks and mortar that make up our cells, organs, and body. Alpha helix proteins are especially important. Their spring-like structure gives them elasticity and resilience, which is why skin, hair, feathers, hooves, and even cell membranes are so durable. But they’re not just tough mechanically, they have built-in antimicrobial properties. With IBM, we’re trying to harness this biochemical trait to create a protein coating that can slow the spoilage of quick-to-rot foods like strawberries.

Q: How did you enlist AI to produce this silk protein?

A: We trained a deep learning model on the Protein Data Bank, which contains the amino acid sequences and three-dimensional shapes of about 120,000 proteins. We then fed the model a snippet of an amino acid chain for honeybee silk and asked it to predict the protein’s shape, atom-by-atom. We validated our work by synthesizing the protein for the first time in a lab — a first step toward developing a thin antimicrobial, structurally-durable coating that can be applied to food. My colleague, Benedetto Marelli, specializes in this part of the process. We also used the platform to predict the structure of proteins that don’t yet exist in nature. That’s how we designed our entirely new protein in the APL Bioengineering study.

Q: How does your model improve on other protein prediction methods?

A: We use end-to-end prediction. The model builds the protein’s structure directly from its sequence, translating amino acid patterns into three-dimensional geometries. It’s like translating a set of IKEA instructions into a built bookshelf, minus the frustration. Through this approach, the model effectively learns how to build a protein from the protein itself, via the language of its amino acids. Remarkably, our method can accurately predict protein structure without a template. It outperforms other folding methods and is significantly faster than physics-based modeling. Because the Protein Data Bank is limited to proteins found in nature, we needed a way to visualize new structures to make new proteins from scratch.

Q: How could the model be used to design an actual protein?

A: We can build atom-by-atom models for sequences found in nature that haven’t yet been studied, as we did in the APL Bioengineering study using a different method. We can visualize the protein’s structure and use other computational methods to assess its function by analyzing its stablity and the other proteins it binds to in cells. Our model could be used in drug design or to interfere with protein-mediated biochemical pathways in infectious disease.

Q: What’s the benefit of translating proteins into sound?

A: Our brains are great at processing sound! In one sweep, our ears pick up all of its hierarchical features: pitch, timbre, volume, melody, rhythm, and chords. We would need a high-powered microscope to see the equivalent detail in an image, and we could never see it all at once. Sound is such an elegant way to access the information stored in a protein.

Typically, sound is made from vibrating a material, like a guitar string, and music is made by arranging sounds in hierarchical patterns. With AI we can combine these concepts, and use molecular vibrations and neural networks to construct new musical forms. We’ve been working on methods to turn protein structures into audible representations, and translate these representations into new materials.

Q: What can the sonification of SARS-CoV-2's ""spike"" protein tell us?

A: Its protein spike contains three protein chains folded into an intriguing pattern. These structures are too small for the eye to see, but they can be heard. We represented the physical protein structure, with its entangled chains, as interwoven melodies that form a multi-layered composition. The spike protein’s amino acid sequence, its secondary structure patterns, and its intricate three-dimensional folds are all featured. The resulting piece is a form of counterpoint music, in which notes are played against notes. Like a symphony, the musical patterns reflect the protein’s intersecting geometry realized by materializing its DNA code.

Q: What did you learn?

A: The virus has an uncanny ability to deceive and exploit the host for its own multiplication. Its genome hijacks the host cell’s protein manufacturing machinery, and forces it to replicate the viral genome and produce viral proteins to make new viruses. As you listen, you may be surprised by the pleasant, even relaxing, tone of the music. But it tricks our ear in the same way the virus tricks our cells. It’s an invader disguised as a friendly visitor. Through music, we can see the SARS-CoV-2 spike from a new angle, and appreciate the urgent need to learn the language of proteins.

Q: Can any of this address Covid-19, and the virus that causes it?

A: In the longer term, yes. Translating proteins into sound gives scientists another tool to understand and design proteins. Even a small mutation can limit or enhance the pathogenic power of SARS-CoV-2. Through sonification, we can also compare the biochemical processes of its spike protein with previous coronaviruses, like SARS or MERS.

In the music we created, we analyzed the vibrational structure of the spike protein that infects the host. Understanding these vibrational patterns is critical for drug design and much more. Vibrations may change as temperatures warm, for example, and they may also tell us why the SARS-CoV-2 spike gravitates toward human cells more than other viruses. We’re exploring these questions in current, ongoing research with my graduate students.

We might also use a compositional approach to design drugs to attack the virus. We could search for a new protein that matches the melody and rhythm of an antibody capable of binding to the spike protein, interfering with its ability to infect.

Q: How can music aid protein design?

A: You can think of music as an algorithmic reflection of structure. Bach’s Goldberg Variations, for example, are a brilliant realization of counterpoint, a principle we’ve also found in proteins. We can now hear this concept as nature composed it, and compare it to ideas in our imagination, or use AI to speak the language of protein design and let it imagine new structures. We believe that the analysis of sound and music can help us understand the material world better. Artistic expression is, after all, just a model of the world within us and around us.

Co-authors of the study in Extreme Mechanics Letters are: Zhao Qin, Hui Sun, Eugene Lim and Benedetto Marelli at MIT; and Lingfei Wu, Siyu Huo, Tengfei Ma and Pin-Yu Chen at IBM Research. Co-author of the study in APL Bioengineering is Chi-Hua Yu. Buehler’s sonification work is supported by MIT’s Center for Art, Science and Technology (CAST) and the Mellon Foundation."
142;news.mit.edu;http://news.mit.edu/2014/mit-cheetah-robot-runs-jumps-0915;;Bound for robotic glory;"Speed and agility are hallmarks of the cheetah: The big predator is the fastest land animal on Earth, able to accelerate to 60 mph in just a few seconds. As it ramps up to top speed, a cheetah pumps its legs in tandem, bounding until it reaches a full gallop.

Now MIT researchers have developed an algorithm for bounding that they’ve successfully implemented in a robotic cheetah — a sleek, four-legged assemblage of gears, batteries, and electric motors that weighs about as much as its feline counterpart. The team recently took the robot for a test run on MIT’s Killian Court, where it bounded across the grass at a steady clip.

In experiments on an indoor track, the robot sprinted up to 10 mph, even continuing to run after clearing a hurdle. The MIT researchers estimate that the current version of the robot may eventually reach speeds of up to 30 mph.

The key to the bounding algorithm is in programming each of the robot’s legs to exert a certain amount of force in the split second during which it hits the ground, in order to maintain a given speed: In general, the faster the desired speed, the more force must be applied to propel the robot forward. Sangbae Kim, an associate professor of mechanical engineering at MIT, hypothesizes that this force-control approach to robotic running is similar, in principle, to the way world-class sprinters race.

“Many sprinters, like Usain Bolt, don’t cycle their legs really fast,” Kim says. “They actually increase their stride length by pushing downward harder and increasing their ground force, so they can fly more while keeping the same frequency.”

Kim says that by adapting a force-based approach, the cheetah-bot is able to handle rougher terrain, such as bounding across a grassy field. In treadmill experiments, the team found that the robot handled slight bumps in its path, maintaining its speed even as it ran over a foam obstacle.

“Most robots are sluggish and heavy, and thus they cannot control force in high-speed situations,” Kim says. “That’s what makes the MIT cheetah so special: You can actually control the force profile for a very short period of time, followed by a hefty impact with the ground, which makes it more stable, agile, and dynamic.”

Kim says what makes the robot so dynamic is a custom-designed, high-torque-density electric motor, designed by Jeffrey Lang, the Vitesse Professor of Electrical Engineering at MIT. These motors are controlled by amplifiers designed by David Otten, a principal research engineer in MIT’s Research Laboratory of Electronics. The combination of such special electric motors and custom-designed, bio-inspired legs allow force control on the ground without relying on delicate force sensors on the feet.

Kim and his colleagues — research scientist Hae-Won Park and graduate student Meng Yee Chuah — will present details of the bounding algorithm this month at the IEEE/RSJ International Conference on Intelligent Robots and Systems in Chicago.

Toward the ultimate gait

The act of running can be parsed into a number of biomechanically distinct gaits, from trotting and cantering to more dynamic bounding and galloping. In bounding, an animal’s front legs hit the ground together, followed by its hind legs, similar to the way that rabbits hop — a relatively simple gait that the researchers chose to model first.

“Bounding is like an entry-level high-speed gait, and galloping is the ultimate gait,” Kim says. “Once you get bounding, you can easily split the two legs and get galloping.”

As an animal bounds, its legs touch the ground for a fraction of a second before cycling through the air again. The percentage of time a leg spends on the ground rather than in the air is referred to in biomechanics as a “duty cycle”; the faster an animal runs, the shorter its duty cycle.

Kim and his colleagues developed an algorithm that determines the amount of force a leg should exert in the short period of each cycle that it spends on the ground. That force, they reasoned, should be enough for the robot to push up against the downward force of gravity, in order to maintain forward momentum.

“Once I know how long my leg is on the ground and how long my body is in the air, I know how much force I need to apply to compensate for the gravitational force,” Kim says. “Now we’re able to control bounding at many speeds. And to jump, we can, say, triple the force, and it jumps over obstacles.”

In experiments, the team ran the robot at progressively smaller duty cycles, finding that, following the algorithm’s force prescriptions, the robot was able to run at higher speeds without falling. Kim says the team’s algorithm enables precise control over the forces a robot can exert while running.

By contrast, he says, similar quadruped robots may exert high force, but with poor efficiency. What’s more, such robots run on gasoline and are powered by a gasoline engine, in order to generate high forces.

“As a result, they’re way louder,” Kim says. “Our robot can be silent and as efficient as animals. The only things you hear are the feet hitting the ground. This is kind of a new paradigm where we’re controlling force in a highly dynamic situation. Any legged robot should be able to do this in the future.”

This work was supported by the Defense Advanced Research Projects Agency."
143;news.mit.edu;http://news.mit.edu/2020/safe-paths-privacy-first-approach-contact-tracing-0410;;Safe Paths: A privacy-first approach to contact tracing;"The research described in this article has been published on a preprint server but has not yet been peer-reviewed by scientific or medical experts.

Fast containment is key to halting the progression of pandemics, and rapid determination of a diagnosed patient’s locations and contact history is a vital step for communities and cities. This process is labor-intensive, susceptible to human memory errors, and fraught with privacy concerns.

Smartphones can aid in this process, though any type of mass surveillance network and analytics can lead to — or be misused by — a surveillance state.

Early contact-tracing tools deployed in certain countries against the current Covid-19 pandemic have indeed helped slow the spread, but have done so at the expense of the privacy of citizens and businesses, exposing even the most private details about individuals.

To help address this urgent challenge, a team led by MIT Media Lab Associate Professor Ramesh Raskar is designing and developing Safe Paths, a citizen-centric, open source, privacy-first set of digital tools and platforms to help stem the spread of Covid-19.

The Safe Paths project is a multi-faculty, cross-MIT effort, with input and expertise from institutes including Harvard University, Stanford University, and the State University of New York at Buffalo; clinical input from Mayo Clinic and Massachusetts General Hospital; and mentors from the World Health Organization, the U.S. Department of Health and Human Services, and the Graduate Institute of International and Development Studies.

A number of leaders and personnel from the global company EY are volunteering their time across many disciplines, including strategy and inclusion on the core initiative leadership team. Numerous additional companies are also participating in this way, including TripleBlind, Public Consulting Group, and Earned Media Consultants.

Experts from government agencies and academic institutes in Canada, Germany, India, Italy, the United Kingdom, and Vietnam are also helping to guide the platform’s development.

The Safe Paths platform, currently in beta, comprises both a smartphone application, PrivateKit, and a web application, Safe Places. The PrivateKit app will enable users to match the personal diary of location data on their smartphone with anonymized, redacted, and blurred location history of infected patients. The digital contact tracing uses overlapped GPS and Bluetooth trails that allow an individual to check if they have crossed paths with someone who was later diagnosed positive for the virus. The PACT Bluetooth protocol, announced earlier by MIT, will be available through Safe Paths. The design of the PACT system has benefited from Safe Paths early work in this area. Through Safe Places, public health officials are equipped to redact location trails of diagnosed carriers and thus broadcast location information with privacy protection for both diagnosed patients and for local businesses.

The platform takes a fundamentally different approach to app-based epidemic analytics, and in the future will use techniques based on Split Learning, research that Raskar’s Camera Culture group at the Media Lab has been developing for the past several years, and which enables distributed deep learning without the sharing of raw data. Safe Paths uses either on-device calculation or encrypted trail match. The Safe Paths platform provides users information on whether they have experienced a near-contact with a diagnosed individual, while maintaining the privacy of both the user and the diagnosed patient. Users long their location history privately on their own phone and remain in control of their data. Diagnosed patients can opt to provide their location history to health officials (providing similar, yet much more accurate, information to the current healthcare intake interviews).

Safe Places also provides a secure tool for public health officials to make infected patient contact history much more efficient, and enables anonymized and safe sharing of patient location history. In the future, this data will also be encrypted.

In the white paper, ""Apps Gone Rogue: Maintaining Personal Privacy in an Epidemic,"" the research team describes the application of contact tracing to slow the spread of epidemics; outlines the current landscape of interventions; and details challenges and risks to data security and privacy protection. Ongoing and collaborative research designed to further explore critical aspects of contact tracing, and to test increasingly robust privacy protection methodologies. Findings will be continuously shared and published.

“We are dedicated to privacy-first solutions — user location and contact history should never leave a user’s phone without direct consent,” Raskar says. “We strongly believe that all users should be in control of their own data, and that we should never need to sacrifice consent for Covid-19 safety.”

Zelalem Temesgen, an infectious disease specialist at Mayo Clinic who has contributed clinical expertise to the project, emphasizes the vital role of contact tracing in stemming an epidemic.

“In conjunction with rapid diagnosis and isolation of suspected or confirmed cases, contact tracing is a critical intervention for controlling outbreaks of infectious diseases,” Temesgen states. “In the best of times, contact tracing is a laborious and difficult task; it becomes even more challenging in situations where individuals without symptoms are able to transmit infection to others.”

Temesgen notes that having tools to enhance contact tracing capabilities by more efficiently, and accurately identifying locations where transmission may have occurred will empower public health officials to intervene expeditiously and offer testing to those who need it, and initiate other measures such as isolation and treatment.

“In situations like we are facing now, where our knowledge about this new infection is incomplete and continuously evolving, having accurate and comprehensive contact tracing capability can also provide crucial information about how the infection is spread,” he adds.

According to Ronald Rivest, Institute Professor at MIT and inventor of the RSA public-key cryptosystem, contact tracing has proven to be an important and effective tool in fighting pandemics. “It’s fortunately possible to achieve good contact tracing using smartphones, which can detect the presence of other nearby smartphones,” Rivest notes. “Furthermore, such contact tracing can be done quite simply in a privacy-preserving manner — one doesn’t need to implement ‘big brother’ systems that hand over everyone’s location history to a big database somewhere. I believe that we can see such systems implemented and fielded quickly.”

MIT Assistant Professor Kevin Esvelt, an evolutionary engineer who specializes in mitigating global catastrophic bio-risks, notes that automated contact tracing becomes increasingly effective as more people adopt it. “Safe Paths uses anonymized GPS, which improves upon traditional contact tracing for everyone using it, as well as Bluetooth, which can only anonymously log an interaction if both people have it. In the long run, it would be best to integrate these capabilities into the operating system of every smartphone as a defense against all pandemics, with each user freely deciding whether or not to share their local data when they learn they’re infected.”

“Until that day,” Esvelt adds, “a statewide emergency message with a download link — or prominent placement by the big tech companies — is likely the best we can do.”

The initial phases of the PrivateKit mobile application and Safe Places web application rollout will emphasize rapid iteration and deployment of solutions for privacy-first, pandemic contact tracing. In the later phases, the goal is the building of encrypted computational methods that can be useful in future types of outbreaks."
144;machinelearningmastery.com;https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/;2018-09-20;How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition;"# multi-headed cnn model

from numpy import mean

from numpy import std

from numpy import dstack

from pandas import read_csv

from matplotlib import pyplot

from keras . utils import to_categorical

from keras . utils . vis_utils import plot_model

from keras . models import Model

from keras . layers import Input

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import Dropout

from keras . layers . convolutional import Conv1D

from keras . layers . convolutional import MaxPooling1D

from keras . layers . merge import concatenate

# load a single file as a numpy array

def load_file ( filepath ) :

dataframe = read_csv ( filepath , header = None , delim_whitespace = True )

return dataframe . values

# load a list of files and return as a 3d numpy array

def load_group ( filenames , prefix = '' ) :

loaded = list ( )

for name in filenames :

data = load_file ( prefix + name )

loaded . append ( data )

# stack group so that features are the 3rd dimension

loaded = dstack ( loaded )

return loaded

# load a dataset group, such as train or test

def load_dataset_group ( group , prefix = '' ) :

filepath = prefix + group + '/Inertial Signals/'

# load all 9 files as a single array

filenames = list ( )

# total acceleration

filenames += [ 'total_acc_x_' + group + '.txt' , 'total_acc_y_' + group + '.txt' , 'total_acc_z_' + group + '.txt' ]

# body acceleration

filenames += [ 'body_acc_x_' + group + '.txt' , 'body_acc_y_' + group + '.txt' , 'body_acc_z_' + group + '.txt' ]

# body gyroscope

filenames += [ 'body_gyro_x_' + group + '.txt' , 'body_gyro_y_' + group + '.txt' , 'body_gyro_z_' + group + '.txt' ]

# load input data

X = load_group ( filenames , filepath )

# load class output

y = load_file ( prefix + group + '/y_' + group + '.txt' )

return X , y

# load the dataset, returns train and test X and y elements

def load_dataset ( prefix = '' ) :

# load all train

trainX , trainy = load_dataset_group ( 'train' , prefix + 'HARDataset/' )

print ( trainX . shape , trainy . shape )

# load all test

testX , testy = load_dataset_group ( 'test' , prefix + 'HARDataset/' )

print ( testX . shape , testy . shape )

# zero-offset class values

trainy = trainy - 1

testy = testy - 1

# one hot encode y

trainy = to_categorical ( trainy )

testy = to_categorical ( testy )

print ( trainX . shape , trainy . shape , testX . shape , testy . shape )

return trainX , trainy , testX , testy

# fit and evaluate a model

def evaluate_model ( trainX , trainy , testX , testy ) :

verbose , epochs , batch_size = 0 , 10 , 32

n_timesteps , n_features , n_outputs = trainX . shape [ 1 ] , trainX . shape [ 2 ] , trainy . shape [ 1 ]

# head 1

inputs1 = Input ( shape = ( n_timesteps , n_features ) )

conv1 = Conv1D ( filters = 64 , kernel_size = 3 , activation = 'relu' ) ( inputs1 )

drop1 = Dropout ( 0.5 ) ( conv1 )

pool1 = MaxPooling1D ( pool_size = 2 ) ( drop1 )

flat1 = Flatten ( ) ( pool1 )

# head 2

inputs2 = Input ( shape = ( n_timesteps , n_features ) )

conv2 = Conv1D ( filters = 64 , kernel_size = 5 , activation = 'relu' ) ( inputs2 )

drop2 = Dropout ( 0.5 ) ( conv2 )

pool2 = MaxPooling1D ( pool_size = 2 ) ( drop2 )

flat2 = Flatten ( ) ( pool2 )

# head 3

inputs3 = Input ( shape = ( n_timesteps , n_features ) )

conv3 = Conv1D ( filters = 64 , kernel_size = 11 , activation = 'relu' ) ( inputs3 )

drop3 = Dropout ( 0.5 ) ( conv3 )

pool3 = MaxPooling1D ( pool_size = 2 ) ( drop3 )

flat3 = Flatten ( ) ( pool3 )

# merge

merged = concatenate ( [ flat1 , flat2 , flat3 ] )

# interpretation

dense1 = Dense ( 100 , activation = 'relu' ) ( merged )

outputs = Dense ( n_outputs , activation = 'softmax' ) ( dense1 )

model = Model ( inputs = [ inputs1 , inputs2 , inputs3 ] , outputs = outputs )

# save a plot of the model

plot_model ( model , show_shapes = True , to_file = 'multichannel.png' )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( [ trainX , trainX , trainX ] , trainy , epochs = epochs , batch_size = batch_size , verbose = verbose )

# evaluate model

_ , accuracy = model . evaluate ( [ testX , testX , testX ] , testy , batch_size = batch_size , verbose = 0 )

return accuracy

# summarize scores

def summarize_results ( scores ) :

print ( scores )

m , s = mean ( scores ) , std ( scores )

print ( 'Accuracy: %.3f%% (+/-%.3f)' % ( m , s ) )

# run an experiment

def run_experiment ( repeats = 10 ) :

# load data

trainX , trainy , testX , testy = load_dataset ( )

# repeat experiment

scores = list ( )

for r in range ( repeats ) :

score = evaluate_model ( trainX , trainy , testX , testy )

score = score * 100.0

print ( '>#%d: %.3f' % ( r + 1 , score ) )

scores . append ( score )

# summarize results

summarize_results ( scores )

# run the experiment"
145;machinelearningmastery.com;http://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/;2016-05-24;Metrics To Evaluate Machine Learning Algorithms in Python;"# Cross Validation Classification Accuracy

import pandas

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

seed = 7

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

model = LogisticRegression ( )

scoring = 'accuracy'

results = model_selection . cross_val_score ( model , X , Y , cv = kfold , scoring = scoring )"
146;news.mit.edu;http://news.mit.edu/2020/machine-learning-picks-out-hidden-vibrations-earthquake-data-0228;;Machine learning picks out hidden vibrations from earthquake data;"Over the last century, scientists have developed methods to map the structures within the Earth’s crust, in order to identify resources such as oil reserves, geothermal sources, and, more recently, reservoirs where excess carbon dioxide could potentially be sequestered. They do so by tracking seismic waves that are produced naturally by earthquakes or artificially via explosives or underwater air guns. The way these waves bounce and scatter through the Earth can give scientists an idea of the type of structures that lie beneath the surface.

There is a narrow range of seismic waves — those that occur at low frequencies of around 1 hertz — that could give scientists the clearest picture of underground structures spanning wide distances. But these waves are often drowned out by Earth’s noisy seismic hum, and are therefore difficult to pick up with current detectors. Specifically generating low-frequency waves would require pumping in enormous amounts of energy. For these reasons, low-frequency seismic waves have largely gone missing in human-generated seismic data.

Now MIT researchers have come up with a machine learning workaround to fill in this gap.

In a paper appearing in the journal Geophysics, they describe a method in which they trained a neural network on hundreds of different simulated earthquakes. When the researchers presented the trained network with only the high-frequency seismic waves produced from a new simulated earthquake, the neural network was able to imitate the physics of wave propagation and accurately estimate the quake’s missing low-frequency waves.

The new method could allow researchers to artificially synthesize the low-frequency waves that are hidden in seismic data, which can then be used to more accurately map the Earth’s internal structures.

“The ultimate dream is to be able to map the whole subsurface, and be able to say, for instance, ‘this is exactly what it looks like underneath Iceland, so now you know where to explore for geothermal sources,’” says co-author Laurent Demanet, professor of applied mathematics at MIT. “Now we’ve shown that deep learning offers a solution to be able to fill in these missing frequencies.”

Demanet’s co-author is lead author Hongyu Sun, a graduate student in MIT’s Department of Earth, Atmospheric and Planetary Sciences.

Speaking another frequency

A neural network is a set of algorithms modeled loosely after the neural workings of the human brain. The algorithms are designed to recognize patterns in data that are fed into the network, and to cluster these data into categories, or labels. A common example of a neural network involves visual processing; the model is trained to classify an image as either a cat or a dog, based on the patterns it recognizes between thousands of images that are specifically labeled as cats, dogs, and other objects.

Sun and Demanet adapted a neural network for signal processing, specifically, to recognize patterns in seismic data. They reasoned that if a neural network was fed enough examples of earthquakes, and the ways in which the resulting high- and low-frequency seismic waves travel through a particular composition of the Earth, the network should be able to, as they write in their paper, “mine the hidden correlations among different frequency components” and extrapolate any missing frequencies if the network were only given an earthquake’s partial seismic profile.

The researchers looked to train a convolutional neural network, or CNN, a class of deep neural networks that is often used to analyze visual information. A CNN very generally consists of an input and output layer, and multiple hidden layers between, that process inputs to identify correlations between them.

Among their many applications, CNNs have been used as a means of generating visual or auditory “deepfakes” — content that has been extrapolated or manipulated through deep-learning and neural networks, to make it seem, for example, as if a woman were talking with a man’s voice.

“If a network has seen enough examples of how to take a male voice and transform it into a female voice or vice versa, you can create a sophisticated box to do that,” Demanet says. “Whereas here we make the Earth speak another frequency — one that didn’t originally go through it.”

Tracking waves

The researchers trained their neural network with inputs that they generated using the Marmousi model, a complex two-dimensional geophysical model that simulates the way seismic waves travel through geological structures of varying density and composition.

In their study, the team used the model to simulate nine “virtual Earths,” each with a different subsurface composition. For each Earth model, they simulated 30 different earthquakes, all with the same strength, but different starting locations. In total, the researchers generated hundreds of different seismic scenarios. They fed the information from almost all of these simulations into their neural network and let the network find correlations between seismic signals.

After the training session, the team introduced to the neural network a new earthquake that they simulated in the Earth model but did not include in the original training data. They only included the high-frequency part of the earthquake’s seismic activity, in hopes that the neural network learned enough from the training data to be able to infer the missing low-frequency signals from the new input.

They found that the neural network produced the same low-frequency values that the Marmousi model originally simulated.

“The results are fairly good,” Demanet says. “It’s impressive to see how far the network can extrapolate to the missing frequencies.”

As with all neural networks, the method has its limitations. Specifically, the neural network is only as good as the data that are fed into it. If a new input is wildly different from the bulk of a network’s training data, there’s no guarantee that the output will be accurate. To contend with this limitation, the researchers say they plan to introduce a wider variety of data to the neural network, such as earthquakes of different strengths, as well as subsurfaces of more varied composition.

As they improve the neural network’s predictions, the team hopes to be able to use the method to extrapolate low-frequency signals from actual seismic data, which can then be plugged into seismic models to more accurately map the geological structures below the Earth’s surface. The low frequencies, in particular, are a key ingredient for solving the big puzzle of finding the correct physical model.

“Using this neural network will help us find the missing frequencies to ultimately improve the subsurface image and find the composition of the Earth,” Demanet says.

This research was supported, in part, by Total SA and the U.S. Air Force Office of Scientific Research."
147;machinelearningmastery.com;https://machinelearningmastery.com/better-deep-learning-neural-networks-crash-course/;2019-02-17;How to Get Better Deep Learning Results (7-Day Mini-Course);"# example of batch gradient descent

from sklearn . datasets import make_circles

from keras . layers import Dense

from keras . models import Sequential

from keras . optimizers import SGD

from matplotlib import pyplot

# generate dataset

X , y = make_circles ( n_samples = 1000 , noise = 0.1 , random_state = 1 )

# split into train and test

n_train = 500

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# define model

model = Sequential ( )

model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = SGD ( lr = 0.01 , momentum = 0.9 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

# fit model

history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 1000 , batch_size = len ( trainX ) , verbose = 0 )

# evaluate the model

_ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 )

_ , test_acc = model . evaluate ( testX , testy , verbose = 0 )

print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) )

# plot loss learning curves

pyplot . subplot ( 211 )

pyplot . title ( 'Cross-Entropy Loss' , pad = - 40 )

pyplot . plot ( history . history [ 'loss' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' )

pyplot . legend ( )

# plot accuracy learning curves

pyplot . subplot ( 212 )

pyplot . title ( 'Accuracy' , pad = - 40 )

pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' )

pyplot . legend ( )"
148;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-baseline-forecasts-for-multi-site-multivariate-air-pollution-time-series-forecasting/;2018-10-14;How to Develop Baseline Forecasts for Multi-Site Multivariate Air Pollution Time Series Forecasting;"# forecast local median per hour of day

from numpy import loadtxt

from numpy import nan

from numpy import isnan

from numpy import unique

from numpy import array

from numpy import nanmedian

from matplotlib import pyplot

# split the dataset by 'chunkID', return a list of chunks

def to_chunks ( values , chunk_ix = 0 ) :

chunks = list ( )

# get the unique chunk ids

chunk_ids = unique ( values [ : , chunk_ix ] )

# group rows by chunk id

for chunk_id in chunk_ids :

selection = values [ : , chunk_ix ] == chunk_id

chunks . append ( values [ selection , : ] )

return chunks

# return a list of relative forecast lead times

def get_lead_times ( ) :

return [ 1 , 2 , 3 , 4 , 5 , 10 , 17 , 24 , 48 , 72 ]

# forecast all lead times for one variable

def forecast_variable ( train_chunks , chunk_train , chunk_test , lead_times , target_ix ) :

forecast = list ( )

# convert target number into column number

col_ix = 3 + target_ix

# enumerate lead times

for i in range ( len ( lead_times ) ) :

# get the hour for this forecast lead time

hour = chunk_test [ i , 2 ]

# check for no test data

if isnan ( hour ) :

forecast . append ( nan )

continue

# select rows in chunk with this hour

selected = chunk_train [ chunk_train [ : , 2 ] == hour ]

# calculate the central tendency for target

value = nanmedian ( selected [ : , col_ix ] )

forecast . append ( value )

return forecast

# forecast for each chunk, returns [chunk][variable][time]

def forecast_chunks ( train_chunks , test_input ) :

lead_times = get_lead_times ( )

predictions = list ( )

# enumerate chunks to forecast

for i in range ( len ( train_chunks ) ) :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 39 ) :

yhat = forecast_variable ( train_chunks , train_chunks [ i ] , test_input [ i ] , lead_times , j )

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# convert the test dataset in chunks to [chunk][variable][time] format

def prepare_test_forecasts ( test_chunks ) :

predictions = list ( )

# enumerate chunks to forecast

for rows in test_chunks :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 3 , rows . shape [ 1 ] ) :

yhat = rows [ : , j ]

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# calculate the error between an actual and predicted value

def calculate_error ( actual , predicted ) :

# give the full actual value if predicted is nan

if isnan ( predicted ) :

return abs ( actual )

# calculate abs difference

return abs ( actual - predicted )

# evaluate a forecast in the format [chunk][variable][time]

def evaluate_forecasts ( predictions , testset ) :

lead_times = get_lead_times ( )

total_mae , times_mae = 0.0 , [ 0.0 for _ in range ( len ( lead_times ) ) ]

total_c , times_c = 0 , [ 0 for _ in range ( len ( lead_times ) ) ]

# enumerate test chunks

for i in range ( len ( test_chunks ) ) :

# convert to forecasts

actual = testset [ i ]

predicted = predictions [ i ]

# enumerate target variables

for j in range ( predicted . shape [ 0 ] ) :

# enumerate lead times

for k in range ( len ( lead_times ) ) :

# skip if actual in nan

if isnan ( actual [ j , k ] ) :

continue

# calculate error

error = calculate_error ( actual [ j , k ] , predicted [ j , k ] )

# update statistics

total_mae += error

times_mae [ k ] += error

total_c += 1

times_c [ k ] += 1

# normalize summed absolute errors

total_mae /= total_c

times_mae = [ times_mae [ i ] / times_c [ i ] for i in range ( len ( times_mae ) ) ]

return total_mae , times_mae

# summarize scores

def summarize_error ( name , total_mae , times_mae ) :

# print summary

lead_times = get_lead_times ( )

formatted = [ '+%d %.3f' % ( lead_times [ i ] , times_mae [ i ] ) for i in range ( len ( lead_times ) ) ]

s_scores = ', ' . join ( formatted )

print ( '%s: [%.3f MAE] %s' % ( name , total_mae , s_scores ) )

# plot summary

pyplot . plot ( [ str ( x ) for x in lead_times ] , times_mae , marker = '.' )

pyplot . show ( )

# load dataset

train = loadtxt ( 'AirQualityPrediction/naive_train.csv' , delimiter = ',' )

test = loadtxt ( 'AirQualityPrediction/naive_test.csv' , delimiter = ',' )

# group data by chunks

train_chunks = to_chunks ( train )

test_chunks = to_chunks ( test )

# forecast

test_input = [ rows [ : , : 3 ] for rows in test_chunks ]

forecast = forecast_chunks ( train_chunks , test_input )

# evaluate forecast

actual = prepare_test_forecasts ( test_chunks )

total_mae , times_mae = evaluate_forecasts ( forecast , actual )

# summarize forecast"
149;machinelearningmastery.com;http://machinelearningmastery.com/applied-deep-learning-in-python-mini-course/;2016-07-05;Applied Deep Learning in Python Mini-Course;"from keras . models import Sequential

from keras . layers import Dense

# Load the dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# Define and Compile

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# Fit the model

model . fit ( X , Y , epochs = 150 , batch_size = 10 )

# Evaluate the model

scores = model . evaluate ( X , Y )"
150;machinelearningmastery.com;https://machinelearningmastery.com/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python/;2018-10-21;How to Grid Search Triple Exponential Smoothing for Time Series Forecasting in Python;"# grid search ets models for monthly car sales

from math import sqrt

from multiprocessing import cpu_count

from joblib import Parallel

from joblib import delayed

from warnings import catch_warnings

from warnings import filterwarnings

from statsmodels . tsa . holtwinters import ExponentialSmoothing

from sklearn . metrics import mean_squared_error

from pandas import read_csv

from numpy import array

# one-step Holt Winter’s Exponential Smoothing forecast

def exp_smoothing_forecast ( history , config ) :

t , d , s , p , b , r = config

# define model

history = array ( history )

model = ExponentialSmoothing ( history , trend = t , damped = d , seasonal = s , seasonal_periods = p )

# fit model

model_fit = model . fit ( optimized = True , use_boxcox = b , remove_bias = r )

# make one step forecast

yhat = model_fit . predict ( len ( history ) , len ( history ) )

return yhat [ 0 ]

# root mean squared error or rmse

def measure_rmse ( actual , predicted ) :

return sqrt ( mean_squared_error ( actual , predicted ) )

# split a univariate dataset into train/test sets

def train_test_split ( data , n_test ) :

return data [ : - n_test ] , data [ - n_test : ]

# walk-forward validation for univariate data

def walk_forward_validation ( data , n_test , cfg ) :

predictions = list ( )

# split dataset

train , test = train_test_split ( data , n_test )

# seed history with training dataset

history = [ x for x in train ]

# step over each time-step in the test set

for i in range ( len ( test ) ) :

# fit model and make forecast for history

yhat = exp_smoothing_forecast ( history , cfg )

# store forecast in list of predictions

predictions . append ( yhat )

# add actual observation to history for the next loop

history . append ( test [ i ] )

# estimate prediction error

error = measure_rmse ( test , predictions )

return error

# score a model, return None on failure

def score_model ( data , n_test , cfg , debug = False ) :

result = None

# convert config to a key

key = str ( cfg )

# show all warnings and fail on exception if debugging

if debug :

result = walk_forward_validation ( data , n_test , cfg )

else :

# one failure during model validation suggests an unstable config

try :

# never show warnings when grid searching, too noisy

with catch_warnings ( ) :

filterwarnings ( ""ignore"" )

result = walk_forward_validation ( data , n_test , cfg )

except :

error = None

# check for an interesting result

if result is not None :

print ( ' > Model[%s] %.3f' % ( key , result ) )

return ( key , result )

# grid search configs

def grid_search ( data , cfg_list , n_test , parallel = True ) :

scores = None

if parallel :

# execute configs in parallel

executor = Parallel ( n_jobs = cpu_count ( ) , backend = 'multiprocessing' )

tasks = ( delayed ( score_model ) ( data , n_test , cfg ) for cfg in cfg_list )

scores = executor ( tasks )

else :

scores = [ score_model ( data , n_test , cfg ) for cfg in cfg_list ]

# remove empty results

scores = [ r for r in scores if r [ 1 ] != None ]

# sort configs by error, asc

scores . sort ( key = lambda tup : tup [ 1 ] )

return scores

# create a set of exponential smoothing configs to try

def exp_smoothing_configs ( seasonal = [ None ] ) :

models = list ( )

# define config lists

t_params = [ 'add' , 'mul' , None ]

d_params = [ True , False ]

s_params = [ 'add' , 'mul' , None ]

p_params = seasonal

b_params = [ True , False ]

r_params = [ True , False ]

# create config instances

for t in t_params :

for d in d_params :

for s in s_params :

for p in p_params :

for b in b_params :

for r in r_params :

cfg = [ t , d , s , p , b , r ]

models . append ( cfg )

return models

if __name__ == '__main__' :

# load dataset

series = read_csv ( 'monthly-car-sales.csv' , header = 0 , index_col = 0 )

data = series . values

# data split

n_test = 12

# model configs

cfg_list = exp_smoothing_configs ( seasonal = [ 0 , 6 , 12 ] )

# grid search

scores = grid_search ( data [ : , 0 ] , cfg_list , n_test )

print ( 'done' )

# list top 3 configs

for cfg , error in scores [ : 3 ] :"
151;machinelearningmastery.com;http://machinelearningmastery.com/how-to-identify-outliers-in-your-data/;2013-12-30;How to Identify Outliers in your Data;"Tweet Share Share

Last Updated on May 30, 2019

Bojan Miletic asked a question about outlier detection in datasets when working with machine learning algorithms. This post is in answer to his question.

If you have a question about machine learning, sign-up to the newsletter and reply to an email or use the contact form and ask, I will answer your question and may even turn it into a blog post.

Outliers

Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately poorer results.

Even before predictive models are prepared on training data, outliers can result in misleading representations and in turn misleading interpretations of collected data. Outliers can skew the summary distribution of attribute values in descriptive statistics like mean and standard deviation and in plots such as histograms and scatterplots, compressing the body of the data.

Finally, outliers can represent examples of data instances that are relevant to the problem such as anomalies in the case of fraud detection and computer security.

Outlier Modeling

Outliers are extreme values that fall a long way outside of the other observations. For example, in a normal distribution, outliers may be values on the tails of the distribution.

The process of identifying outliers has many names in data mining and machine learning such as outlier mining, outlier modeling and novelty detection and anomaly detection.

In his book Outlier Analysis (affiliate link), Aggarwal provides a useful taxonomy of outlier detection methods, as follows:

Extreme Value Analysis : Determine the statistical tails of the underlying distribution of the data. For example, statistical methods like the z-scores on univariate data.

: Determine the statistical tails of the underlying distribution of the data. For example, statistical methods like the z-scores on univariate data. Probabilistic and Statistical Models : Determine unlikely instances from a probabilistic model of the data. For example, gaussian mixture models optimized using expectation-maximization.

: Determine unlikely instances from a probabilistic model of the data. For example, gaussian mixture models optimized using expectation-maximization. Linear Models : Projection methods that model the data into lower dimensions using linear correlations. For example, principle component analysis and data with large residual errors may be outliers.

: Projection methods that model the data into lower dimensions using linear correlations. For example, principle component analysis and data with large residual errors may be outliers. Proximity-based Models : Data instances that are isolated from the mass of the data as determined by cluster, density or nearest neighbor analysis.

: Data instances that are isolated from the mass of the data as determined by cluster, density or nearest neighbor analysis. Information Theoretic Models : Outliers are detected as data instances that increase the complexity (minimum code length) of the dataset.

: Outliers are detected as data instances that increase the complexity (minimum code length) of the dataset. High-Dimensional Outlier Detection: Methods that search subspaces for outliers give the breakdown of distance based measures in higher dimensions (curse of dimensionality).

Aggarwal comments that the interpretability of an outlier model is critically important. Context or rationale is required around decisions why a specific data instance is or is not an outlier.

In his contributing chapter to Data Mining and Knowledge Discovery Handbook (affiliate link), Irad Ben-Gal proposes a taxonomy of outlier models as univariate or multivariate and parametric and nonparametric. This is a useful way to structure methods based on what is known about the data. For example:

Are you considered with outliers in one or more than one attributes (univariate or multivariate methods)?

Can you assume a statistical distribution from which the observations were sampled or not (parametric or nonparametric)?

Get Started

There are many methods and much research put into outlier detection. Start by making some assumptions and design experiments where you can clearly observe the effects of the those assumptions against some performance or accuracy measure.

I recommend working through a stepped process from extreme value analysis, proximity methods and projection methods.

Extreme Value Analysis

You do not need to know advanced statistical methods to look for, analyze and filter out outliers from your data. Start out simple with extreme value analysis.

Focus on univariate methods

Visualize the data using scatterplots, histograms and box and whisker plots and look for extreme values

Assume a distribution (Gaussian) and look for values more than 2 or 3 standard deviations from the mean or 1.5 times from the first or third quartile

Filter out outliers candidate from training dataset and assess your models performance

Proximity Methods

Once you have explore simpler extreme value methods, consider moving onto proximity-based methods.

Use clustering methods to identify the natural clusters in the data (such as the k-means algorithm)

Identify and mark the cluster centroids

Identify data instances that are a fixed distance or percentage distance from cluster centroids

Filter out outliers candidate from training dataset and assess your models performance

Projection Methods

Projection methods are relatively simple to apply and quickly highlight extraneous values.

Use projection methods to summarize your data to two dimensions (such as PCA, SOM or Sammon’s mapping)

Visualize the mapping and identify outliers by hand

Use proximity measures from projected values or codebook vectors to identify outliers

Filter out outliers candidate from training dataset and assess your models performance

Methods Robust to Outliers

An alternative strategy is to move to models that are robust to outliers. There are robust forms of regression that minimize the median least square errors rather than mean (so-called robust regression), but are more computationally intensive. There are also methods like decision trees that are robust to outliers.

You could spot check some methods that are robust to outliers. If there are significant model accuracy benefits then there may be an opportunity to model and filter out outliers from your training data.

Resources

There are a lot of webpages that discuss outlier detection, but I recommend reading through a good book on the subject, something more authoritative. Even looking through introductory books on machine learning and data mining won’t be that useful to you. For a classical treatment of outliers by statisticians, check out:

Robust Regression and Outlier Detection (affiliate link) by Rousseeuw and Leroy published in 2003

Outliers in Statistical Data (affiliate link) by Barnett and Lewis, published in 1994

Identification of Outliers (affiliate link) a monograph by Hawkins published in 1980

For a modern treatment of outliers by data mining community, see:

Outlier Analysis (affiliate link) by Aggarwal, published in 2013

Chapter 7 by Irad Ben-Gal in Data Mining and Knowledge Discovery Handbook (affiliate link) edited by Maimon and Rokach, published in 2010"
152;machinelearningmastery.com;https://machinelearningmastery.com/relationship-between-applied-statistics-and-machine-learning/;2018-06-28;The Close Relationship Between Applied Statistics and Machine Learning;"Tweet Share Share

Last Updated on August 8, 2019

The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability.

Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus on the behavior of models and explainability of predictions.

The very close relationship between the two approaches to the same problem means that both fields have a lot to learn from each other. The statisticians need to consider algorithmic methods was called out in the classic “two cultures” paper. Machine learning practitioners must also take heed, keep an open mind, and learn both the terminology and relevant methods from applied statistics.

In this post, you will discover that machine learning and statistical learning are two closely related but different perspectives on the same problem.

After reading this post, you will know:

“Machine learning” and “predictive modeling” are a computer science perspective on modeling data with a focus on algorithmic methods and model skill.

“Statistics” and “statistical learning” are a mathematical perspective on modeling data with a focus on data models and on goodness of fit.

Machine learning practitioners must keep an open mind and leverage methods and understand the terminology from the closely related fields of applied statistics and statistical learning.

Discover statistical hypothesis testing, resampling methods, estimation statistics and nonparametric methods in my new book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Machine Learning

Machine learning is a subfield of artificial intelligence and is related to the broader field of computer science.

When it comes to developing machine learning models in order to make predictions, there is a heavy focus on algorithms, code, and results.

Machine learning is a lot broader than developing models in order to make predictions, as can be seen by the definition in the classic 1997 textbook by Tom Mitchell.

The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience.

— Page xv, Machine Learning, 1997.

Here, we can see that from a research perspective, machine learning is really the study of learning with computer programs. It just so happens that some of these learning programs are useful for predictive modeling problems, and some in fact have been borrowed from other fields, such as statistics.

Linear regression is a perfect example. It is a more-than-a-century-old method from the (at the time: nascent) field of statistics that is used for fitting a line or plane to real-valued data. From a machine learning perspective, we look at it as a system for learning weights (coefficients) in response to examples from a domain.

Many methods have been developed in the field of artificial intelligence and machine learning, sometimes by statisticians, that prove very useful for the task of predictive modeling. A good example is classification and regression trees that bears no resemblance to classical methods in statistics.

Need help with Statistics for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Predictive Modeling

The useful part of machine learning for the practitioner may be called predictive modeling.

This explicitly ignores distinctions between statistics and machine learning. It also shucks off the broader objectives of statistics (understanding data) and machine learning (understanding learning in software) and only concerns itself, as its name suggests, with developing models that make predictions.

The term predictive modeling may stir associations such as machine learning, pattern recognition, and data mining. Indeed, these associations are appropriate and the methods implied by these terms are an integral piece of the predictive modeling process. But predictive modeling encompasses much more than the tools and techniques for uncovering patterns within data. The practice of predictive modeling defines the process of developing a model in a way that we can understand and quantify the model’s prediction accuracy on future, yet-to-be-seen data.

— Page vii, Applied Predictive Modeling, 2013

Predictive modeling provides a laser-focus on developing models with the objective of getting the best possible results with regard to some measure of model skill. This pragmatic approach often means that results in the form of maximum skill or minimum error are sought at the expense of almost everything else.

It doesn’t really matter what we call the process, machine learning or predictive modeling. In some sense it is marketing and group identification. Getting results and delivering value matters more to the practitioner.

Statistical Learning

The process of working with a dataset and developing a predictive model is also a task in statistics.

A statistician may have traditionally referred to the activity as applied statistics.

Statistics is a subfield of mathematics, and this heritage gives a focus of well defined, carefully chosen methods. A need to understand not only why a specific model was chosen, but also how and why specific predictions are made.

From this perspective, often model skill is important, but less important than the interpretability of the model.

Nevertheless, modern statisticians have formulated a new perspective as a subfield of applied statistics called “statistical learning“. It may be the statistics equivalent of “predictive modeling” where model skill is important, but perhaps a stronger emphasis is given to careful selection and introduction of the learning models.

Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning.

— Page vii, An Introduction to Statistical Learning with Applications in R, 2013.

We can see that there is a bleeding of ideas between fields and subfields in statistics. The machine learning practitioner must be aware of both the machine learning and statistical-based approach to the problem. This is especially important given the use of different terminology in both domains.

In his course on statistics, Rob Tibshirani, a statistician who also has a foot in machine learning, provides a glossary that maps terms in statistics to terms in machine learning, reproduced below.

This highlights the deeper need for the machine learning practitioner to focus on predictive modeling and stay open to methods, ideas, and terminology, regardless of the field of origin. This may apply to modern fields like bioinformatics and econometrics but applies more so to the tightly related and much older field of statistics.

Two Cultures

Recently, and perhaps still now, applied statisticians looked down the field of machine learning and the practice of results-at-any-cost predictive modeling.

Both fields offer tremendous value, but perhaps on subtly different flavors of the same general problem of predictive modeling.

Real and valuable contributions have been made to modeling from the computer science perspective of machine learning such as decision trees mentioned above and artificial neural networks, more recently relabeled deep learning, to name two well known examples.

Just as the machine learning practitioner must keep an eye on applied statistics and statistical learning, the statistician must keep an eye on machine learning.

This call was made clearly in the now (perhaps famous) 2001 paper titled “Statistical Modeling: The Two Cultures” by Leo Breiman.

In it, he contrasts the “data modeling culture” of statisticians to the “algorithmic modeling culture” of all other fields, to which machine learning belongs. He highlights these cultures as ways of thinking about the same problem of mapping inputs to outputs, where the statistical approach is to focus on goodness of fit tests and the algorithmic approach focuses on predictive accuracy.

He suggests that the field of statistics will suffer both by losing relevance and in the fragility of the methods by ignoring the algorithmic approach. The classical approach he refers to as “data models,” a subtle but important shift in focus where a practitioner chooses and focuses on the behavior of the model (e.g. logistic regression) rather than the data and processes that may have generated it.

This might be characterized (perhaps unfairly) as focusing on making the data fit the model rather than choosing or adapting the model to fit the data.

The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. […] If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.

It’s an important paper, still relevant and a great read more than 15 years later.

The emergence of subfields like “statistical learning” by statisticians suggests that headway is being made.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this post, you discovered that machine learning and statistical learning are two closely related but different perspectives on the same problem.

Specifically, you learned:

“Machine learning” and “predictive modeling” are a computer science perspective on modeling data with a focus on algorithmic methods and model skill.

“Statistics” and “statistical learning” are a mathematical perspective on modeling data with a focus on data models and on goodness of fit.

Machine learning practitioners must keep an open mind and leverage methods and understand the terminology from the closely related fields of applied statistics and statistical learning.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Statistics for Machine Learning! Develop a working understanding of statistics ...by writing lines of code in python Discover how in my new Ebook:

Statistical Methods for Machine Learning It provides self-study tutorials on topics like:

Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more... Discover how to Transform Data into Knowledge Skip the Academics. Just Results. See What's Inside"
153;news.mit.edu;http://news.mit.edu/2019/mit-briefing-bold-action-needed-make-technology-work-society;;MIT Briefing: Bold action needed to make technology work for society;"CAMBRIDGE, Mass. -- On Tuesday, September 10, the MIT Task Force on the Work of the Future will detail their findings on how automation, robotics and artificial intelligence are likely to impact the American workforce, and describe public and private action they say is needed to harness new technologies for shared prosperity. The briefing will be held at the National Press Club in Washington, D.C. at 8:30 A.M., and will feature remarks from MIT President L. Rafael Reif and MIT researchers, as well as a panel discussion and opportunity for questions.

The MIT Task Force on the Work of the Future – which is led by David Autor, the Ford Professor of Economics; David Mindell, the Frances and David Dibner Professor of the History of Engineering and Manufacturing, and a professor of aeronautics and astronautics; and Elisabeth Reynolds, executive director of the Task Force on the Work of the Future and a lecturer in the Department of Urban Studies and Planning – consists of dozens of MIT scholars, as well as an advisory board of business executives and policy leaders. The Task Force report draws on new data, the expert knowledge of many technology sectors, and a close analysis of both technology-centered firms and economic data spanning the postwar era.

In recent decades, technology has contributed to the polarization of employment, helping high-skilled professionals while reducing opportunities for many other workers. A critical challenge is not necessarily a lack of jobs, but the low quality of jobs and the resulting lack of viable careers for many people. With this in mind, the MIT Task Force on the Work of the Future finds that the future of work can be shaped beneficially by new policies, renewed support for labor, and reformed institutions, not just new technologies.

Eduardo Porter, an economics reporter for The New York Times, will moderate the panel discussion. Participants include: John E. Kelly III, executive vice president of IBM; Juan Salgado, chancellor of the City Colleges of Chicago; and Liz Shuler, secretary-treasurer of the American Federation of Labor and Congress of Industrial Organizations (AFL-CIO).

WHAT:

MIT Task Force on the Work of the Future Briefing

WHO:

MIT speakers:

L. Rafael Reif , MIT President;

, MIT President; David Autor , Ford Professor of Economics

, Ford Professor of Economics David Mindell , Frances and David Dibner Professor of the History of Engineering and Manufacturing, and a professor of aeronautics and astronautics;

, Frances and David Dibner Professor of the History of Engineering and Manufacturing, and a professor of aeronautics and astronautics; Elisabeth Reynolds, executive director of the Task Force on the Work of the Future and the MIT Industrial Performance Center (IPC), and a lecturer in the Department of Urban Studies and Planning

Guest speakers and participants:

Eduardo Porter , economics reporter, The New York Times;

, economics reporter, The New York Times; John E. Kelly III , executive vice President, IBM;

, executive vice President, IBM; Juan Salgado , chancellor, City Colleges of Chicago;

, chancellor, City Colleges of Chicago; Liz Shuler, secretary-treasurer, AFL-CIO

WHERE:

National Press Club

First Amendment Lounge

529 14th Street, NW, 13th Floor

Washington, D.C. 20045

WHEN:

Tuesday, September 10, 2019

8:00 A.M.: Coffee and breakfast

8:30 A.M. – 9:45 A.M.: Remarks and panel discussion

Media RSVP:

Reporters interested in attending should email Abby Abazorius at abbya@mit.edu or expertrequests@mit.edu to RSVP and for more information.

###"
154;machinelearningmastery.com;https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/;2017-07-16;Gentle Introduction to Models for Sequence Prediction with RNNs;"Tweet Share Share

Last Updated on August 25, 2019

Sequence prediction is a problem that involves using historical sequence information to predict the next value or values in the sequence.

The sequence may be symbols like letters in a sentence or real values like those in a time series of prices. Sequence prediction may be easiest to understand in the context of time series forecasting as the problem is already generally understood.

In this post, you will discover the standard sequence prediction models that you can use to frame your own sequence prediction problems.

After reading this post, you will know:

How sequence prediction problems are modeled with recurrent neural networks.

The 4 standard sequence prediction models used by recurrent neural networks.

The 2 most common misunderstandings made by beginners when applying sequence prediction models.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Tutorial Overview

This tutorial is divided into 4 parts; they are:

Sequence Prediction with Recurrent Neural Networks Models for Sequence Prediction Cardinality from Timesteps not Features Two Common Misunderstandings by Practitioners

Sequence Prediction with Recurrent Neural Networks

Recurrent Neural Networks, like Long Short-Term Memory (LSTM) networks, are designed for sequence prediction problems.

In fact, at the time of writing, LSTMs achieve state-of-the-art results in challenging sequence prediction problems like neural machine translation (translating English to French).

LSTMs work by learning a function (f(…)) that maps input sequence values (X) onto output sequence values (y).

y(t) = f(X(t)) 1 y(t) = f(X(t))

The learned mapping function is static and may be thought of as a program that takes input variables and uses internal variables. Internal variables are represented by an internal state maintained by the network and built up or accumulated over each value in the input sequence.

… RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables.

— Andrej Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks, 2015

The static mapping function may be defined with a different number of inputs or outputs, as we will review in the next section.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Models for Sequence Prediction

In this section, will review the 4 primary models for sequence prediction.

We will use the following terminology:

X: The input sequence value, may be delimited by a time step, e.g. X(1).

u: The hidden state value, may be delimited by a time step, e.g. u(1).

y: The output sequence value, may be delimited by a time step, e.g. y(1).

One-to-One Model

A one-to-one model produces one output value for each input value.

The internal state for the first time step is zero; from that point onward, the internal state is accumulated over the prior time steps.

In the case of a sequence prediction, this model would produce one time step forecast for each observed time step received as input.

This is a poor use for RNNs as the model has no chance to learn over input or output time steps (e.g. BPTT). If you find implementing this model for sequence prediction, you may intend to be using a many-to-one model instead.

One-to-Many Model

A one-to-many model produces multiple output values for one input value.

The internal state is accumulated as each value in the output sequence is produced.

This model can be used for image captioning where one image is provided as input and a sequence of words are generated as output.

Many-to-One Model

A many-to-one model produces one output value after receiving multiple input values.

The internal state is accumulated with each input value before a final output value is produced.

In the case of time series, this model would use a sequence of recent observations to forecast the next time step. This architecture would represent the classical autoregressive time series model.

Many-to-Many Model

A many-to-many model produces multiple outputs after receiving multiple input values.

As with the many-to-one case, state is accumulated until the first output is created, but in this case multiple time steps are output.

Importantly, the number of input time steps do not have to match the number of output time steps. Think of the input and output time steps operating at different rates.

In the case of time series forecasting, this model would use a sequence of recent observations to make a multi-step forecast.

In a sense, it combines the capabilities of the many-to-one and one-to-many models.

Cardinality from Timesteps (not Features!)

A common point of confusion is to conflate the above examples of sequence mapping models with multiple input and output features.

A sequence may be comprised of single values, one for each time step.

Alternately, a sequence could just as easily represent a vector of multiple observations at the time step. Each item in the vector for a time step may be thought of as its own separate time series. It does not affect the description of the models above.

For example, a model that takes as input one time step of temperature and pressure and predicts one time step of temperature and pressure is a one-to-one model, not a many-to-many model.

The model does take two values as input and predicts two values, but there is only a single sequence time step expressed for the input and predicted as output.

The cardinality of the sequence prediction models defined above refers to time steps, not features (e.g. univariate or multivariate sequences).

Two Common Misunderstandings by Practitioners

The confusion of features vs time steps leads to two main misunderstandings when implementing recurrent neural networks by practitioners:

1. Timesteps as Input Features

Observations at previous timesteps are framed as input features to the model.

This is the classical fixed-window-based approach of inputting sequence prediction problems used by multilayer Perceptrons. Instead, the sequence should be fed in one time step at a time.

This confusion may lead you to think you have implemented a many-to-one or many-to-many sequence prediction model when in fact you only have a single vector input for one time step.

2. Timesteps as Output Features

Predictions at multiple future time steps are framed as output features to the model.

This is the classical fixed-window approach of making multi-step predictions used by multilayer Perceptrons and other machine learning algorithms. Instead, the sequence predictions should be generated one time step at a time.

This confusion may lead you to think you have implemented a one-to-many or many-to-many sequence prediction model when in fact you only have a single vector output for one time step (e.g. seq2vec not seq2seq).

Note: framing timesteps as features in sequence prediction problems is a valid strategy, and could lead to improved performance even when using recurrent neural networks (try it!). The important point here is to understand the common pitfalls and not trick yourself when framing your own prediction problems.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Summary

In this tutorial, you discovered the standard models for sequence prediction with recurrent neural networks.

Specifically, you learned:

How sequence prediction problems are modeled with recurrent neural networks.

The 4 standard sequence prediction models used by recurrent neural networks.

The 2 most common misunderstandings made by beginners when applying sequence prediction models.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside"
155;machinelearningmastery.com;http://machinelearningmastery.com/persistence-time-series-forecasting-with-python/;2016-12-25;How to Make Baseline Predictions for Time Series Forecasting with Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41

from pandas import read_csv from pandas import datetime from pandas import DataFrame from pandas import concat from matplotlib import pyplot from sklearn . metrics import mean_squared_error def parser ( x ) : return datetime . strptime ( '190' + x , '%Y-%m' ) series = read_csv ( 'shampoo-sales.csv' , header = 0 , parse_dates = [ 0 ] , index_col = 0 , squeeze = True , date_parser = parser ) # Create lagged dataset values = DataFrame ( series . values ) dataframe = concat ( [ values . shift ( 1 ) , values ] , axis = 1 ) dataframe . columns = [ 't-1' , 't+1' ] print ( dataframe . head ( 5 ) ) # split into train and test sets X = dataframe . values train_size = int ( len ( X ) * 0.66 ) train , test = X [ 1 : train_size ] , X [ train_size : ] train_X , train_y = train [ : , 0 ] , train [ : , 1 ] test_X , test_y = test [ : , 0 ] , test [ : , 1 ] # persistence model def model_persistence ( x ) : return x # walk-forward validation predictions = list ( ) for x in test_X : yhat = model_persistence ( x ) predictions . append ( yhat ) test_score = mean_squared_error ( test_y , predictions ) print ( 'Test MSE: %.3f' % test_score ) # plot predictions and expected results pyplot . plot ( train_y ) pyplot . plot ( [ None for i in train_y ] + [ x for x in test_y ] ) pyplot . plot ( [ None for i in train_y ] + [ x for x in predictions ] ) pyplot . show ( )"
156;machinelearningmastery.com;https://machinelearningmastery.com/indoor-movement-time-series-classification-with-machine-learning-algorithms/;2018-09-09;Indoor Movement Time Series Classification with Machine Learning Algorithms;"# spot check for ES2

from pandas import read_csv

from matplotlib import pyplot

from sklearn . metrics import accuracy_score

from sklearn . linear_model import LogisticRegression

from sklearn . neighbors import KNeighborsClassifier

from sklearn . tree import DecisionTreeClassifier

from sklearn . svm import SVC

from sklearn . ensemble import RandomForestClassifier

from sklearn . ensemble import GradientBoostingClassifier

from sklearn . pipeline import Pipeline

from sklearn . preprocessing import StandardScaler

# load dataset

train = read_csv ( 'es2_train.csv' , header = None )

test = read_csv ( 'es2_test.csv' , header = None )

# split into inputs and outputs

trainX , trainy = train . values [ : , : - 1 ] , train . values [ : , - 1 ]

testX , testy = test . values [ : , : - 1 ] , test . values [ : , - 1 ]

# create a list of models to evaluate

models , names = list ( ) , list ( )

# logistic

models . append ( LogisticRegression ( ) )

names . append ( 'LR' )

# knn

models . append ( KNeighborsClassifier ( ) )

names . append ( 'KNN' )

# knn

models . append ( KNeighborsClassifier ( n_neighbors = 7 ) )

names . append ( 'KNN-7' )

# cart

models . append ( DecisionTreeClassifier ( ) )

names . append ( 'CART' )

# svm

models . append ( SVC ( ) )

names . append ( 'SVM' )

# random forest

models . append ( RandomForestClassifier ( ) )

names . append ( 'RF' )

# gbm

models . append ( GradientBoostingClassifier ( ) )

names . append ( 'GBM' )

# evaluate models

all_scores = list ( )

for i in range ( len ( models ) ) :

# create a pipeline for the model

scaler = StandardScaler ( )

model = Pipeline ( steps = [ ( 's' , scaler ) , ( 'm' , models [ i ] ) ] )

# fit

# model = models[i]

model . fit ( trainX , trainy )

# predict

yhat = model . predict ( testX )

# evaluate

score = accuracy_score ( testy , yhat ) * 100

all_scores . append ( score )

# summarize

print ( '%s %.3f%%' % ( names [ i ] , score ) )

# plot

pyplot . bar ( names , all_scores )"
157;machinelearningmastery.com;https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/;2018-12-20;How to Develop an Ensemble of Deep Learning Models in Keras;"Tweet Share Share

Last Updated on January 10, 2020

Deep learning neural network models are highly flexible nonlinear algorithms capable of learning a near infinite number of mapping functions.

A frustration with this flexibility is the high variance in a final model. The same neural network model trained on the same dataset may find one of many different possible “good enough” solutions each time it is run.

Model averaging is an ensemble learning technique that reduces the variance in a final neural network model, sacrificing spread in the performance of the model for a confidence in what performance to expect from the model.

In this tutorial, you will discover how to develop a model averaging ensemble in Keras to reduce the variance in a final model.

After completing this tutorial, you will know:

Model averaging is an ensemble learning technique that can be used to reduce the expected variance of deep learning neural network models.

How to implement model averaging in Keras for classification and regression predictive modeling problems.

How to work through a multi-class classification problem and use model averaging to reduce the variance of the final model.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Updated Oct/2019 : Updated for Keras 2.3 and TensorFlow 2.0.

: Updated for Keras 2.3 and TensorFlow 2.0. Updated Jan/2020: Updated for changes in scikit-learn v0.22 API.

Tutorial Overview

This tutorial is divided into six parts; they are:

Model Averaging How to Average Models in Keras Multi-Class Classification Problem MLP Model for Multi-Class Classification High Variance of MLP Model Model Averaging Ensemble

Model Averaging

Deep learning neural network models are nonlinear methods that learn via a stochastic training algorithm.

This means that they are highly flexible, capable of learning complex relationships between variables and approximating any mapping function, given enough resources. A downside of this flexibility is that the models suffer high variance.

This means that the models are highly dependent on the specific training data used to train the model and on the initial conditions (random initial weights) and serendipity during the training process. The result is a final model that makes different predictions each time the same model configuration is trained on the same dataset.

This can be frustrating when training a final model for use in making predictions on new data, such as operationally or in a machine learning competition.

The high variance of the approach can be addressed by training multiple models for the problem and combining their predictions. This approach is called model averaging and belongs to a family of techniques called ensemble learning.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Average Models in Keras

The simplest way to develop a model averaging ensemble in Keras is to train multiple models on the same dataset then combine the predictions from each of the trained models.

Train Multiple Models

Training multiple models may be resource intensive, depending on the size of the model and the size of the training data.

You may have to train the models sequentially on the same hardware. For very large models, it may be worth training the models in parallel using cloud infrastructure such as Amazon Web Services.

The number of models required for the ensemble may vary based on the complexity of the problem and model. A benefit of the approach is that you can continue to create models, add them to the ensemble, and evaluate their impact on the performance by making predictions on a holdout test set.

For small models, you can train the models sequentially and keep them in memory for use in your experiment. For example:

... # train models and keep them in memory n_members = 10 models = list() for _ in range(n_members): # define and fit model model = ... # store model in memory as ensemble member models.add(models) ... 1 2 3 4 5 6 7 8 9 10 . . . # train models and keep them in memory n_members = 10 models = list ( ) for _ in range ( n_members ) : # define and fit model model = . . . # store model in memory as ensemble member models . add ( models ) . . .

For large models, perhaps trained on different hardware, you can save each model to file.

... # train models and keep them to file n_members = 10 for i in range(n_members): # define and fit model model = ... # save model to file filename = 'model_' + str(i + 1) + '.h5' model.save(filename) print('Saved: %s' % filename) ... 1 2 3 4 5 6 7 8 9 10 11 . . . # train models and keep them to file n_members = 10 for i in range ( n_members ) : # define and fit model model = . . . # save model to file filename = 'model_' + str ( i + 1 ) + '.h5' model . save ( filename ) print ( 'Saved: %s' % filename ) . . .

Models can then be loaded later.

Small models can all be loaded into memory at the same time, whereas very large models may have to be loaded one at a time to make a prediction, then later to have the predictions combined.

from keras.models import load_model ... # load pre-trained ensemble members n_members = 10 models = list() for i in range(n_members): # load model filename = 'model_' + str(i + 1) + '.h5' model = load_model(filename) # store in memory models.append(model) ... 1 2 3 4 5 6 7 8 9 10 11 12 from keras . models import load _ model . . . # load pre-trained ensemble members n_members = 10 models = list ( ) for i in range ( n_members ) : # load model filename = 'model_' + str ( i + 1 ) + '.h5' model = load_model ( filename ) # store in memory models . append ( model ) . . .

Combine Predictions

Once the models have been prepared, each model can be used to make a prediction and the predictions can be combined.

In the case of a regression problem where each model is predicting a real-valued output, the values can be collected and the average calculated.

... # make predictions yhats = [model.predict(testX) for model in models] yhats = array(yhats) # calculate average outcomes = mean(yhats) 1 2 3 4 5 6 . . . # make predictions yhats = [ model . predict ( testX ) for model in models ] yhats = array ( yhats ) # calculate average outcomes = mean ( yhats )

In the case of a classification problem, there are two options.

The first is to calculate the mode of the predicted integer class values.

... # make predictions yhats = [model.predict_classes(testX) for model in models] yhats = array(yhats) # calculate mode outcomes, _ = mode(yhats) 1 2 3 4 5 6 . . . # make predictions yhats = [ model . predict_classes ( testX ) for model in models ] yhats = array ( yhats ) # calculate mode outcomes , _ = mode ( yhats )

A downside of this approach is that for small ensembles or problems with a large number of classes, the sample of predictions may not be large enough for the mode to be meaningful.

In the case of a binary classification problem, a sigmoid activation function is used on the output layer and the average of the predicted probabilities can be calculated much like a regression problem.

In the case of a multi-class classification problem with more than two classes, a softmax activation function is used on the output layer and the sum of the probabilities for each predicted class can be calculated before taking the argmax to get the class value.

... # make predictions yhats = [model.predict(testX) for model in models] yhats = array(yhats) # sum across ensembles summed = numpy.sum(yhats, axis=0) # argmax across classes outcomes = argmax(summed, axis=1) 1 2 3 4 5 6 7 8 . . . # make predictions yhats = [ model . predict ( testX ) for model in models ] yhats = array ( yhats ) # sum across ensembles summed = numpy . sum ( yhats , axis = 0 ) # argmax across classes outcomes = argmax ( summed , axis = 1 )

These approaches for combining predictions of Keras models will work just as well for Multilayer Perceptron, Convolutional, and Recurrent Neural Networks.

Now that we know how to average predictions from multiple neural network models in Keras, let’s work through a case study.

Multi-Class Classification Problem

We will use a small multi-class classification problem as the basis to demonstrate a model averaging ensemble.

The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class.

We use this problem with 500 examples, with input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same 500 points.

# generate 2d classification dataset X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2) 1 2 # generate 2d classification dataset X , y = make_blobs ( n_samples = 500 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 )

The results are the input and output elements of a dataset that we can model.

In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value.

The complete example is listed below.

# scatter plot of blobs dataset from sklearn.datasets import make_blobs from matplotlib import pyplot from pandas import DataFrame # generate 2d classification dataset X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2) # scatter plot, dots colored by class value df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y)) colors = {0:'red', 1:'blue', 2:'green'} fig, ax = pyplot.subplots() grouped = df.groupby('label') for key, group in grouped: group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key]) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # scatter plot of blobs dataset from sklearn . datasets import make_blobs from matplotlib import pyplot from pandas import DataFrame # generate 2d classification dataset X , y = make_blobs ( n_samples = 500 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 ) # scatter plot, dots colored by class value df = DataFrame ( dict ( x = X [ : , 0 ] , y = X [ : , 1 ] , label = y ) ) colors = { 0 : 'red' , 1 : 'blue' , 2 : 'green' } fig , ax = pyplot . subplots ( ) grouped = df . groupby ( 'label' ) for key , group in grouped : group . plot ( ax = ax , kind = 'scatter' , x = 'x' , y = 'y' , label = key , color = colors [ key ] ) pyplot . show ( )

Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points.

This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different “good enough” candidate solutions resulting in a high variance.

MLP Model for Multi-Class Classification

Now that we have defined a problem, we can define a model to address it.

We will define a model that is perhaps under-constrained and not tuned to the problem. This is intentional to demonstrate the high variance of a neural network model seen on truly large and challenging supervised learning problems.

The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with 3 elements with the probability that the sample belongs to each of the 3 classes. Therefore, the first step is to one hot encode the class values.

y = to_categorical(y) 1 y = to_categorical ( y )

Next, we must split the dataset into training and test sets. We will use the test set both to evaluate the performance of the model and to plot its performance during training with a learning curve. We will use 30% of the data for training and 70% for the test set.

This is an example of a challenging problem where we have more unlabeled examples than we do labeled examples.

# split into train and test n_train = int(0.3 * X.shape[0]) trainX, testX = X[:n_train, :], X[n_train:, :] trainy, testy = y[:n_train], y[n_train:] 1 2 3 4 # split into train and test n_train = int ( 0.3 * X . shape [ 0 ] ) trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ]

Next, we can define and compile the model.

The model will expect samples with two input variables. The model then has a single hidden layer with 15 modes and a rectified linear activation function, then an output layer with 3 nodes to predict the probability of each of the 3 classes and a softmax activation function.

Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent.

# define model model = Sequential() model.add(Dense(15, input_dim=2, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) 1 2 3 4 5 # define model model = Sequential ( ) model . add ( Dense ( 15 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

The model is fit for 200 training epochs and we will evaluate the model each epoch on the test set, using the test set as a validation set.

# fit model history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0) 1 2 # fit model history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 200 , verbose = 0 )

At the end of the run, we will evaluate the performance of the model on both the train and the test sets.

# evaluate the model _, train_acc = model.evaluate(trainX, trainy, verbose=0) _, test_acc = model.evaluate(testX, testy, verbose=0) print('Train: %.3f, Test: %.3f' % (train_acc, test_acc)) 1 2 3 4 # evaluate the model _ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 ) _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) )

Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and test dataset.

# plot history pyplot.plot(history.history['accuracy'], label='train') pyplot.plot(history.history['val_accuracy'], label='test') pyplot.legend() pyplot.show() 1 2 3 4 5 # plot history pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' ) pyplot . legend ( ) pyplot . show ( )

The complete example is listed below.

# fit high variance mlp on blobs classification problem from sklearn.datasets import make_blobs from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense from matplotlib import pyplot # generate 2d classification dataset X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2) y = to_categorical(y) # split into train and test n_train = int(0.3 * X.shape[0]) trainX, testX = X[:n_train, :], X[n_train:, :] trainy, testy = y[:n_train], y[n_train:] # define model model = Sequential() model.add(Dense(15, input_dim=2, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0) # evaluate the model _, train_acc = model.evaluate(trainX, trainy, verbose=0) _, test_acc = model.evaluate(testX, testy, verbose=0) print('Train: %.3f, Test: %.3f' % (train_acc, test_acc)) # learning curves of model accuracy pyplot.plot(history.history['accuracy'], label='train') pyplot.plot(history.history['val_accuracy'], label='test') pyplot.legend() pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # fit high variance mlp on blobs classification problem from sklearn . datasets import make_blobs from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Dense from matplotlib import pyplot # generate 2d classification dataset X , y = make_blobs ( n_samples = 500 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 ) y = to_categorical ( y ) # split into train and test n_train = int ( 0.3 * X . shape [ 0 ] ) trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] # define model model = Sequential ( ) model . add ( Dense ( 15 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 200 , verbose = 0 ) # evaluate the model _ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 ) _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) ) # learning curves of model accuracy pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' ) pyplot . legend ( ) pyplot . show ( )

Running the example first prints the performance of the final model on the train and test datasets.

Your specific results will vary (by design!) given the high variance nature of the model.

In this case, we can see that the model achieved about 84% accuracy on the training dataset and about 76% accuracy on the test dataset; not terrible.

Train: 0.847, Test: 0.766 1 Train: 0.847, Test: 0.766

A line plot is also created showing the learning curves for the model accuracy on the train and test sets over each training epoch.

We can see that the model is not really overfit, but is perhaps a little underfit and may benefit from an increase in capacity, more training, and perhaps some regularization. All of these improvements of which we intentionally hold back to force the high variance for our case study.

High Variance of MLP Model

It is important to demonstrate that the model indeed has a variance in its prediction.

We can demonstrate this by repeating the fit and evaluation of the same model configuration on the same dataset and summarizing the final performance of the model.

To do this, we first split the fit and evaluation of the model out as a function that we can call repeatedly. The evaluate_model() function below takes the train and test dataset, fits a model, then evaluates it, retuning the accuracy of the model on the test dataset.

# fit and evaluate a neural net model on the dataset def evaluate_model(trainX, trainy, testX, testy): # define model model = Sequential() model.add(Dense(15, input_dim=2, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model model.fit(trainX, trainy, epochs=200, verbose=0) # evaluate the model _, test_acc = model.evaluate(testX, testy, verbose=0) return test_acc 1 2 3 4 5 6 7 8 9 10 11 12 # fit and evaluate a neural net model on the dataset def evaluate_model ( trainX , trainy , testX , testy ) : # define model model = Sequential ( ) model . add ( Dense ( 15 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( trainX , trainy , epochs = 200 , verbose = 0 ) # evaluate the model _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) return test_acc

We can call this function 30 times, saving the test accuracy scores.

# repeated evaluation n_repeats = 30 scores = list() for _ in range(n_repeats): score = evaluate_model(trainX, trainy, testX, testy) print('> %.3f' % score) scores.append(score) 1 2 3 4 5 6 7 # repeated evaluation n_repeats = 30 scores = list ( ) for _ in range ( n_repeats ) : score = evaluate_model ( trainX , trainy , testX , testy ) print ( '> %.3f' % score ) scores . append ( score )

Once collected, we can summarize the distribution scores, first in terms of the mean and standard deviation, assuming the distribution is Gaussian, which is very reasonable.

# summarize the distribution of scores print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores))) 1 2 # summarize the distribution of scores print ( 'Scores Mean: %.3f, Standard Deviation: %.3f' % ( mean ( scores ) , std ( scores ) ) )

We can then summarize the distribution both as a histogram to show the shape of the distribution and as a box and whisker plot to show the spread and body of the distribution.

# histogram of distribution pyplot.hist(scores, bins=10) pyplot.show() # boxplot of distribution pyplot.boxplot(scores) pyplot.show() 1 2 3 4 5 6 # histogram of distribution pyplot . hist ( scores , bins = 10 ) pyplot . show ( ) # boxplot of distribution pyplot . boxplot ( scores ) pyplot . show ( )

The complete example of summarizing the variance of the MLP model on the chosen blobs dataset is listed below.

# demonstrate high variance of mlp model on blobs classification problem from sklearn.datasets import make_blobs from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense from numpy import mean from numpy import std from matplotlib import pyplot # fit and evaluate a neural net model on the dataset def evaluate_model(trainX, trainy, testX, testy): # define model model = Sequential() model.add(Dense(15, input_dim=2, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model model.fit(trainX, trainy, epochs=200, verbose=0) # evaluate the model _, test_acc = model.evaluate(testX, testy, verbose=0) return test_acc # generate 2d classification dataset X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2) y = to_categorical(y) # split into train and test n_train = int(0.3 * X.shape[0]) trainX, testX = X[:n_train, :], X[n_train:, :] trainy, testy = y[:n_train], y[n_train:] # repeated evaluation n_repeats = 30 scores = list() for _ in range(n_repeats): score = evaluate_model(trainX, trainy, testX, testy) print('> %.3f' % score) scores.append(score) # summarize the distribution of scores print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores))) # histogram of distribution pyplot.hist(scores, bins=10) pyplot.show() # boxplot of distribution pyplot.boxplot(scores) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # demonstrate high variance of mlp model on blobs classification problem from sklearn . datasets import make_blobs from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Dense from numpy import mean from numpy import std from matplotlib import pyplot # fit and evaluate a neural net model on the dataset def evaluate_model ( trainX , trainy , testX , testy ) : # define model model = Sequential ( ) model . add ( Dense ( 15 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( trainX , trainy , epochs = 200 , verbose = 0 ) # evaluate the model _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) return test_acc # generate 2d classification dataset X , y = make_blobs ( n_samples = 500 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 ) y = to_categorical ( y ) # split into train and test n_train = int ( 0.3 * X . shape [ 0 ] ) trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] # repeated evaluation n_repeats = 30 scores = list ( ) for _ in range ( n_repeats ) : score = evaluate_model ( trainX , trainy , testX , testy ) print ( '> %.3f' % score ) scores . append ( score ) # summarize the distribution of scores print ( 'Scores Mean: %.3f, Standard Deviation: %.3f' % ( mean ( scores ) , std ( scores ) ) ) # histogram of distribution pyplot . hist ( scores , bins = 10 ) pyplot . show ( ) # boxplot of distribution pyplot . boxplot ( scores ) pyplot . show ( )

Running the example first prints the accuracy of each model on the test set, finishing with the mean and standard deviation of the sample of accuracy scores.

The specifics of your sample may differ, but the summary statistics should be similar.

In this case, we can see that the average of the sample is 77% with a standard deviation of about 1.4%. Assuming a Gaussian distribution, we would expect 99% of accuracy scores to fall between about 73% and 81% (i.e. 3 standard deviations above and below the mean).

We can take the standard deviation of the accuracy of the model on the test set as an estimate for the variance of the predictions made by the model.

> 0.749 > 0.771 > 0.763 > 0.760 > 0.783 > 0.780 > 0.769 > 0.754 > 0.766 > 0.786 > 0.766 > 0.774 > 0.757 > 0.754 > 0.771 > 0.749 > 0.763 > 0.800 > 0.774 > 0.777 > 0.766 > 0.794 > 0.797 > 0.757 > 0.763 > 0.751 > 0.789 > 0.791 > 0.766 > 0.766 Scores Mean: 0.770, Standard Deviation: 0.014 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 > 0.749 > 0.771 > 0.763 > 0.760 > 0.783 > 0.780 > 0.769 > 0.754 > 0.766 > 0.786 > 0.766 > 0.774 > 0.757 > 0.754 > 0.771 > 0.749 > 0.763 > 0.800 > 0.774 > 0.777 > 0.766 > 0.794 > 0.797 > 0.757 > 0.763 > 0.751 > 0.789 > 0.791 > 0.766 > 0.766 Scores Mean: 0.770, Standard Deviation: 0.014

A histogram of the accuracy scores is also created, showing a very rough Gaussian shape, perhaps with a longer right tail.

A large sample and a different number of bins on the plot might better expose the true underlying shape of the distribution.

A box and whisker plot is also created showing a line at the median at about 76.5% accuracy on the test set and the interquartile range or middle 50% of the samples between about 78% and 76%.

The analysis of the sample of test scores clearly demonstrates a variance in the performance of the same model trained on the same dataset.

A spread of likely scores of about 8 percentage points (81% – 73%) on the test set could reasonably be considered large, e.g. a high variance result.

Model Averaging Ensemble

We can use model averaging to both reduce the variance of the model and possibly reduce the generalization error of the model.

Specifically, this would result in a smaller standard deviation on the holdout test set and a better performance on the training set. We can check both of these assumptions.

First, we must develop a function to prepare and return a fit model on the training dataset.

# fit model on dataset def fit_model(trainX, trainy): # define model model = Sequential() model.add(Dense(15, input_dim=2, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model model.fit(trainX, trainy, epochs=200, verbose=0) return model 1 2 3 4 5 6 7 8 9 10 # fit model on dataset def fit_model ( trainX , trainy ) : # define model model = Sequential ( ) model . add ( Dense ( 15 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( trainX , trainy , epochs = 200 , verbose = 0 ) return model

Next, we need a function that can take a list of ensemble members and make a prediction for an out of sample dataset. This could be one or more samples arranged in a two-dimensional array of samples and input features.

Hint: you can use this function yourself for testing ensembles and for making predictions with ensembles on new data.

# make an ensemble prediction for multi-class classification def ensemble_predictions(members, testX): # make predictions yhats = [model.predict(testX) for model in members] yhats = array(yhats) # sum across ensemble members summed = numpy.sum(yhats, axis=0) # argmax across classes result = argmax(summed, axis=1) return result 1 2 3 4 5 6 7 8 9 10 # make an ensemble prediction for multi-class classification def ensemble_predictions ( members , testX ) : # make predictions yhats = [ model . predict ( testX ) for model in members ] yhats = array ( yhats ) # sum across ensemble members summed = numpy . sum ( yhats , axis = 0 ) # argmax across classes result = argmax ( summed , axis = 1 ) return result

We don’t know how many ensemble members will be appropriate for this problem.

Therefore, we can perform a sensitivity analysis of the number of ensemble members and how it impacts test accuracy. This means we need a function that can evaluate a specified number of ensemble members and return the accuracy of a prediction combined from those members.

# evaluate a specific number of members in an ensemble def evaluate_n_members(members, n_members, testX, testy): # select a subset of members subset = members[:n_members] print(len(subset)) # make prediction yhat = ensemble_predictions(subset, testX) # calculate accuracy return accuracy_score(testy, yhat) 1 2 3 4 5 6 7 8 9 # evaluate a specific number of members in an ensemble def evaluate_n_members ( members , n_members , testX , testy ) : # select a subset of members subset = members [ : n_members ] print ( len ( subset ) ) # make prediction yhat = ensemble_predictions ( subset , testX ) # calculate accuracy return accuracy_score ( testy , yhat )

Finally, we can create a line plot of the number of ensemble members (x-axis) versus the accuracy of a prediction averaged across that many members on the test dataset (y-axis).

# plot score vs number of ensemble members x_axis = [i for i in range(1, n_members+1)] pyplot.plot(x_axis, scores) pyplot.show() 1 2 3 4 # plot score vs number of ensemble members x_axis = [ i for i in range ( 1 , n_members + 1 ) ] pyplot . plot ( x_axis , scores ) pyplot . show ( )

The complete example is listed below.

# model averaging ensemble and a study of ensemble size on test accuracy from sklearn.datasets import make_blobs from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense import numpy from numpy import array from numpy import argmax from sklearn.metrics import accuracy_score from matplotlib import pyplot # fit model on dataset def fit_model(trainX, trainy): # define model model = Sequential() model.add(Dense(15, input_dim=2, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model model.fit(trainX, trainy, epochs=200, verbose=0) return model # make an ensemble prediction for multi-class classification def ensemble_predictions(members, testX): # make predictions yhats = [model.predict(testX) for model in members] yhats = array(yhats) # sum across ensemble members summed = numpy.sum(yhats, axis=0) # argmax across classes result = argmax(summed, axis=1) return result # evaluate a specific number of members in an ensemble def evaluate_n_members(members, n_members, testX, testy): # select a subset of members subset = members[:n_members] print(len(subset)) # make prediction yhat = ensemble_predictions(subset, testX) # calculate accuracy return accuracy_score(testy, yhat) # generate 2d classification dataset X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2) # split into train and test n_train = int(0.3 * X.shape[0]) trainX, testX = X[:n_train, :], X[n_train:, :] trainy, testy = y[:n_train], y[n_train:] trainy = to_categorical(trainy) # fit all models n_members = 20 members = [fit_model(trainX, trainy) for _ in range(n_members)] # evaluate different numbers of ensembles scores = list() for i in range(1, n_members+1): score = evaluate_n_members(members, i, testX, testy) print('> %.3f' % score) scores.append(score) # plot score vs number of ensemble members x_axis = [i for i in range(1, n_members+1)] pyplot.plot(x_axis, scores) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 # model averaging ensemble and a study of ensemble size on test accuracy from sklearn . datasets import make_blobs from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Dense import numpy from numpy import array from numpy import argmax from sklearn . metrics import accuracy_score from matplotlib import pyplot # fit model on dataset def fit_model ( trainX , trainy ) : # define model model = Sequential ( ) model . add ( Dense ( 15 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( trainX , trainy , epochs = 200 , verbose = 0 ) return model # make an ensemble prediction for multi-class classification def ensemble_predictions ( members , testX ) : # make predictions yhats = [ model . predict ( testX ) for model in members ] yhats = array ( yhats ) # sum across ensemble members summed = numpy . sum ( yhats , axis = 0 ) # argmax across classes result = argmax ( summed , axis = 1 ) return result # evaluate a specific number of members in an ensemble def evaluate_n_members ( members , n_members , testX , testy ) : # select a subset of members subset = members [ : n_members ] print ( len ( subset ) ) # make prediction yhat = ensemble_predictions ( subset , testX ) # calculate accuracy return accuracy_score ( testy , yhat ) # generate 2d classification dataset X , y = make_blobs ( n_samples = 500 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 ) # split into train and test n_train = int ( 0.3 * X . shape [ 0 ] ) trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] trainy = to_categorical ( trainy ) # fit all models n_members = 20 members = [ fit_model ( trainX , trainy ) for _ in range ( n_members ) ] # evaluate different numbers of ensembles scores = list ( ) for i in range ( 1 , n_members + 1 ) : score = evaluate_n_members ( members , i , testX , testy ) print ( '> %.3f' % score ) scores . append ( score ) # plot score vs number of ensemble members x_axis = [ i for i in range ( 1 , n_members + 1 ) ] pyplot . plot ( x_axis , scores ) pyplot . show ( )

Running the example first fits 20 models on the same training dataset, which may take less than a minute on modern hardware.

Then, different sized ensembles are tested from 1 member to all 20 members and test accuracy results are printed for each ensemble size.

1 > 0.740 2 > 0.754 3 > 0.754 4 > 0.760 5 > 0.763 6 > 0.763 7 > 0.763 8 > 0.763 9 > 0.760 10 > 0.760 11 > 0.763 12 > 0.763 13 > 0.766 14 > 0.763 15 > 0.760 16 > 0.760 17 > 0.763 18 > 0.766 19 > 0.763 20 > 0.763 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 1 > 0.740 2 > 0.754 3 > 0.754 4 > 0.760 5 > 0.763 6 > 0.763 7 > 0.763 8 > 0.763 9 > 0.760 10 > 0.760 11 > 0.763 12 > 0.763 13 > 0.766 14 > 0.763 15 > 0.760 16 > 0.760 17 > 0.763 18 > 0.766 19 > 0.763 20 > 0.763

Finally, a line plot is created showing the relationship between ensemble size and performance on the test set.

We can see that performance improves to about five members, after which performance plateaus around 76% accuracy. This is close to the average test set performance observed during the analysis of the repeated evaluation of the model.

Finally, we can update the repeated evaluation experiment to use an ensemble of five models instead of a single model and compare the distribution of scores.

The complete example of a repeated evaluated five-member ensemble of the blobs dataset is listed below.

# repeated evaluation of model averaging ensemble on blobs dataset from sklearn.datasets import make_blobs from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense import numpy from numpy import array from numpy import argmax from numpy import mean from numpy import std from sklearn.metrics import accuracy_score # fit model on dataset def fit_model(trainX, trainy): # define model model = Sequential() model.add(Dense(15, input_dim=2, activation='relu')) model.add(Dense(3, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model model.fit(trainX, trainy, epochs=200, verbose=0) return model # make an ensemble prediction for multi-class classification def ensemble_predictions(members, testX): # make predictions yhats = [model.predict(testX) for model in members] yhats = array(yhats) # sum across ensemble members summed = numpy.sum(yhats, axis=0) # argmax across classes result = argmax(summed, axis=1) return result # evaluate ensemble model def evaluate_members(members, testX, testy): # make prediction yhat = ensemble_predictions(members, testX) # calculate accuracy return accuracy_score(testy, yhat) # generate 2d classification dataset X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2) # split into train and test n_train = int(0.3 * X.shape[0]) trainX, testX = X[:n_train, :], X[n_train:, :] trainy, testy = y[:n_train], y[n_train:] trainy = to_categorical(trainy) # repeated evaluation n_repeats = 30 n_members = 5 scores = list() for _ in range(n_repeats): # fit all models members = [fit_model(trainX, trainy) for _ in range(n_members)] # evaluate ensemble score = evaluate_members(members, testX, testy) print('> %.3f' % score) scores.append(score) # summarize the distribution of scores print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores))) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 # repeated evaluation of model averaging ensemble on blobs dataset from sklearn . datasets import make_blobs from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Dense import numpy from numpy import array from numpy import argmax from numpy import mean from numpy import std from sklearn . metrics import accuracy_score # fit model on dataset def fit_model ( trainX , trainy ) : # define model model = Sequential ( ) model . add ( Dense ( 15 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( trainX , trainy , epochs = 200 , verbose = 0 ) return model # make an ensemble prediction for multi-class classification def ensemble_predictions ( members , testX ) : # make predictions yhats = [ model . predict ( testX ) for model in members ] yhats = array ( yhats ) # sum across ensemble members summed = numpy . sum ( yhats , axis = 0 ) # argmax across classes result = argmax ( summed , axis = 1 ) return result # evaluate ensemble model def evaluate_members ( members , testX , testy ) : # make prediction yhat = ensemble_predictions ( members , testX ) # calculate accuracy return accuracy_score ( testy , yhat ) # generate 2d classification dataset X , y = make_blobs ( n_samples = 500 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 ) # split into train and test n_train = int ( 0.3 * X . shape [ 0 ] ) trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] trainy = to_categorical ( trainy ) # repeated evaluation n_repeats = 30 n_members = 5 scores = list ( ) for _ in range ( n_repeats ) : # fit all models members = [ fit_model ( trainX , trainy ) for _ in range ( n_members ) ] # evaluate ensemble score = evaluate_members ( members , testX , testy ) print ( '> %.3f' % score ) scores . append ( score ) # summarize the distribution of scores print ( 'Scores Mean: %.3f, Standard Deviation: %.3f' % ( mean ( scores ) , std ( scores ) ) )

Running the example may take a few minutes as five models are fit and evaluated and this process is repeated 30 times.

The performance of each model on the test set is printed to provide an indication of progress.

The mean and standard deviation of the model performance is printed at the end of the run. Your specific results may vary, but not by much.

> 0.769 > 0.757 > 0.754 > 0.780 > 0.771 > 0.774 > 0.766 > 0.769 > 0.774 > 0.771 > 0.760 > 0.766 > 0.766 > 0.769 > 0.766 > 0.771 > 0.763 > 0.760 > 0.771 > 0.780 > 0.769 > 0.757 > 0.769 > 0.771 > 0.771 > 0.766 > 0.763 > 0.766 > 0.771 > 0.769 Scores Mean: 0.768, Standard Deviation: 0.006 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 > 0.769 > 0.757 > 0.754 > 0.780 > 0.771 > 0.774 > 0.766 > 0.769 > 0.774 > 0.771 > 0.760 > 0.766 > 0.766 > 0.769 > 0.766 > 0.771 > 0.763 > 0.760 > 0.771 > 0.780 > 0.769 > 0.757 > 0.769 > 0.771 > 0.771 > 0.766 > 0.763 > 0.766 > 0.771 > 0.769 Scores Mean: 0.768, Standard Deviation: 0.006

In this case, we can see that the average performance of a five-member ensemble on the dataset is 76%. This is very close to the average of 77% seen for a single model.

The important difference is the standard deviation shrinking from 1.4% for a single model to 0.6% with an ensemble of five models. We might expect that a given ensemble of five models on this problem to have a performance fall between about 74% and about 78% with a likelihood of 99%.

Averaging the same model trained on the same dataset gives us a spread for improved reliability, a property often highly desired in a final model to be used operationally.

More models in the ensemble will further decrease the standard deviation of the accuracy of an ensemble on the test dataset given the law of large numbers, at least to a point of diminishing returns.

This demonstrates that for this specific model and prediction problem, that a model averaging ensemble with five members is sufficient to reduce the variance of the model. This reduction in variance, in turn, also means a better on-average performance when preparing a final model.

Extensions

This section lists some ideas for extending the tutorial that you may wish to explore.

Average Class Prediction . Update the example to average the class integer prediction instead of the class probability prediction and compare results.

. Update the example to average the class integer prediction instead of the class probability prediction and compare results. Save and Load Models . Update the example to save ensemble members to file, then load them from a separate script for evaluation.

. Update the example to save ensemble members to file, then load them from a separate script for evaluation. Sensitivity of Variance. Create a new example that performs a sensitivity analysis of the number of ensemble members on the standard deviation of model performance on the test set over a given number of repeats and report the point of diminishing returns.

If you explore any of these extensions, I’d love to know.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this tutorial, you discovered how to develop a model averaging ensemble in Keras to reduce the variance in a final model.

Specifically, you learned:

Model averaging is an ensemble learning technique that can be used to reduce the expected variance of deep learning neural network models.

How to implement model averaging in Keras for classification and regression predictive modeling problems.

How to work through a multi-class classification problem and use model averaging to reduce the variance of the final model.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
158;machinelearningmastery.com;https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/;2017-04-06;Time Series Forecasting with the Long Short-Term Memory Network in Python;"from pandas import DataFrame

from pandas import Series

from pandas import concat

from pandas import read_csv

from pandas import datetime

from sklearn . metrics import mean_squared_error

from sklearn . preprocessing import MinMaxScaler

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from math import sqrt

from matplotlib import pyplot

import numpy

# date-time parsing function for loading the dataset

def parser ( x ) :

return datetime . strptime ( '190' + x , '%Y-%m' )

# frame a sequence as a supervised learning problem

def timeseries_to_supervised ( data , lag = 1 ) :

df = DataFrame ( data )

columns = [ df . shift ( i ) for i in range ( 1 , lag + 1 ) ]

columns . append ( df )

df = concat ( columns , axis = 1 )

df . fillna ( 0 , inplace = True )

return df

# create a differenced series

def difference ( dataset , interval = 1 ) :

diff = list ( )

for i in range ( interval , len ( dataset ) ) :

value = dataset [ i ] - dataset [ i - interval ]

diff . append ( value )

return Series ( diff )

# invert differenced value

def inverse_difference ( history , yhat , interval = 1 ) :

return yhat + history [ - interval ]

# scale train and test data to [-1, 1]

def scale ( train , test ) :

# fit scaler

scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) )

scaler = scaler . fit ( train )

# transform train

train = train . reshape ( train . shape [ 0 ] , train . shape [ 1 ] )

train_scaled = scaler . transform ( train )

# transform test

test = test . reshape ( test . shape [ 0 ] , test . shape [ 1 ] )

test_scaled = scaler . transform ( test )

return scaler , train_scaled , test_scaled

# inverse scaling for a forecasted value

def invert_scale ( scaler , X , value ) :

new_row = [ x for x in X ] + [ value ]

array = numpy . array ( new_row )

array = array . reshape ( 1 , len ( array ) )

inverted = scaler . inverse_transform ( array )

return inverted [ 0 , - 1 ]

# fit an LSTM network to training data

def fit_lstm ( train , batch_size , nb_epoch , neurons ) :

X , y = train [ : , 0 : - 1 ] , train [ : , - 1 ]

X = X . reshape ( X . shape [ 0 ] , 1 , X . shape [ 1 ] )

model = Sequential ( )

model . add ( LSTM ( neurons , batch_input_shape = ( batch_size , X . shape [ 1 ] , X . shape [ 2 ] ) , stateful = True ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

for i in range ( nb_epoch ) :

model . fit ( X , y , epochs = 1 , batch_size = batch_size , verbose = 0 , shuffle = False )

model . reset_states ( )

return model

# make a one-step forecast

def forecast_lstm ( model , batch_size , X ) :

X = X . reshape ( 1 , 1 , len ( X ) )

yhat = model . predict ( X , batch_size = batch_size )

return yhat [ 0 , 0 ]

# load dataset

series = read_csv ( 'shampoo-sales.csv' , header = 0 , parse_dates = [ 0 ] , index_col = 0 , squeeze = True , date_parser = parser )

# transform data to be stationary

raw_values = series . values

diff_values = difference ( raw_values , 1 )

# transform data to be supervised learning

supervised = timeseries_to_supervised ( diff_values , 1 )

supervised_values = supervised . values

# split data into train and test-sets

train , test = supervised_values [ 0 : - 12 ] , supervised_values [ - 12 : ]

# transform the scale of the data

scaler , train_scaled , test_scaled = scale ( train , test )

# repeat experiment

repeats = 30

error_scores = list ( )

for r in range ( repeats ) :

# fit the model

lstm_model = fit_lstm ( train_scaled , 1 , 3000 , 4 )

# forecast the entire training dataset to build up state for forecasting

train_reshaped = train_scaled [ : , 0 ] . reshape ( len ( train_scaled ) , 1 , 1 )

lstm_model . predict ( train_reshaped , batch_size = 1 )

# walk-forward validation on the test data

predictions = list ( )

for i in range ( len ( test_scaled ) ) :

# make one-step forecast

X , y = test_scaled [ i , 0 : - 1 ] , test_scaled [ i , - 1 ]

yhat = forecast_lstm ( lstm_model , 1 , X )

# invert scaling

yhat = invert_scale ( scaler , X , yhat )

# invert differencing

yhat = inverse_difference ( raw_values , yhat , len ( test_scaled ) + 1 - i )

# store forecast

predictions . append ( yhat )

# report performance

rmse = sqrt ( mean_squared_error ( raw_values [ - 12 : ] , predictions ) )

print ( '%d) Test RMSE: %.3f' % ( r + 1 , rmse ) )

error_scores . append ( rmse )

# summarize results

results = DataFrame ( )

results [ 'rmse' ] = error_scores

print ( results . describe ( ) )

results . boxplot ( )"
159;machinelearningmastery.com;http://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/;2016-04-05;Linear Discriminant Analysis for Machine Learning;"Tweet Share Share

Last Updated on February 4, 2020

Logistic regression is a classification algorithm traditionally limited to only two-class classification problems.

If you have more than two classes then Linear Discriminant Analysis is the preferred linear classification technique.

In this post you will discover the Linear Discriminant Analysis (LDA) algorithm for classification predictive modeling problems. After reading this post you will know:

The limitations of logistic regression and the need for linear discriminant analysis.

The representation of the model that is learned from data and can be saved to file.

How the model is estimated from your data.

How to make predictions from a learned LDA model.

How to prepare your data to get the most from the LDA model.

This post is intended for developers interested in applied machine learning, how the models work and how to use them well. As such no background in statistics or linear algebra is required, although it does help if you know about the mean and variance of a distribution.

LDA is a simple model in both preparation and application. There is some interesting statistics behind how the model is setup and how the prediction equation is derived, but is not covered in this post.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Limitations of Logistic Regression

Logistic regression is a simple and powerful linear classification algorithm. It also has limitations that suggest at the need for alternate linear classification algorithms.

Two-Class Problems . Logistic regression is intended for two-class or binary classification problems. It can be extended for multi-class classification, but is rarely used for this purpose.

. Logistic regression is intended for two-class or binary classification problems. It can be extended for multi-class classification, but is rarely used for this purpose. Unstable With Well Separated Classes . Logistic regression can become unstable when the classes are well separated.

. Logistic regression can become unstable when the classes are well separated. Unstable With Few Examples. Logistic regression can become unstable when there are few examples from which to estimate the parameters.

Linear Discriminant Analysis does address each of these points and is the go-to linear method for multi-class classification problems. Even with binary-classification problems, it is a good idea to try both logistic regression and linear discriminant analysis.

Representation of LDA Models

The representation of LDA is straight forward.

It consists of statistical properties of your data, calculated for each class. For a single input variable (x) this is the mean and the variance of the variable for each class. For multiple variables, this is the same properties calculated over the multivariate Gaussian, namely the means and the covariance matrix.

These statistical properties are estimated from your data and plug into the LDA equation to make predictions. These are the model values that you would save to file for your model.

Let’s look at how these parameters are estimated.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Learning LDA Models

LDA makes some simplifying assumptions about your data:

That your data is Gaussian, that each variable is is shaped like a bell curve when plotted. That each attribute has the same variance, that values of each variable vary around the mean by the same amount on average.

With these assumptions, the LDA model estimates the mean and variance from your data for each class. It is easy to think about this in the univariate (single input variable) case with two classes.

The mean (mu) value of each input (x) for each class (k) can be estimated in the normal way by dividing the sum of values by the total number of values.

muk = 1/nk * sum(x)

Where muk is the mean value of x for the class k, nk is the number of instances with class k. The variance is calculated across all classes as the average squared difference of each value from the mean.

sigma^2 = 1 / (n-K) * sum((x – mu)^2)

Where sigma^2 is the variance across all inputs (x), n is the number of instances, K is the number of classes and mu is the mean for input x.

Making Predictions with LDA

LDA makes predictions by estimating the probability that a new set of inputs belongs to each class. The class that gets the highest probability is the output class and a prediction is made.

The model uses Bayes Theorem to estimate the probabilities. Briefly Bayes’ Theorem can be used to estimate the probability of the output class (k) given the input (x) using the probability of each class and the probability of the data belonging to each class:

P(Y=x|X=x) = (PIk * fk(x)) / sum(PIl * fl(x))

Where PIk refers to the base probability of each class (k) observed in your training data (e.g. 0.5 for a 50-50 split in a two class problem). In Bayes’ Theorem this is called the prior probability.

PIk = nk/n

The f(x) above is the estimated probability of x belonging to the class. A Gaussian distribution function is used for f(x). Plugging the Gaussian into the above equation and simplifying we end up with the equation below. This is called a discriminate function and the class is calculated as having the largest value will be the output classification (y):

Dk(x) = x * (muk/siga^2) – (muk^2/(2*sigma^2)) + ln(PIk)

Dk(x) is the discriminate function for class k given input x, the muk, sigma^2 and PIk are all estimated from your data.

How to Prepare Data for LDA

This section lists some suggestions you may consider when preparing your data for use with LDA.

Classification Problems . This might go without saying, but LDA is intended for classification problems where the output variable is categorical. LDA supports both binary and multi-class classification.

. This might go without saying, but LDA is intended for classification problems where the output variable is categorical. LDA supports both binary and multi-class classification. Gaussian Distribution . The standard implementation of the model assumes a Gaussian distribution of the input variables. Consider reviewing the univariate distributions of each attribute and using transforms to make them more Gaussian-looking (e.g. log and root for exponential distributions and Box-Cox for skewed distributions).

. The standard implementation of the model assumes a Gaussian distribution of the input variables. Consider reviewing the univariate distributions of each attribute and using transforms to make them more Gaussian-looking (e.g. log and root for exponential distributions and Box-Cox for skewed distributions). Remove Outliers . Consider removing outliers from your data. These can skew the basic statistics used to separate classes in LDA such the mean and the standard deviation.

. Consider removing outliers from your data. These can skew the basic statistics used to separate classes in LDA such the mean and the standard deviation. Same Variance. LDA assumes that each input variable has the same variance. It is almost always a good idea to standardize your data before using LDA so that it has a mean of 0 and a standard deviation of 1.

Extensions to LDA

Linear Discriminant Analysis is a simple and effective method for classification. Because it is simple and so well understood, there are many extensions and variations to the method. Some popular extensions include:

Quadratic Discriminant Analysis (QDA) : Each class uses its own estimate of variance (or covariance when there are multiple input variables).

: Each class uses its own estimate of variance (or covariance when there are multiple input variables). Flexible Discriminant Analysis (FDA) : Where non-linear combinations of inputs is used such as splines.

: Where non-linear combinations of inputs is used such as splines. Regularized Discriminant Analysis (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA.

The original development was called the Linear Discriminant or Fisher’s Discriminant Analysis. The multi-class version was referred to Multiple Discriminant Analysis. These are all simply referred to as Linear Discriminant Analysis now.

Further Reading

This section provides some additional resources if you are looking to go deeper. I have to credit the book An Introduction to Statistical Learning: with Applications in R, some description and the notation in this post was taken from this text, it’s excellent.

Books

Other

Summary

In this post you discovered Linear Discriminant Analysis for classification predictive modeling problems. You learned:

The model representation for LDA and what is actually distinct about a learned model.

How the parameters of the LDA model can be estimated from training data.

How the model can be used to make predictions on new data.

How to prepare your data to get the most from the method.

Do you have any questions about this post?

Leave a comment and ask, I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
160;news.mit.edu;http://news.mit.edu/2020/neural-networks-optimize-materials-search-0326;;Neural networks facilitate optimization in the search for new materials;"When searching through theoretical lists of possible new materials for particular applications, such as batteries or other energy-related devices, there are often millions of potential materials that could be considered, and multiple criteria that need to be met and optimized at once. Now, researchers at MIT have found a way to dramatically streamline the discovery process, using a machine learning system.

As a demonstration, the team arrived at a set of the eight most promising materials, out of nearly 3 million candidates, for an energy storage system called a flow battery. This culling process would have taken 50 years by conventional analytical methods, they say, but they accomplished it in five weeks.

The findings are reported in the journal ACS Central Science, in a paper by MIT professor of chemical engineering Heather Kulik, Jon Paul Janet PhD ’19, Sahasrajit Ramesh, and graduate student Chenru Duan.

The study looked at a set of materials called transition metal complexes. These can exist in a vast number of different forms, and Kulik says they “are really fascinating, functional materials that are unlike a lot of other material phases. The only way to understand why they work the way they do is to study them using quantum mechanics.”

To predict the properties of any one of millions of these materials would require either time-consuming and resource-intensive spectroscopy and other lab work, or time-consuming, highly complex physics-based computer modeling for each possible candidate material or combination of materials. Each such study could consume hours to days of work.

Instead, Kulik and her team took a small number of different possible materials and used them to teach an advanced machine-learning neural network about the relationship between the materials’ chemical compositions and their physical properties. That knowledge was then applied to generate suggestions for the next generation of possible materials to be used for the next round of training of the neural network. Through four successive iterations of this process, the neural network improved significantly each time, until reaching a point where it was clear that further iterations would not yield any further improvements.

This iterative optimization system greatly streamlined the process of arriving at potential solutions that satisfied the two conflicting criteria being sought. This kind of process of finding the best solutions in situations, where improving one factor tends to worsen the other, is known as a Pareto front, representing a graph of the points such that any further improvement of one factor would make the other worse. In other words, the graph represents the best possible compromise points, depending on the relative importance assigned to each factor.

Training typical neural networks requires very large data sets, ranging from thousands to millions of examples, but Kulik and her team were able to use this iterative process, based on the Pareto front model, to streamline the process and provide reliable results using only the few hundred samples.

In the case of screening for the flow battery materials, the desired characteristics were in conflict, as is often the case: The optimum material would have high solubility and a high energy density (the ability to store energy for a given weight). But increasing solubility tends to decrease the energy density, and vice versa.

Not only was the neural network able to rapidly come up with promising candidates, it also was able to assign levels of confidence to its different predictions through each iteration, which helped to allow the refinement of the sample selection at each step. “We developed a better than best-in-class uncertainty quantification technique for really knowing when these models were going to fail,” Kulik says.

The challenge they chose for the proof-of-concept trial was materials for use in redox flow batteries, a type of battery that holds promise for large, grid-scale batteries that could play a significant role in enabling clean, renewable energy. Transition metal complexes are the preferred category of materials for such batteries, Kulik says, but there are too many possibilities to evaluate by conventional means. They started out with a list of 3 million such complexes before ultimately whittling that down to the eight good candidates, along with a set of design rules that should enable experimentalists to explore the potential of these candidates and their variations.

“Through that process, the neural net both gets increasingly smarter about the [design] space, but also increasingly pessimistic that anything beyond what we’ve already characterized can further improve on what we already know,” she says.

Apart from the specific transition metal complexes suggested for further investigation using this system, she says, the method itself could have much broader applications. “We do view it as the framework that can be applied to any materials design challenge where you're really trying to address multiple objectives at once. You know, all of the most interesting materials design challenges are ones where you have one thing you're trying to improve, but improving that worsens another. And for us, the redox flow battery redox couple was just a good demonstration of where we think we can go with this machine learning and accelerated materials discovery.”

For example, optimizing catalysts for various chemical and industrial processes is another kind of such complex materials search, Kulik says. Presently used catalysts often involve rare and expensive elements, so finding similarly effective compounds based on abundant and inexpensive materials could be a significant advantage.

“This paper represents, I believe, the first application of multidimensional directed improvement in the chemical sciences,” she says. But the long-term significance of the work is in the methodology itself, because of things that might not be possible at all otherwise. “You start to realize that even with parallel computations, these are cases where we wouldn't have come up with a design principle in any other way. And these leads that are coming out of our work, these are not necessarily at all ideas that were already known from the literature or that an expert would have been able to point you to.”

“This is a beautiful combination of concepts in statistics, applied math, and physical science that is going to be extremely useful in engineering applications,” says George Schatz, a professor of chemistry and of chemical and biological engineering at Northwestern University, who was not associated with this work. He says this research addresses “how to do machine learning when there are multiple objectives. Kulik’s approach uses leading edge methods to train an artificial neural network that is used to predict which combination of transition metal ions and organic ligands will be best for redox flow battery electrolytes.”

Schatz says “this method can be used in many different contexts, so it has the potential to transform machine learning, which is a major activity around the world.”

The work was supported by the Office of Naval Research, the Defense Advanced Research Projects Agency (DARPA), the U.S. Department of Energy, the Burroughs Wellcome Fund, and the AAAS Mar ion Milligan Mason Award."
161;machinelearningmastery.com;http://machinelearningmastery.com/how-to-use-r-for-machine-learning/;2016-01-17;How To Use R For Machine Learning;"Tweet Share Share

Last Updated on August 22, 2019

There are a ton of packages for R. Which ones are best to use for your machine learning project?

In this post you will discover the exact R functions and packages recommended for each sub task in a machine learning journey.

This is useful. Bookmark this page. I’m sure you will be checking back time and again.

If you’re an R user and know a better way, share it in the comments and I will update the list.

Discover how to prepare data, fit machine learning models and evaluate their predictions in R with my new book, including 14 step-by-step tutorials, 3 projects, and full source code.

Let’s get started.

What R Packages Should You Use?

There are more than 6,000 third party packages for R. The vast number of packages available is one of the benefits of the R platform. It is also the frustration.

Which packages should you use?

There are specific tasks that you need to perform as part of a machine learning project. Tasks like loading data, evaluating algorithms and improving accuracy. You can use multiple techniques for each task and multiple packages may provide those techniques.

Given that there are so many different ways to complete a given subtask, you need to discover those functions and packages that best meet your needs.

Map Best-Of-Breed Packages Onto Project Tasks

The way to solve this problem is to create a mapping of all of the sub-tasks you are likely to work on during a machine learning project and find the best-of-breed packages and functions that you can use.

You start by listing all of the sub-tasks in a machine learning project. You can take a close look at the process of applied machine learning and the machine learning project checklist.

Given that R is a statistical language, it provides a lot of tools that you can use for data analysis as well as predictive models that you can train and use to generate predictions.

Using your favorite search engine, you can locate all of the packages and functions in packages that you can use to complete each task. This can be exhaustive and you can end up with many different candidate solutions.

You need to reduce each list of options down to the one preferred way of completing a task. You could experiment with each and see what works for you. You could also carefully review you search results and tease out the most popular functions used by practitioners.

Next up is a mapping from R packages and functions to the tasks of a machine learning project that you can use to get started today using R for machine learning.

Need more Help with R for Machine Learning? Take my free 14-day email course and discover how to use R on your project (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

How To Use R For Machine Learning Projects

This section lists many of the main sub-tasks of a generic machine learning project. Each task lists the specific function and parent package that you can use in R to complete the task.

Some properties of the chosen functions are:

Minimum : the list is a bare minimum of both the machine learning tasks in a project and only the function and package name that you can use. More homework is required to actually use each of the functions listed.

: the list is a bare minimum of both the machine learning tasks in a project and only the function and package name that you can use. More homework is required to actually use each of the functions listed. Simple : functions were chosen for simplicity in delivering a direct result for the task. One function was preferred over multiple function calls.

: functions were chosen for simplicity in delivering a direct result for the task. One function was preferred over multiple function calls. Preference: functions were chosen based on my preferences and best estimation. Other practitioners may have different alternatives (share in the comments!).

Tasks are organized into three broad groups:

Data preparation tasks for getting data ready for modeling. Evaluating algorithm tasks for racing and evaluating predictive modeling algorithms. Improve results tasks for getting more out of well performing algorithms.

1. Data Preparation Tasks

Data Loading

Load a dataset from your file.

CSV: read.csv function from the utils package

Data Cleaning

Clean up a dataset to ensure that the data is reasonable and consistent ready for analysis and modeling.

Imputing: impute from the Hmisc package.

Outliers: various functions from the outliers package.

Rebalance: SMOTE function from the DMwR package.

Data Summary

Summarize a dataset using descriptive statistics.

Summarize Distributions: summary function from base package.

Summarize Correlations: cor function from the stats package

Data Visualization

Summarize a dataset visually.

Scatterplot Matrix: pairs function from the graphics package.

Histogram: hist function from the graphics package.

Density Plot: densityplot function from the lattice package.

Box and Whisker Plot: boxplot function from the graphics package

Honorable mentions:

ggpairs function from the GGally package which can do it all on one plot

ggplot2 and lattice packages in general are excellent for plotting

Feature Selection

Select those features in the dataset that are most relevant for building a predictive model.

RFE: rfe function from the caret package

Correlated: findCorrelation function from the caret package

The caret package provides a suite of feature selection methods, see Evaluate Algorithm Tasks.

Honorable mentions:

Data Transformation

Create transforms of the dataset to best expose the structure of the problem to the learning algorithms.

Normalize: custom written function

Standardize: scale function from the base package.

The caret package provides data transforms as part of the test harness, see the next section.

2. Evaluate Algorithm Tasks

Functions from the caret package should be used to evaluate models on your dataset.

The caret package supports various performance measures and test options such as data splits and cross validation. Pre-processing can also be configured as part of the test harness.

Model Evaluation

Model Evaluation: train function from the caret package.

Test Options: trainControl function from the caret package.

Preprocessing Options: preProcess function from the caret package.

Note that many modern predictive models (such as flavors of advanced decision trees) provide some form of feature selection, parameter tuning and ensembling built in.

Predictive Models

The caret package provides access to all of the best of breed predictive modeling algorithms.

3. Improve Result Tasks

Techniques for getting the most out of well performing models in service of making accurate predictions.

Algorithm Tuning

The caret package provides algorithm tuning as part of the test harness and includes techniques such as random, grid and adaptive search.

Model Ensembles

Many modern predictive modeling algorithms provide ensembling built-in. A suite of bagging and boosting functions are provided in the caret package.

Blend: caretEnsemble from the caretEnsemble package.

Stacking: caretStack from the caretEnsemble package.

Bagging: bagging function from the ipred package.

Summary

In this post you discovered that the best way to use R for machine learning is to map specific R functions and packages onto the tasks of a machine learning project.

You discovered the specific packages and functions that you can use for the most common tasks of a machine learning project, including links to further documentation.

Your Next Step

Get started using R for machine learning. Use the suggestions above on your current or next machine learning project.

Did I miss an important package? Did I miss a key task in a machine learning project? Leave a comment and let me know what I missed.

Do you have a question? Email me or leave a comment.

Discover Faster Machine Learning in R! Develop Your Own Models in Minutes ...with just a few lines of R code Discover how in my new Ebook:

Machine Learning Mastery With R Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
162;news.mit.edu;http://news.mit.edu/2019/g-anthony-grant-named-director-athletics-daper-department-head-1209;;G. Anthony Grant named director of athletics, DAPER department head;"G. Anthony Grant has been named MIT’s new athletic director and head of the Department of Athletics, Physical Education, and Recreation (DAPER).

“This role is a big one, and I am excited for Dr. Grant as he begins an exciting new phase of his career, and for the MIT community as we welcome a new leader to our team,” says Suzy M. Nelson, vice president and dean for student life.

Grant joins MIT from Metropolitan State University (MSU Denver) in Colorado, where he has been director of athletics. In more than four years at MSU Denver, Grant worked very closely with student-athletes and university leadership to align the athletics department’s values with those of the university overall. Grant also collaborated with internal and external constituents to develop a three-year strategic plan that set the department’s future direction.

“It is an honor to be selected as the director of athletics and department head of DAPER at MIT,” says Grant. “I would like to thank Suzy Nelson, Judy Robinson, and the members of the search committee for this tremendous opportunity. There is important work being done in DAPER and the Division of Student Life. I am eager to collaborate with faculty, staff, students, and supporters to continue to build on the academic and athletic success that has been established within the department, as well as to provide a service to the broader community as it relates to promoting an environment of health and wellness throughout campus.”

In competition, teams in Grant’s program were competitive and successful in the Rocky Mountain Athletic Conference (RMAC), as well as on the regional and national level within NCAA Division II. The program also boasted an excellent academic record, with an average of 78 student-athletes on the athletic director’s honor roll each semester. These academic achievements rank among Grants’ proudest achievements during his MSU Denver tenure.

As a thought leader among collegiate athletic directors, Grant committed himself to supporting underrepresented minority student-athletes and administrators by serving on the NCAA Minority Opportunities and Interests Committee, and the RMAC Diversity and Inclusion Committee. Before MSU Denver, Grant held positions of increasing responsibility in the athletic departments at Millersville University, Pennsylvania, and the University of Iowa, where he completed his PhD. He also holds a bachelor’s degree from Penn State University and a master’s from Temple University."
163;machinelearningmastery.com;https://machinelearningmastery.com/dont-start-with-open-source-code-when-implementing-machine-learning-algorithms/;2014-10-30;Don’t Start with Open-Source Code When Implementing Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

Edward Raff is the author of the Java Machine Learning library called JSAT (which is an acronym for Java Statistical Analysis Tool).

Edward has implemented many algorithms in creating this library and I recently reached out to him and asked what advice he could give to beginners implementing machine learning algorithms from scratch.

In this post we take a look at tips on implementing machine learning algorithms based on Edwards advice.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Don’t Read Other Peoples Source Code

At least, not initially.

What drew me to ask Edward questions about his advice on implementing machine learning algorithms from scratch was his comment on a Reddit question, titled appropriately “Implementing Machine Learning Algorithms“.

In his comment, Edward suggested that beginners avoid looking at source code of other open source implementations as much as possible. He knew this was counter to most advice (even my own) and it really caught my attention.

Edward start’s out suggesting that there are two quite different tasks when implementing machine learning algorithms:

Implementing Well Known Algorithms. These are well described in many papers, books, websites lecture notes and so on. You have many sources, they algorithms are relatively straight forward and they are good case studies for self education. Implementing Algorithms From Papers. These are algorithms that have limited and sparse descriptions in literature and require significant work and prior knowledge to understand and implement.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Implementing Well Known Algorithms

Edward suggests reading code is a bad idea if you are interested in implementing well known algorithms.

I distilled at least 3 key reasons for why this is the case from his comments:

Code Optimization : Code in open source libraries is very likely highly optimized for execution speed, memory and accuracy. This means that it implements all kinds of mathematical and programming tricks. As a result, the code will be very difficult to follow. You will spend the majority of your time figuring out the tricks rather than figuring out the algorithm, which was your goal in the first place.

: Code in open source libraries is very likely highly optimized for execution speed, memory and accuracy. This means that it implements all kinds of mathematical and programming tricks. As a result, the code will be very difficult to follow. You will spend the majority of your time figuring out the tricks rather than figuring out the algorithm, which was your goal in the first place. Project Centric : The code will not be a generic implementation ready for you to run in isolation, it will be carefully designed to work within the project’s framework. It is also very likely that details will be abstracted and hidden from you “conveniently” by the framework. You will spend your time learning that framework and it’s design in order to understand the algorithm implementation.

: The code will not be a generic implementation ready for you to run in isolation, it will be carefully designed to work within the project’s framework. It is also very likely that details will be abstracted and hidden from you “conveniently” by the framework. You will spend your time learning that framework and it’s design in order to understand the algorithm implementation. Limited Understanding: Studying an implementation of an algorithm does not help you in your ambition to understand the algorithm, it can teach you tricks of efficient algorithm implementation. In the beginning, the most critical time, other peoples code will confuse you.

I think there is deep wisdom here.

I would point out that open source implementations can sometimes help in the understanding of a specific technical detail, such as an update rule or other modular piece of mathematics that may be poorly described, but realized in a single function in code. I have experienced this myself many times, but it is a heuristic, not a rule.

Edward suggests algorithms like k-means and stochastic gradient descent as good examples to start with.

Implementing Algorithms From Papers

Edward suggests that implementing machine learning algorithms from papers is a big jump if you have not first implemented well known algorithms, as described above.

From Edwards comments you can sketch out a process for learning machine learning algorithms by implementing them from scratch. My interpretation of that process looks something like the following:

Implement the algorithm yourself from scratch. Compare performance to off-the-shelf implementations. Work hard to meet performance and results. Look at open source code to understand advanced tips and tricks.

He suggests that creating your own un-optimized implementation will point out to you where the inefficiencies are, motivate you to fix them, motivating you to understand them in depth and seek out how they have been solved elsewhere.

He further suggests that simply coping an implementation won’t teach you what you need to know, that you will miss out on that deep understanding of the algorithm and its unoptimized performance characteristics and how optimizations can be generalized across algorithms of a similar class.

Advice for Beginners

After some discussion over email, Edward expanded on his comments and wrote up his thoughts in a blog post titled “Beginner Advice on Learning to Implement ML Algorithms“.

This is a great post. In it he addresses three key questions: how to implement machine learning algorithms from scratch, common traps for beginners and resources that may help.

The post is not just great because the advice comes from hard earned wisdom (Edward does machine learning the hard way – he practices it, as you should), but there are few if any posts out there like it. No one is talking about how to implement machine learning algorithms from scratch. It is my mission to work on this problem at the moment.

Edward’s key message is that you need to practice. Implementing machine learning algorithms requires that you understand the background to each algorithm, including the theory, mathematics and history of the field and the algorithm. This does not come quickly or easily. You must work at it, iterate on your understanding and practice, a lot.

If you are a professional programmer, than you know, mastery takes nothing less.

Tips for Implementing Algorithms

In his blog post, Edward provides 4 master tips that can help you implement machine learning algorithms from scratch. In summary, they are:

Read the whole paper. Read the whole paper, slowly. Then soak in the ideas for a while, say a few days. Read it again later, but not until you have a first-cut of your own mental model for how the algorithm works, the data flow and how it all hangs together. Read with purpose. Subsequent reads must correct and improve upon your existing understanding of the algorithm. Devise a test problem. Locate, select or devise a test problem that is simple enough for you to understand and visualize the results or behavior of the algorithm, but complex enough to force the procedure to exhibit a differentiating characteristic or result. This problem will be your litmus test, telling you when the implementation is correct and when optimizations have not introduced fatal bugs. Edward calls it a “useful unit test of macro functionality“. Optimize last. Understand the procedure and logic of the algorithm first by implementing the whole thing from scratch, leveraging little existing code or tricks. Only after you understand and have a working implementation should you consider improving performance in terms of space or time complexity, or accuracy with algorithm extensions. Understand the foundations. When it comes to production-grade implementations, you can leverage existing libraries. Edward points to examples such as LIBSVM and LIBLINEAR. These are powerful libraries that incorporate decades of bug fixing and optimizations. Before adopting them, be confident you understand exactly what it is you are leveraging, how it works and the characterize the benefits it provides. Optimize your implementations purposefully, use the best and understand what it does.

Again, there is great wisdom in these tips. I could not have put it better myself. In particular. I strongly agree with the need for implementing algorithms inefficiently from scratch to maximize learning. Algorithm optimization is an import but wholly different skill for a wholly different purpose.

Remember this.

Avoid the Beginner Pitfalls

Edward goes on to highlight common traps that beginners fall into. In summary, they are:

Don’t assume the research paper is correct, peer review is not perfect and mistakes (sometimes large ones) make into publications.

Don’t try and get a math-free understanding of an algorithm, maths can describe salient parts of an algorithm process efficiently and unambiguously and this is critically important.

Don’t start with other peoples source code, as described above.

You cannot know how to apply an algorithm to a problem effectively a priori, look for transferable application ideas from similar papers.

Default random number generates often don’t cut it, use something better, but not cryptographic strength.

Scripting languages don’t cut the mustard when optimizing (His personal, and stated possibly controversial, opinion. I personally find static types save a lot of headache in large production systems).

Summary

Implementing machine learning algorithms is an excellent (if not the best) way of learning machine learning. The knowledge is visceral because you have to sweat the details, they become intimate. This helps when you are trying to get the most from an algorithm.

In this post you discovered that the often suggested advice of “read open source implementations” is not wrong, but needs to fit carefully within your learning strategy.

Edward suggests that you learn machine learning algorithms the hard way, figure them out yourself so that you grow, then turn to open source implementations to learn the efficient mathematical and programatic tricks to optimize the implementation, if and when you need those efficiencies.

This is nuanced and valuable advice.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
164;machinelearningmastery.com;https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-computer-vision-7-day-mini-course/;2019-04-08;How to Get Started With Deep Learning for Computer Vision (7-Day Mini-Course);"# fit a cnn on the fashion mnist dataset

from keras . datasets import fashion_mnist

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Conv2D

from keras . layers import MaxPooling2D

from keras . layers import Dense

from keras . layers import Flatten

# load dataset

( trainX , trainY ) , ( testX , testY ) = fashion_mnist . load_data ( )

# reshape dataset to have a single channel

trainX = trainX . reshape ( ( trainX . shape [ 0 ] , 28 , 28 , 1 ) )

testX = testX . reshape ( ( testX . shape [ 0 ] , 28 , 28 , 1 ) )

# convert from integers to floats

trainX , testX = trainX . astype ( 'float32' ) , testX . astype ( 'float32' )

# normalize to range 0-1

trainX , testX = trainX / 255.0 , testX / 255.0

# one hot encode target values

trainY , testY = to_categorical ( trainY ) , to_categorical ( testY )

# define model

model = Sequential ( )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , input_shape = ( 28 , 28 , 1 ) ) )

model . add ( MaxPooling2D ( ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 100 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 10 , activation = 'softmax' ) )

model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( trainX , trainY , epochs = 10 , batch_size = 32 , verbose = 2 )

# evaluate model

loss , acc = model . evaluate ( testX , testY , verbose = 0 )"
165;machinelearningmastery.com;https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/;2016-10-25;How To Implement Simple Linear Regression From Scratch With Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98

# Simple Linear Regression on the Swedish Insurance Dataset from random import seed from random import randrange from csv import reader from math import sqrt # Load a CSV file def load_csv ( filename ) : dataset = list ( ) with open ( filename , 'r' ) as file : csv_reader = reader ( file ) for row in csv_reader : if not row : continue dataset . append ( row ) return dataset # Convert string column to float def str_column_to_float ( dataset , column ) : for row in dataset : row [ column ] = float ( row [ column ] . strip ( ) ) # Split a dataset into a train and test set def train_test_split ( dataset , split ) : train = list ( ) train_size = split * len ( dataset ) dataset_copy = list ( dataset ) while len ( train ) < train_size : index = randrange ( len ( dataset_copy ) ) train . append ( dataset_copy . pop ( index ) ) return train , dataset_copy # Calculate root mean squared error def rmse_metric ( actual , predicted ) : sum_error = 0.0 for i in range ( len ( actual ) ) : prediction_error = predicted [ i ] - actual [ i ] sum_error += ( prediction_error * * 2 ) mean_error = sum_error / float ( len ( actual ) ) return sqrt ( mean_error ) # Evaluate an algorithm using a train/test split def evaluate_algorithm ( dataset , algorithm , split , * args ) : train , test = train_test_split ( dataset , split ) test_set = list ( ) for row in test : row_copy = list ( row ) row_copy [ - 1 ] = None test_set . append ( row_copy ) predicted = algorithm ( train , test_set , * args ) actual = [ row [ - 1 ] for row in test ] rmse = rmse_metric ( actual , predicted ) return rmse # Calculate the mean value of a list of numbers def mean ( values ) : return sum ( values ) / float ( len ( values ) ) # Calculate covariance between x and y def covariance ( x , mean_x , y , mean_y ) : covar = 0.0 for i in range ( len ( x ) ) : covar += ( x [ i ] - mean_x ) * ( y [ i ] - mean_y ) return covar # Calculate the variance of a list of numbers def variance ( values , mean ) : return sum ( [ ( x - mean ) * * 2 for x in values ] ) # Calculate coefficients def coefficients ( dataset ) : x = [ row [ 0 ] for row in dataset ] y = [ row [ 1 ] for row in dataset ] x_mean , y_mean = mean ( x ) , mean ( y ) b1 = covariance ( x , x_mean , y , y_mean ) / variance ( x , x_mean ) b0 = y_mean - b1 * x_mean return [ b0 , b1 ] # Simple linear regression algorithm def simple_linear_regression ( train , test ) : predictions = list ( ) b0 , b1 = coefficients ( train ) for row in test : yhat = b0 + b1 * row [ 0 ] predictions . append ( yhat ) return predictions # Simple linear regression on insurance dataset seed ( 1 ) # load and prepare data filename = 'insurance.csv' dataset = load_csv ( filename ) for i in range ( len ( dataset [ 0 ] ) ) : str_column_to_float ( dataset , i ) # evaluate algorithm split = 0.6 rmse = evaluate_algorithm ( dataset , simple_linear_regression , split ) print ( 'RMSE: %.3f' % ( rmse ) )"
166;news.mit.edu;http://news.mit.edu/press/filming-guidelines;;Filming Guidelines;"The MIT News Office media relations team is responsible for approving requests for non-commercial filming on the MIT campus. If you are not a member of the media, please send your request to Peter Bebergal in the MIT Technology Licensing Office.

When the details of a given request have been agreed upon, a location agreement must be signed and returned to the MIT News Office prior to the crew's arrival. Once a location agreement is approved, video crews are welcome to film on campus property, as long as the reporting activities do not disrupt Institute activities, interfere with the privacy of students, faculty or staff, or jeopardize the safety of Institute personnel, visitors or facilities.

Guidelines

Filming is prohibited along the Infinite Corridor, in Lobby 7, Lobby 10 and in residence halls. In the Stata Center, filming is only permitted on the first floor (or ""student street"") and in individual offices and labs, with permission from occupants.

When filming students, we ask that the media respect the right of students not to be interviewed, if they so decline. Media may not take or use pictures of students or film students without first getting their permission to be filmed. Students must also be given complete details about what is being filmed and how their photograph might be used. All other locations/subjects must be approved prior to filming.

Submitting Requests

Please e-mail requests to film on campus to Sarah McDonnell with at least 72 business hours (three business days) of advanced notice, and include the following information:

Name and description of the organization or individual making the request.

Name, address, and phone number of the contact person.

Project description and the intended use of the resulting material.

Date(s) and time(s) requested.

Location of photo shoot.

Number of people, and amount and type of equipment involved.

Proof of adequate insurance coverage and indemnity.

Contact

Sarah McDonnell

Media Relations Manager

s_mcd@mit.edu

617-253-8923"
167;machinelearningmastery.com;http://machinelearningmastery.com/perform-feature-selection-machine-learning-data-weka/;2016-07-12;How to Perform Feature Selection With Machine Learning Data in Weka;"Tweet Share Share

Last Updated on December 13, 2019

Raw machine learning data contains a mixture of attributes, some of which are relevant to making predictions.

How do you know which features to use and which to remove? The process of selecting features in your data to model your problem is called feature selection.

In this post you will discover how to perform feature selection with your machine learning data in Weka.

After reading this post you will know:

About the importance of feature selection when working through a machine learning problem.

How feature selection is supported on the Weka platform.

How to use various different feature selection techniques in Weka on your dataset.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Update March/2018: Added alternate link to download the dataset as the original appears to have been taken down.

Predict the Onset of Diabetes

The dataset used for this example is the Pima Indians onset of diabetes dataset.

It is a classification problem where each instance represents medical details for one patient and the task is to predict whether the patient will have an onset of diabetes within the next five years.

You can learn more about the dataset here:

You can also access this dataset in your Weka installation, under the data/ directory in the file called diabetes.arff.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Feature Selection in Weka

Many feature selection techniques are supported in Weka.

A good place to get started exploring feature selection in Weka is in the Weka Explorer.

Open the Weka GUI Chooser. Click the “Explorer” button to launch the Explorer. Open the Pima Indians dataset. Click the “Select attributes” tab to access the feature selection methods.

Feature selection is divided into two parts:

Attribute Evaluator

Search Method.

Each section has multiple techniques from which to choose.

The attribute evaluator is the technique by which each attribute in your dataset (also called a column or feature) is evaluated in the context of the output variable (e.g. the class). The search method is the technique by which to try or navigate different combinations of attributes in the dataset in order to arrive on a short list of chosen features.

Some Attribute Evaluator techniques require the use of specific Search Methods. For example, the CorrelationAttributeEval technique used in the next section can only be used with a Ranker Search Method, that evaluates each attribute and lists the results in a rank order. When selecting different Attribute Evaluators, the interface may ask you to change the Search Method to something compatible with the chosen technique.

Both the Attribute Evaluator and Search Method techniques can be configured. Once chosen, click on the name of the technique to get access to its configuration details.

Click the “More” button to get more documentation on the feature selection technique and configuration parameters. Hover your mouse cursor over a configuration parameter to get a tooltip containing more details.

Now that we know how to access feature selection techniques in Weka, let’s take a look at how to use some popular methods on our chosen standard dataset.

Correlation Based Feature Selection

A popular technique for selecting the most relevant attributes in your dataset is to use correlation.

Correlation is more formally referred to as Pearson’s correlation coefficient in statistics.

You can calculate the correlation between each attribute and the output variable and select only those attributes that have a moderate-to-high positive or negative correlation (close to -1 or 1) and drop those attributes with a low correlation (value close to zero).

Weka supports correlation based feature selection with the CorrelationAttributeEval technique that requires use of a Ranker search method.

Running this on our Pima Indians dataset suggests that one attribute (plas) has the highest correlation with the output class. It also suggests a host of attributes with some modest correlation (mass, age, preg). If we use 0.2 as our cut-off for relevant attributes, then the remaining attributes could possibly be removed (pedi, insu, skin and pres).

Information Gain Based Feature Selection

Another popular feature selection technique is to calculate the information gain.

You can calculate the information gain (also called entropy) for each attribute for the output variable. Entry values vary from 0 (no information) to 1 (maximum information). Those attributes that contribute more information will have a higher information gain value and can be selected, whereas those that do not add much information will have a lower score and can be removed.

Weka supports feature selection via information gain using the InfoGainAttributeEval Attribute Evaluator. Like the correlation technique above, the Ranker Search Method must be used.

Running this technique on our Pima Indians we can see that one attribute contributes more information than all of the others (plas). If we use an arbitrary cutoff of 0.05, then we would also select the mass, age and insu attributes and drop the rest from our dataset.

Learner Based Feature Selection

A popular feature selection technique is to use a generic but powerful learning algorithm and evaluate the performance of the algorithm on the dataset with different subsets of attributes selected.

The subset that results in the best performance is taken as the selected subset. The algorithm used to evaluate the subsets does not have to be the algorithm that you intend to use to model your problem, but it should be generally quick to train and powerful, like a decision tree method.

In Weka this type of feature selection is supported by the WrapperSubsetEval technique and must use a GreedyStepwise or BestFirst Search Method. The latter, BestFirst, is preferred if you can spare the compute time.

1. First select the “WrapperSubsetEval” technique.

2. Click on the name “WrapperSubsetEval” to open the configuration for the method.

3. Click the “Choose” button for the “classifier” and change it to J48 under “trees”.

4. Click “OK” to accept the configuration.

5. Change the “Search Method” to “BestFirst”.

6. Click the “Start” button to evaluate the features.

Running this feature selection technique on the Pima Indians dataset selects 4 of the 8 input variables: plas, pres, mass and age.

Select Attributes in Weka

Looking back over the three techniques, we can see some overlap in the selected features (e.g. plas), but also differences.

It is a good idea to evaluate a number of different “views” of your machine learning dataset. A view of your dataset is nothing more than a subset of features selected by a given feature selection technique. It is a copy of your dataset that you can easily make in Weka.

For example, taking the results from the last feature selection technique, let’s say we wanted to create a view of the Pima Indians dataset with only the following attributes: plas, pres, mass and age:

1. Click the “Preprocess” tab.

2. In the “Attributes” selection Tick all but the plas, pres, mass, age and class attributes.

3. Click the “Remove” button.

4. Click the “Save” button and enter a filename.

You now have a new view of your dataset to explore.

What Feature Selection Techniques To Use

You cannot know which views of your data will produce the most accurate models.

Therefore, it is a good idea to try a number of different feature selection techniques on your data and in turn create many different views of your data.

Select a good generic technique, like a decision tree, and build a model for each view of your data.

Compare the results to get an idea of which view of your data results in the best performance. This will give you an idea of the view or more specifically features that best expose the structure of your problem to learning algorithms in general.

Summary

In this post you discovered the importance of feature selection and how to use feature selection on your data with Weka.

Specifically, you learned:

How to perform feature selection using correlation.

How to perform feature selection using information gain.

How to perform feature selection by training a model on different subsets of features.

Do you have any questions about feature selection in Weka or about this post? Ask your questions in the comments and I will do my best to answer.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
168;machinelearningmastery.com;https://machinelearningmastery.com/how-to-scale-data-for-long-short-term-memory-networks-in-python/;2017-07-06;How to Scale Data for Long Short-Term Memory Networks in Python;"from pandas import Series

from sklearn . preprocessing import MinMaxScaler

# define contrived series

data = [ 10.0 , 20.0 , 30.0 , 40.0 , 50.0 , 60.0 , 70.0 , 80.0 , 90.0 , 100.0 ]

series = Series ( data )

print ( series )

# prepare data for normalization

values = series . values

values = values . reshape ( ( len ( values ) , 1 ) )

# train the normalization

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

scaler = scaler . fit ( values )

print ( 'Min: %f, Max: %f' % ( scaler . data_min_ , scaler . data_max_ ) )

# normalize the dataset and print

normalized = scaler . transform ( values )

print ( normalized )

# inverse transform and print

inversed = scaler . inverse_transform ( normalized )"
169;news.mit.edu;http://news.mit.edu/2020/how-covid-19-tests-work-why-they-are-in-short-supply-0410;;3 Questions: How Covid-19 tests work and why they’re in short supply;"One key to stopping the spread of Covid-19 is knowing who has it. A delay in reliable tests and Covid-19 diagnostics in the United States has painted an unreliable picture of just how many people are infected and how the epidemic is evolving. But new testing options are now becoming available and the information from these diagnostics will help guide decisions and actions important for public health.

McGovern Institute research scientists Omar Abuddayeh and Jonathan Gootenberg have been developing CRISPR technologies to rapidly diagnose Covid-19 and other infectious diseases. They recently described the current state of Covid-19 testing.

Q: How do Covid-19 tests work?

A: There are three main types of tests. The first uses the detection of nucleic acid. These tests directly test for the RNA genome of the virus in a variety of sample types, such as nasopharyngeal swabs or sputum. These tests are most commonly performed using polymerase chain reaction (PCR), which can amplify a small part of the virus RNA sequence billions-of-fold higher to allow detection with a fluorescence measuring instrument. These types of tests are highly sensitive, allowing for early detection of the virus days after infection. PCR tests require complex instrumentation and are usually performed by skilled personnel in an advanced laboratory setting. An alternative method is SHERLOCK, a nucleic acid-based test developed here at MIT stemming from the CRISPR gene editing tool that does not need complex instrumentation and can be read out using a paper strip akin to a pregnancy test, without any loss of sensitivity or specificity. The test is also low-cost and can be performed in less than an hour. Because of these features, we are hoping to gain FDA approval that allows deployment at the point of care or at home testing with our Covid-19 SHERLOCK test kit.

The second type of Covid-19 test detects viral proteins. Some tests use a paper strip that have antibodies against Covid-19 proteins. These allow for easy detection of the virus in less than an hour but are at least a million-fold less sensitive than nucleic acid-based tests because there is no amplification step. This makes them less ideal for screening purposes, as many patients will not have enough viral load in sputum or swabs and will receive false negative results.

The third category is serology tests that detect antibodies against the virus. These tests can also be used as a paper strip with antibodies that detect other antibodies that develop in someone’s blood in response to Covid-19 infection. Antibodies do not show up in blood until one to two weeks after symptoms present, so these tests are not great for catching infection at early stages. Serology tests are more useful for determining if someone has had the infection, recovered, and developed immunity. They may serve a purpose for finding immune people and deciding whether they can go back to work, or for developing antibody-based therapies.

Q: Why aren’t there more Covid-19 tests available?

A: The difficulties in getting nucleic acid detection tests stem from a confluence of multiple factors, including limited supplies of tests, limited supplies of other consumables needed for testing (such as nasal swabs or RNA purification kits), insufficient testing bandwidth at sites that can perform tests (often due to bottlenecks in labor or instruments), and complications behind the logistics of assigning tests or reporting back results. As a result, just producing more testing material would not solve the issue outright, and either more instrumentation and labor is required, or newer, more rapid tests need to be developed that can be performed in a more distributed manner with reduced dependence on equipment, centralized labs, or RNA purification kits.

Q: What kind of Covid-19 test are you developing now?

A: We are working on a nucleic acid-based test that does not require complex instrumentation, rapidly returns results (with a goal of under one hour), and can be performed at a point-of-care location without trained professionals. We hope to accomplish this using a combination of techniques. First, we are incorporating isothermal amplification technologies, which, unlike current PCR-based tests, do not require intricate heating and cooling to operate. We are combining this with our CRISPR-based diagnostics, allowing for sensitive detection and readout in a simple visual format, akin to a pregnancy test. We hope that this test will significantly lower the barrier for accurate diagnosis and provide another approach for Covid-19 surveillance."
170;machinelearningmastery.com;https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/;2017-08-13;Multivariate Time Series Forecasting with LSTMs in Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95

from math import sqrt from numpy import concatenate from matplotlib import pyplot from pandas import read_csv from pandas import DataFrame from pandas import concat from sklearn . preprocessing import MinMaxScaler from sklearn . preprocessing import LabelEncoder from sklearn . metrics import mean_squared_error from keras . models import Sequential from keras . layers import Dense from keras . layers import LSTM # convert series to supervised learning def series_to_supervised ( data , n_in = 1 , n_out = 1 , dropnan = True ) : n_vars = 1 if type ( data ) is list else data . shape [ 1 ] df = DataFrame ( data ) cols , names = list ( ) , list ( ) # input sequence (t-n, ... t-1) for i in range ( n_in , 0 , - 1 ) : cols . append ( df . shift ( i ) ) names += [ ( 'var%d(t-%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ] # forecast sequence (t, t+1, ... t+n) for i in range ( 0 , n_out ) : cols . append ( df . shift ( - i ) ) if i == 0 : names += [ ( 'var%d(t)' % ( j + 1 ) ) for j in range ( n_vars ) ] else : names += [ ( 'var%d(t+%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ] # put it all together agg = concat ( cols , axis = 1 ) agg . columns = names # drop rows with NaN values if dropnan : agg . dropna ( inplace = True ) return agg # load dataset dataset = read_csv ( 'pollution.csv' , header = 0 , index_col = 0 ) values = dataset . values # integer encode direction encoder = LabelEncoder ( ) values [ : , 4 ] = encoder . fit_transform ( values [ : , 4 ] ) # ensure all data is float values = values . astype ( 'float32' ) # normalize features scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) ) scaled = scaler . fit_transform ( values ) # frame as supervised learning reframed = series_to_supervised ( scaled , 1 , 1 ) # drop columns we don't want to predict reframed . drop ( reframed . columns [ [ 9 , 10 , 11 , 12 , 13 , 14 , 15 ] ] , axis = 1 , inplace = True ) print ( reframed . head ( ) ) # split into train and test sets values = reframed . values n_train_hours = 365 * 24 train = values [ : n_train_hours , : ] test = values [ n_train_hours : , : ] # split into input and outputs train_X , train_y = train [ : , : - 1 ] , train [ : , - 1 ] test_X , test_y = test [ : , : - 1 ] , test [ : , - 1 ] # reshape input to be 3D [samples, timesteps, features] train_X = train_X . reshape ( ( train_X . shape [ 0 ] , 1 , train_X . shape [ 1 ] ) ) test_X = test_X . reshape ( ( test_X . shape [ 0 ] , 1 , test_X . shape [ 1 ] ) ) print ( train_X . shape , train_y . shape , test_X . shape , test_y . shape ) # design network model = Sequential ( ) model . add ( LSTM ( 50 , input_shape = ( train_X . shape [ 1 ] , train_X . shape [ 2 ] ) ) ) model . add ( Dense ( 1 ) ) model . compile ( loss = 'mae' , optimizer = 'adam' ) # fit network history = model . fit ( train_X , train_y , epochs = 50 , batch_size = 72 , validation_data = ( test_X , test_y ) , verbose = 2 , shuffle = False ) # plot history pyplot . plot ( history . history [ 'loss' ] , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' ) pyplot . legend ( ) pyplot . show ( ) # make a prediction yhat = model . predict ( test_X ) test_X = test_X . reshape ( ( test_X . shape [ 0 ] , test_X . shape [ 2 ] ) ) # invert scaling for forecast inv_yhat = concatenate ( ( yhat , test_X [ : , 1 : ] ) , axis = 1 ) inv_yhat = scaler . inverse_transform ( inv_yhat ) inv_yhat = inv_yhat [ : , 0 ] # invert scaling for actual test_y = test_y . reshape ( ( len ( test_y ) , 1 ) ) inv_y = concatenate ( ( test_y , test_X [ : , 1 : ] ) , axis = 1 ) inv_y = scaler . inverse_transform ( inv_y ) inv_y = inv_y [ : , 0 ] # calculate RMSE rmse = sqrt ( mean_squared_error ( inv_y , inv_yhat ) ) print ( 'Test RMSE: %.3f' % rmse )"
171;machinelearningmastery.com;https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/;2019-07-30;How to Implement Pix2Pix GAN Models From Scratch With Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144

# example of defining a composite model for training the generator model from keras . optimizers import Adam from keras . initializers import RandomNormal from keras . models import Model from keras . models import Input from keras . layers import Conv2D from keras . layers import Conv2DTranspose from keras . layers import LeakyReLU from keras . layers import Activation from keras . layers import Concatenate from keras . layers import Dropout from keras . layers import BatchNormalization from keras . layers import LeakyReLU from keras . utils . vis_utils import plot_model # define the discriminator model def define_discriminator ( image_shape ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # source image input in_src_image = Input ( shape = image_shape ) # target image input in_target_image = Input ( shape = image_shape ) # concatenate images channel-wise merged = Concatenate ( ) ( [ in_src_image , in_target_image ] ) # C64 d = Conv2D ( 64 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( merged ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C128 d = Conv2D ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C256 d = Conv2D ( 256 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C512 d = Conv2D ( 512 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # second last output layer d = Conv2D ( 512 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # patch output d = Conv2D ( 1 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) patch_out = Activation ( 'sigmoid' ) ( d ) # define model model = Model ( [ in_src_image , in_target_image ] , patch_out ) # compile model opt = Adam ( lr = 0.0002 , beta_1 = 0.5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = opt , loss_weights = [ 0.5 ] ) return model # define an encoder block def define_encoder_block ( layer_in , n_filters , batchnorm = True ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # add downsampling layer g = Conv2D ( n_filters , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( layer_in ) # conditionally add batch normalization if batchnorm : g = BatchNormalization ( ) ( g , training = True ) # leaky relu activation g = LeakyReLU ( alpha = 0.2 ) ( g ) return g # define a decoder block def decoder_block ( layer_in , skip_in , n_filters , dropout = True ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # add upsampling layer g = Conv2DTranspose ( n_filters , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( layer_in ) # add batch normalization g = BatchNormalization ( ) ( g , training = True ) # conditionally add dropout if dropout : g = Dropout ( 0.5 ) ( g , training = True ) # merge with skip connection g = Concatenate ( ) ( [ g , skip_in ] ) # relu activation g = Activation ( 'relu' ) ( g ) return g # define the standalone generator model def define_generator ( image_shape = ( 256 , 256 , 3 ) ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # image input in_image = Input ( shape = image_shape ) # encoder model: C64-C128-C256-C512-C512-C512-C512-C512 e1 = define_encoder_block ( in_image , 64 , batchnorm = False ) e2 = define_encoder_block ( e1 , 128 ) e3 = define_encoder_block ( e2 , 256 ) e4 = define_encoder_block ( e3 , 512 ) e5 = define_encoder_block ( e4 , 512 ) e6 = define_encoder_block ( e5 , 512 ) e7 = define_encoder_block ( e6 , 512 ) # bottleneck, no batch norm and relu b = Conv2D ( 512 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( e7 ) b = Activation ( 'relu' ) ( b ) # decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128 d1 = decoder_block ( b , e7 , 512 ) d2 = decoder_block ( d1 , e6 , 512 ) d3 = decoder_block ( d2 , e5 , 512 ) d4 = decoder_block ( d3 , e4 , 512 , dropout = False ) d5 = decoder_block ( d4 , e3 , 256 , dropout = False ) d6 = decoder_block ( d5 , e2 , 128 , dropout = False ) d7 = decoder_block ( d6 , e1 , 64 , dropout = False ) # output g = Conv2DTranspose ( 3 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d7 ) out_image = Activation ( 'tanh' ) ( g ) # define model model = Model ( in_image , out_image ) return model # define the combined generator and discriminator model, for updating the generator def define_gan ( g_model , d_model , image_shape ) : # make weights in the discriminator not trainable d_model . trainable = False # define the source image in_src = Input ( shape = image_shape ) # connect the source image to the generator input gen_out = g_model ( in_src ) # connect the source input and generator output to the discriminator input dis_out = d_model ( [ in_src , gen_out ] ) # src image as input, generated image and classification output model = Model ( in_src , [ dis_out , gen_out ] ) # compile model opt = Adam ( lr = 0.0002 , beta_1 = 0.5 ) model . compile ( loss = [ 'binary_crossentropy' , 'mae' ] , optimizer = opt , loss_weights = [ 1 , 100 ] ) return model # define image shape image_shape = ( 256 , 256 , 3 ) # define the models d_model = define_discriminator ( image_shape ) g_model = define_generator ( image_shape ) # define the composite model gan_model = define_gan ( g_model , d_model , image_shape ) # summarize the model gan_model . summary ( ) # plot the model plot_model ( gan_model , to_file = 'gan_model_plot.png' , show_shapes = True , show_layer_names = True )"
172;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/;2018-11-13;How to Develop LSTM Models for Time Series Forecasting;"Tweet Share Share

Last Updated on January 6, 2020

Long Short-Term Memory networks, or LSTMs for short, can be applied to time series forecasting.

There are many types of LSTM models that can be used for each specific type of time series forecasting problem.

In this tutorial, you will discover how to develop a suite of LSTM models for a range of standard time series forecasting problems.

The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem.

After completing this tutorial, you will know:

How to develop LSTM models for univariate time series forecasting.

How to develop LSTM models for multivariate time series forecasting.

How to develop LSTM models for multi-step time series forecasting.

This is a large and important post; you may want to bookmark it for future reference.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

In this tutorial, we will explore how to develop a suite of different types of LSTM models for time series forecasting.

The models are demonstrated on small contrived time series problems intended to give the flavor of the type of time series problem being addressed. The chosen configuration of the models is arbitrary and not optimized for each problem; that was not the goal.

This tutorial is divided into four parts; they are:

Univariate LSTM Models Data Preparation Vanilla LSTM Stacked LSTM Bidirectional LSTM CNN LSTM ConvLSTM Multivariate LSTM Models Multiple Input Series. Multiple Parallel Series. Multi-Step LSTM Models Data Preparation Vector Output Model Encoder-Decoder Model Multivariate Multi-Step LSTM Models Multiple Input Multi-Step Output. Multiple Parallel Input and Multi-Step Output.

Univariate LSTM Models

LSTMs can be used to model univariate time series forecasting problems.

These are problems comprised of a single series of observations and a model is required to learn from the series of past observations to predict the next value in the sequence.

We will demonstrate a number of variations of the LSTM model for univariate time series forecasting.

This section is divided into six parts; they are:

Data Preparation Vanilla LSTM Stacked LSTM Bidirectional LSTM CNN LSTM ConvLSTM

Each of these models are demonstrated for one-step univariate time series forecasting, but can easily be adapted and used as the input part of a model for other types of time series forecasting problems.

Data Preparation

Before a univariate series can be modeled, it must be prepared.

The LSTM model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the LSTM can learn.

Consider a given univariate sequence:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.

X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ... 1 2 3 4 5 X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ...

The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.

# split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on our small contrived dataset above.

The complete example is listed below.

# univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step.

[10 20 30] 40 [20 30 40] 50 [30 40 50] 60 [40 50 60] 70 [50 60 70] 80 [60 70 80] 90 1 2 3 4 5 6 [10 20 30] 40 [20 30 40] 50 [30 40 50] 60 [40 50 60] 70 [50 60 70] 80 [60 70 80] 90

Now that we know how to prepare a univariate series for modeling, let’s look at developing LSTM models that can learn the mapping of inputs to outputs, starting with a Vanilla LSTM.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Vanilla LSTM

A Vanilla LSTM is an LSTM model that has a single hidden layer of LSTM units, and an output layer used to make a prediction.

We can define a Vanilla LSTM for univariate time series forecasting as follows.

# define model model = Sequential() model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 # define model model = Sequential ( ) model . add ( LSTM ( 50 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

Key in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps and the number of features.

We are working with a univariate series, so the number of features is one, for one variable.

The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function.

The shape of the input for each sample is specified in the input_shape argument on the definition of first hidden layer.

We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape:

[samples, timesteps, features] 1 [samples, timesteps, features]

Our split_sequence() function in the previous section outputs the X with the shape [samples, timesteps], so we easily reshape it to have an additional dimension for the one feature.

# reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) 1 2 3 # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) )

In this case, we define a model with 50 LSTM units in the hidden layer and an output layer that predicts a single numerical value.

The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ‘mse‘ loss function.

Once the model is defined, we can fit it on the training dataset.

# fit model model.fit(X, y, epochs=200, verbose=0) 1 2 # fit model model . fit ( X , y , epochs = 200 , verbose = 0 )

After the model is fit, we can use it to make a prediction.

We can predict the next value in the sequence by providing the input:

[70, 80, 90] 1 [70, 80, 90]

And expecting the model to predict something like:

[100] 1 [100]

The model expects the input shape to be three-dimensional with [samples, timesteps, features], therefore, we must reshape the single input sample before making the prediction.

# demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

We can tie all of this together and demonstrate how to develop a Vanilla LSTM for univariate time series forecasting and make a single prediction.

# univariate lstm example from numpy import array from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) # define model model = Sequential() model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=200, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # univariate lstm example from numpy import array from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) ) # define model model = Sequential ( ) model . add ( LSTM ( 50 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 200 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

Your results may vary given the stochastic nature of the algorithm; try running the example a few times.

We can see that the model predicts the next value in the sequence.

[[102.09213]] 1 [[102.09213]]

Stacked LSTM

Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.

An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.

We can address this by having the LSTM output a value for each time step in the input data by setting the return_sequences=True argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next.

We can therefore define a Stacked LSTM as follows.

# define model model = Sequential() model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features))) model.add(LSTM(50, activation='relu')) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 # define model model = Sequential ( ) model . add ( LSTM ( 50 , activation = 'relu' , return_sequences = True , input_shape = ( n_steps , n_features ) ) ) model . add ( LSTM ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

We can tie this together; the complete code example is listed below.

# univariate stacked lstm example from numpy import array from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense # split a univariate sequence def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) # define model model = Sequential() model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features))) model.add(LSTM(50, activation='relu')) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=200, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # univariate stacked lstm example from numpy import array from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense # split a univariate sequence def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) ) # define model model = Sequential ( ) model . add ( LSTM ( 50 , activation = 'relu' , return_sequences = True , input_shape = ( n_steps , n_features ) ) ) model . add ( LSTM ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 200 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example predicts the next value in the sequence, which we expect would be 100.

[[102.47341]] 1 [[102.47341]]

Bidirectional LSTM

On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.

This is called a Bidirectional LSTM.

We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.

An example of defining a Bidirectional LSTM to read input both forward and backward is as follows.

# define model model = Sequential() model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 # define model model = Sequential ( ) model . add ( Bidirectional ( LSTM ( 50 , activation = 'relu' ) , input_shape = ( n_steps , n_features ) ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

The complete example of the Bidirectional LSTM for univariate time series forecasting is listed below.

# univariate bidirectional lstm example from numpy import array from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import Bidirectional # split a univariate sequence def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) # define model model = Sequential() model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=200, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # univariate bidirectional lstm example from numpy import array from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import Bidirectional # split a univariate sequence def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) ) # define model model = Sequential ( ) model . add ( Bidirectional ( LSTM ( 50 , activation = 'relu' ) , input_shape = ( n_steps , n_features ) ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 200 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example predicts the next value in the sequence, which we expect would be 100.

[[101.48093]] 1 [[101.48093]]

CNN LSTM

A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.

The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.

A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.

The first step is to split the input sequences into subsequences that can be processed by the CNN model. For example, we can first split our univariate time series data into input/output samples with four steps as input and one as output. Each sample can then be split into two sub-samples, each with two time steps. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input.

We can parameterize this and define the number of subsequences as n_seq and the number of time steps per subsequence as n_steps. The input data can then be reshaped to have the required structure:

[samples, subsequences, timesteps, features] 1 [samples, subsequences, timesteps, features]

For example:

# choose a number of time steps n_steps = 4 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, subsequences, timesteps, features] n_features = 1 n_seq = 2 n_steps = 2 X = X.reshape((X.shape[0], n_seq, n_steps, n_features)) 1 2 3 4 5 6 7 8 9 # choose a number of time steps n_steps = 4 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, subsequences, timesteps, features] n_features = 1 n_seq = 2 n_steps = 2 X = X . reshape ( ( X . shape [ 0 ] , n_seq , n_steps , n_features ) )

We want to reuse the same CNN model when reading in each sub-sequence of data separately.

This can be achieved by wrapping the entire CNN model in a TimeDistributed wrapper that will apply the entire model once per input, in this case, once per input subsequence.

The CNN model first has a convolutional layer for reading across the subsequence that requires a number of filters and a kernel size to be specified. The number of filters is the number of reads or interpretations of the input sequence. The kernel size is the number of time steps included of each ‘read’ operation of the input sequence.

The convolution layer is followed by a max pooling layer that distills the filter maps down to 1/2 of their size that includes the most salient features. These structures are then flattened down to a single one-dimensional vector to be used as a single input time step to the LSTM layer.

model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features))) model.add(TimeDistributed(MaxPooling1D(pool_size=2))) model.add(TimeDistributed(Flatten())) 1 2 3 model . add ( TimeDistributed ( Conv1D ( filters = 64 , kernel_size = 1 , activation = 'relu' ) , input_shape = ( None , n_steps , n_features ) ) ) model . add ( TimeDistributed ( MaxPooling1D ( pool_size = 2 ) ) ) model . add ( TimeDistributed ( Flatten ( ) ) )

Next, we can define the LSTM part of the model that interprets the CNN model’s read of the input sequence and makes a prediction.

model.add(LSTM(50, activation='relu')) model.add(Dense(1)) 1 2 model . add ( LSTM ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) )

We can tie all of this together; the complete example of a CNN-LSTM model for univariate time series forecasting is listed below.

# univariate cnn lstm example from numpy import array from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import Flatten from keras.layers import TimeDistributed from keras.layers.convolutional import Conv1D from keras.layers.convolutional import MaxPooling1D # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 4 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, subsequences, timesteps, features] n_features = 1 n_seq = 2 n_steps = 2 X = X.reshape((X.shape[0], n_seq, n_steps, n_features)) # define model model = Sequential() model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features))) model.add(TimeDistributed(MaxPooling1D(pool_size=2))) model.add(TimeDistributed(Flatten())) model.add(LSTM(50, activation='relu')) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=500, verbose=0) # demonstrate prediction x_input = array([60, 70, 80, 90]) x_input = x_input.reshape((1, n_seq, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # univariate cnn lstm example from numpy import array from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import Flatten from keras . layers import TimeDistributed from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 4 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, subsequences, timesteps, features] n_features = 1 n_seq = 2 n_steps = 2 X = X . reshape ( ( X . shape [ 0 ] , n_seq , n_steps , n_features ) ) # define model model = Sequential ( ) model . add ( TimeDistributed ( Conv1D ( filters = 64 , kernel_size = 1 , activation = 'relu' ) , input_shape = ( None , n_steps , n_features ) ) ) model . add ( TimeDistributed ( MaxPooling1D ( pool_size = 2 ) ) ) model . add ( TimeDistributed ( Flatten ( ) ) ) model . add ( LSTM ( 50 , activation = 'relu' ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 500 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 60 , 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_seq , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example predicts the next value in the sequence, which we expect would be 100.

[[101.69263]] 1 [[101.69263]]

ConvLSTM

A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit.

The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting.

The layer expects input as a sequence of two-dimensional images, therefore the shape of input data must be:

[samples, timesteps, rows, columns, features] 1 [samples, timesteps, rows, columns, features]

For our purposes, we can split each sample into subsequences where timesteps will become the number of subsequences, or n_seq, and columns will be the number of time steps for each subsequence, or n_steps. The number of rows is fixed at 1 as we are working with one-dimensional data.

We can now reshape the prepared samples into the required structure.

# choose a number of time steps n_steps = 4 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features] n_features = 1 n_seq = 2 n_steps = 2 X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features)) 1 2 3 4 5 6 7 8 9 # choose a number of time steps n_steps = 4 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features] n_features = 1 n_seq = 2 n_steps = 2 X = X . reshape ( ( X . shape [ 0 ] , n_seq , 1 , n_steps , n_features ) )

We can define the ConvLSTM as a single layer in terms of the number of filters and a two-dimensional kernel size in terms of (rows, columns). As we are working with a one-dimensional series, the number of rows is always fixed to 1 in the kernel.

The output of the model must then be flattened before it can be interpreted and a prediction made.

model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features))) model.add(Flatten()) 1 2 model . add ( ConvLSTM2D ( filters = 64 , kernel_size = ( 1 , 2 ) , activation = 'relu' , input_shape = ( n_seq , 1 , n_steps , n_features ) ) ) model . add ( Flatten ( ) )

The complete example of a ConvLSTM for one-step univariate time series forecasting is listed below.

# univariate convlstm example from numpy import array from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import Flatten from keras.layers import ConvLSTM2D # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 4 # split into samples X, y = split_sequence(raw_seq, n_steps) # reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features] n_features = 1 n_seq = 2 n_steps = 2 X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features)) # define model model = Sequential() model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features))) model.add(Flatten()) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=500, verbose=0) # demonstrate prediction x_input = array([60, 70, 80, 90]) x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # univariate convlstm example from numpy import array from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import Flatten from keras . layers import ConvLSTM2D # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 4 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features] n_features = 1 n_seq = 2 n_steps = 2 X = X . reshape ( ( X . shape [ 0 ] , n_seq , 1 , n_steps , n_features ) ) # define model model = Sequential ( ) model . add ( ConvLSTM2D ( filters = 64 , kernel_size = ( 1 , 2 ) , activation = 'relu' , input_shape = ( n_seq , 1 , n_steps , n_features ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 500 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 60 , 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_seq , 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example predicts the next value in the sequence, which we expect would be 100.

[[103.68166]] 1 [[103.68166]]

Now that we have looked at LSTM models for univariate data, let’s turn our attention to multivariate data.

Multivariate LSTM Models

Multivariate time series data means data where there is more than one observation for each time step.

There are two main models that we may require with multivariate time series data; they are:

Multiple Input Series. Multiple Parallel Series.

Let’s take a look at each in turn.

Multiple Input Series

A problem may have two or more parallel input time series and an output time series that is dependent on the input time series.

The input time series are parallel because each series has an observation at the same time steps.

We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series.

# define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) 1 2 3 4 # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] )

We can reshape these three arrays of data as a single dataset where each row is a time step, and each column is a separate time series. This is a standard way of storing parallel time series in a CSV file.

# convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) 1 2 3 4 5 6 # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) )

The complete example is listed below.

# multivariate data preparation from numpy import array from numpy import hstack # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) print(dataset) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # multivariate data preparation from numpy import array from numpy import hstack # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) print ( dataset )

Running the example prints the dataset with one row per time step and one column for each of the two input and one output parallel time series.

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

As with the univariate time series, we must structure these data into samples with input and output elements.

An LSTM model needs sufficient context to learn a mapping from an input sequence to an output value. LSTMs can support parallel input time series as separate variables or features. Therefore, we need to split the data into samples maintaining the order of observations across the two input sequences.

If we chose three input time steps, then the first sample would look as follows:

Input:

10, 15 20, 25 30, 35 1 2 3 10, 15 20, 25 30, 35

Output:

65 1 65

That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case, 65.

We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used.

We can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input/output samples.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can test this function on our dataset using three time steps for each input time series as input.

The complete example is listed below.

# multivariate data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # multivariate data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the X and y components.

We can see that the X component has a three-dimensional structure.

The first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series.

This is the exact three-dimensional structure expected by an LSTM as input. The data is ready to use without further reshaping.

We can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample.

(7, 3, 2) (7,) [[10 15] [20 25] [30 35]] 65 [[20 25] [30 35] [40 45]] 85 [[30 35] [40 45] [50 55]] 105 [[40 45] [50 55] [60 65]] 125 [[50 55] [60 65] [70 75]] 145 [[60 65] [70 75] [80 85]] 165 [[70 75] [80 85] [90 95]] 185 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 (7, 3, 2) (7,) [[10 15] [20 25] [30 35]] 65 [[20 25] [30 35] [40 45]] 85 [[30 35] [40 45] [50 55]] 105 [[40 45] [50 55] [60 65]] 125 [[50 55] [60 65] [70 75]] 145 [[60 65] [70 75] [80 85]] 165 [[70 75] [80 85] [90 95]] 185

We are now ready to fit an LSTM model on this data.

Any of the varieties of LSTMs in the previous section can be used, such as a Vanilla, Stacked, Bidirectional, CNN, or ConvLSTM model.

We will use a Vanilla LSTM where the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument.

# define model model = Sequential() model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 # define model model = Sequential ( ) model . add ( LSTM ( 50 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

When making a prediction, the model expects three time steps for two input time series.

We can predict the next value in the output series providing the input values of:

80, 85 90, 95 100, 105 1 2 3 80, 85 90, 95 100, 105

The shape of the one sample with three time steps and two variables must be [1, 3, 2].

We would expect the next value in the sequence to be 100 + 105, or 205.

# demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

The complete example is listed below.

# multivariate lstm example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features))) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=200, verbose=0) # demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # multivariate lstm example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # define model model = Sequential ( ) model . add ( LSTM ( 50 , activation = 'relu' , input_shape = ( n_steps , n_features ) ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 200 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[[208.13531]] 1 [[208.13531]]

Multiple Parallel Series

An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each.

For example, given the data from the previous section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may want to predict the value for each of the three time series for the next time step.

This might be referred to as multivariate forecasting.

Again, the data must be split into input/output samples in order to train a model.

The first sample of this dataset would be:

Input:

10, 15, 25 20, 25, 45 30, 35, 65 1 2 3 10, 15, 25 20, 25, 45 30, 35, 65

Output:

40, 45, 85 1 40, 45, 85

The split_sequences() function below will split multiple parallel time series with rows for time steps and one series per column into the required input/output shape.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this on the contrived problem; the complete example is listed below.

# multivariate output data prep from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # multivariate output data prep from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared X and y components.

The shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3).

The shape of y is two-dimensional as we might expect for the number of samples (6) and the number of time variables per sample to be predicted (3).

The data is ready to use in an LSTM model that expects three-dimensional input and two-dimensional output shapes for the X and y components of each sample.

Then, each of the samples is printed showing the input and output components of each sample.

(6, 3, 3) (6, 3) [[10 15 25] [20 25 45] [30 35 65]] [40 45 85] [[20 25 45] [30 35 65] [40 45 85]] [ 50 55 105] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [ 60 65 125] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [ 70 75 145] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [ 80 85 165] [[ 60 65 125] [ 70 75 145] [ 80 85 165]] [ 90 95 185] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (6, 3, 3) (6, 3) [[10 15 25] [20 25 45] [30 35 65]] [40 45 85] [[20 25 45] [30 35 65] [40 45 85]] [ 50 55 105] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [ 60 65 125] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [ 70 75 145] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [ 80 85 165] [[ 60 65 125] [ 70 75 145] [ 80 85 165]] [ 90 95 185]

We are now ready to fit an LSTM model on this data.

Any of the varieties of LSTMs in the previous section can be used, such as a Vanilla, Stacked, Bidirectional, CNN, or ConvLSTM model.

We will use a Stacked LSTM where the number of time steps and parallel series (features) are specified for the input layer via the input_shape argument. The number of parallel series is also used in the specification of the number of values to predict by the model in the output layer; again, this is three.

# define model model = Sequential() model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features))) model.add(LSTM(100, activation='relu')) model.add(Dense(n_features)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 # define model model = Sequential ( ) model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True , input_shape = ( n_steps , n_features ) ) ) model . add ( LSTM ( 100 , activation = 'relu' ) ) model . add ( Dense ( n_features ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

We can predict the next value in each of the three parallel series by providing an input of three time steps for each series.

70, 75, 145 80, 85, 165 90, 95, 185 1 2 3 70, 75, 145 80, 85, 165 90, 95, 185

The shape of the input for making a single prediction must be 1 sample, 3 time steps, and 3 features, or [1, 3, 3]

# demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

We would expect the vector output to be:

[100, 105, 205] 1 [100, 105, 205]

We can tie all of this together and demonstrate a Stacked LSTM for multivariate output time series forecasting below.

# multivariate output stacked lstm example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features))) model.add(LSTM(100, activation='relu')) model.add(Dense(n_features)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=400, verbose=0) # demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_steps, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # multivariate output stacked lstm example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # define model model = Sequential ( ) model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True , input_shape = ( n_steps , n_features ) ) ) model . add ( LSTM ( 100 , activation = 'relu' ) ) model . add ( Dense ( n_features ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 400 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[[101.76599 108.730484 206.63577 ]] 1 [[101.76599 108.730484 206.63577 ]]

Multi-Step LSTM Models

A time series forecasting problem that requires a prediction of multiple time steps into the future can be referred to as multi-step time series forecasting.

Specifically, these are problems where the forecast horizon or interval is more than one time step.

There are two main types of LSTM models that can be used for multi-step forecasting; they are:

Vector Output Model Encoder-Decoder Model

Before we look at these models, let’s first look at the preparation of data for multi-step forecasting.

Data Preparation

As with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components.

Both the input and output components will be comprised of multiple time steps and may or may not have the same number of steps.

For example, given the univariate time series:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We could use the last three time steps as input and forecast the next two time steps.

The first sample would look as follows:

Input:

[10, 20, 30] 1 [10, 20, 30]

Output:

[40, 50] 1 [40, 50]

The split_sequence() function below implements this behavior and will split a given univariate time series into samples with a specified number of input and output time steps.

# split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on the small contrived dataset.

The complete example is listed below.

# multi-step data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # split into samples X, y = split_sequence(raw_seq, n_steps_in, n_steps_out) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # multi-step data preparation from numpy import array # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # split into samples X , y = split_sequence ( raw_seq , n_steps_in , n_steps_out ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example splits the univariate series into input and output time steps and prints the input and output components of each.

[10 20 30] [40 50] [20 30 40] [50 60] [30 40 50] [60 70] [40 50 60] [70 80] [50 60 70] [80 90] 1 2 3 4 5 [10 20 30] [40 50] [20 30 40] [50 60] [30 40 50] [60 70] [40 50 60] [70 80] [50 60 70] [80 90]

Now that we know how to prepare data for multi-step forecasting, let’s look at some LSTM models that can learn this mapping.

Vector Output Model

Like other types of neural network models, the LSTM can output a vector directly that can be interpreted as a multi-step forecast.

This approach was seen in the previous section were one time step of each output time series was forecasted as a vector.

As with the LSTMs for univariate data in a prior section, the prepared samples must first be reshaped. The LSTM expects data to have a three-dimensional structure of [samples, timesteps, features], and in this case, we only have one feature so the reshape is straightforward.

# reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) 1 2 3 # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) )

With the number of input and output steps specified in the n_steps_in and n_steps_out variables, we can define a multi-step time-series forecasting model.

Any of the presented LSTM model types could be used, such as Vanilla, Stacked, Bidirectional, CNN-LSTM, or ConvLSTM. Below defines a Stacked LSTM for multi-step forecasting.

# define model model = Sequential() model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features))) model.add(LSTM(100, activation='relu')) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 # define model model = Sequential ( ) model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True , input_shape = ( n_steps_in , n_features ) ) ) model . add ( LSTM ( 100 , activation = 'relu' ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

The model can make a prediction for a single sample. We can predict the next two steps beyond the end of the dataset by providing the input:

[70, 80, 90] 1 [70, 80, 90]

We would expect the predicted output to be:

[100, 110] 1 [100, 110]

As expected by the model, the shape of the single sample of input data when making the prediction must be [1, 3, 1] for the 1 sample, 3 time steps of the input, and the single feature.

# demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 )

Tying all of this together, the Stacked LSTM for multi-step forecasting with a univariate time series is listed below.

# univariate multi-step vector-output stacked lstm example from numpy import array from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense # split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # split into samples X, y = split_sequence(raw_seq, n_steps_in, n_steps_out) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) # define model model = Sequential() model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features))) model.add(LSTM(100, activation='relu')) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=50, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # univariate multi-step vector-output stacked lstm example from numpy import array from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # split into samples X , y = split_sequence ( raw_seq , n_steps_in , n_steps_out ) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) ) # define model model = Sequential ( ) model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True , input_shape = ( n_steps_in , n_features ) ) ) model . add ( LSTM ( 100 , activation = 'relu' ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 50 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example forecasts and prints the next two time steps in the sequence.

[[100.98096 113.28924]] 1 [[100.98096 113.28924]]

Encoder-Decoder Model

A model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM.

The model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another.

This model can be used for multi-step time series forecasting.

As its name suggests, the model is comprised of two sub-models: the encoder and the decoder.

The encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model’s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models.

model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features))) 1 model . add ( LSTM ( 100 , activation = 'relu' , input_shape = ( n_steps_in , n_features ) ) )

The decoder uses the output of the encoder as an input.

First, the fixed-length output of the encoder is repeated, once for each required time step in the output sequence.

model.add(RepeatVector(n_steps_out)) 1 model . add ( RepeatVector ( n_steps_out ) )

This sequence is then provided to an LSTM decoder model. The model must output a value for each value in the output time step, which can be interpreted by a single output model.

model.add(LSTM(100, activation='relu', return_sequences=True)) 1 model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True ) )

We can use the same output layer or layers to make each one-step prediction in the output sequence. This can be achieved by wrapping the output part of the model in a TimeDistributed wrapper.

model.add(TimeDistributed(Dense(1))) 1 model . add ( TimeDistributed ( Dense ( 1 ) ) )

The full definition for an Encoder-Decoder model for multi-step time series forecasting is listed below.

# define model model = Sequential() model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features))) model.add(RepeatVector(n_steps_out)) model.add(LSTM(100, activation='relu', return_sequences=True)) model.add(TimeDistributed(Dense(1))) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 6 7 # define model model = Sequential ( ) model . add ( LSTM ( 100 , activation = 'relu' , input_shape = ( n_steps_in , n_features ) ) ) model . add ( RepeatVector ( n_steps_out ) ) model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True ) ) model . add ( TimeDistributed ( Dense ( 1 ) ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

As with other LSTM models, the input data must be reshaped into the expected three-dimensional shape of [samples, timesteps, features].

X = X.reshape((X.shape[0], X.shape[1], n_features)) 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) )

In the case of the Encoder-Decoder model, the output, or y part, of the training dataset must also have this shape. This is because the model will predict a given number of time steps with a given number of features for each input sample.

y = y.reshape((y.shape[0], y.shape[1], n_features)) 1 y = y . reshape ( ( y . shape [ 0 ] , y . shape [ 1 ] , n_features ) )

The complete example of an Encoder-Decoder LSTM for multi-step time series forecasting is listed below.

# univariate multi-step encoder-decoder lstm example from numpy import array from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import RepeatVector from keras.layers import TimeDistributed # split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # split into samples X, y = split_sequence(raw_seq, n_steps_in, n_steps_out) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X.reshape((X.shape[0], X.shape[1], n_features)) y = y.reshape((y.shape[0], y.shape[1], n_features)) # define model model = Sequential() model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features))) model.add(RepeatVector(n_steps_out)) model.add(LSTM(100, activation='relu', return_sequences=True)) model.add(TimeDistributed(Dense(1))) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=100, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # univariate multi-step encoder-decoder lstm example from numpy import array from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import RepeatVector from keras . layers import TimeDistributed # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # split into samples X , y = split_sequence ( raw_seq , n_steps_in , n_steps_out ) # reshape from [samples, timesteps] into [samples, timesteps, features] n_features = 1 X = X . reshape ( ( X . shape [ 0 ] , X . shape [ 1 ] , n_features ) ) y = y . reshape ( ( y . shape [ 0 ] , y . shape [ 1 ] , n_features ) ) # define model model = Sequential ( ) model . add ( LSTM ( 100 , activation = 'relu' , input_shape = ( n_steps_in , n_features ) ) ) model . add ( RepeatVector ( n_steps_out ) ) model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True ) ) model . add ( TimeDistributed ( Dense ( 1 ) ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 100 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example forecasts and prints the next two time steps in the sequence.

[[[101.9736 [116.213615]]] 1 2 [[[101.9736 [116.213615]]]

Multivariate Multi-Step LSTM Models

In the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting.

It is possible to mix and match the different types of LSTM models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging.

In this section, we will provide short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically:

Multiple Input Multi-Step Output. Multiple Parallel Input and Multi-Step Output.

Perhaps the biggest stumbling block is in the preparation of data, so this is where we will focus our attention.

Multiple Input Multi-Step Output

There are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series.

For example, consider our multivariate time series from a prior section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may use three prior time steps of each of the two input time series to predict two time steps of the output time series.

Input:

10, 15 20, 25 30, 35 1 2 3 10, 15 20, 25 30, 35

Output:

65 85 1 2 65 85

The split_sequences() function below implements this behavior.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this on our contrived dataset.

The complete example is listed below.

# multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # covert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # covert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared training data.

We can see that the shape of the input portion of the samples is three-dimensional, comprised of six samples, with three time steps, and two variables for the 2 input time series.

The output portion of the samples is two-dimensional for the six samples and the two time steps for each sample to be predicted.

The prepared samples are then printed to confirm that the data was prepared as we specified.

(6, 3, 2) (6, 2) [[10 15] [20 25] [30 35]] [65 85] [[20 25] [30 35] [40 45]] [ 85 105] [[30 35] [40 45] [50 55]] [105 125] [[40 45] [50 55] [60 65]] [125 145] [[50 55] [60 65] [70 75]] [145 165] [[60 65] [70 75] [80 85]] [165 185] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (6, 3, 2) (6, 2) [[10 15] [20 25] [30 35]] [65 85] [[20 25] [30 35] [40 45]] [ 85 105] [[30 35] [40 45] [50 55]] [105 125] [[40 45] [50 55] [60 65]] [125 145] [[50 55] [60 65] [70 75]] [145 165] [[60 65] [70 75] [80 85]] [165 185]

We can now develop an LSTM model for multi-step predictions.

A vector output or an encoder-decoder model could be used. In this case, we will demonstrate a vector output with a Stacked LSTM.

The complete example is listed below.

# multivariate multi-step stacked lstm example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # covert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features))) model.add(LSTM(100, activation='relu')) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=200, verbose=0) # demonstrate prediction x_input = array([[70, 75], [80, 85], [90, 95]]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # multivariate multi-step stacked lstm example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # covert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) # the dataset knows the number of features, e.g. 2 n_features = X . shape [ 2 ] # define model model = Sequential ( ) model . add ( LSTM ( 100 , activation = 'relu' , return_sequences = True , input_shape = ( n_steps_in , n_features ) ) ) model . add ( LSTM ( 100 , activation = 'relu' ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 200 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 ] , [ 80 , 85 ] , [ 90 , 95 ] ] ) x_input = x_input . reshape ( ( 1 , n_steps_in , n_features ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example fits the model and predicts the next two time steps of the output sequence beyond the dataset.

We would expect the next two steps to be: [185, 205]

It is a challenging framing of the problem with very little data, and the arbitrarily configured version of the model gets close.

[[188.70619 210.16513]] 1 [[188.70619 210.16513]]

Multiple Parallel Input and Multi-Step Output

A problem with parallel time series may require the prediction of multiple time steps of each time series.

For example, consider our multivariate time series from a prior section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may use the last three time steps from each of the three time series as input to the model and predict the next time steps of each of the three time series as output.

The first sample in the training dataset would be the following.

Input:

10, 15, 25 20, 25, 45 30, 35, 65 1 2 3 10, 15, 25 20, 25, 45 30, 35, 65

Output:

40, 45, 85 50, 55, 105 1 2 40, 45, 85 50, 55, 105

The split_sequences() function below implements this behavior.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on the small contrived dataset.

The complete example is listed below.

# multivariate multi-step data preparation from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import RepeatVector from keras.layers import TimeDistributed # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # covert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # multivariate multi-step data preparation from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import RepeatVector from keras . layers import TimeDistributed # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # covert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared training dataset.

We can see that both the input (X) and output (Y) elements of the dataset are three dimensional for the number of samples, time steps, and variables or parallel time series respectively.

The input and output elements of each series are then printed side by side so that we can confirm that the data was prepared as we expected.

(5, 3, 3) (5, 2, 3) [[10 15 25] [20 25 45] [30 35 65]] [[ 40 45 85] [ 50 55 105]] [[20 25 45] [30 35 65] [40 45 85]] [[ 50 55 105] [ 60 65 125]] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [[ 60 65 125] [ 70 75 145]] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [[ 70 75 145] [ 80 85 165]] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [[ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (5, 3, 3) (5, 2, 3) [[10 15 25] [20 25 45] [30 35 65]] [[ 40 45 85] [ 50 55 105]] [[20 25 45] [30 35 65] [40 45 85]] [[ 50 55 105] [ 60 65 125]] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [[ 60 65 125] [ 70 75 145]] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [[ 70 75 145] [ 80 85 165]] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [[ 80 85 165] [ 90 95 185]]

We can use either the Vector Output or Encoder-Decoder LSTM to model this problem. In this case, we will use the Encoder-Decoder model.

The complete example is listed below.

# multivariate multi-step encoder-decoder lstm example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import LSTM from keras.layers import Dense from keras.layers import RepeatVector from keras.layers import TimeDistributed # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # covert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) # the dataset knows the number of features, e.g. 2 n_features = X.shape[2] # define model model = Sequential() model.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features))) model.add(RepeatVector(n_steps_out)) model.add(LSTM(200, activation='relu', return_sequences=True)) model.add(TimeDistributed(Dense(n_features))) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=300, verbose=0) # demonstrate prediction x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]]) x_input = x_input.reshape((1, n_steps_in, n_features)) yhat = model.predict(x_input, verbose=0) print(yhat"
173;machinelearningmastery.com;http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/;2016-08-03;Text Generation With LSTM Recurrent Neural Networks in Python with Keras;"# Load Larger LSTM network and generate text

import sys

import numpy

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Dropout

from keras . layers import LSTM

from keras . callbacks import ModelCheckpoint

from keras . utils import np_utils

# load ascii text and covert to lowercase

filename = ""wonderland.txt""

raw_text = open ( filename , 'r' , encoding = 'utf-8' ) . read ( )

raw_text = raw_text . lower ( )

# create mapping of unique chars to integers, and a reverse mapping

chars = sorted ( list ( set ( raw_text ) ) )

char_to_int = dict ( ( c , i ) for i , c in enumerate ( chars ) )

int_to_char = dict ( ( i , c ) for i , c in enumerate ( chars ) )

# summarize the loaded data

n_chars = len ( raw_text )

n_vocab = len ( chars )

print ""Total Characters: "" , n_chars

print ""Total Vocab: "" , n_vocab

# prepare the dataset of input to output pairs encoded as integers

seq_length = 100

dataX = [ ]

dataY = [ ]

for i in range ( 0 , n_chars - seq_length , 1 ) :

seq_in = raw_text [ i : i + seq_length ]

seq_out = raw_text [ i + seq_length ]

dataX . append ( [ char_to_int [ char ] for char in seq_in ] )

dataY . append ( char_to_int [ seq_out ] )

n_patterns = len ( dataX )

print ""Total Patterns: "" , n_patterns

# reshape X to be [samples, time steps, features]

X = numpy . reshape ( dataX , ( n_patterns , seq_length , 1 ) )

# normalize

X = X / float ( n_vocab )

# one hot encode the output variable

y = np_utils . to_categorical ( dataY )

# define the LSTM model

model = Sequential ( )

model . add ( LSTM ( 256 , input_shape = ( X . shape [ 1 ] , X . shape [ 2 ] ) , return_sequences = True ) )

model . add ( Dropout ( 0.2 ) )

model . add ( LSTM ( 256 ) )

model . add ( Dropout ( 0.2 ) )

model . add ( Dense ( y . shape [ 1 ] , activation = 'softmax' ) )

# load the network weights

filename = ""weights-improvement-47-1.2219-bigger.hdf5""

model . load_weights ( filename )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' )

# pick a random seed

start = numpy . random . randint ( 0 , len ( dataX ) - 1 )

pattern = dataX [ start ]

print ""Seed:""

print ""\"""" , '' . join ( [ int_to_char [ value ] for value in pattern ] ) , ""\""""

# generate characters

for i in range ( 1000 ) :

x = numpy . reshape ( pattern , ( 1 , len ( pattern ) , 1 ) )

x = x / float ( n_vocab )

prediction = model . predict ( x , verbose = 0 )

index = numpy . argmax ( prediction )

result = int_to_char [ index ]

seq_in = [ int_to_char [ value ] for value in pattern ]

sys . stdout . write ( result )

pattern . append ( index )

pattern = pattern [ 1 : len ( pattern ) ]"
174;machinelearningmastery.com;https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/;2019-06-13;18 Impressive Applications of Generative Adversarial Networks (GANs);"Tweet Share Share

Last Updated on July 12, 2019

A Generative Adversarial Network, or GAN, is a type of neural network architecture for generative modeling.

Generative modeling involves using a model to generate new examples that plausibly come from an existing distribution of samples, such as generating new photographs that are similar but specifically different from a dataset of existing photographs.

A GAN is a generative model that is trained using two neural network models. One model is called the “generator” or “generative network” model that learns to generate new plausible samples. The other model is called the “discriminator” or “discriminative network” and learns to differentiate generated examples from real examples.

The two models are set up in a contest or a game (in a game theory sense) where the generator model seeks to fool the discriminator model, and the discriminator is provided with both examples of real and generated samples.

After training, the generative model can then be used to create new plausible samples on demand.

GANs have very specific use cases and it can be difficult to understand these use cases when getting started.

In this post, we will review a large number of interesting applications of GANs to help you develop an intuition for the types of problems where GANs can be used and useful. It’s not an exhaustive list, but it does contain many example uses of GANs that have been in the media.

We will divide these applications into the following areas:

Generate Examples for Image Datasets

Generate Photographs of Human Faces

Generate Realistic Photographs

Generate Cartoon Characters

Image-to-Image Translation

Text-to-Image Translation

Semantic-Image-to-Photo Translation

Face Frontal View Generation

Generate New Human Poses

Photos to Emojis

Photograph Editing

Face Aging

Photo Blending

Super Resolution

Photo Inpainting

Clothing Translation

Video Prediction

3D Object Generation

Did I miss an interesting application of GANs or great paper on a specific GAN application?

Please let me know in the comments.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Generate Examples for Image Datasets

Generating new plausible samples was the application described in the original paper by Ian Goodfellow, et al. in the 2014 paper “Generative Adversarial Networks” where GANs were used to generate new plausible examples for the MNIST handwritten digit dataset, the CIFAR-10 small object photograph dataset, and the Toronto Face Database.

This was also the demonstration used in the important 2015 paper titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” by Alec Radford, et al. called DCGAN that demonstrated how to train stable GANs at scale. They demonstrated models for generating new examples of bedrooms.

Importantly, in this paper, they also demonstrated the ability to perform vector arithmetic with the input to the GANs (in the latent space) both with generated bedrooms and with generated faces.

Generate Photographs of Human Faces

Tero Karras, et al. in their 2017 paper titled “Progressive Growing of GANs for Improved Quality, Stability, and Variation” demonstrate the generation of plausible realistic photographs of human faces. They are so real looking, in fact, that it is fair to call the result remarkable. As such, the results received a lot of media attention. The face generations were trained on celebrity examples, meaning that there are elements of existing celebrities in the generated faces, making them seem familiar, but not quite.

Their methods were also used to demonstrate the generation of objects and scenes.

Examples from this paper were used in a 2018 report titled “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation” to demonstrate the rapid progress of GANs from 2014 to 2017 (found via this tweet by Ian Goodfellow).

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Generate Realistic Photographs

Andrew Brock, et al. in their 2018 paper titled “Large Scale GAN Training for High Fidelity Natural Image Synthesis” demonstrate the generation of synthetic photographs with their technique BigGAN that are practically indistinguishable from real photographs.

Generate Cartoon Characters

Yanghua Jin, et al. in their 2017 paper titled “Towards the Automatic Anime Characters Creation with Generative Adversarial Networks” demonstrate the training and use of a GAN for generating faces of anime characters (i.e. Japanese comic book characters).

Inspired by the anime examples, a number of people have tried to generate Pokemon characters, such as the pokeGAN project and the Generate Pokemon with DCGAN project, with limited success.

Image-to-Image Translation

This is a bit of a catch-all task, for those papers that present GANs that can do many image translation tasks.

Phillip Isola, et al. in their 2016 paper titled “Image-to-Image Translation with Conditional Adversarial Networks” demonstrate GANs, specifically their pix2pix approach for many image-to-image translation tasks.

Examples include translation tasks such as:

Translation of semantic images to photographs of cityscapes and buildings.

Translation of satellite photographs to Google Maps.

Translation of photos from day to night.

Translation of black and white photographs to color.

Translation of sketches to color photographs.

Jun-Yan Zhu in their 2017 paper titled “Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks” introduce their famous CycleGAN and a suite of very impressive image-to-image translation examples.

The example below demonstrates four image translation cases:

Translation from photograph to artistic painting style.

Translation of horse to zebra.

Translation of photograph from summer to winter.

Translation of satellite photograph to Google Maps view.

The paper also provides many other examples, such as:

Translation of painting to photograph.

Translation of sketch to photograph.

Translation of apples to oranges.

Translation of photograph to artistic painting.

Text-to-Image Translation (text2image)

Han Zhang, et al. in their 2016 paper titled “StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks” demonstrate the use of GANs, specifically their StackGAN to generate realistic looking photographs from textual descriptions of simple objects like birds and flowers.

Scott Reed, et al. in their 2016 paper titled “Generative Adversarial Text to Image Synthesis” also provide an early example of text to image generation of small objects and scenes including birds, flowers, and more.

Ayushman Dash, et al. provide more examples on seemingly the same dataset in their 2017 paper titled “TAC-GAN – Text Conditioned Auxiliary Classifier Generative Adversarial Network“.

Scott Reed, et al. in their 2016 paper titled “Learning What and Where to Draw” expand upon this capability and use GANs to both generate images from text and use bounding boxes and key points as hints as to where to draw a described object, like a bird.

Semantic-Image-to-Photo Translation

Ting-Chun Wang, et al. in their 2017 paper titled “High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs” demonstrate the use of conditional GANs to generate photorealistic images given a semantic image or sketch as input.

Specific examples included:

Cityscape photograph, given semantic image.

Bedroom photograph, given semantic image.

Human face photograph, given semantic image.

Human face photograph, given sketch.

They also demonstrate an interactive editor for manipulating the generated image.

Face Frontal View Generation

Rui Huang, et al. in their 2017 paper titled “Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis” demonstrate the use of GANs for generating frontal-view (i.e. face on) photographs of human faces given photographs taken at an angle. The idea is that the generated front-on photos can then be used as input to a face verification or face identification system.

Generate New Human Poses

Liqian Ma, et al. in their 2017 paper titled “Pose Guided Person Image Generation” provide an example of generating new photographs of human models with new poses.

Photos to Emojis

Yaniv Taigman, et al. in their 2016 paper titled “Unsupervised Cross-Domain Image Generation” used a GAN to translate images from one domain to another, including from street numbers to MNIST handwritten digits, and from photographs of celebrities to what they call emojis or small cartoon faces.

Photograph Editing

Guim Perarnau, et al. in their 2016 paper titled “Invertible Conditional GANs For Image Editing” use a GAN, specifically their IcGAN, to reconstruct photographs of faces with specific specified features, such as changes in hair color, style, facial expression, and even gender.

Ming-Yu Liu, et al. in their 2016 paper titled “Coupled Generative Adversarial Networks” also explore the generation of faces with specific properties such as hair color, facial expression, and glasses. They also explore the generation of other images, such as scenes with varied color and depth.

Andrew Brock, et al. in their 2016 paper titled “Neural Photo Editing with Introspective Adversarial Networks” present a face photo editor using a hybrid of variational autoencoders and GANs. The editor allows rapid realistic modification of human faces including changing hair color, hairstyles, facial expression, poses, and adding facial hair.

He Zhang, et al. in their 2017 paper titled “Image De-raining Using a Conditional Generative Adversarial Network” use GANs for image editing, including examples such as removing rain and snow from photographs.

Face Aging

Grigory Antipov, et al. in their 2017 paper titled “Face Aging With Conditional Generative Adversarial Networks” use GANs to generate photographs of faces with different apparent ages, from younger to older.

Zhifei Zhang, in their 2017 paper titled “Age Progression/Regression by Conditional Adversarial Autoencoder” use a GAN based method for de-aging photographs of faces.

Photo Blending

Huikai Wu, et al. in their 2017 paper titled “GP-GAN: Towards Realistic High-Resolution Image Blending” demonstrate the use of GANs in blending photographs, specifically elements from different photographs such as fields, mountains, and other large structures.

Super Resolution

Christian Ledig, et al. in their 2016 paper titled “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network” demonstrate the use of GANs, specifically their SRGAN model, to generate output images with higher, sometimes much higher, pixel resolution.

Huang Bin, et al. in their 2017 paper tilted “High-Quality Face Image SR Using Conditional Generative Adversarial Networks” use GANs for creating versions of photographs of human faces.

Subeesh Vasu, et al. in their 2018 paper tilted “Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network” provide an example of GANs for creating high-resolution photographs, focusing on street scenes.

Photo Inpainting

Deepak Pathak, et al. in their 2016 paper titled “Context Encoders: Feature Learning by Inpainting” describe the use of GANs, specifically Context Encoders, to perform photograph inpainting or hole filling, that is filling in an area of a photograph that was removed for some reason.

Raymond A. Yeh, et al. in their 2016 paper titled “Semantic Image Inpainting with Deep Generative Models” use GANs to fill in and repair intentionally damaged photographs of human faces.

Yijun Li, et al. in their 2017 paper titled “Generative Face Completion” also use GANs for inpainting and reconstructing damaged photographs of human faces.

Clothing Translation

Donggeun Yoo, et al. in their 2016 paper titled “Pixel-Level Domain Transfer” demonstrate the use of GANs to generate photographs of clothing as may be seen in a catalog or online store, based on photographs of models wearing the clothing.

Video Prediction

Carl Vondrick, et al. in their 2016 paper titled “Generating Videos with Scene Dynamics” describe the use of GANs for video prediction, specifically predicting up to a second of video frames with success, mainly for static elements of the scene.

3D Object Generation

Jiajun Wu, et al. in their 2016 paper titled “Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling” demonstrate a GAN for generating new three-dimensional objects (e.g. 3D models) such as chairs, cars, sofas, and tables.

Matheus Gadelha, et al. in their 2016 paper titled “3D Shape Induction from 2D Views of Multiple Objects” use GANs to generate three-dimensional models given two-dimensional pictures of objects from multiple perspectives.

Further Reading

This section provides more lists of GAN applications to complement this list.

Summary

In this post, you discovered a large number of applications of Generative Adversarial Networks, or GANs.

Did I miss an interesting application of GANs or a great paper on specific GAN application?

Please let me know in the comments.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Generative Adversarial Networks Today! Develop Your GAN Models in Minutes ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Generative Adversarial Networks with Python It provides self-study tutorials and end-to-end projects on:

DCGAN, conditional GANs, image translation, Pix2Pix, CycleGAN

and much more... Finally Bring GAN Models to your Vision Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
175;machinelearningmastery.com;http://machinelearningmastery.com/spot-check-regression-machine-learning-algorithms-python-scikit-learn/;2016-05-29;Spot-Check Regression Machine Learning Algorithms in Python with scikit-learn;"# Linear Regression

import pandas

from sklearn import model_selection

from sklearn . linear_model import LinearRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data""

names = [ 'CRIM' , 'ZN' , 'INDUS' , 'CHAS' , 'NOX' , 'RM' , 'AGE' , 'DIS' , 'RAD' , 'TAX' , 'PTRATIO' , 'B' , 'LSTAT' , 'MEDV' ]

dataframe = pandas . read_csv ( url , delim_whitespace = True , names = names )

array = dataframe . values

X = array [ : , 0 : 13 ]

Y = array [ : , 13 ]

seed = 7

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

model = LinearRegression ( )

scoring = 'neg_mean_squared_error'

results = model_selection . cross_val_score ( model , X , Y , cv = kfold , scoring = scoring )"
176;machinelearningmastery.com;http://machinelearningmastery.com/how-to-improve-machine-learning-results/;2013-12-29;How to Improve Machine Learning Results;"Tweet Share Share

Last Updated on May 14, 2019

Having one or two algorithms that perform reasonably well on a problem is a good start, but sometimes you may be incentivised to get the best result you can given the time and resources you have available.

In this post, you will review methods you can use to squeeze out extra performance and improve the results you are getting from machine learning algorithms.

When tuning algorithms you must have a high confidence in the results given by your test harness. This means that you should be using techniques that reduce the variance of the performance measure you are using to assess algorithm runs. I suggest cross validation with a reasonably high number of folds (the exact number of which depends on your dataset).

The three strategies you will learn about in this post are:

Algorithm Tuning

Ensembles

Extreme Feature Engineering

Algorithm Tuning

The place to start is to get better results from algorithms that you already know perform well on your problem. You can do this by exploring and fine tuning the configuration for those algorithms.

Machine learning algorithms are parameterized and modification of those parameters can influence the outcome of the learning process. Think of each algorithm parameter as a dimension on a graph with the values of a given parameter as a point along the axis. Three parameters would be a cube of possible configurations for the algorithm, and n-parameters would be an n-dimensional hypercube of possible configurations for the algorithm.

The objective of algorithm tuning is to find the best point or points in that hypercube for your problem. You will be optimizing against your test harness, so again you cannot underestimate the importance of spending the time to build a trusted test harness.

You can approach this search problem by using automated methods that impose a grid on the possibility space and sample where good algorithm configuration might be. You can then use those points in an optimization algorithm to zoom in on the best performance.

You can repeat this process with a number of well performing methods and explore the best you can achieve with each. I strongly advise that the process is automated and reasonably coarse grained as you can quickly reach points of diminishing returns (fractional percentage performance increases) that may not translate to the production system.

The more tuned the parameters of an algorithm, the more biased the algorithm will be to the training data and test harness. This strategy can be effective, but it can also lead to more fragile models that overfit your test harness and don’t perform as well in practice.

Ensembles

Ensemble methods are concerned with combining the results of multiple methods in order to get improved results. Ensemble methods work well when you have multiple “good enough” models that specialize in different parts of the problem.

This may be achieved through many ways. Three ensemble strategies you can explore are:

Bagging : Known more formally as Bootstrapped Aggregation is where the same algorithm has different perspectives on the problem by being trained on different subsets of the training data.

: Known more formally as Bootstrapped Aggregation is where the same algorithm has different perspectives on the problem by being trained on different subsets of the training data. Boosting : Different algorithms are trained on the same training data.

: Different algorithms are trained on the same training data. Blending: Known more formally as Stacked Generalization or Stacking is where a variety of models whose predictions are taken as input to a new model that learns how to combine the predictions into an overall prediction.

It is a good idea to get into ensemble methods after you have exhausted more traditional methods. There are two good reasons for this, they are generally more complex than traditional methods and the traditional methods give you a good base level from which you can improve and draw from to create your ensembles.

Extreme Feature Engineering

The previous two strategies have looked at getting more from machine learning algorithms. This strategy is about exposing more structure in the problem for the algorithms to learn. In data preparation learned about feature decomposition and aggregation in order to better normalize the data for machine learning algorithms. In this strategy, we push that idea to the limits. I call this strategy extreme feature engineering, when really the term “feature engineering” would suffice.

Think of your data as having complex multi-dimensional structures embedded in it that machine learning algorithms know how to find and exploit to make decisions. You want to best expose those structures to algorithms so that the algorithms can do their best work. A difficulty is that some of those structures may be too dense or too complex for the algorithms to find without help. You may also have some knowledge of such structures from your domain expertise.

Take attributes and decompose them widely into multiple features. Technically, what you are doing with this strategy is reducing dependencies and non-linear relationships into simpler independent linear relationships.

This is might be a foreign idea, so here are three examples:

Categorical : You have a categorical attribute that had the values [red, green blue], you could split that into 3 binary attributes of red, green and blue and give each instance a 1 or 0 value for each.

: You have a categorical attribute that had the values [red, green blue], you could split that into 3 binary attributes of red, green and blue and give each instance a 1 or 0 value for each. Real: You have a real valued quantity that has values ranging from 0 to 1000. You could create 10 binary attributes, each representing a bin of values (0-99 for bin 1, 100-199 for bin 2, etc.) and assign each instance a binary value (1/0) for the bins.

I recommend performing this process one step at a time and creating a new test/train dataset for each modification you make and then test algorithms on the dataset. This will start to give you an intuition for attributes and features in the database that are exposing more or less information to the algorithms and the effects on the performance measure. You can use these results to guide further extreme decompositions or aggregations.

Summary

I this post you learned about three strategies for getting improved results from machine learning algorithms on your problem:

Algorithm Tuning where discovering the best models is treated like a search problem through model parameter space.

Ensembles where the predictions made by multiple models are combined.

Extreme Feature Engineering where the attribute decomposition and aggregation seen in data preparation is pushed to the limits.

Resources

If you are looking to dive deeper into this subject, take a look at the resources below.

Update

For 20 tips and tricks for getting more from your algorithms, see the post:"
177;machinelearningmastery.com;http://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/;;Time Series Forecasting With Python;"Introduction to Time Series Forecasting With Python

Discover How to Prepare Data and Develop Models to Predict the Future

$37 USD Time series forecasting is different from other machine learning problems. The key difference is the fixed sequence of observations and the constraints and additional structure this provides. In this mega Ebook written in the friendly Machine Learning Mastery style that you’re used to, finally cut through the math and specialized methods for time series forecasting. Using clear explanations, standard Python libraries and step-by-step tutorials you will discover how to load and prepare data, evaluate model skill, and implement forecasting models for time series data. Technical Details About the Book PDF format Ebook.

8 parts, 34 chapters, 367 pages.

28 step-by-step tutorial lessons.

3 end-to-end projects.

181 Python (.py) files. Clear and Complete Examples.

No Math. Nothing Hidden. Convinced?

Click to jump straight to the packages. Nothing comes close to the level of detail and practicality of these masterpieces. Nader Nazemi Bioinformatician

Time Series Problems are Important

Time series forecasting is an important area of machine learning that is often neglected.

It is important because there are so many prediction problems that involve a time component. These problems are neglected because it is this time component that makes time series problems more difficult to handle.

You can’t just fire a machine learning algorithm at a time series dataset.

Time series data must be transformed into a supervised learning problem.

Time series data has temporal structure like trends and seasonality that must be handled.

Time series data has a forecast horizon.

There are a few conceptual steps you must make before you can start developing forecasting models.

There are also specialized terminology and algorithms to consider and use when working with time series data.

It can feel overwhelming for a beginner and standard machine learning libraries like scikit-learn do not make it easy to get started.

Introducing: “ Time Series Forecasting With Python “

This is the book I wish I had when I was getting started with univariate time series forecasting.

It is designed for the practical and hands-on way you prefer to learn.

The goal of this book is to:

Show you how to get results on univariate time series forecasting problems using the Python ecosystem.

It is a cookbook designed for immediate use.

This book was developed using five principles.

They are:

Application : The focus is on the application of forecasting rather than the theory.

: The focus is on the application of forecasting rather than the theory. Lessons : The book is broken down into short lessons, each focused on a specific topic.

: The book is broken down into short lessons, each focused on a specific topic. Value : Lessons focus on the most used and most useful aspects of a forecasting project.

: Lessons focus on the most used and most useful aspects of a forecasting project. Results : Each lesson provides a path to a usable and reproducible result.

: Each lesson provides a path to a usable and reproducible result. Speed: Each lesson is designed to provide the shortest path to a result.

These principles shape the structure and organization of the book.

What You Will Know and Be Able to Do (Reading Outcomes)

If you choose to work through all of the lessons and projects of this book, you can set some reasonable expectations on your new found capabilities.

They are:

Time Series Foundations : You will be able to identify time series forecasting problems as distinct from other predictive modeling problems and how time series can be framed as supervised learning.

: You will be able to identify time series forecasting problems as distinct from other predictive modeling problems and how time series can be framed as supervised learning. Transform Data For Modeling : You will be able to transform, rescale, smooth and engineer features from time series data in order to best expose the underlying inherent properties of the problem (the signal) to learning algorithms for forecasting.

: You will be able to transform, rescale, smooth and engineer features from time series data in order to best expose the underlying inherent properties of the problem (the signal) to learning algorithms for forecasting. Harness Temporal Structure : You will be able to analyze time series data and understand the temporal structure inherent in it such as trends and seasonality and how these structures may be addressed, removed and harnessed when forecasting.

: You will be able to analyze time series data and understand the temporal structure inherent in it such as trends and seasonality and how these structures may be addressed, removed and harnessed when forecasting. Evaluate Models : You will be able to devise a model test harness for a univariate forecasting problem and estimate the baseline skill and expected model performance on unseen data with various performance measures.

: You will be able to devise a model test harness for a univariate forecasting problem and estimate the baseline skill and expected model performance on unseen data with various performance measures. Apply Classical Methods: You will be able to select, apply and interpret the results from classical linear methods such as Autoregression, Moving Average and ARIMA models on univariate time series forecasting problems.

You will be a capable predictive modeler for univariate time series forecasting problems using the Python ecosystem.

‘Time Series Forecasting With Python‘ is for Python Developers…

This book makes some assumptions about you.

They are:

You’re a Developer : This is a book for developers. You are a developer of some sort. You know how to read and write code. You know how to develop and debug a program.

: This is a book for developers. You are a developer of some sort. You know how to read and write code. You know how to develop and debug a program. You know Python : This is a book for Python people. You know the Python program- ming language, or you’re a skilled enough developer that you can pick it up as you go along.

: This is a book for Python people. You know the Python program- ming language, or you’re a skilled enough developer that you can pick it up as you go along. You know some Machine Learning: This is a book for novice machine learning practitioners. You know some basic practical machine learning, or you can figure it out quickly.

No mathematical prerequisites are needed.

No scikit-learn prerequisites are needed.

This is a playbook, a cookbook, a field guide, not a textbook for academics.

Time Series Forecasting for Beginners

It is an introductory book for time series forecasting.

As such, it focuses on univariate (one variable) data, rather than more complex multivariate problems. It also focuses on using powerful linear methods like ARIMA, rather than more exotic methods.

Everything You Need to Know to Develop Time Series Forecasting Models

You Will Get:

28 Lessons on Python Best Practices for Time Series Forecasting and

3 Project Tutorials that Tie it All Together

This Ebook was written around two themes designed to get you started and using Python for applied time series forecasting effectively and quickly.

These two parts are Lessons and Projects:

Lessons : Learn how the sub-tasks of time series forecasting projects map onto Python and the best practice way of working through each task.

: Learn how the sub-tasks of time series forecasting projects map onto Python and the best practice way of working through each task. Projects: Tie together all of the knowledge from the lessons by working through case study predictive modeling problems.

1. Lessons Here is an overview of the 28 step-by-step lessons you will complete: Each lesson was designed to be completed in about 30 minutes by the average developer. Part I. Fundamentals Python Environment

What is Time Series Forecasting?

Time Series as Supervised Learning Part II. Data Preparation Load and Explore Time Series Data

Data Visualization

Resampling and Interpolation

Power Transforms

Moving Average Smoothing Part III. Temporal Structure A Gentle Introduction to White Noise

A Gentle Introduction to the Random Walk

Decompose Time Series Data

Use and Remove Trends

Use and Remove Seasonality

Stationarity in Time Series Data Part IV. Evaluate Models Backtest Forecast Models

Forecasting Performance Measures

Persistence Model for Forecasting

Visualize Residual Forecast Errors

Reframe Time Series Forecasting Problems Part V. Forecast Models A Gentle Introduction to the Box-Jenkins Method

Autoregression Models for Forecasting

Moving Average Models for Forecasting

ARIMA Model for Forecasting

Autocorrelation and Partial Autocorrelation

Grid Search ARIMA Model Hyperparameters

Save Models and Make Predictions

Forecast Confidence Intervals 2. Projects Here is an overview of the 3 end-to-end projects you will complete: Project 1: Monthly Armed Robberies in Boston.

Project 2: Annual Water Usage in Baltimore.

Project 3: Monthly Sales of French Champagne. Each project was designed to be completed in about 60 minutes by the average developer.

Take a Sneak Peek Inside The Ebook

Click image to Enlarge.

BONUS: Time Series Forecasting Code Recipes

…you also get 181 fully working time series forecasting scripts

Sample Code Recipes Each recipe presented in the book is standalone, meaning that you can copy and paste it into your project and use it immediately. You get one Python script (.py) for each example provided in the book.

You get the datasets used throughout the book. Your Time Series Code Recipe Library covers the following topics: Loading data from CSV files.

Feature engineering.

Power transforms like log and sqrt.

Upsampling and downsampling data.

Interpolating missing values.

Moving average smoothing

Stationarity statistical tests.

Walk-forward model validation.

Performance measures like RMSE.

Naive forecast model.

Data visualization like line plots and ACF.

AR forecast models.

MA forecast models

ARIMA forecast models.

Grid search model parameters

Save forecast models to file.

Calculate confidence intervals. This means that you can follow along and compare your answers to a known working implementation of each example in the provided Python files. This helps a lot to speed up your progress when working through the details of a specific task. Python Technical Details This section provides some technical details about the book. Python Version : You can use Python 2 or 3.

: You can use Python 2 or 3. SciPy : You will use NumPy, Pandas and scikit-learn.

: You will use NumPy, Pandas and scikit-learn. Statsmodels : You can use Statsmodels 0.6 or 0.8.

: You can use Statsmodels 0.6 or 0.8. Operating System : You can use Windows, Linux or Mac OS X.

: You can use Windows, Linux or Mac OS X. Editor: You can use a text editor and run example from the command line.

About The Author

Hi, I'm Jason Brownlee. I run this site and I wrote and published this book.

I live in Australia with my wife and sons. I love to read books, write tutorials, and develop systems.

I have a computer science and software engineering background as well as Masters and PhD degrees in Artificial Intelligence with a focus on stochastic optimization.

I've written books on algorithms, won and ranked well in competitions, consulted for startups, and spent years in industry. (Yes, I have spend a long time building and maintaining REAL operational systems!)

I get a lot of satisfaction helping developers get started and get really good at applied machine learning.

I teach an unconventional top-down and results-first approach to machine learning where we start by working through tutorials and problems, then later wade into theory as we need it.

I'm here to help if you ever have any questions. I want you to be awesome at machine learning.





Download Your Sample Chapter Do you want to take a closer look at the book? Download a free sample chapter PDF. Enter your email address and your sample chapter will be sent to your inbox. >> Click Here to Download Your Sample Chapter

Check Out What Customers Are Saying:

In a few hours of reading this e-book I have learned that would have taken me weeks or months of digging around to find on the the internet. Erich Glinker Sessional Professor As a beginner in time series forecasting, I found this book helpful and direct to the focal points of the subject. I liked most the projects part. The framework for the implementation was made clearly and well-organized. Samih Eisa Researcher I am just getting started on the book and I use it as a support to prepare myself for maybe starting on time series forecasting on my job. I have read the first two chapters and then jumped to the projects. Lykke Pedersen Data Scientist

I’ve really enjoyed this book. It’s pragmatic and doesn’t assume a lot from the reader, meaning I can make sure all of my thinking and approaches are working well and I can confidently have more junior people on my team take this up and get up to speed with time series forecasting. One of the important things that Jason does with this book is make it accessible from a machine learning perspective, meaning we can use the tools and analysis we use on other problems with time series problems as well. Jason covers ARIMA models and similar models, but also shows the jumping off point between setting a problem up and using regular supervised learning approaches to our work. This book builds right to that crucial point. Finally, if you haven’t followed Jason’s approach to teaching these topics, I think you’ll find their structure reassuring and accessible. The structure of each chapter makes things easy to find and use for real projects. There is always enough background information to get projects working with references to the literature if you’d like to dive deeper into algorithms or theory. I’ve found that I read Jason’s work quickly first to understand where he’s taking me and then I refer back to them while I’m working through actual projects and this book structure is ideal for this kind of practice. David Richards Senior Software Engineer

You're Not Alone in Choosing Machine Learning Mastery

Trusted by Over 47,354 Practitioners

...including employees from companies like:

...students and faculty from universities like:

and many thousands more...

Absolutely No Risk with...

100% Money Back Guarantee

Plus, as you should expect of any great product on the market, every Machine Learning Mastery Ebook

comes with the surest sign of confidence: my gold-standard 100% money-back guarantee.

100% Money-Back Guarantee

If you're not happy with your purchase of any of the Machine Learning Mastery Ebooks,

just email me within 90 days of buying, and I'll give you your money back ASAP.

No waiting. No questions asked. No risk.

Start Forecasting With Time Series Today! Choose Your Package:

Basic Package You will get: Time Series Forecasting With Python (including bonus source code) Buy Now for $37 (a great deal!)



Python ML Bundle TOP SELLER You get the 5-Ebook set: Machine Learning Algorithms From Scratch Machine Learning Mastery With Python Imbalanced Classification with Python XGBoost With Python Time Series Forecasting With Python (includes all bonus source code) Buy Now for $137 That's $195.00 of Value! (You get a huge 29.74% discount)

Super Bundle BEST VALUE You get the complete 18-Ebook set: Statistics Methods for Machine Learning Linear Algebra for Machine Learning Probability for Machine Learning Master Machine Learning Algorithms ML Algorithms From Scratch Machine Learning Mastery With Weka Machine Learning Mastery With R Machine Learning Mastery With Python Imbalanced Classification With Python Time Series Forecasting With Python Deep Learning With Python Deep Learning for CV Deep Learning for NLP Deep Learning for Time Series Forecasting Generative Adversarial Networks with Python Better Deep Learning LSTM Networks With Python XGBoost With Python (includes all bonus source code) Buy Now for $447 That's $646.00 of Value! (You save a massive $199.00)

All prices are in US Dollars (USD).

(1) Click the button. (2) Enter your details. (3) Download immediately.

Secure Payment Processing With SSL Encryption

Are you a Student, Teacher or Retiree? Contact me about a discount.

Do you have any Questions? See the FAQ.

What Are Skills in Machine Learning Worth?

Your boss asks you:

Hey, can you build a predictive model for this?

Imagine you had the skills and confidence to say:

""YES!""

...and follow through .

I have been there. It feels great!

How much is that worth to you?

The industry is demanding skills in machine learning.

The market wants people that can deliver results, not write academic papers.

Business knows what these skills are worth and are paying sky-high starting salaries.

A Data Scientists Salary Begins at:

$100,000 to $150,000.

A Machine Learning Engineers Salary is Even Higher.

What Are Your Alternatives?

You made it this far.

You're ready to take action.

But, what are your alternatives? What options are there?

(1) A Theoretical Textbook for $100+

...it's boring, math-heavy and you'll probably never finish it.

(2) An On-site Boot Camp for $10,000+

...it's full of young kids, you must travel and it can take months.

(3) A Higher Degree for $100,000+

...it's expensive, takes years, and you'll be an academic.

OR...

For the Hands-On Skills You Get...

And the Speed of Results You See...

And the Low Price You Pay...

Machine Learning Mastery Ebooks are

Amazing Value!

And they work. That's why I offer the money-back guarantee.

You're A Professional

The field moves quickly,

...how long can you wait? You think you have all the time in the world, but... New methods are devised and algorithms change .

. New books get released and prices increase .

. New graduates come along and jobs get filled . Right Now is the Best Time to make your start.

Bottom-up is Slow and Frustrating,

...don't you want a faster way? Can you really go on another day, week or month... Scraping ideas and code from incomplete posts .

. Skimming theory and insight from short videos .

. Parsing Greek letters from academic textbooks . Targeted Training is your Shortest Path to a result.

Professionals Stay On Top Of Their Field

Get The Training You Need!

You don't want to fall behind or miss the opportunity.

Frequently Asked Questions

Previous Next

Do you have another question?

Please contact me."
178;machinelearningmastery.com;http://machinelearningmastery.com/machine-learning-in-python-step-by-step/;2019-02-09;Your First Machine Learning Project in Python Step-By-Step;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42

# compare algorithms from pandas import read_csv from matplotlib import pyplot from sklearn . model_selection import train_test_split from sklearn . model_selection import cross_val_score from sklearn . model_selection import StratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . tree import DecisionTreeClassifier from sklearn . neighbors import KNeighborsClassifier from sklearn . discriminant_analysis import LinearDiscriminantAnalysis from sklearn . naive_bayes import GaussianNB from sklearn . svm import SVC # Load dataset url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"" names = [ 'sepal-length' , 'sepal-width' , 'petal-length' , 'petal-width' , 'class' ] dataset = read_csv ( url , names = names ) # Split-out validation dataset array = dataset . values X = array [ : , 0 : 4 ] y = array [ : , 4 ] X_train , X_validation , Y_train , Y_validation = train_test_split ( X , y , test_size = 0.20 , random_state = 1 , shuffle = True ) # Spot Check Algorithms models = [ ] models . append ( ( 'LR' , LogisticRegression ( solver = 'liblinear' , multi_class = 'ovr' ) ) ) models . append ( ( 'LDA' , LinearDiscriminantAnalysis ( ) ) ) models . append ( ( 'KNN' , KNeighborsClassifier ( ) ) ) models . append ( ( 'CART' , DecisionTreeClassifier ( ) ) ) models . append ( ( 'NB' , GaussianNB ( ) ) ) models . append ( ( 'SVM' , SVC ( gamma = 'auto' ) ) ) # evaluate each model in turn results = [ ] names = [ ] for name , model in models : kfold = StratifiedKFold ( n_splits = 10 , random_state = 1 ) cv_results = cross_val_score ( model , X_train , Y_train , cv = kfold , scoring = 'accuracy' ) results . append ( cv_results ) names . append ( name ) print ( '%s: %f (%f)' % ( name , cv_results . mean ( ) , cv_results . std ( ) ) ) # Compare Algorithms pyplot . boxplot ( results , labels = names ) pyplot . title ( 'Algorithm Comparison' ) pyplot . show ( )"
179;machinelearningmastery.com;https://machinelearningmastery.com/examples-of-linear-algebra-in-machine-learning/;2018-03-08;10 Examples of Linear Algebra in Machine Learning;"Tweet Share Share

Last Updated on August 9, 2019

Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms.

It is a key foundation to the field of machine learning, from notations used to describe the operation of algorithms to the implementation of algorithms in code.

Although linear algebra is integral to the field of machine learning, the tight relationship is often left unexplained or explained using abstract concepts such as vector spaces or specific matrix operations.

In this post, you will discover 10 common examples of machine learning that you may be familiar with that use, require and are really best understood using linear algebra.

After reading this post, you will know:

The use of linear algebra structures when working with data, such as tabular datasets and images.

Linear algebra concepts when working with data preparation, such as one hot encoding and dimensionality reduction.

The ingrained use of linear algebra notation and methods in sub-fields such as deep learning, natural language processing, and recommender systems.

Discover vectors, matrices, tensors, matrix types, matrix factorization, PCA, SVD and much more in my new book, with 19 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we will review 10 obvious and concrete examples of linear algebra in machine learning.

I tried to pick examples that you may be familiar with or have even worked with before. They are:

Dataset and Data Files Images and Photographs One-Hot Encoding Linear Regression Regularization Principal Component Analysis Singular-Value Decomposition Latent Semantic Analysis Recommender Systems Deep Learning

Do you have your own favorite example of linear algebra in machine learning?

Let me know in the comments below.

Need help with Linear Algebra for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Dataset and Data Files

In machine learning, you fit a model on a dataset.

This is the table-like set of numbers where each row represents an observation and each column represents a feature of the observation.

For example, below is a snippet of the Iris flowers dataset:

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 1 2 3 4 5 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa

This data is in fact a matrix: a key data structure in linear algebra.

Further, when you split the data into inputs and outputs to fit a supervised machine learning model, such as the measurements and the flower species, you have a matrix (X) and a vector (y). The vector is another key data structure in linear algebra.

Each row has the same length, i.e. the same number of columns, therefore we can say that the data is vectorized where rows can be provided to a model one at a time or in a batch and the model can be pre-configured to expect rows of a fixed width.

For help loading data files as NumPy arrays, see the tutorial:

2. Images and Photographs

Perhaps you are more used to working with images or photographs in computer vision applications.

Each image that you work with is itself a table structure with a width and height and one pixel value in each cell for black and white images or 3 pixel values in each cell for a color image.

A photo is yet another example of a matrix from linear algebra.

Operations on the image, such as cropping, scaling, shearing, and so on are all described using the notation and operations of linear algebra.

For help loading images as NumPy arrays, see the tutorial:

3. One Hot Encoding

Sometimes you work with categorical data in machine learning.

Perhaps the class labels for classification problems, or perhaps categorical input variables.

It is common to encode categorical variables to make them easier to work with and learn by some techniques. A popular encoding for categorical variables is the one hot encoding.

A one hot encoding is where a table is created to represent the variable with one column for each category and a row for each example in the dataset. A check, or one-value, is added in the column for the categorical value for a given row, and a zero-value is added to all other columns.

For example, the color variable with the 3 rows:

red green blue ... 1 2 3 4 red green blue ...

Might be encoded as:

red, green, blue 1, 0, 0 0, 1, 0 0, 0, 1 ... 1 2 3 4 5 red, green, blue 1, 0, 0 0, 1, 0 0, 0, 1 ...

Each row is encoded as a binary vector, a vector with zero or one values and this is an example of a sparse representation, a whole sub-field of linear algebra.

For more on one hot encoding, see the tutorial:

4. Linear Regression

Linear regression is an old method from statistics for describing the relationships between variables.

It is often used in machine learning for predicting numerical values in simpler regression problems.

There are many ways to describe and solve the linear regression problem, i.e. finding a set of coefficients that when multiplied by each of the input variables and added together results in the best prediction of the output variable.

If you have used a machine learning tool or library, the most common way of solving linear regression is via a least squares optimization that is solved using matrix factorization methods from linear regression, such as an LU decomposition or a singular-value decomposition, or SVD.

Even the common way of summarizing the linear regression equation uses linear algebra notation:

y = A . b 1 y = A . b

Where y is the output variable A is the dataset and b are the model coefficients.

For more on linear regression from a linear algebra perspective, see the tutorial:

5. Regularization

In applied machine learning, we often seek the simplest possible models that achieve the best skill on our problem.

Simpler models are often better at generalizing from specific examples to unseen data.

In many methods that involve coefficients, such as regression methods and artificial neural networks, simpler models are often characterized by models that have smaller coefficient values.

A technique that is often used to encourage a model to minimize the size of coefficients while it is being fit on data is called regularization. Common implementations include the L2 and L1 forms of regularization.

Both of these forms of regularization are in fact a measure of the magnitude or length of the coefficients as a vector and are methods lifted directly from linear algebra called the vector norm.

For more on vector norms used in regularization, see the tutorial:

6. Principal Component Analysis

Often, a dataset has many columns, perhaps tens, hundreds, thousands, or more.

Modeling data with many features is challenging, and models built from data that include irrelevant features are often less skillful than models trained from the most relevant data.

It is hard to know which features of the data are relevant and which are not.

Methods for automatically reducing the number of columns of a dataset are called dimensionality reduction, and perhaps the most popular method is called the principal component analysis, or PCA for short.

This method is used in machine learning to create projections of high-dimensional data for both visualization and for training models.

The core of the PCA method is a matrix factorization method from linear algebra. The eigendecomposition can be used and more robust implementations may use the singular-value decomposition, or SVD.

For more on principal component analysis, see the tutorial:

7. Singular-Value Decomposition

Another popular dimensionality reduction method is the singular-value decomposition method, or SVD for short.

As mentioned, and as the name of the method suggests, it is a matrix factorization method from the field of linear algebra.

It has wide use in linear algebra and can be used directly in applications such as feature selection, visualization, noise reduction, and more.

We will see two more cases below of using the SVD in machine learning.

For more on the singular-value decomposition, see the tutorial:

8. Latent Semantic Analysis

In the sub-field of machine learning for working with text data called natural language processing, it is common to represent documents as large matrices of word occurrences.

For example, the columns of the matrix may be the known words in the vocabulary and rows may be sentences, paragraphs, pages, or documents of text with cells in the matrix marked as the count or frequency of the number of times the word occurred.

This is a sparse matrix representation of the text. Matrix factorization methods, such as the singular-value decomposition can be applied to this sparse matrix, which has the effect of distilling the representation down to its most relevant essence. Documents processed in this way are much easier to compare, query, and use as the basis for a supervised machine learning model.

This form of data preparation is called Latent Semantic Analysis, or LSA for short, and is also known by the name Latent Semantic Indexing, or LSI.

9. Recommender Systems

Predictive modeling problems that involve the recommendation of products are called recommender systems, a sub-field of machine learning.

Examples include the recommendation of books based on previous purchases and purchases by customers like you on Amazon, and the recommendation of movies and TV shows to watch based on your viewing history and viewing history of subscribers like you on Netflix.

The development of recommender systems is primarily concerned with linear algebra methods. A simple example is in the calculation of the similarity between sparse customer behavior vectors using distance measures such as Euclidean distance or dot products.

Matrix factorization methods like the singular-value decomposition are used widely in recommender systems to distill item and user data to their essence for querying and searching and comparison.

10. Deep Learning

Artificial neural networks are nonlinear machine learning algorithms that are inspired by elements of the information processing in the brain and have proven effective at a range of problems, not the least of which is predictive modeling.

Deep learning is the recent resurgence in the use of artificial neural networks with newer methods and faster hardware that allow for the development and training of larger and deeper (more layers) networks on very large datasets. Deep learning methods are routinely achieving state-of-the-art results on a range of challenging problems such as machine translation, photo captioning, speech recognition, and much more.

At their core, the execution of neural networks involves linear algebra data structures multiplied and added together. Scaled up to multiple dimensions, deep learning methods work with vectors, matrices, and even tensors of inputs and coefficients, where a tensor is a matrix with more than two dimensions.

Linear algebra is central to the description of deep learning methods via matrix notation to the implementation of deep learning methods such as Google’s TensorFlow Python library that has the word “tensor” in its name.

For more on tensors, see the tutorial:

Summary

In this post, you discovered 10 common examples of machine learning that you may be familiar with that use and require linear algebra.

Specifically, you learned:

The use of linear algebra structures when working with data such as tabular datasets and images.

Linear algebra concepts when working with data preparation such as one hot encoding and dimensionality reduction.

The ingrained use of linear algebra notation and methods in sub-fields such as deep learning, natural language processing, and recommender systems.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Linear Algebra for Machine Learning! Develop a working understand of linear algebra ...by writing lines of code in python Discover how in my new Ebook:

Linear Algebra for Machine Learning It provides self-study tutorials on topics like:

Vector Norms, Matrix Multiplication, Tensors, Eigendecomposition, SVD, PCA and much more... Finally Understand the Mathematics of Data Skip the Academics. Just Results. See What's Inside"
180;machinelearningmastery.com;https://machinelearningmastery.com/how-to-model-human-activity-from-smartphone-data/;2018-09-16;How to Model Human Activity From Smartphone Data;"# plot durations of each activity by subject

from numpy import array

from numpy import dstack

from numpy import unique

from pandas import read_csv

from matplotlib import pyplot

# load a single file as a numpy array

def load_file ( filepath ) :

dataframe = read_csv ( filepath , header = None , delim_whitespace = True )

return dataframe . values

# load a list of files, such as x, y, z data for a given variable

def load_group ( filenames , prefix = '' ) :

loaded = list ( )

for name in filenames :

data = load_file ( prefix + name )

loaded . append ( data )

# stack group so that features are the 3rd dimension

loaded = dstack ( loaded )

return loaded

# load a dataset group, such as train or test

def load_dataset ( group , prefix = '' ) :

filepath = prefix + group + '/Inertial Signals/'

# load all 9 files as a single array

filenames = list ( )

# total acceleration

filenames += [ 'total_acc_x_' + group + '.txt' , 'total_acc_y_' + group + '.txt' , 'total_acc_z_' + group + '.txt' ]

# body acceleration

filenames += [ 'body_acc_x_' + group + '.txt' , 'body_acc_y_' + group + '.txt' , 'body_acc_z_' + group + '.txt' ]

# body gyroscope

filenames += [ 'body_gyro_x_' + group + '.txt' , 'body_gyro_y_' + group + '.txt' , 'body_gyro_z_' + group + '.txt' ]

# load input data

X = load_group ( filenames , filepath )

# load class output

y = load_file ( prefix + group + '/y_' + group + '.txt' )

return X , y

# get all data for one subject

def data_for_subject ( X , y , sub_map , sub_id ) :

# get row indexes for the subject id

ix = [ i for i in range ( len ( sub_map ) ) if sub_map [ i ] == sub_id ]

# return the selected samples

return X [ ix , : , : ] , y [ ix ]

# convert a series of windows to a 1D list

def to_series ( windows ) :

series = list ( )

for window in windows :

# remove the overlap from the window

half = int ( len ( window ) / 2 ) - 1

for value in window [ - half : ] :

series . append ( value )

return series

# group data by activity

def data_by_activity ( X , y , activities ) :

# group windows by activity

return { a : X [ y [ : , 0 ] == a , : , : ] for a in activities }

# plot activity durations by subject

def plot_activity_durations_by_subject ( X , y , sub_map ) :

# get unique subjects and activities

subject_ids = unique ( sub_map [ : , 0 ] )

activity_ids = unique ( y [ : , 0 ] )

# enumerate subjects

activity_windows = { a : list ( ) for a in activity_ids }

for sub_id in subject_ids :

# get data for one subject

_ , subj_y = data_for_subject ( X , y , sub_map , sub_id )

# count windows by activity

for a in activity_ids :

activity_windows [ a ] . append ( len ( subj_y [ subj_y [ : , 0 ] == a ] ) )

# organize durations into a list of lists

durations = [ activity_windows [ a ] for a in activity_ids ]

pyplot . boxplot ( durations , labels = activity_ids )

pyplot . show ( )

# load training dataset

X , y = load_dataset ( 'train' , 'HARDataset/' )

# load mapping of rows to subjects

sub_map = load_file ( 'HARDataset/train/subject_train.txt' )

# plot durations"
181;news.mit.edu;http://news.mit.edu/2018/blind-cheetah-robot-climb-stairs-obstacles-disaster-zones-0705;;“Blind” Cheetah 3 robot can climb stairs littered with obstacles;"MIT’s Cheetah 3 robot can now leap and gallop across rough terrain, climb a staircase littered with debris, and quickly recover its balance when suddenly yanked or shoved, all while essentially blind.

The 90-pound mechanical beast — about the size of a full-grown Labrador — is intentionally designed to do all this without relying on cameras or any external environmental sensors. Instead, it nimbly “feels” its way through its surroundings in a way that engineers describe as “blind locomotion,” much like making one’s way across a pitch-black room.

“There are many unexpected behaviors the robot should be able to handle without relying too much on vision,” says the robot’s designer, Sangbae Kim, associate professor of mechanical engineering at MIT. “Vision can be noisy, slightly inaccurate, and sometimes not available, and if you rely too much on vision, your robot has to be very accurate in position and eventually will be slow. So we want the robot to rely more on tactile information. That way, it can handle unexpected obstacles while moving fast.”

Researchers will present the robot’s vision-free capabilities in October at the International Conference on Intelligent Robots, in Madrid. In addition to blind locomotion, the team will demonstrate the robot’s improved hardware, including an expanded range of motion compared to its predecessor Cheetah 2, that allows the robot to stretch backwards and forwards, and twist from side to side, much like a cat limbering up to pounce.

Within the next few years, Kim envisions the robot carrying out tasks that would otherwise be too dangerous or inaccessible for humans to take on.

“Cheetah 3 is designed to do versatile tasks such as power plant inspection, which involves various terrain conditions including stairs, curbs, and obstacles on the ground,” Kim says. ""I think there are countless occasions where we [would] want to send robots to do simple tasks instead of humans. Dangerous, dirty, and difficult work can be done much more safely through remotely controlled robots.”

Making a commitment

The Cheetah 3 can blindly make its way up staircases and through unstructured terrain, and can quickly recover its balance in the face of unexpected forces, thanks to two new algorithms developed by Kim’s team: a contact detection algorithm, and a model-predictive control algorithm.

The contact detection algorithm helps the robot determine the best time for a given leg to switch from swinging in the air to stepping on the ground. For example, if the robot steps on a light twig versus a hard, heavy rock, how it reacts — and whether it continues to carry through with a step, or pulls back and swings its leg instead — can make or break its balance.

“When it comes to switching from the air to the ground, the switching has to be very well-done,” Kim says. “This algorithm is really about, ‘When is a safe time to commit my footstep?’”

The contact detection algorithm helps the robot determine the best time to transition a leg between swing and step, by constantly calculating for each leg three probabilities: the probability of a leg making contact with the ground, the probability of the force generated once the leg hits the ground, and the probability that the leg will be in midswing. The algorithm calculates these probabilities based on data from gyroscopes, accelerometers, and joint positions of the legs, which record the leg’s angle and height with respect to the ground.

If, for example, the robot unexpectedly steps on a wooden block, its body will suddenly tilt, shifting the angle and height of the robot. That data will immediately feed into calculating the three probabilities for each leg, which the algorithm will combine to estimate whether each leg should commit to pushing down on the ground, or lift up and swing away in order to keep its balance — all while the robot is virtually blind.

“If humans close our eyes and make a step, we have a mental model for where the ground might be, and can prepare for it. But we also rely on the feel of touch of the ground,” Kim says. “We are sort of doing the same thing by combining multiple [sources of] information to determine the transition time.”

The researchers tested the algorithm in experiments with the Cheetah 3 trotting on a laboratory treadmill and climbing on a staircase. Both surfaces were littered with random objects such as wooden blocks and rolls of tape.

“It doesn’t know the height of each step, and doesn’t know there are obstacles on the stairs, but it just plows through without losing its balance,” Kim says. “Without that algorithm, the robot was very unstable and fell easily.”

Future force

The robot’s blind locomotion was also partly due to the model-predictive control algorithm, which predicts how much force a given leg should apply once it has committed to a step.

“The contact detection algorithm will tell you, ‘this is the time to apply forces on the ground,’” Kim says. “But once you’re on the ground, now you need to calculate what kind of forces to apply so you can move the body in the right way.”

The model-predictive control algorithm calculates the multiplicative positions of the robot’s body and legs a half-second into the future, if a certain force is applied by any given leg as it makes contact with the ground.

“Say someone kicks the robot sideways,” Kim says. “When the foot is already on the ground, the algorithm decides, ‘How should I specify the forces on the foot? Because I have an undesirable velocity on the left, so I want to apply a force in the opposite direction to kill that velocity. If I apply 100 newtons in this opposite direction, what will happen a half second later?”

The algorithm is designed to make these calculations for each leg every 50 milliseconds, or 20 times per second. In experiments, researchers introduced unexpected forces by kicking and shoving the robot as it trotted on a treadmill, and yanking it by the leash as it climbed up an obstacle-laden staircase. They found that the model-predictive algorithm enabled the robot to quickly produce counter-forces to regain its balance and keep moving forward, without tipping too far in the opposite direction.

“It’s thanks to that predictive control that can apply the right forces on the ground, combined with this contact transition algorithm that makes each contact very quick and secure,” Kim says.

The team had already added cameras to the robot to give it visual feedback of its surroundings. This will help in mapping the general environment, and will give the robot a visual heads-up on larger obstacles such as doors and walls. But for now, the team is working to further improve the robot’s blind locomotion

“We want a very good controller without vision first,” Kim says. “And when we do add vision, even if it might give you the wrong information, the leg should be able to handle (obstacles). Because what if it steps on something that a camera can’t see? What will it do? That’s where blind locomotion can help. We don’t want to trust our vision too much.”

This research was supported, in part, by Naver, Toyota Research Institute, Foxconn, and Air Force Office of Scientific Research."
182;machinelearningmastery.com;http://machinelearningmastery.com/normalize-standardize-machine-learning-data-weka/;2016-07-04;How to Normalize and Standardize Your Machine Learning Data in Weka;"Tweet Share Share

Last Updated on December 11, 2019

Machine learning algorithms make assumptions about the dataset you are modeling.

Often, raw data is comprised of attributes with varying scales. For example, one attribute may be in kilograms and another may be a count. Although not required, you can often get a boost in performance by carefully choosing methods to rescale your data.

In this post you will discover how you can rescale your data so that all of the data has the same scale.

After reading this post you will know:

How to normalize your numeric attributes between the range of 0 and 1.

How to standardize your numeric attributes to have a 0 mean and unit variance.

When to choose normalization or standardization.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Update March/2018: Added alternate link to download the dataset as the original appears to have been taken down.

Predict the Onset of Diabetes

The dataset used for this example is the Pima Indians onset of diabetes dataset.

It is a classification problem where each instance represents medical details for one patient and the task is to predict whether the patient will have an onset of diabetes within the next five years.

This is a good dataset to practice scaling as the 8 input variables all have varying scales, such as the count of the number of times the patient was pregnant (preg) and the calculation of the patients body mass index (mass).

Download the dataset and place it in your current working directory.

You can also access this dataset in your Weka installation, under the data/ directory in the file called diabetes.arff.

About Data Filters in Weka

Weka provides filters for transforming your dataset. The best way to see what filters are supported and to play with them on your dataset is to use the Weka Explorer.

The “Filter” pane allows you to choose a filter.

Filters are divided into two types:

Supervised Filters : That can be applied but require user control in some way. Such as rebalancing instances for a class.

: That can be applied but require user control in some way. Such as rebalancing instances for a class. Unsupervised Filters: That can be applied in an undirected manner. For example, rescale all values to the range 0-to-1.

Personally, I think the distinction between these two types of filters is a little arbitrary and confusing. Nevertheless, that is how they are laid out.

Within these two groups, filters are further divided into filters for Attributes and Instances:

Attribute Filters : Apply an operation on attributes or one attribute at a time.

: Apply an operation on attributes or one attribute at a time. Instance Filters: Apply an operation on instance or one instance at a time.

This distinction makes a lot more sense.

After you have selected a filter, its name will appear in the box next to the “Choose” button.

You can configure a filter by clicking its name which will open the configuration window. You can change the parameters of the filter and even save or load the configuration of the filter itself. This is great for reproducibility.

You can learn more about each configuration option by hovering over it and reading the tooltip.

You can also read all of the details about the filter including the configuration, papers and books for further reading and more information about the filter works by clicking the “More” button.

You can close the help and apply the configuration by clicking the “OK” button.

You can apply a filter to your loaded dataset by clicking the “Apply” button next to the filter name.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Normalize Your Numeric Attributes

Data normalization is the process of rescaling one or more attributes to the range of 0 to 1. This means that the largest value for each attribute is 1 and the smallest value is 0.

Normalization is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve).

You can normalize all of the attributes in your dataset with Weka by choosing the Normalize filter and applying it to your dataset.

You can use the following recipe to normalize your dataset:

1. Open the Weka Explorer.

2. Load your dataset.

3. Click the “Choose” button to select a Filter and select unsupervised.attribute.Normalize.

4. Click the “Apply” button to normalize your dataset.

5. Click the “Save” button and type a filename to save the normalized copy of your dataset.

Reviewing the details of each attribute in the “Selected attribute” window will give you confidence that the filter was successful and that each attribute was rescaled to the range of 0 to 1.

You can use other scales such as -1 to 1, which is useful when using support vector machines and adaboost.

Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.

Standardize Your Numeric Attributes

Data standardization is the process of rescaling one or more attributes so that they have a mean value of 0 and a standard deviation of 1.

Standardization assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian.

You can standardize all of the attributes in your dataset with Weka by choosing the Standardize filter and applying it your dataset.

You can use the following recipe to standardize your dataset:

1. Open the Weka Explorer

2. Load your dataset.

3. Click the “Choose” button to select a Filter and select unsupervised.attribute.Standardize.

4. Click the “Apply” button to normalize your dataset.

5. Click the “Save” button and type a filename to save the standardized copy of your dataset.

Reviewing the details of each attribute in the “Selected attribute” window will give you confidence that the filter was successful and that each attribute has a mean of 0 and a standard deviation of 1.

Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression and linear discriminant analysis.

Summary

In this post you discovered how to rescale your dataset in Weka.

Specifically, you learned:

How to normalize your dataset to the range 0 to 1.

How to standardize your data to have a mean of 0 and a standard deviation of 1.

When to use normalization and standardization.

Do you have any questions about scaling your data or about this post? Ask your questions in the comments and I will do my best to answer.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
183;machinelearningmastery.com;https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/;2017-11-05;How to Develop a Character-Based Neural Language Model in Keras;"from numpy import array

from pickle import dump

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# load doc into memory

def load_doc ( filename ) :

# open the file as read only

file = open ( filename , 'r' )

# read all text

text = file . read ( )

# close the file

file . close ( )

return text

# load

in_filename = 'char_sequences.txt'

raw_text = load_doc ( in_filename )

lines = raw_text . split ( '

' )

# integer encode sequences of characters

chars = sorted ( list ( set ( raw_text ) ) )

mapping = dict ( ( c , i ) for i , c in enumerate ( chars ) )

sequences = list ( )

for line in lines :

# integer encode line

encoded_seq = [ mapping [ char ] for char in line ]

# store

sequences . append ( encoded_seq )

# vocabulary size

vocab_size = len ( mapping )

print ( 'Vocabulary Size: %d' % vocab_size )

# separate into input and output

sequences = array ( sequences )

X , y = sequences [ : , : - 1 ] , sequences [ : , - 1 ]

sequences = [ to_categorical ( x , num_classes = vocab_size ) for x in X ]

X = array ( sequences )

y = to_categorical ( y , num_classes = vocab_size )

# define model

model = Sequential ( )

model . add ( LSTM ( 75 , input_shape = ( X . shape [ 1 ] , X . shape [ 2 ] ) ) )

model . add ( Dense ( vocab_size , activation = 'softmax' ) )

print ( model . summary ( ) )

# compile model

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( X , y , epochs = 100 , verbose = 2 )

# save the model to file

model . save ( 'model.h5' )

# save the mapping"
184;machinelearningmastery.com;http://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/;2016-05-26;Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn;"# Logistic Regression Classification

import pandas

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

seed = 7

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

model = LogisticRegression ( )

results = model_selection . cross_val_score ( model , X , Y , cv = kfold )"
185;machinelearningmastery.com;https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/;2017-11-06;How to Get Started with Deep Learning for Natural Language Processing;"from gensim . models import Word2Vec

from sklearn . decomposition import PCA

from matplotlib import pyplot

# define training data

sentences = [ [ 'this' , 'is' , 'the' , 'first' , 'sentence' , 'for' , 'word2vec' ] ,

[ 'this' , 'is' , 'the' , 'second' , 'sentence' ] ,

[ 'yet' , 'another' , 'sentence' ] ,

[ 'one' , 'more' , 'sentence' ] ,

[ 'and' , 'the' , 'final' , 'sentence' ] ]

# train model

model = Word2Vec ( sentences , min_count = 1 )

# fit a 2D PCA model to the vectors

X = model [ model . wv . vocab ]

pca = PCA ( n_components = 2 )

result = pca . fit_transform ( X )

# create a scatter plot of the projection

pyplot . scatter ( result [ : , 0 ] , result [ : , 1 ] )

words = list ( model . wv . vocab )

for i , word in enumerate ( words ) :

pyplot . annotate ( word , xy = ( result [ i , 0 ] , result [ i , 1 ] ) )"
186;machinelearningmastery.com;https://machinelearningmastery.com/how-to-load-and-manipulate-images-for-deep-learning-in-python-with-pil-pillow/;2019-03-21;How to Load and Manipulate Images for Deep Learning in Python With PIL/Pillow;"Tweet Share Share

Last Updated on September 12, 2019

Before you can develop predictive models for image data, you must learn how to load and manipulate images and photographs.

The most popular and de facto standard library in Python for loading and working with image data is Pillow. Pillow is an updated version of the Python Image Library, or PIL, and supports a range of simple and sophisticated image manipulation functionality. It is also the basis for simple image support in other Python libraries such as SciPy and Matplotlib.

In this tutorial, you will discover how to load and manipulate image data using the Pillow Python library.

After completing this tutorial, you will know:

How to install the Pillow library and confirm it is working correctly.

How to load images from file, convert loaded images to NumPy arrays, and save images in new formats.

How to perform basic transforms to image data such as resize, flips, rotations, and cropping.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Updated Sep/2019: Updated to reflect minor changes to Pillow API.

Tutorial Overview

This tutorial is divided into six parts; they are:

How to Install Pillow How to Load and Display Images How to Convert Images to NumPy Arrays and Back How to Save Images to File How to Resize Images How to Flip, Rotate, and Crop Images

How to Install Pillow

The Python Imaging Library, or PIL for short, is an open source library for loading and manipulating images.

It was developed and made available more than 25 years ago and has become a de facto standard API for working with images in Python. The library is now defunct and no longer updated and does not support Python 3.

Pillow is a PIL library that supports Python 3 and is the preferred modern library for image manipulation in Python. It is even required for simple image loading and saving in other Python scientific libraries such as SciPy and Matplotlib.

The Pillow library is installed as a part of most SciPy installations; for example, if you are using Anaconda.

For help setting up your SciPy environment, see the step-by-step tutorial:

If you manage the installation of Python software packages yourself for your workstation, you can easily install Pillow using pip; for example:

sudo pip install Pillow 1 sudo pip install Pillow

For more help installing Pillow manually, see:

Pillow is built on top of the older PIL and you can confirm that the library was installed correctly by printing the version number; for example:

# check Pillow version number import PIL print('Pillow Version:', PIL.__version__) 1 2 3 # check Pillow version number import PIL print ( 'Pillow Version:' , PIL . __version__ )

Running the example will print the version number for Pillow; your version number should be the same or higher.

Pillow Version: 6.1.0 1 Pillow Version: 6.1.0

Now that your environment is set up, let’s look at how to load an image.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Load and Display Images

We need a test image to demonstrate some important features of using the Pillow library.

In this tutorial, we will use a photograph of the Sydney Opera House, taken by Ed Dunens and made available on Flickr under a creative commons license, some rights reserved.

Download the photograph and save it in your current working directory with the file name “opera_house.jpg“.

Images are typically in PNG or JPEG format and can be loaded directly using the open() function on Image class. This returns an Image object that contains the pixel data for the image as well as details about the image. The Image class is the main workhorse for the Pillow library and provides a ton of properties about the image as well as functions that allow you to manipulate the pixels and format of the image.

The ‘format‘ property on the image will report the image format (e.g. JPEG), the ‘mode‘ will report the pixel channel format (e.g. RGB or CMYK), and the ‘size‘ will report the dimensions of the image in pixels (e.g. 640×480).

The show() function will display the image using your operating systems default application.

The example below demonstrates how to load and show an image using the Image class in the Pillow library.

# load and show an image with Pillow from PIL import Image # load the image image = Image.open('opera_house.jpg') # summarize some details about the image print(image.format) print(image.mode) print(image.size) # show the image image.show() 1 2 3 4 5 6 7 8 9 10 # load and show an image with Pillow from PIL import Image # load the image image = Image . open ( 'opera_house.jpg' ) # summarize some details about the image print ( image . format ) print ( image . mode ) print ( image . size ) # show the image image . show ( )

Running the example will first load the image, report the format, mode, and size, then show the image on your desktop.

JPEG RGB (640, 360) 1 2 3 JPEG RGB (640, 360)

The image is shown using the default image preview application for your operating system, such as Preview on MacOS.

Now that you know how to load an image, let’s look at how you can access the pixel data of images.

How to Convert Images to NumPy Arrays and Back

Often in machine learning, we want to work with images as NumPy arrays of pixel data.

With Pillow installed, you can also use the Matplotlib library to load the image and display it within a Matplotlib frame.

This can be achieved using the imread() function that loads the image an array of pixels directly and the imshow() function that will display an array of pixels as an image.

The example below loads and displays the same image using Matplotlib that, in turn, will use Pillow under the covers.

# load and display an image with Matplotlib from matplotlib import image from matplotlib import pyplot # load image as pixel array data = image.imread('opera_house.jpg') # summarize shape of the pixel array print(data.dtype) print(data.shape) # display the array of pixels as an image pyplot.imshow(data) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 # load and display an image with Matplotlib from matplotlib import image from matplotlib import pyplot # load image as pixel array data = image . imread ( 'opera_house.jpg' ) # summarize shape of the pixel array print ( data . dtype ) print ( data . shape ) # display the array of pixels as an image pyplot . imshow ( data ) pyplot . show ( )

Running the example first loads the image and then reports the data type of the array, in this case, 8-bit unsigned integers, then reports the shape of the array, in this case, 360 pixels wide by 640 pixels high and three channels for red, green, and blue.

uint8 (360, 640, 3) 1 2 uint8 (360, 640, 3)

Finally, the image is displayed using Matplotlib.

The Matplotlib wrapper functions can be more effective than using Pillow directly.

Nevertheless, you can access the pixel data from a Pillow Image. Perhaps the simplest way is to construct a NumPy array and pass in the Image object. The process can be reversed converting a given array of pixel data into a Pillow Image object using the Image.fromarray() function. This can be useful if image data is manipulated as a NumPy array and you then want to save it later as a PNG or JPEG file.

The example below loads the photo as a Pillow Image object and converts it to a NumPy array, then converts it back to an Image object again.

# load image and convert to and from NumPy array from PIL import Image from numpy import asarray # load the image image = Image.open('opera_house.jpg') # convert image to numpy array data = asarray(image) # summarize shape print(data.shape) # create Pillow image image2 = Image.fromarray(data) # summarize image details print(image2.format) print(image2.mode) print(image2.size) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # load image and convert to and from NumPy array from PIL import Image from numpy import asarray # load the image image = Image . open ( 'opera_house.jpg' ) # convert image to numpy array data = asarray ( image ) # summarize shape print ( data . shape ) # create Pillow image image2 = Image . fromarray ( data ) # summarize image details print ( image2 . format ) print ( image2 . mode ) print ( image2 . size )

Running the example first loads the photo as a Pillow image then converts it to a NumPy array and reports the shape of the array. Finally, the array is converted back into a Pillow image and the details are reported.

(360, 640, 3) JPEG RGB (640, 360) 1 2 3 4 (360, 640, 3) JPEG RGB (640, 360)

Both approaches are effective for loading image data into NumPy arrays, although the Matplotlib imread() function uses fewer lines of code than loading and converting a Pillow Image object and may be preferred.

For example, you could easily load all images in a directory as a list as follows:

# load all images in a directory from os import listdir from matplotlib import image # load all images in a directory loaded_images = list() for filename in listdir('images'): # load image img_data = image.imread('images/' + filename) # store loaded image loaded_images.append(img_data) print('> loaded %s %s' % (filename, img_data.shape)) 1 2 3 4 5 6 7 8 9 10 11 # load all images in a directory from os import listdir from matplotlib import image # load all images in a directory loaded_images = list ( ) for filename in listdir ( 'images' ) : # load image img_data = image . imread ( 'images/' + filename ) # store loaded image loaded_images . append ( img_data ) print ( '> loaded %s %s' % ( filename , img_data . shape ) )

Now that we know how to load images as NumPy arrays, let’s look at how to save images to file.

How to Save Images to File

An image object can be saved by calling the save() function.

This can be useful if you want to save an image in a different format, in which case the ‘format‘ argument can be specified, such as PNG, GIF, or PEG.

For example, the code listing below loads the photograph in JPEG format and saves it in PNG format.

# example of saving an image in another format from PIL import Image # load the image image = Image.open('opera_house.jpg') # save as PNG format image.save('opera_house.png', format='PNG') # load the image again and inspect the format image2 = Image.open('opera_house.png') print(image2.format) 1 2 3 4 5 6 7 8 9 # example of saving an image in another format from PIL import Image # load the image image = Image . open ( 'opera_house.jpg' ) # save as PNG format image . save ( 'opera_house.png' , format = 'PNG' ) # load the image again and inspect the format image2 = Image . open ( 'opera_house.png' ) print ( image2 . format )

Running the example loads the JPEG image, saves it in PNG format, then loads the newly saved image again, and confirms that the format is indeed PNG.

PNG 1 PNG

Saving images is useful if you perform some data preparation on the image before modeling. One example is converting color images (RGB channels) to grayscale (1 channel).

There are a number of ways to convert an image to grayscale, but Pillow provides the convert() function and the mode ‘L‘ will convert an image to grayscale.

# example of saving a grayscale version of a loaded image from PIL import Image # load the image image = Image.open('opera_house.jpg') # convert the image to grayscale gs_image = image.convert(mode='L') # save in jpeg format gs_image.save('opera_house_grayscale.jpg') # load the image again and show it image2 = Image.open('opera_house_grayscale.jpg') # show the image image2.show() 1 2 3 4 5 6 7 8 9 10 11 12 # example of saving a grayscale version of a loaded image from PIL import Image # load the image image = Image . open ( 'opera_house.jpg' ) # convert the image to grayscale gs_image = image . convert ( mode = 'L' ) # save in jpeg format gs_image . save ( 'opera_house_grayscale.jpg' ) # load the image again and show it image2 = Image . open ( 'opera_house_grayscale.jpg' ) # show the image image2 . show ( )

Running the example loads the photograph, converts it to grayscale, saves the image in a new file, then loads it again and shows it to confirm that the photo is now grayscale instead of color.

How to Resize Images

It is important to be able to resize images before modeling.

Sometimes it is desirable to thumbnail all images to have the same width or height. This can be achieved with Pillow using the thumbnail() function. The function takes a tuple with the width and height and the image will be resized so that the width and height of the image are equal or smaller than the specified shape.

For example, the test photograph we have been working with has the width and height of (640, 360). We can resize it to (100, 100), in which case the largest dimension, in this case, the width, will be reduced to 100, and the height will be scaled in order to retain the aspect ratio of the image.

The example below will load the photograph and create a smaller thumbnail with a width and height of 100 pixels.

# create a thumbnail of an image from PIL import Image # load the image image = Image.open('opera_house.jpg') # report the size of the image print(image.size) # create a thumbnail and preserve aspect ratio image.thumbnail((100,100)) # report the size of the thumbnail print(image.size) 1 2 3 4 5 6 7 8 9 10 # create a thumbnail of an image from PIL import Image # load the image image = Image . open ( 'opera_house.jpg' ) # report the size of the image print ( image . size ) # create a thumbnail and preserve aspect ratio image . thumbnail ( ( 100 , 100 ) ) # report the size of the thumbnail print ( image . size )

Running the example first loads the photograph and reports the width and height. The image is then resized, in this case, the width is reduced to 100 pixels and the height is reduced to 56 pixels, maintaining the aspect ratio of the original image.

(640, 360) (100, 56) 1 2 (640, 360) (100, 56)

We may not want to preserve the aspect ratio, and instead, we may want to force the pixels into a new shape.

This can be achieved using the resize() function that allows you to specify the width and height in pixels and the image will be reduced or stretched to fit the new shape.

The example below demonstrates how to resize a new image and ignore the original aspect ratio.

# resize image and force a new shape from PIL import Image # load the image image = Image.open('opera_house.jpg') # report the size of the image print(image.size) # resize image and ignore original aspect ratio img_resized = image.resize((200,200)) # report the size of the thumbnail print(img_resized.size) 1 2 3 4 5 6 7 8 9 10 # resize image and force a new shape from PIL import Image # load the image image = Image . open ( 'opera_house.jpg' ) # report the size of the image print ( image . size ) # resize image and ignore original aspect ratio img_resized = image . resize ( ( 200 , 200 ) ) # report the size of the thumbnail print ( img_resized . size )

Running the example loads the image, reports the shape of the image, then resizes it to have a width and height of 200 pixels.

(640, 360) (200, 200) 1 2 (640, 360) (200, 200)

The sized of the image is shown and we can see that the wide photograph has been compressed into a square, although all of the features are still quite visible and obvious.

Standard resampling algorithms are used to invent or remove pixels when resizing, and you can specify a technique, although default is a bicubic resampling algorithm that suits most general applications.

How to Flip, Rotate, and Crop Images

Simple image manipulation can be used to create new versions of images that, in turn, can provide a richer training dataset when modeling.

Generally, this is referred to as data augmentation and may involve creating flipped, rotated, cropped, or other modified versions of the original images with the hope that the algorithm will learn to extract the same features from the image data regardless of where they might appear.

You may want to implement your own data augmentation schemes, in which case you need to know how to perform basic manipulations of your image data.

Flip Image

An image can be flipped by calling the flip() function and passing in a method such as FLIP_LEFT_RIGHT for a horizontal flip or FLIP_TOP_BOTTOM for a vertical flip. Other flips are also available

The example below creates both horizontal and vertical flipped versions of the image.

# create flipped versions of an image from PIL import Image from matplotlib import pyplot # load image image = Image.open('opera_house.jpg') # horizontal flip hoz_flip = image.transpose(Image.FLIP_LEFT_RIGHT) # vertical flip ver_flip = image.transpose(Image.FLIP_TOP_BOTTOM) # plot all three images using matplotlib pyplot.subplot(311) pyplot.imshow(image) pyplot.subplot(312) pyplot.imshow(hoz_flip) pyplot.subplot(313) pyplot.imshow(ver_flip) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # create flipped versions of an image from PIL import Image from matplotlib import pyplot # load image image = Image . open ( 'opera_house.jpg' ) # horizontal flip hoz_flip = image . transpose ( Image . FLIP_LEFT_RIGHT ) # vertical flip ver_flip = image . transpose ( Image . FLIP_TOP_BOTTOM ) # plot all three images using matplotlib pyplot . subplot ( 311 ) pyplot . imshow ( image ) pyplot . subplot ( 312 ) pyplot . imshow ( hoz_flip ) pyplot . subplot ( 313 ) pyplot . imshow ( ver_flip ) pyplot . show ( )

Running the example loads the photograph and creates horizontal and vertical flipped versions of the photograph, then plots all three versions as subplots using Matplotlib.

You will note that the imshow() function can plot the Image object directly without having to convert it to a NumPy array.

Rotate Image

An image can be rotated using the rotate() function and passing in the angle for the rotation.

The function offers additional control such as whether or not to expand the dimensions of the image to fit the rotated pixel values (default is to clip to the same size), where to center the rotation the image (default is the center), and the fill color for pixels outside of the image (default is black).

The example below creates a few rotated versions of the image.

# create rotated versions of an image from PIL import Image from matplotlib import pyplot # load image image = Image.open('opera_house.jpg') # plot original image pyplot.subplot(311) pyplot.imshow(image) # rotate 45 degrees pyplot.subplot(312) pyplot.imshow(image.rotate(45)) # rotate 90 degrees pyplot.subplot(313) pyplot.imshow(image.rotate(90)) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # create rotated versions of an image from PIL import Image from matplotlib import pyplot # load image image = Image . open ( 'opera_house.jpg' ) # plot original image pyplot . subplot ( 311 ) pyplot . imshow ( image ) # rotate 45 degrees pyplot . subplot ( 312 ) pyplot . imshow ( image . rotate ( 45 ) ) # rotate 90 degrees pyplot . subplot ( 313 ) pyplot . imshow ( image . rotate ( 90 ) ) pyplot . show ( )

Running the example plots the original photograph, then a version of the photograph rotated 45 degrees, and another rotated 90 degrees.

You can see that in both rotations, the pixels are clipped to the original dimensions of the image and that the empty pixels are filled with black color.

Cropped Image

An image can be cropped: that is, a piece can be cut out to create a new image, using the crop() function.

The crop function takes a tuple argument that defines the two x/y coordinates of the box to crop out of the image. For example, if the image is 2,000 by 2,000 pixels, we can clip out a 100 by 100 box in the middle of the image by defining a tuple with the top-left and bottom-right points of (950, 950, 1050, 1050).

The example below demonstrates how to create a new image as a crop from a loaded image.

# example of cropping an image from PIL import Image # load image image = Image.open('opera_house.jpg') # create a cropped image cropped = image.crop((100, 100, 200, 200)) # show cropped image cropped.show() 1 2 3 4 5 6 7 8 # example of cropping an image from PIL import Image # load image image = Image . open ( 'opera_house.jpg' ) # create a cropped image cropped = image . crop ( ( 100 , 100 , 200 , 200 ) ) # show cropped image cropped . show ( )

Running the example creates a cropped square image of 100 pixels starting at 100,100 and extending down and left to 200,200. The cropped square is then displayed.

Extensions

This section lists some ideas for extending the tutorial that you may wish to explore.

Your Own Images . Experiment with Pillow functions for reading and manipulating images with your own image data.

. Experiment with Pillow functions for reading and manipulating images with your own image data. More Transforms . Review the Pillow API documentation and experiment with additional image manipulation functions.

. Review the Pillow API documentation and experiment with additional image manipulation functions. Image Pre-processing. Write a function to create augmented versions of an image ready for use with a deep learning neural network.

If you explore any of these extensions, I’d love to know.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this tutorial, you discovered how to load and manipulate image data using the Pillow Python library.

Specifically, you learned:

How to install the Pillow library and confirm it is working correctly.

How to load images from file, convert loaded images to NumPy arrays, and save images in new formats.

How to perform basic transforms to image data such as resize, flips, rotations, and cropping.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning Models for Vision Today! Develop Your Own Vision Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Computer Vision It provides self-study tutorials on topics like:

classification, object detection (yolo and rcnn), face recognition (vggface and facenet), data preparation and much more... Finally Bring Deep Learning to your Vision Projects Skip the Academics. Just Results. See What's Inside"
187;machinelearningmastery.com;https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/;2017-12-07;Encoder-Decoder Models for Text Summarization in Keras;"vocab_size = . . .

src_txt_length = . . .

sum_txt_length = . . .

# encoder input model

inputs = Input ( shape = ( src_txt_length , ) )

encoder1 = Embedding ( vocab_size , 128 ) ( inputs )

encoder2 = LSTM ( 128 ) ( encoder1 )

encoder3 = RepeatVector ( sum_txt_length ) ( encoder2 )

# decoder output model

decoder1 = LSTM ( 128 , return_sequences = True ) ( encoder3 )

outputs = TimeDistributed ( Dense ( vocab_size , activation = 'softmax' ) ) ( decoder1 )

# tie it together

model = Model ( inputs = inputs , outputs = outputs )"
188;machinelearningmastery.com;https://machinelearningmastery.com/machine-learning-in-python-step-by-step/;2019-02-09;Your First Machine Learning Project in Python Step-By-Step;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42

# compare algorithms from pandas import read_csv from matplotlib import pyplot from sklearn . model_selection import train_test_split from sklearn . model_selection import cross_val_score from sklearn . model_selection import StratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . tree import DecisionTreeClassifier from sklearn . neighbors import KNeighborsClassifier from sklearn . discriminant_analysis import LinearDiscriminantAnalysis from sklearn . naive_bayes import GaussianNB from sklearn . svm import SVC # Load dataset url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"" names = [ 'sepal-length' , 'sepal-width' , 'petal-length' , 'petal-width' , 'class' ] dataset = read_csv ( url , names = names ) # Split-out validation dataset array = dataset . values X = array [ : , 0 : 4 ] y = array [ : , 4 ] X_train , X_validation , Y_train , Y_validation = train_test_split ( X , y , test_size = 0.20 , random_state = 1 , shuffle = True ) # Spot Check Algorithms models = [ ] models . append ( ( 'LR' , LogisticRegression ( solver = 'liblinear' , multi_class = 'ovr' ) ) ) models . append ( ( 'LDA' , LinearDiscriminantAnalysis ( ) ) ) models . append ( ( 'KNN' , KNeighborsClassifier ( ) ) ) models . append ( ( 'CART' , DecisionTreeClassifier ( ) ) ) models . append ( ( 'NB' , GaussianNB ( ) ) ) models . append ( ( 'SVM' , SVC ( gamma = 'auto' ) ) ) # evaluate each model in turn results = [ ] names = [ ] for name , model in models : kfold = StratifiedKFold ( n_splits = 10 , random_state = 1 ) cv_results = cross_val_score ( model , X_train , Y_train , cv = kfold , scoring = 'accuracy' ) results . append ( cv_results ) names . append ( name ) print ( '%s: %f (%f)' % ( name , cv_results . mean ( ) , cv_results . std ( ) ) ) # Compare Algorithms pyplot . boxplot ( results , labels = names ) pyplot . title ( 'Algorithm Comparison' ) pyplot . show ( )"
189;news.mit.edu;http://news.mit.edu/2020/mit-scientist-jill-crittenden-helps-build-covid-19-resource-addressing-face-mask-shortage-0403;;MIT scientist helps build Covid-19 resource to address shortage of face masks;"When the Covid-19 crisis hit the United States this March, MIT neuroscientist Jill Crittenden wanted to help. One of her greatest concerns was the shortage of face masks, which are a key weapon for health care providers, frontline service workers, and the public to protect against respiratory transmission of Covid-19. For those caring for Covid-19 patients, face masks that provide a near-100 percent seal are essential. These critical pieces of equipment, called N95 masks, are now scarce, and health-care workers are now faced with reusing potentially contaminated masks.

To address this, Crittenden joined a team of 60 scientists and engineers, students, and clinicians drawn from universities and the private sector to synthesize the scientific literature about mask decontamination and create a set of best practices for bad times. The group has now unveiled a website, N95decon.org, which provides a summary of this critical information.

“I first heard about the group from Larissa Little, a Harvard graduate student with John Doyle,” explains Crittenden, who is a research scientist in Ann Graybiel's lab at the McGovern Institute for Brain Research at MIT. “The three of us began communicating because we are all also members of the Boston-based MGB Covid-19 Innovation Center, and we agreed that helping to assess the flood of information on N95 decontamination would be an important contribution.”

The team members who came together over several weeks scoured hundreds of peer-reviewed publications and held continuous online meetings to review studies of decontamination methods that had been used to inactivate previous viral and bacterial pathogens, and to then assess the potential for these methods to neutralize the novel SARS-CoV-2 virus that causes Covid-19.

“This group is absolutely amazing,” says Crittenden. “The Zoom meetings are very productive because it is all data- and solutions-driven. Everyone throws out ideas, what they know and what the literature source is, with the only goal being to get to a data-based consensus efficiently.”

Reliable resource

The goal of the consortium was to provide overwhelmed health officials, who don’t have the time to study the literature for themselves, reliable, pre-digested scientific information about the pros and cons of three decontamination methods that offer the best options should local shortages force a choice between decontamination and reuse, or going unmasked.

The three methods involve (1) heat and humidity, (2) a specific wavelength of light called ultraviolet C (UVC), and (3) treatment with hydrogen peroxide vapors (HPV). The scientists did not endorse any one method, but instead sought to describe the circumstances under which each could inactivate the virus provided rigorous procedures were followed. Devices that rely on heat, for instance, could be used under specific temperature, humidity, and time parameters. With UVC devices — which emit a particular wavelength and energy level of light — considerations involve making sure masks are properly oriented to the light so the entire surface is bathed in sufficient energy. The HPV method has the potential advantage of decontaminating masks in volume, as the U.S. Food and Drug Administration, acting in this emergency, has certified certain vendors to offer hydrogen peroxide vapor treatments on a large scale. In addition to giving health officials the scientific information to assess the methods best suited to their circumstances, N95decon.org points decision-makers to sources of reliable and detailed how-to information provided by other organizations, institutions, and commercial services.

“While there is no perfect method for decontamination of N95 masks, it is crucial that decision-makers and users have as much information as possible about the strengths and weaknesses of various approaches,” says Manu Prakash, an associate professor of bioengineering at Stanford University, who helped coordinate this ad hoc, volunteer undertaking. “Manufacturers currently do not recommend N95 mask reuse. We aim to provide information and evidence in this critical time to help those on the front lines of this crisis make risk-management decisions given the specific conditions and limitations they face.”

The researchers stressed that decontamination does not solve the N95 shortage, and expressed the hope that new masks should be made available in large numbers as soon as possible so that health-care workers and first providers could be issued fresh protective gear whenever needed as specified by the non-emergency guidelines set by the U.S. Centers for Disease Control and Prevention.

Forward thinking

Meanwhile, these ad hoc volunteers have pledged to continue working together to update the N95decon.org website as new information becomes available, and to coordinate their efforts to do research to plug the gaps in current knowledge to avoid duplication of effort.

“We are, at heart, a group of people that want to help better equip hospitals and health-care personnel in this time of crisis,” says Brian Fleischer, a surgeon at the University of Chicago Medical Center and a member of the N95DECON consortium. “As a health care provider, many of my colleagues across the country have expressed concern with a lack of quality information in this ever-evolving landscape. I have learned a great deal from this team and I look forward to our continued collaboration to positively affect change.”

Crittenden is hopeful that the new website will help health-care workers make informed decisions about the safest methods available for decontamination and reuse of N95 masks. “I know physicians personally who are very grateful that teams of scientists are doing the in-depth data analysis so that they can feel confident in what is best for their own health,” she says.

Members of the team come from institutions including the University of California at Berkeley, the University of Chicago, Stanford University, Georgetown University, Harvard University, Seattle University, University of Utah, MIT, the University of Michigan, and from Consolidated Sterilizers and X, the Moonshot Factory."
190;machinelearningmastery.com;https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/;2017-06-29;Attention in Long Short-Term Memory Recurrent Neural Networks;"Tweet Share Share

Last Updated on August 14, 2019

The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains.

A limitation of the architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences.

In this post, you will discover the attention mechanism for recurrent neural networks that seeks to overcome this limitation.

After reading this post, you will know:

The limitation of the encode-decoder architecture and the fixed-length internal representation.

The attention mechanism to overcome the limitation that allows the network to learn where to pay attention in the input sequence for each item in the output sequence.

5 applications of the attention mechanism with recurrent neural networks in domains such as text translation, speech recognition, and more.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Problem With Long Sequences

The encoder-decoder recurrent neural network is an architecture where one set of LSTMs learn to encode input sequences into a fixed-length internal representation, and second set of LSTMs read the internal representation and decode it into an output sequence.

This architecture has shown state-of-the-art results on difficult sequence prediction problems like text translation and quickly became the dominant approach.

For example, see:

The encoder-decoder architecture still achieves excellent results on a wide range of problems. Nevertheless, it suffers from the constraint that all input sequences are forced to be encoded to a fixed length internal vector.

This is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems.

A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.

— Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Attention within Sequences

Attention is the idea of freeing the encoder-decoder architecture from the fixed-length internal representation.

This is achieved by keeping the intermediate outputs from the encoder LSTM from each step of the input sequence and training the model to learn to pay selective attention to these inputs and relate them to items in the output sequence.

Put another way, each item in the output sequence is conditional on selective items in the input sequence.

Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words. … it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector.

— Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015

This increases the computational burden of the model, but results in a more targeted and better-performing model.

In addition, the model is also able to show how attention is paid to the input sequence when predicting the output sequence. This can help in understanding and diagnosing exactly what the model is considering and to what degree for specific input-output pairs.

The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation and those in a source sentence. This is done by visualizing the annotation weights… Each row of a matrix in each plot indicates the weights associated with the annotations. From this we see which positions in the source sentence were considered more important when generating the target word.

— Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015

Problem with Large Images

Convolutional neural networks applied to computer vision problems also suffer from similar limitations, where it can be difficult to learn models on very large images.

As a result, a series of glimpses can be taken of a large image to formulate an approximate impression of the image before making a prediction.

One important property of human perception is that one does not tend to process a whole scene in its entirety at once. Instead humans focus attention selectively on parts of the visual space to acquire information when and where it is needed, and combine information from different fixations over time to build up an internal representation of the scene, guiding future eye movements and decision making.

— Recurrent Models of Visual Attention, 2014

These glimpse-based modifications may also be considered attention, but are not considered in this post.

See the papers.

5 Examples of Attention in Sequence Prediction

This section provides some specific examples of how attention is used for sequence prediction with recurrent neural networks.

1. Attention in Text Translation

The motivating example mentioned above is text translation.

Given an input sequence of a sentence in French, translate and output a sentence in English. Attention is used to pay attention to specific words in the input sequence for each word in the output sequence.

We extended the basic encoder–decoder by letting a model (soft-)search for a set of input words, or their annotations computed by an encoder, when generating each target word. This frees the model from having to encode a whole source sentence into a fixed-length vector, and also lets the model focus only on information relevant to the generation of the next target word.

— Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015

2. Attention in Image Descriptions

Different from the glimpse approach, the sequence-based attentional mechanism can be applied to computer vision problems to help get an idea of how to best use the convolutional neural network to pay attention to images when outputting a sequence, such as a caption.

Given an input of an image, output an English description of the image. Attention is used to pay focus on different parts of the image for each word in the output sequence.

We propose an attention based approach that gives state of the art performance on three benchmark datasets … We also show how the learned attention can be exploited to give more interpretability into the models generation process, and demonstrate that the learned alignments correspond very well to human intuition.

— Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2016

3. Attention in Entailment

Given a premise scenario and a hypothesis about the scenario in English, output whether the premise contradicts, is not related, or entails the hypothesis.

For example:

premise: “A wedding party taking pictures“

hypothesis: “Someone got married“

Attention is used to relate each word in the hypothesis to words in the premise, and vise-versa.

We present a neural model based on LSTMs that reads two sentences in one go to determine entailment, as opposed to mapping each sentence independently into a semantic space. We extend this model with a neural word-by-word attention mechanism to encourage reasoning over entailments of pairs of words and phrases. … An extension with word-by-word neural attention surpasses this strong benchmark LSTM result by 2.6 percentage points, setting a new state-of-the-art accuracy…

— Reasoning about Entailment with Neural Attention, 2016

4. Attention in Speech Recognition

Given an input sequence of English speech snippets, output a sequence of phonemes.

Attention is used to relate each phoneme in the output sequence to specific frames of audio in the input sequence.

… a novel end-to-end trainable speech recognition architecture based on a hybrid attention mechanism which combines both content and location information in order to select the next position in the input sequence for decoding. One desirable property of the proposed model is that it can recognize utterances much longer than the ones it was trained on.

— Attention-Based Models for Speech Recognition, 2015.

5. Attention in Text Summarization

Given an input sequence of an English article, output a sequence of English words that summarize the input.

Attention is used to relate each word in the output summary to specific words in the input document.

… a neural attention-based model for abstractive summarization, based on recent developments in neural machine translation. We combine this probabilistic model with a generation algorithm which produces accurate abstractive summaries.

— A Neural Attention Model for Abstractive Sentence Summarization, 2015

Further Reading

This section provides additional resources if you would like to learn more about adding attention to LSTMs.

Keras does not offer attention out of the box at the time of writing, but there are few third-party implementations. See:

Do you know of some good resources on attention in recurrent neural networks?

Let me know in the comments.

Summary

In this post, you discovered the attention mechanism for sequence prediction problems with LSTM recurrent neural networks.

Specifically, you learned:

That the encoder-decoder architecture for recurrent neural networks uses a fixed-length internal representation that imposes a constraint that limits learning very long sequences.

That attention overcomes the limitation in the encode-decoder architecture by allowing the network to learn where to pay attention to the input for each item in the output sequence.

That the approach has been used across different types sequence prediction problems include text translation, speech recognition, and more.

Do you have any questions about attention in recurrent neural networks?

Ask your questions in the comments below and I will do my best to answer.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside"
191;web.mit.edu;http://web.mit.edu/notice-proposed-settlement-class-action-lawsuit;;MIT - Massachusetts Institute of Technology;"NOTICE OF PROPOSED SETTLEMENT OF CLASS ACTION LAWSUIT

ATTENTION: ALL PEOPLE WHO ARE DEAF OR HARD OF HEARING WHO WANT CAPTIONING OF MIT’S ONLINE CONTENT

If you are deaf or hard of hearing and have tried to access or would like to access online video content of the Massachusetts Institute of Technology (“MIT”) with captions or to access MIT’s online audio content with a transcript, you may be a member of the proposed Settlement Class affected by this lawsuit. The Settlement Class in this case does not include students of MIT. This is a court-authorized notice.

PLEASE READ THIS NOTICE CAREFULLY. YOUR RIGHTS MAY BE AFFECTED BY LEGAL PROCEEDINGS IN THIS CASE.

NOTICE OF CLASS ACTION

The purpose of this notice is to inform you of a proposed settlement in a pending class action lawsuit brought by the National Association of the Deaf (“NAD”) and three Deaf plaintiffs on behalf of deaf and hard of hearing individuals against MIT. The case is titled National Association of the Deaf v. Massachusetts Institute of Technology, No. 3:15-cv-30024-KAR, and is pending in the United States District Court for the District of Massachusetts. The proposed class action settlement (“Settlement”) is set forth in a proposed Consent Decree, which must be approved by the United States District Court.

BACKGROUND

This lawsuit alleges that MIT violated the Americans with Disabilities Act and the Rehabilitation Act by failing to provide captioning for its publicly available online content. Plaintiffs and other deaf and hard of hearing individuals alleged that they attempted to access MIT’s publicly available online content but were unable to do so because it did not have captions or had inaccurate captions.

This is a class action. In a class action, one or more people or organizations, called Class Representatives (in this case the National Association of the Deaf, C. Wayne Dore, Christy Smith, and Lee Nettles (“Plaintiffs”)), sue on behalf of people who have similar legal claims. All of these people are a Class or Class Members. One court resolves the issues for all Class Members. United States Magistrate Judge Katherine A. Robertson is in charge of this class action.

The Court did not decide in favor of either Plaintiffs or MIT in this case. Instead, both sides agreed to a settlement. That way, they avoid the cost, delay, and uncertainty of a trial. The settlement provides benefits that go to the Class Members. The Class Representatives and Class Counsel (the attorneys appointed by the Court to represent the Class) think the proposed settlement is in the best interests of the Class Members, taking into account the benefits of the settlement, the risks of continued litigation, and the delay in obtaining relief for the Class if the litigation continues.

THE SETTLEMENT CLASS

The Settlement Class includes all persons (other than students of MIT) who, at any time between February 11, 2012 and the date of preliminary approval of this settlement, have claimed or could have claimed to assert a right under Title III of the ADA, Section 504 of the Rehabilitation Act, and/or other federal, state or local statutes or regulations that set forth standards or obligations coterminous with or equivalent to Title III of the Americans with Disabilities Act or any of the rules or regulations promulgated thereunder, alleging that they are deaf or hard of hearing and that MIT has failed to make accessible to persons who are deaf or hard of hearing online content posted and available for the general public that is produced, created, hosted, linked to, or embedded by MIT.

SUMMARY OF THE PROPOSED SETTLEMENT

The following is a summary of certain provisions of the Settlement. The complete Settlement, set forth in the proposed Consent Decree, is available as set forth below.

The Settlement requires MIT to caption content on Covered MIT Webpages as follows:

Content posted by faculty or employees acting within the scope of their employment, or any MIT Sponsored Student Group (as defined by the Association of Student Activities), on or after the date 60 days after the Effective Date will include captioning when posted.

Content posted by faculty or employees acting within the scope of their employment, or any MIT Sponsored Student Group (as defined by the Association of Student Activities), prior to January 1, 2019 will be captioned upon request within seven business days.

Content posted by faculty or employees acting within in the scope of their employment, or any MIT Sponsored Student Group (as defined by the Association of Student Activities), after January 1, 2019 but before the date 60 days after the Effective Date will be captioned as soon as practicable but no later than one year from the Effective Date, or upon request within seven days.

“Covered MIT Webpages” means public webpages within the MIT.edu domain and corresponding public platforms such as YouTube, Vimeo, and Soundcloud channels operated by MIT, with the exception of certain specific student, alumni, and organization pages.

MIT will also provide live captioning for certain university-wide events for which live streaming is made publicly available.

The settlement also requires MIT to report to NAD on its compliance with these terms and establishes a process by which members of the public can request that content be captioned.

RELEASE OF CLAIMS

The Settlement resolves and releases all claims for injunctive, declaratory, or other non-monetary relief and attorneys’ fees and costs that were brought or could have been brought against MIT relating to the lack of captioning or accurate captioning of free online audio or video content for the general public that is produced, created, hosted, linked to, or embedded by MIT.

REASONABLE ATTORNEYS’ FEES, COSTS AND EXPENSES

The settlement class is represented by the Civil Rights Education and Enforcement Center, the Disability Law Center, the Disability Rights Education and Defense Fund, the National Association of the Deaf, and the law firm of Cohen Milstein Sellers & Toll PLLC (“Class Counsel”). MIT has agreed not to oppose Class Counsel’s request for an award of their reasonable attorneys’ fees, expenses, and costs in the amount of $1,050,000. This amount is subject to the approval by the Court.

FAIRNESS OF SETTLEMENT

The Class Representatives and Class Counsel have concluded that the terms and conditions of the proposed Settlement are fair, reasonable, adequate, and in the best interests of the Settlement Class. In reaching this conclusion, the Class Representatives and Class Counsel have considered the benefits of the settlement, the possible outcomes of continued litigation of these issues, the expense and length of continued litigation, and actual and possible appeals.

THE COURT’S FINAL APPROVAL/FAIRNESS HEARING

The Court has preliminarily approved the Settlement, and has scheduled a hearing for July 14, 2020 at 11:00 am in the Hampshire Courtroom, 300 State Street, Springfield, Massachusetts 01105 to decide whether the proposed Settlement is fair, reasonable, and adequate, and should be finally approved. Although you are not required to attend, as a Settlement Class Member, you have the right to attend and be heard at this hearing, as specified in the next section below. At the hearing, the Court will consider any objections to the Settlement. Judge Robertson will listen to people who have asked to speak at the hearing. After the hearing, the Court will decide whether to approve the Settlement. The Court will also consider the agreed upon amount to award Class Counsel as reasonable attorneys’ fees, costs and litigation expenses. We do not know how long this decision will take.

If the Court approves the Settlement, all Class members will be bound by the provisions of the Settlement with respect to claims against MIT for injunctive relief and attorneys’ fees and relating to captioning of online content.

OBJECTIONS TO THE SETTLEMENT

If you wish to object to the Settlement or to speak at the hearing, you must send any objection and/or notice of your intent to appear at the hearing to the Court in writing on or before May 19, 2020, and include the case number (Civil Action No. 3:15-cv-30024), to the following address: Clerk of the Court, US Courthouse, 300 State Street, Springfield, Massachusetts 01105.

You may also object by filling out this form: https://public.mad.uscourts.gov/FairnessHearing.html

Please note that the Court can only approve or deny the Settlement. The Court cannot change the Settlement’s terms.

All objections must be submitted or postmarked on or before May 19, 2020

Any Class Member who does not object at or before the Final Approval Hearing will be deemed to have approved the Settlement and to have waived such objections and shall not be able to make any objections (by appeal or otherwise) to the Settlement.

IF YOU DO NOT OPPOSE THIS SETTLEMENT, YOU NEED NOT APPEAR OR FILE ANYTHING IN WRITING.

FURTHER INFORMATION

The terms of the Settlement are only summarized in this notice. For the precise and full terms and conditions of the Settlement, please see the proposed Consent Decree available at www.MITcaptioningsettlement.com/consentdecree, by accessing the Court docket on this case through the Court’s Public Access to Electronic Records (PACER) system at https://www.pacer.gov, or by visiting the office of the Clerk of the Court for the United States District Court for the District of Massachusetts, 300 State Street, Springfield, Massachusetts 01105, between 8:30 a.m. and 4:30 p.m., Monday through Friday, excluding Court holidays.

You can also obtain more detailed information about the Settlement or a copy of the Settlement Agreement by calling 240-468-7109 (videophone) or 800-308-1878 (voice), by emailing MITsettlement@creeclaw.org, or by contacting Class Counsel by mail at any of the following addresses:

Thomas P. Murphy

Disability Law Center, Inc.

32 Industrial Drive East

Northampton, MA 01060 Amy F. Robertson

Civil Rights Education and Enforcement Center

1245 E. Colfax Ave., Suite 400

Denver, CO 80218 Joseph M. Sellers Shaylyn Cochran

Cohen Milstein Sellers & Toll PLLC

1100 New York Ave NW, Fifth Floor

Washington DC 20005 Arlene Mayerson Carly Myers

Disability Rights Education And Defense Fund, Inc.

3075 Adeline Street Suite 210

Berkeley, CA 94703 Howard Rosenblum

The National Association of The Deaf Law and Advocacy Center

8630 Fenton Street, Suite 820

Silver Spring, MD 20910

Please do not direct questions to the District Court. To obtain copies of this Notice or the Consent Decree in alternative accessible formats, please contact Class Counsel listed above."
192;machinelearningmastery.com;http://machinelearningmastery.com/prepare-data-machine-learning-python-scikit-learn/;2016-05-17;How To Prepare Your Data For Machine Learning in Python with Scikit-Learn;"# Rescale data (between 0 and 1)

import pandas

import scipy

import numpy

from sklearn . preprocessing import MinMaxScaler

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

# separate array into input and output components

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

rescaledX = scaler . fit_transform ( X )

# summarize transformed data

numpy . set_printoptions ( precision = 3 )"
193;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-1-dimensional-function-from-scratch-in-keras/;2019-06-25;How to Develop a 1D Generative Adversarial Network From Scratch in Keras;"# train a generative adversarial network on a one-dimensional function

from numpy import hstack

from numpy import zeros

from numpy import ones

from numpy . random import rand

from numpy . random import randn

from keras . models import Sequential

from keras . layers import Dense

from matplotlib import pyplot

# define the standalone discriminator model

def define_discriminator ( n_inputs = 2 ) :

model = Sequential ( )

model . add ( Dense ( 25 , activation = 'relu' , kernel_initializer = 'he_uniform' , input_dim = n_inputs ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

return model

# define the standalone generator model

def define_generator ( latent_dim , n_outputs = 2 ) :

model = Sequential ( )

model . add ( Dense ( 15 , activation = 'relu' , kernel_initializer = 'he_uniform' , input_dim = latent_dim ) )

model . add ( Dense ( n_outputs , activation = 'linear' ) )

return model

# define the combined generator and discriminator model, for updating the generator

def define_gan ( generator , discriminator ) :

# make weights in the discriminator not trainable

discriminator . trainable = False

# connect them

model = Sequential ( )

# add generator

model . add ( generator )

# add the discriminator

model . add ( discriminator )

# compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' )

return model

# generate n real samples with class labels

def generate_real_samples ( n ) :

# generate inputs in [-0.5, 0.5]

X1 = rand ( n ) - 0.5

# generate outputs X^2

X2 = X1 * X1

# stack arrays

X1 = X1 . reshape ( n , 1 )

X2 = X2 . reshape ( n , 1 )

X = hstack ( ( X1 , X2 ) )

# generate class labels

y = ones ( ( n , 1 ) )

return X , y

# generate points in latent space as input for the generator

def generate_latent_points ( latent_dim , n ) :

# generate points in the latent space

x_input = randn ( latent_dim * n )

# reshape into a batch of inputs for the network

x_input = x_input . reshape ( n , latent_dim )

return x_input

# use the generator to generate n fake examples, with class labels

def generate_fake_samples ( generator , latent_dim , n ) :

# generate points in latent space

x_input = generate_latent_points ( latent_dim , n )

# predict outputs

X = generator . predict ( x_input )

# create class labels

y = zeros ( ( n , 1 ) )

return X , y

# evaluate the discriminator and plot real and fake points

def summarize_performance ( epoch , generator , discriminator , latent_dim , n = 100 ) :

# prepare real samples

x_real , y_real = generate_real_samples ( n )

# evaluate discriminator on real examples

_ , acc_real = discriminator . evaluate ( x_real , y_real , verbose = 0 )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( generator , latent_dim , n )

# evaluate discriminator on fake examples

_ , acc_fake = discriminator . evaluate ( x_fake , y_fake , verbose = 0 )

# summarize discriminator performance

print ( epoch , acc_real , acc_fake )

# scatter plot real and fake data points

pyplot . scatter ( x_real [ : , 0 ] , x_real [ : , 1 ] , color = 'red' )

pyplot . scatter ( x_fake [ : , 0 ] , x_fake [ : , 1 ] , color = 'blue' )

pyplot . show ( )

# train the generator and discriminator

def train ( g_model , d_model , gan_model , latent_dim , n_epochs = 10000 , n_batch = 128 , n_eval = 2000 ) :

# determine half the size of one batch, for updating the discriminator

half_batch = int ( n_batch / 2 )

# manually enumerate epochs

for i in range ( n_epochs ) :

# prepare real samples

x_real , y_real = generate_real_samples ( half_batch )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( g_model , latent_dim , half_batch )

# update discriminator

d_model . train_on_batch ( x_real , y_real )

d_model . train_on_batch ( x_fake , y_fake )

# prepare points in latent space as input for the generator

x_gan = generate_latent_points ( latent_dim , n_batch )

# create inverted labels for the fake samples

y_gan = ones ( ( n_batch , 1 ) )

# update the generator via the discriminator's error

gan_model . train_on_batch ( x_gan , y_gan )

# evaluate the model every n_eval epochs

if ( i + 1 ) % n_eval == 0 :

summarize_performance ( i , g_model , d_model , latent_dim )

# size of the latent space

latent_dim = 5

# create the discriminator

discriminator = define_discriminator ( )

# create the generator

generator = define_generator ( latent_dim )

# create the gan

gan_model = define_gan ( generator , discriminator )

# train model"
194;machinelearningmastery.com;https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/;2019-06-26;How to Develop a Deep Learning Photo Caption Generator from Scratch;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179

from numpy import array from pickle import load from keras . preprocessing . text import Tokenizer from keras . preprocessing . sequence import pad_sequences from keras . utils import to_categorical from keras . utils import plot_model from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers import LSTM from keras . layers import Embedding from keras . layers import Dropout from keras . layers . merge import add from keras . callbacks import ModelCheckpoint # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text # load a pre-defined list of photo identifiers def load_set ( filename ) : doc = load_doc ( filename ) dataset = list ( ) # process line by line for line in doc . split ( '

' ) : # skip empty lines if len ( line ) < 1 : continue # get the image identifier identifier = line . split ( '.' ) [ 0 ] dataset . append ( identifier ) return set ( dataset ) # load clean descriptions into memory def load_clean_descriptions ( filename , dataset ) : # load document doc = load_doc ( filename ) descriptions = dict ( ) for line in doc . split ( '

' ) : # split line by white space tokens = line . split ( ) # split id from description image_id , image_desc = tokens [ 0 ] , tokens [ 1 : ] # skip images not in the set if image_id in dataset : # create list if image_id not in descriptions : descriptions [ image_id ] = list ( ) # wrap description in tokens desc = 'startseq ' + ' ' . join ( image_desc ) + ' endseq' # store descriptions [ image_id ] . append ( desc ) return descriptions # load photo features def load_photo_features ( filename , dataset ) : # load all features all_features = load ( open ( filename , 'rb' ) ) # filter features features = { k : all_features [ k ] for k in dataset } return features # covert a dictionary of clean descriptions to a list of descriptions def to_lines ( descriptions ) : all_desc = list ( ) for key in descriptions . keys ( ) : [ all_desc . append ( d ) for d in descriptions [ key ] ] return all_desc # fit a tokenizer given caption descriptions def create_tokenizer ( descriptions ) : lines = to_lines ( descriptions ) tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( lines ) return tokenizer # calculate the length of the description with the most words def max_length ( descriptions ) : lines = to_lines ( descriptions ) return max ( len ( d . split ( ) ) for d in lines ) # create sequences of images, input sequences and output words for an image def create_sequences ( tokenizer , max_length , descriptions , photos , vocab_size ) : X1 , X2 , y = list ( ) , list ( ) , list ( ) # walk through each image identifier for key , desc_list in descriptions . items ( ) : # walk through each description for the image for desc in desc_list : # encode the sequence seq = tokenizer . texts_to_sequences ( [ desc ] ) [ 0 ] # split one sequence into multiple X,y pairs for i in range ( 1 , len ( seq ) ) : # split into input and output pair in_seq , out_seq = seq [ : i ] , seq [ i ] # pad input sequence in_seq = pad_sequences ( [ in_seq ] , maxlen = max_length ) [ 0 ] # encode output sequence out_seq = to_categorical ( [ out_seq ] , num_classes = vocab_size ) [ 0 ] # store X1 . append ( photos [ key ] [ 0 ] ) X2 . append ( in_seq ) y . append ( out_seq ) return array ( X1 ) , array ( X2 ) , array ( y ) # define the captioning model def define_model ( vocab_size , max_length ) : # feature extractor model inputs1 = Input ( shape = ( 4096 , ) ) fe1 = Dropout ( 0.5 ) ( inputs1 ) fe2 = Dense ( 256 , activation = 'relu' ) ( fe1 ) # sequence model inputs2 = Input ( shape = ( max_length , ) ) se1 = Embedding ( vocab_size , 256 , mask_zero = True ) ( inputs2 ) se2 = Dropout ( 0.5 ) ( se1 ) se3 = LSTM ( 256 ) ( se2 ) # decoder model decoder1 = add ( [ fe2 , se3 ] ) decoder2 = Dense ( 256 , activation = 'relu' ) ( decoder1 ) outputs = Dense ( vocab_size , activation = 'softmax' ) ( decoder2 ) # tie it together [image, seq] [word] model = Model ( inputs = [ inputs1 , inputs2 ] , outputs = outputs ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' ) # summarize model print ( model . summary ( ) ) plot_model ( model , to_file = 'model.png' , show_shapes = True ) return model # train dataset # load training dataset (6K) filename = 'Flickr8k_text/Flickr_8k.trainImages.txt' train = load_set ( filename ) print ( 'Dataset: %d' % len ( train ) ) # descriptions train_descriptions = load_clean_descriptions ( 'descriptions.txt' , train ) print ( 'Descriptions: train=%d' % len ( train_descriptions ) ) # photo features train_features = load_photo_features ( 'features.pkl' , train ) print ( 'Photos: train=%d' % len ( train_features ) ) # prepare tokenizer tokenizer = create_tokenizer ( train_descriptions ) vocab_size = len ( tokenizer . word_index ) + 1 print ( 'Vocabulary Size: %d' % vocab_size ) # determine the maximum sequence length max_length = max_length ( train_descriptions ) print ( 'Description Length: %d' % max_length ) # prepare sequences X1train , X2train , ytrain = create_sequences ( tokenizer , max_length , train_descriptions , train_features , vocab_size ) # dev dataset # load test set filename = 'Flickr8k_text/Flickr_8k.devImages.txt' test = load_set ( filename ) print ( 'Dataset: %d' % len ( test ) ) # descriptions test_descriptions = load_clean_descriptions ( 'descriptions.txt' , test ) print ( 'Descriptions: test=%d' % len ( test_descriptions ) ) # photo features test_features = load_photo_features ( 'features.pkl' , test ) print ( 'Photos: test=%d' % len ( test_features ) ) # prepare sequences X1test , X2test , ytest = create_sequences ( tokenizer , max_length , test_descriptions , test_features , vocab_size ) # fit model # define the model model = define_model ( vocab_size , max_length ) # define checkpoint callback filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5' checkpoint = ModelCheckpoint ( filepath , monitor = 'val_loss' , verbose = 1 , save_best_only = True , mode = 'min' ) # fit model model . fit ( [ X1train , X2train ] , ytrain , epochs = 20 , verbose = 2 , callbacks = [ checkpoint ] , validation_data = ( [ X1test , X2test ] , ytest ) )"
195;news.mit.edu;http://news.mit.edu/2020/reducing-delays-wireless-networks-csail-0410;;Reducing delays in wireless networks;"MIT researchers have designed a congestion-control scheme for wireless networks that could help reduce lag times and increase quality in video streaming, video chat, mobile gaming, and other web services.

To keep web services running smoothly, congestion-control schemes infer information about a network’s bandwidth capacity and congestion based on feedback from the network routers, which is encoded in data packets. That information determines how fast data packets are sent through the network.

Deciding a good sending rate can be a tough balancing act. Senders don’t want to be overly conservative: If a network’s capacity constantly varies from, say, 2 megabytes per second to 500 kilobytes per second, the sender could always send traffic at the lowest rate. But then your Netflix video, for example, will be unnecessarily low-quality. On the other hand, if the sender constantly maintains a high rate, even when network capacity dips, it could overwhelm the network, creating a massive queue of data packets waiting to be delivered. Queued packets can increase the network’s delay, causing, say, your Skype call to freeze.

Things get even more complicated in wireless networks, which have “time-varying links,” with rapid, unpredictable capacity shifts. Depending on various factors, such as the number of network users, cell tower locations, and even surrounding buildings, capacities can double or drop to zero within fractions of a second. In a paper at the USENIX Symposium on Networked Systems Design and Implementation, the researchers presented “Accel-Brake Control” (ABC), a simple scheme that achieves about 50 percent higher throughput, and about half the network delays, on time-varying links.

The scheme relies on a novel algorithm that enables the routers to explicitly communicate how many data packets should flow through a network to avoid congestion but fully utilize the network. It provides that detailed information from bottlenecks — such as packets queued between cell towers and senders — by repurposing a single bit already available in internet packets. The researchers are already in talks with mobile network operators to test the scheme.

“In cellular networks, your fraction of data capacity changes rapidly, causing lags in your service. Traditional schemes are too slow to adapt to those shifts,” says first author Prateesh Goyal, a graduate student in CSAIL. “ABC provides detailed feedback about those shifts, whether it’s gone up or down, using a single data bit.”

Joining Goyal on the paper are Anup Agarwal, now a graduate student at Carnegie Melon University; Ravi Netravali, now an assistant professor of computer science at the University of California at Los Angeles; Mohammad Alizadeh, an associate professor in MIT’s Department of Electrical Engineering (EECS) and CSAIL; and Hari Balakrishnan, the Fujitsu Professor in EECS. The authors have all been members of the Networks and Mobile Systems group at CSAIL.

Achieving explicit control

Traditional congestion-control schemes rely on either packet losses or information from a single “congestion” bit in internet packets to infer congestion and slow down. A router, such as a base station, will mark the bit to alert a sender — say, a video server — that its sent data packets are in a long queue, signaling congestion. In response, the sender will then reduce its rate by sending fewer packets. The sender also reduces its rate if it detects a pattern of packets being dropped before reaching the receiver.

In attempts to provide greater information about bottlenecked links on a network path, researchers have proposed “explicit” schemes that include multiple bits in packets that specify current rates. But this approach would mean completely changing the way the internet sends data, and it has proved impossible to deploy.

“It’s a tall task,” Alizadeh says. “You’d have to make invasive changes to the standard Internet Protocol (IP) for sending data packets. You’d have to convince all Internet parties, mobile network operators, ISPs, and cell towers to change the way they send and receive data packets. That’s not going to happen.”

With ABC, the researchers still use the available single bit in each data packet, but they do so in such a way that the bits, aggregated across multiple data packets, can provide the needed real-time rate information to senders. The scheme tracks each data packet in a round-trip loop, from sender to base station to receiver. The base station marks the bit in each packet with “accelerate” or “brake,” based on the current network bandwidth. When the packet is received, the marked bit tells the sender to increase or decrease the “in-flight” packets — packets sent but not received — that can be in the network.

If it receives an accelerate command, it means the packet made good time and the network has spare capacity. The sender then sends two packets: one to replace the packet that was received and another to utilize the spare capacity. When told to brake, the sender decreases its in-flight packets by one — meaning it doesn’t replace the packet that was received.

Used across all packets in the network, that one bit of information becomes a powerful feedback tool that tells senders their sending rates with high precision. Within a couple hundred milliseconds, it can vary a sender’s rate between zero and double. “You’d think one bit wouldn’t carry enough information,” Alizadeh says. “But, by aggregating single-bit feedback across a stream of packets, we can get the same effect as that of a multibit signal.”

Staying one step ahead

At the core of ABC is an algorithm that predicts the aggregate rate of the senders one round-trip ahead to better compute the accelerate/brake feedback.

The idea is that an ABC-equipped base station knows how senders will behave — maintaining, increasing, or decreasing their in-flight packets — based on how it marked the packet it sent to a receiver. The moment the base station sends a packet, it knows how many packets it will receive from the sender in exactly one round-trip’s time in the future. It uses that information to mark the packets to more accurately match the sender’s rate to the current network capacity.

In simulations of cellular networks, compared to traditional congestion control schemes, ABC achieves around 30 to 40 percent greater throughput for roughly the same delays. Alternatively, it can reduce delays by around 200 to 400 percent by maintaining the same throughput as traditional schemes. Compared to existing explicit schemes that were not designed for time-varying links, ABC reduces delays by half for the same throughput. “Basically, existing schemes get low throughput and low delays, or high throughput and high delays, whereas ABC achieves high throughput with low delays,” Goyal says.

Next, the researchers are trying to see if apps and web services can use ABC to better control the quality of content. For example, “a video content provider could use ABC’s information about congestion and data rates to pick the resolution of streaming video more intelligently,” Alizadeh says. “If it doesn’t have enough capacity, the video server could lower the resolution temporarily, so the video will continue playing at the highest possible quality without freezing.”"
196;machinelearningmastery.com;http://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/;2013-12-24;How to Prepare Data For Machine Learning;"Tweet Share Share

Last Updated on June 7, 2016

Machine learning algorithms learn from data. It is critical that you feed them the right data for the problem you want to solve. Even if you have good data, you need to make sure that it is in a useful scale, format and even that meaningful features are included.

In this post you will learn how to prepare data for a machine learning algorithm. This is a big topic and you will cover the essentials.

Data Preparation Process

The more disciplined you are in your handling of data, the more consistent and better results you are like likely to achieve. The process for getting data ready for a machine learning algorithm can be summarized in three steps:

Step 1 : Select Data

: Select Data Step 2 : Preprocess Data

: Preprocess Data Step 3: Transform Data

You can follow this process in a linear manner, but it is very likely to be iterative with many loops.

Step 1: Select Data

This step is concerned with selecting the subset of all available data that you will be working with. There is always a strong desire for including all data that is available, that the maxim “more is better” will hold. This may or may not be true.

You need to consider what data you actually need to address the question or problem you are working on. Make some assumptions about the data you require and be careful to record those assumptions so that you can test them later if needed.

Below are some questions to help you think through this process:

What is the extent of the data you have available? For example through time, database tables, connected systems. Ensure you have a clear picture of everything that you can use.

What data is not available that you wish you had available? For example data that is not recorded or cannot be recorded. You may be able to derive or simulate this data.

What data don’t you need to address the problem? Excluding data is almost always easier than including data. Note down which data you excluded and why.

It is only in small problems, like competition or toy datasets where the data has already been selected for you.

Step 2: Preprocess Data

After you have selected the data, you need to consider how you are going to use the data. This preprocessing step is about getting the selected data into a form that you can work.

Three common data preprocessing steps are formatting, cleaning and sampling:

Formatting : The data you have selected may not be in a format that is suitable for you to work with. The data may be in a relational database and you would like it in a flat file, or the data may be in a proprietary file format and you would like it in a relational database or a text file.

: The data you have selected may not be in a format that is suitable for you to work with. The data may be in a relational database and you would like it in a flat file, or the data may be in a proprietary file format and you would like it in a relational database or a text file. Cleaning : Cleaning data is the removal or fixing of missing data. There may be data instances that are incomplete and do not carry the data you believe you need to address the problem. These instances may need to be removed. Additionally, there may be sensitive information in some of the attributes and these attributes may need to be anonymized or removed from the data entirely.

: Cleaning data is the removal or fixing of missing data. There may be data instances that are incomplete and do not carry the data you believe you need to address the problem. These instances may need to be removed. Additionally, there may be sensitive information in some of the attributes and these attributes may need to be anonymized or removed from the data entirely. Sampling: There may be far more selected data available than you need to work with. More data can result in much longer running times for algorithms and larger computational and memory requirements. You can take a smaller representative sample of the selected data that may be much faster for exploring and prototyping solutions before considering the whole dataset.

It is very likely that the machine learning tools you use on the data will influence the preprocessing you will be required to perform. You will likely revisit this step.

Step 3: Transform Data

The final step is to transform the process data. The specific algorithm you are working with and the knowledge of the problem domain will influence this step and you will very likely have to revisit different transformations of your preprocessed data as you work on your problem.

Three common data transformations are scaling, attribute decompositions and attribute aggregations. This step is also referred to as feature engineering.

Scaling : The preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. Many machine learning methods like data attributes to have the same scale such as between 0 and 1 for the smallest and largest value for a given feature. Consider any feature scaling you may need to perform.

: The preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. Many machine learning methods like data attributes to have the same scale such as between 0 and 1 for the smallest and largest value for a given feature. Consider any feature scaling you may need to perform. Decomposition : There may be features that represent a complex concept that may be more useful to a machine learning method when split into the constituent parts. An example is a date that may have day and time components that in turn could be split out further. Perhaps only the hour of day is relevant to the problem being solved. consider what feature decompositions you can perform.

: There may be features that represent a complex concept that may be more useful to a machine learning method when split into the constituent parts. An example is a date that may have day and time components that in turn could be split out further. Perhaps only the hour of day is relevant to the problem being solved. consider what feature decompositions you can perform. Aggregation: There may be features that can be aggregated into a single feature that would be more meaningful to the problem you are trying to solve. For example, there may be a data instances for each time a customer logged into a system that could be aggregated into a count for the number of logins allowing the additional instances to be discarded. Consider what type of feature aggregations could perform.

You can spend a lot of time engineering features from your data and it can be very beneficial to the performance of an algorithm. Start small and build on the skills you learn.

Summary

In this post you learned the essence of data preparation for machine learning. You discovered a three step framework for data preparation and tactics in each step:

Step 1: Data Selection Consider what data is available, what data is missing and what data can be removed.

Consider what data is available, what data is missing and what data can be removed. Step 2: Data Preprocessing Organize your selected data by formatting, cleaning and sampling from it.

Organize your selected data by formatting, cleaning and sampling from it. Step 3: Data Transformation Transform preprocessed data ready for machine learning by engineering features using scaling, attribute decomposition and attribute aggregation.

Data preparation is a large subject that can involve a lot of iterations, exploration and analysis. Getting good at data preparation will make you a master at machine learning. For now, just consider the questions raised in this post when preparing data and always be looking for clearer ways of representing the problem you are trying to solve.

Resources

If you are looking to dive deeper into this subject, you can learn more in the resources below.

Do you have some data preparation process tips and tricks. Please leave a comment and share your experiences."
197;machinelearningmastery.com;https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/;2019-06-16;A Gentle Introduction to Generative Adversarial Networks (GANs);"Tweet Share Share

Last Updated on July 19, 2019

Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.

Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.

GANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated). The two models are trained together in a zero-sum game, adversarial, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples.

GANs are an exciting and rapidly changing field, delivering on the promise of generative models in their ability to generate realistic examples across a range of problem domains, most notably in image-to-image translation tasks such as translating photos of summer to winter or day to night, and in generating photorealistic photos of objects, scenes, and people that even humans cannot tell are fake.

In this post, you will discover a gentle introduction to Generative Adversarial Networks, or GANs.

After reading this post, you will know:

Context for GANs, including supervised vs. unsupervised learning and discriminative vs. generative modeling.

GANs are an architecture for automatically training a generative model by treating the unsupervised problem as supervised and using both a generative and a discriminative model.

GANs provide a path to sophisticated domain-specific data augmentation and a solution to problems that require a generative solution, such as image-to-image translation.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

What Are Generative Models? What Are Generative Adversarial Networks? Why Generative Adversarial Networks?

What Are Generative Models?

In this section, we will review the idea of generative models, stepping over the supervised vs. unsupervised learning paradigms and discriminative vs. generative modeling.

Supervised vs. Unsupervised Learning

A typical machine learning problem involves using a model to make a prediction, e.g. predictive modeling.

This requires a training dataset that is used to train a model, comprised of multiple examples, called samples, each with input variables (X) and output class labels (y). A model is trained by showing examples of inputs, having it predict outputs, and correcting the model to make the outputs more like the expected outputs.

In the predictive or supervised learning approach, the goal is to learn a mapping from inputs x to outputs y, given a labeled set of input-output pairs …

— Page 2, Machine Learning: A Probabilistic Perspective, 2012.

This correction of the model is generally referred to as a supervised form of learning, or supervised learning.

Examples of supervised learning problems include classification and regression, and examples of supervised learning algorithms include logistic regression and random forest.

There is another paradigm of learning where the model is only given the input variables (X) and the problem does not have any output variables (y).

A model is constructed by extracting or summarizing the patterns in the input data. There is no correction of the model, as the model is not predicting anything.

The second main type of machine learning is the descriptive or unsupervised learning approach. Here we are only given inputs, and the goal is to find “interesting patterns” in the data. […] This is a much less well-defined problem, since we are not told what kinds of patterns to look for, and there is no obvious error metric to use (unlike supervised learning, where we can compare our prediction of y for a given x to the observed value).

— Page 2, Machine Learning: A Probabilistic Perspective, 2012.

This lack of correction is generally referred to as an unsupervised form of learning, or unsupervised learning.

Examples of unsupervised learning problems include clustering and generative modeling, and examples of unsupervised learning algorithms are K-means and Generative Adversarial Networks.

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Discriminative vs. Generative Modeling

In supervised learning, we may be interested in developing a model to predict a class label given an example of input variables.

This predictive modeling task is called classification.

Classification is also traditionally referred to as discriminative modeling.

… we use the training data to find a discriminant function f(x) that maps each x directly onto a class label, thereby combining the inference and decision stages into a single learning problem.

— Page 44, Pattern Recognition and Machine Learning, 2006.

This is because a model must discriminate examples of input variables across classes; it must choose or make a decision as to what class a given example belongs.

Alternately, unsupervised models that summarize the distribution of input variables may be able to be used to create or generate new examples in the input distribution.

As such, these types of models are referred to as generative models.

For example, a single variable may have a known data distribution, such as a Gaussian distribution, or bell shape. A generative model may be able to sufficiently summarize this data distribution, and then be used to generate new variables that plausibly fit into the distribution of the input variable.

Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.

— Page 43, Pattern Recognition and Machine Learning, 2006.

In fact, a really good generative model may be able to generate new examples that are not just plausible, but indistinguishable from real examples from the problem domain.

Examples of Generative Models

Naive Bayes is an example of a generative model that is more often used as a discriminative model.

For example, Naive Bayes works by summarizing the probability distribution of each input variable and the output class. When a prediction is made, the probability for each possible outcome is calculated for each variable, the independent probabilities are combined, and the most likely outcome is predicted. Used in reverse, the probability distributions for each variable can be sampled to generate new plausible (independent) feature values.

Other examples of generative models include Latent Dirichlet Allocation, or LDA, and the Gaussian Mixture Model, or GMM.

Deep learning methods can be used as generative models. Two popular examples include the Restricted Boltzmann Machine, or RBM, and the Deep Belief Network, or DBN.

Two modern examples of deep learning generative modeling algorithms include the Variational Autoencoder, or VAE, and the Generative Adversarial Network, or GAN.

What Are Generative Adversarial Networks?

Generative Adversarial Networks, or GANs, are a deep-learning-based generative model.

More generally, GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture.

The GAN architecture was first described in the 2014 paper by Ian Goodfellow, et al. titled “Generative Adversarial Networks.”

A standardized approach called Deep Convolutional Generative Adversarial Networks, or DCGAN, that led to more stable models was later formalized by Alec Radford, et al. in the 2015 paper titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks“.

Most GANs today are at least loosely based on the DCGAN architecture …

— NIPS 2016 Tutorial: Generative Adversarial Networks, 2016.

The GAN model architecture involves two sub-models: a generator model for generating new examples and a discriminator model for classifying whether generated examples are real, from the domain, or fake, generated by the generator model.

Generator . Model that is used to generate new plausible examples from the problem domain.

. Model that is used to generate new plausible examples from the problem domain. Discriminator. Model that is used to classify examples as real (from the domain) or fake (generated).

Generative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary. The generator network directly produces samples. Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples drawn from the generator.

— Page 699, Deep Learning, 2016.

The Generator Model

The generator model takes a fixed-length random vector as input and generates a sample in the domain.

The vector is drawn from randomly from a Gaussian distribution, and the vector is used to seed the generative process. After training, points in this multidimensional vector space will correspond to points in the problem domain, forming a compressed representation of the data distribution.

This vector space is referred to as a latent space, or a vector space comprised of latent variables. Latent variables, or hidden variables, are those variables that are important for a domain but are not directly observable.

A latent variable is a random variable that we cannot observe directly.

— Page 67, Deep Learning, 2016.

We often refer to latent variables, or a latent space, as a projection or compression of a data distribution. That is, a latent space provides a compression or high-level concepts of the observed raw data such as the input data distribution. In the case of GANs, the generator model applies meaning to points in a chosen latent space, such that new points drawn from the latent space can be provided to the generator model as input and used to generate new and different output examples.

Machine-learning models can learn the statistical latent space of images, music, and stories, and they can then sample from this space, creating new artworks with characteristics similar to those the model has seen in its training data.

— Page 270, Deep Learning with Python, 2017.

After training, the generator model is kept and used to generate new samples.

The Discriminator Model

The discriminator model takes an example from the domain as input (real or generated) and predicts a binary class label of real or fake (generated).

The real example comes from the training dataset. The generated examples are output by the generator model.

The discriminator is a normal (and well understood) classification model.

After the training process, the discriminator model is discarded as we are interested in the generator.

Sometimes, the generator can be repurposed as it has learned to effectively extract features from examples in the problem domain. Some or all of the feature extraction layers can be used in transfer learning applications using the same or similar input data.

We propose that one way to build good image representations is by training Generative Adversarial Networks (GANs), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks

— Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, 2015.

GANs as a Two Player Game

Generative modeling is an unsupervised learning problem, as we discussed in the previous section, although a clever property of the GAN architecture is that the training of the generative model is framed as a supervised learning problem.

The two models, the generator and discriminator, are trained together. The generator generates a batch of samples, and these, along with real examples from the domain, are provided to the discriminator and classified as real or fake.

The discriminator is then updated to get better at discriminating real and fake samples in the next round, and importantly, the generator is updated based on how well, or not, the generated samples fooled the discriminator.

We can think of the generator as being like a counterfeiter, trying to make fake money, and the discriminator as being like police, trying to allow legitimate money and catch counterfeit money. To succeed in this game, the counterfeiter must learn to make money that is indistinguishable from genuine money, and the generator network must learn to create samples that are drawn from the same distribution as the training data.

— NIPS 2016 Tutorial: Generative Adversarial Networks, 2016.

In this way, the two models are competing against each other, they are adversarial in the game theory sense, and are playing a zero-sum game.

Because the GAN framework can naturally be analyzed with the tools of game theory, we call GANs “adversarial”.

— NIPS 2016 Tutorial: Generative Adversarial Networks, 2016.

In this case, zero-sum means that when the discriminator successfully identifies real and fake samples, it is rewarded or no change is needed to the model parameters, whereas the generator is penalized with large updates to model parameters.

Alternately, when the generator fools the discriminator, it is rewarded, or no change is needed to the model parameters, but the discriminator is penalized and its model parameters are updated.

At a limit, the generator generates perfect replicas from the input domain every time, and the discriminator cannot tell the difference and predicts “unsure” (e.g. 50% for real and fake) in every case. This is just an example of an idealized case; we do not need to get to this point to arrive at a useful generator model.

[training] drives the discriminator to attempt to learn to correctly classify samples as real or fake. Simultaneously, the generator attempts to fool the classifier into believing its samples are real. At convergence, the generator’s samples are indistinguishable from real data, and the discriminator outputs 1/2 everywhere. The discriminator may then be discarded.

— Page 700, Deep Learning, 2016.

GANs and Convolutional Neural Networks

GANs typically work with image data and use Convolutional Neural Networks, or CNNs, as the generator and discriminator models.

The reason for this may be both because the first description of the technique was in the field of computer vision and used CNNs and image data, and because of the remarkable progress that has been seen in recent years using CNNs more generally to achieve state-of-the-art results on a suite of computer vision tasks such as object detection and face recognition.

Modeling image data means that the latent space, the input to the generator, provides a compressed representation of the set of images or photographs used to train the model. It also means that the generator generates new images or photographs, providing an output that can be easily viewed and assessed by developers or users of the model.

It may be this fact above others, the ability to visually assess the quality of the generated output, that has both led to the focus of computer vision applications with CNNs and on the massive leaps in the capability of GANs as compared to other generative models, deep learning based or otherwise.

Conditional GANs

An important extension to the GAN is in their use for conditionally generating an output.

The generative model can be trained to generate new examples from the input domain, where the input, the random vector from the latent space, is provided with (conditioned by) some additional input.

The additional input could be a class value, such as male or female in the generation of photographs of people, or a digit, in the case of generating images of handwritten digits.

Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information y. y could be any kind of auxiliary information, such as class labels or data from other modalities. We can perform the conditioning by feeding y into the both the discriminator and generator as [an] additional input layer.

— Conditional Generative Adversarial Nets, 2014.

The discriminator is also conditioned, meaning that it is provided both with an input image that is either real or fake and the additional input. In the case of a classification label type conditional input, the discriminator would then expect that the input would be of that class, in turn teaching the generator to generate examples of that class in order to fool the discriminator.

In this way, a conditional GAN can be used to generate examples from a domain of a given type.

Taken one step further, the GAN models can be conditioned on an example from the domain, such as an image. This allows for applications of GANs such as text-to-image translation, or image-to-image translation. This allows for some of the more impressive applications of GANs, such as style transfer, photo colorization, transforming photos from summer to winter or day to night, and so on.

In the case of conditional GANs for image-to-image translation, such as transforming day to night, the discriminator is provided examples of real and generated nighttime photos as well as (conditioned on) real daytime photos as input. The generator is provided with a random vector from the latent space as well as (conditioned on) real daytime photos as input.

Why Generative Adversarial Networks?

One of the many major advancements in the use of deep learning methods in domains such as computer vision is a technique called data augmentation.

Data augmentation results in better performing models, both increasing model skill and providing a regularizing effect, reducing generalization error. It works by creating new, artificial but plausible examples from the input problem domain on which the model is trained.

The techniques are primitive in the case of image data, involving crops, flips, zooms, and other simple transforms of existing images in the training dataset.

Successful generative modeling provides an alternative and potentially more domain-specific approach for data augmentation. In fact, data augmentation is a simplified version of generative modeling, although it is rarely described this way.

… enlarging the sample with latent (unobserved) data. This is called data augmentation. […] In other problems, the latent data are actual data that should have been observed but are missing.

— Page 276, The Elements of Statistical Learning, 2016.

In complex domains or domains with a limited amount of data, generative modeling provides a path towards more training for modeling. GANs have seen much success in this use case in domains such as deep reinforcement learning.

There are many research reasons why GANs are interesting, important, and require further study. Ian Goodfellow outlines a number of these in his 2016 conference keynote and associated technical report titled “NIPS 2016 Tutorial: Generative Adversarial Networks.”

Among these reasons, he highlights GANs’ successful ability to model high-dimensional data, handle missing data, and the capacity of GANs to provide multi-modal outputs or multiple plausible answers.

Perhaps the most compelling application of GANs is in conditional GANs for tasks that require the generation of new examples. Here, Goodfellow indicates three main examples:

Image Super-Resolution . The ability to generate high-resolution versions of input images.

. The ability to generate high-resolution versions of input images. Creating Art . The ability to great new and artistic images, sketches, painting, and more.

. The ability to great new and artistic images, sketches, painting, and more. Image-to-Image Translation. The ability to translate photographs across domains, such as day to night, summer to winter, and more.

Perhaps the most compelling reason that GANs are widely studied, developed, and used is because of their success. GANs have been able to generate photos so realistic that humans are unable to tell that they are of objects, scenes, and people that do not exist in real life.

Astonishing is not a sufficient adjective for their capability and success.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Posts

Books

Papers

Articles

Summary

In this post, you discovered a gentle introduction to Generative Adversarial Networks, or GANs.

Specifically, you learned:

Context for GANs, including supervised vs. unsupervised learning and discriminative vs. generative modeling.

GANs are an architecture for automatically training a generative model by treating the unsupervised problem as supervised and using both a generative and a discriminative model.

GANs provide a path to sophisticated domain-specific data augmentation and a solution to problems that require a generative solution, such as image-to-image translation.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Generative Adversarial Networks Today! Develop Your GAN Models in Minutes ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Generative Adversarial Networks with Python It provides self-study tutorials and end-to-end projects on:

DCGAN, conditional GANs, image translation, Pix2Pix, CycleGAN

and much more... Finally Bring GAN Models to your Vision Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
198;machinelearningmastery.com;http://machinelearningmastery.com/time-series-datasets-for-machine-learning/;2016-11-29;7 Time Series Datasets for Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

Machine learning can be applied to time series datasets.

These are problems where a numeric or categorical value must be predicted, but the rows of data are ordered by time.

A problem when getting started in time series forecasting with machine learning is finding good quality standard datasets on which to practice.

In this post, you will discover 8 standard time series datasets that you can use to get started and practice time series forecasting with machine learning.

After reading this post, you will know:

4 univariate time series datasets.

3 multivariate time series datasets.

Websites that you can use to search and download more datasets.

Discover how to prepare and visualize time series data and develop autoregressive forecasting models in my new book, with 28 step-by-step tutorials, and full python code.

Let’s get started.

Updated Apr/2019: Updated the links to the datasets.

Univariate Time Series Datasets

Time series datasets that only have one variable are called univariate datasets.

These datasets are a great place to get started because:

They are so simple and easy to understand.

You can plot them easily in excel or your favorite plotting tool.

You can easily plot the predictions compared to the expected results.

You can quickly try and evaluate a suite of traditional and newer methods.

There are many sources of time series dataset, such as the “Time Series Data Library” created by Rob Hyndman, Professor of Statistics at Monash University, Australia

Below are 4 univariate time series datasets that you can download from a range of fields such as Sales, Meteorology, Physics and Demography.

Stop learning Time Series Forecasting the slow way! Take my free 7-day email course and discover how to get started (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Shampoo Sales Dataset

This dataset describes the monthly number of sales of shampoo over a 3 year period.

The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright and Hyndman (1998).

Below is a sample of the first 5 rows of data including the header row.

""Month"",""Sales of shampoo over a three year period"" ""1-01"",266.0 ""1-02"",145.9 ""1-03"",183.1 ""1-04"",119.3 ""1-05"",180.3 1 2 3 4 5 6 ""Month"",""Sales of shampoo over a three year period"" ""1-01"",266.0 ""1-02"",145.9 ""1-03"",183.1 ""1-04"",119.3 ""1-05"",180.3

Below is a plot of the entire dataset.

The dataset shows an increasing trend and possibly some seasonal component.

Minimum Daily Temperatures Dataset

This dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.

The units are in degrees Celsius and there are 3650 observations. The source of the data is credited as the Australian Bureau of Meteorology.

Below is a sample of the first 5 rows of data including the header row.

""Date"",""Daily minimum temperatures in Melbourne, Australia, 1981-1990"" ""1981-01-01"",20.7 ""1981-01-02"",17.9 ""1981-01-03"",18.8 ""1981-01-04"",14.6 ""1981-01-05"",15.8 1 2 3 4 5 6 ""Date"",""Daily minimum temperatures in Melbourne, Australia, 1981-1990"" ""1981-01-01"",20.7 ""1981-01-02"",17.9 ""1981-01-03"",18.8 ""1981-01-04"",14.6 ""1981-01-05"",15.8

Below is a plot of the entire dataset.

The dataset shows a strong seasonality component and has a nice fine grained detail to work with.

Monthly Sunspot Dataset

This dataset describes a monthly count of the number of observed sunspots for just over 230 years (1749-1983).

The units are a count and there are 2,820 observations. The source of the dataset is credited to Andrews & Herzberg (1985).

Below is a sample of the first 5 rows of data including the header row.

""Month"",""Zuerich monthly sunspot numbers 1749-1983"" ""1749-01"",58.0 ""1749-02"",62.6 ""1749-03"",70.0 ""1749-04"",55.7 ""1749-05"",85.0 1 2 3 4 5 6 ""Month"",""Zuerich monthly sunspot numbers 1749-1983"" ""1749-01"",58.0 ""1749-02"",62.6 ""1749-03"",70.0 ""1749-04"",55.7 ""1749-05"",85.0

Below is a plot of the entire dataset.

The dataset shows seasonality with large differences between seasons.

Daily Female Births Dataset

This dataset describes the number of daily female births in California in 1959.

The units are a count and there are 365 observations. The source of the dataset is credited to Newton (1988).

Below is a sample of the first 5 rows of data including the header row.

""Date"",""Daily total female births in California, 1959"" ""1959-01-01"",35 ""1959-01-02"",32 ""1959-01-03"",30 ""1959-01-04"",31 ""1959-01-05"",44 1 2 3 4 5 6 ""Date"",""Daily total female births in California, 1959"" ""1959-01-01"",35 ""1959-01-02"",32 ""1959-01-03"",30 ""1959-01-04"",31 ""1959-01-05"",44

Below is a plot of the entire dataset.

Multivariate Time Series Datasets

Multivariate datasets are generally more challenging and are the sweet spot for machine learning methods.

A great source of multivariate time series data is the UCI Machine Learning Repository. At the time of writing, there are

At the time of writing, there are 63 time series datasets that you can download for free and work with.

Below is a selection of 3 recommended multivariate time series datasets from Meteorology, Medicine and Monitoring domains.

EEG Eye State Dataset

This dataset describes EEG data for an individual and whether their eyes were open or closed. The objective of the problem is to predict whether eyes are open or closed given EEG data alone.

The objective of the problem is to predict whether eyes are open or closed given EEG data alone.

This is a classification predictive modeling problems and there are a total of 14,980 observations and 15 input variables. The class value of ‘1’ indicates the eye-closed and ‘0’ the eye-open state. Data is ordered by time and observations were recorded over a period of 117 seconds.

Data is ordered by time and observations were recorded over a period of 117 seconds.

Below is a sample of the first 5 rows with no header row.

4329.23,4009.23,4289.23,4148.21,4350.26,4586.15,4096.92,4641.03,4222.05,4238.46,4211.28,4280.51,4635.9,4393.85,0 4324.62,4004.62,4293.85,4148.72,4342.05,4586.67,4097.44,4638.97,4210.77,4226.67,4207.69,4279.49,4632.82,4384.1,0 4327.69,4006.67,4295.38,4156.41,4336.92,4583.59,4096.92,4630.26,4207.69,4222.05,4206.67,4282.05,4628.72,4389.23,0 4328.72,4011.79,4296.41,4155.9,4343.59,4582.56,4097.44,4630.77,4217.44,4235.38,4210.77,4287.69,4632.31,4396.41,0 4326.15,4011.79,4292.31,4151.28,4347.69,4586.67,4095.9,4627.69,4210.77,4244.1,4212.82,4288.21,4632.82,4398.46,0 1 2 3 4 5 4329.23,4009.23,4289.23,4148.21,4350.26,4586.15,4096.92,4641.03,4222.05,4238.46,4211.28,4280.51,4635.9,4393.85,0 4324.62,4004.62,4293.85,4148.72,4342.05,4586.67,4097.44,4638.97,4210.77,4226.67,4207.69,4279.49,4632.82,4384.1,0 4327.69,4006.67,4295.38,4156.41,4336.92,4583.59,4096.92,4630.26,4207.69,4222.05,4206.67,4282.05,4628.72,4389.23,0 4328.72,4011.79,4296.41,4155.9,4343.59,4582.56,4097.44,4630.77,4217.44,4235.38,4210.77,4287.69,4632.31,4396.41,0 4326.15,4011.79,4292.31,4151.28,4347.69,4586.67,4095.9,4627.69,4210.77,4244.1,4212.82,4288.21,4632.82,4398.46,0

Occupancy Detection Dataset

This dataset describes measurements of a room and the objective is to predict whether or not the room is occupied.

There are 20,560 one-minute observations taken over the period of a few weeks. This is a classification prediction problem. There are 7 attributes including various light and climate properties of the room.

The source for the data is credited to Luis Candanedo from UMONS.

Below is a sample of the first 5 rows of data including the header row.

""date"",""Temperature"",""Humidity"",""Light"",""CO2"",""HumidityRatio"",""Occupancy"" ""1"",""2015-02-04 17:51:00"",23.18,27.272,426,721.25,0.00479298817650529,1 ""2"",""2015-02-04 17:51:59"",23.15,27.2675,429.5,714,0.00478344094931065,1 ""3"",""2015-02-04 17:53:00"",23.15,27.245,426,713.5,0.00477946352442199,1 ""4"",""2015-02-04 17:54:00"",23.15,27.2,426,708.25,0.00477150882608175,1 ""5"",""2015-02-04 17:55:00"",23.1,27.2,426,704.5,0.00475699293331518,1 ""6"",""2015-02-04 17:55:59"",23.1,27.2,419,701,0.00475699293331518,1 1 2 3 4 5 6 7 ""date"",""Temperature"",""Humidity"",""Light"",""CO2"",""HumidityRatio"",""Occupancy"" ""1"",""2015-02-04 17:51:00"",23.18,27.272,426,721.25,0.00479298817650529,1 ""2"",""2015-02-04 17:51:59"",23.15,27.2675,429.5,714,0.00478344094931065,1 ""3"",""2015-02-04 17:53:00"",23.15,27.245,426,713.5,0.00477946352442199,1 ""4"",""2015-02-04 17:54:00"",23.15,27.2,426,708.25,0.00477150882608175,1 ""5"",""2015-02-04 17:55:00"",23.1,27.2,426,704.5,0.00475699293331518,1 ""6"",""2015-02-04 17:55:59"",23.1,27.2,419,701,0.00475699293331518,1

The data is provided in 3 files that suggest the splits that may be used for training and testing a model.

Ozone Level Detection Dataset

This dataset describes 6 years of ground ozone concentration observations and the objective is to predict whether it is an “ozone day” or not.

The dataset contains 2,536 observations and 73 attributes. This is a classification prediction problem and the final attribute indicates the class value as “1” for an ozone day and “0” for a normal day.

Two versions of the data are provided, eight-hour peak set and one-hour peak set. I would suggest using the one hour peak set for now.

Below is a sample of the first 5 rows with no header row.

1/1/1998,0.8,1.8,2.4,2.1,2,2.1,1.5,1.7,1.9,2.3,3.7,5.5,5.1,5.4,5.4,4.7,4.3,3.5,3.5,2.9,3.2,3.2,2.8,2.6,5.5,3.1,5.2,6.1,6.1,6.1,6.1,5.6,5.2,5.4,7.2,10.6,14.5,17.2,18.3,18.9,19.1,18.9,18.3,17.3,16.8,16.1,15.4,14.9,14.8,15,19.1,12.5,6.7,0.11,3.83,0.14,1612,-2.3,0.3,7.18,0.12,3178.5,-15.5,0.15,10.67,-1.56,5795,-12.1,17.9,10330,-55,0,0. 1/2/1998,2.8,3.2,3.3,2.7,3.3,3.2,2.9,2.8,3.1,3.4,4.2,4.5,4.5,4.3,5.5,5.1,3.8,3,2.6,3,2.2,2.3,2.5,2.8,5.5,3.4,15.1,15.3,15.6,15.6,15.9,16.2,16.2,16.2,16.6,17.8,19.4,20.6,21.2,21.8,22.4,22.1,20.8,19.1,18.1,17.2,16.5,16.1,16,16.2,22.4,17.8,9,0.25,-0.41,9.53,1594.5,-2.2,0.96,8.24,7.3,3172,-14.5,0.48,8.39,3.84,5805,14.05,29,10275,-55,0,0. 1/3/1998,2.9,2.8,2.6,2.1,2.2,2.5,2.5,2.7,2.2,2.5,3.1,4,4.4,4.6,5.6,5.4,5.2,4.4,3.5,2.7,2.9,3.9,4.1,4.6,5.6,3.5,16.6,16.7,16.7,16.8,16.8,16.8,16.9,16.9,17.1,17.6,19.1,21.3,21.8,22,22.1,22.2,21.3,19.8,18.6,18,18,18.2,18.3,18.4,22.2,18.7,9,0.56,0.89,10.17,1568.5,0.9,0.54,3.8,4.42,3160,-15.9,0.6,6.94,9.8,5790,17.9,41.3,10235,-40,0,0. 1/4/1998,4.7,3.8,3.7,3.8,2.9,3.1,2.8,2.5,2.4,3.1,3.3,3.1,2.3,2.1,2.2,3.8,2.8,2.4,1.9,3.2,4.1,3.9,4.5,4.3,4.7,3.2,18.3,18.2,18.3,18.4,18.6,18.6,18.5,18.7,18.6,18.8,19,19,19.3,19.4,19.6,19.2,18.9,18.8,18.6,18.5,18.3,18.5,18.8,18.9,19.6,18.7,9.9,0.89,-0.34,8.58,1546.5,3,0.77,4.17,8.11,3145.5,-16.8,0.49,8.73,10.54,5775,31.15,51.7,10195,-40,2.08,0. 1/5/1998,2.6,2.1,1.6,1.4,0.9,1.5,1.2,1.4,1.3,1.4,2.2,2,3,3,3.1,3.1,2.7,3,2.4,2.8,2.5,2.5,3.7,3.4,3.7,2.3,18.8,18.6,18.5,18.5,18.6,18.9,19.2,19.4,19.8,20.5,21.1,21.9,23.8,25.1,25.8,26,25.6,24.2,22.9,21.6,20,19.5,19.1,19.1,26,21.1,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,0.58,0. 1/6/1998,3.1,3.5,3.3,2.5,1.6,1.7,1.6,1.6,2.3,1.8,2.5,3.9,3.4,2.7,3.4,2.5,2.2,4.4,4.3,3.2,6.2,6.8,5.1,4,6.8,3.2,18.9,19.5,19.6,19.5,19.5,19.5,19.4,19.2,19.1,19.5,19.6,18.6,18.6,18.9,19.2,19.3,19.2,18.8,17.6,16.9,15.6,15.4,15.9,15.8,19.6,18.5,14.4,0.68,1.52,8.62,1499.5,4.3,0.61,9.04,10.81,3111,-11.8,0.09,11.98,11.28,5770,27.95,46.25,10120,?,5.84,0. 1 2 3 4 5 6 1/1/1998,0.8,1.8,2.4,2.1,2,2.1,1.5,1.7,1.9,2.3,3.7,5.5,5.1,5.4,5.4,4.7,4.3,3.5,3.5,2.9,3.2,3.2,2.8,2.6,5.5,3.1,5.2,6.1,6.1,6.1,6.1,5.6,5.2,5.4,7.2,10.6,14.5,17.2,18.3,18.9,19.1,18.9,18.3,17.3,16.8,16.1,15.4,14.9,14.8,15,19.1,12.5,6.7,0.11,3.83,0.14,1612,-2.3,0.3,7.18,0.12,3178.5,-15.5,0.15,10.67,-1.56,5795,-12.1,17.9,10330,-55,0,0. 1/2/1998,2.8,3.2,3.3,2.7,3.3,3.2,2.9,2.8,3.1,3.4,4.2,4.5,4.5,4.3,5.5,5.1,3.8,3,2.6,3,2.2,2.3,2.5,2.8,5.5,3.4,15.1,15.3,15.6,15.6,15.9,16.2,16.2,16.2,16.6,17.8,19.4,20.6,21.2,21.8,22.4,22.1,20.8,19.1,18.1,17.2,16.5,16.1,16,16.2,22.4,17.8,9,0.25,-0.41,9.53,1594.5,-2.2,0.96,8.24,7.3,3172,-14.5,0.48,8.39,3.84,5805,14.05,29,10275,-55,0,0. 1/3/1998,2.9,2.8,2.6,2.1,2.2,2.5,2.5,2.7,2.2,2.5,3.1,4,4.4,4.6,5.6,5.4,5.2,4.4,3.5,2.7,2.9,3.9,4.1,4.6,5.6,3.5,16.6,16.7,16.7,16.8,16.8,16.8,16.9,16.9,17.1,17.6,19.1,21.3,21.8,22,22.1,22.2,21.3,19.8,18.6,18,18,18.2,18.3,18.4,22.2,18.7,9,0.56,0.89,10.17,1568.5,0.9,0.54,3.8,4.42,3160,-15.9,0.6,6.94,9.8,5790,17.9,41.3,10235,-40,0,0. 1/4/1998,4.7,3.8,3.7,3.8,2.9,3.1,2.8,2.5,2.4,3.1,3.3,3.1,2.3,2.1,2.2,3.8,2.8,2.4,1.9,3.2,4.1,3.9,4.5,4.3,4.7,3.2,18.3,18.2,18.3,18.4,18.6,18.6,18.5,18.7,18.6,18.8,19,19,19.3,19.4,19.6,19.2,18.9,18.8,18.6,18.5,18.3,18.5,18.8,18.9,19.6,18.7,9.9,0.89,-0.34,8.58,1546.5,3,0.77,4.17,8.11,3145.5,-16.8,0.49,8.73,10.54,5775,31.15,51.7,10195,-40,2.08,0. 1/5/1998,2.6,2.1,1.6,1.4,0.9,1.5,1.2,1.4,1.3,1.4,2.2,2,3,3,3.1,3.1,2.7,3,2.4,2.8,2.5,2.5,3.7,3.4,3.7,2.3,18.8,18.6,18.5,18.5,18.6,18.9,19.2,19.4,19.8,20.5,21.1,21.9,23.8,25.1,25.8,26,25.6,24.2,22.9,21.6,20,19.5,19.1,19.1,26,21.1,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,0.58,0. 1/6/1998,3.1,3.5,3.3,2.5,1.6,1.7,1.6,1.6,2.3,1.8,2.5,3.9,3.4,2.7,3.4,2.5,2.2,4.4,4.3,3.2,6.2,6.8,5.1,4,6.8,3.2,18.9,19.5,19.6,19.5,19.5,19.5,19.4,19.2,19.1,19.5,19.6,18.6,18.6,18.9,19.2,19.3,19.2,18.8,17.6,16.9,15.6,15.4,15.9,15.8,19.6,18.5,14.4,0.68,1.52,8.62,1499.5,4.3,0.61,9.04,10.81,3111,-11.8,0.09,11.98,11.28,5770,27.95,46.25,10120,?,5.84,0.

Summary

In this post, you discovered a suite of standard time series forecast datasets that you can use to get started and practice time series forecasting with machine learning methods.

Specifically, you learned about:

4 univariate time series forecasting datasets.

3 multivariate time series forecasting datasets.

Two websites where you can download many more datasets.

Did you use one of the above datasets in your own project?

Share your findings in the comments below.

Want to Develop Time Series Forecasts with Python? Develop Your Own Forecasts in Minutes ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Introduction to Time Series Forecasting With Python It covers self-study tutorials and end-to-end projects on topics like: Loading data, visualization, modeling, algorithm tuning, and much more... Finally Bring Time Series Forecasting to

Your Own Projects Skip the Academics. Just Results. See What's Inside"
199;machinelearningmastery.com;https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/;2019-10-06;How to Develop a Naive Bayes Classifier from Scratch in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41

# example of preparing and making a prediction with a naive bayes model from sklearn . datasets import make_blobs from scipy . stats import norm from numpy import mean from numpy import std # fit a probability distribution to a univariate data sample def fit_distribution ( data ) : # estimate parameters mu = mean ( data ) sigma = std ( data ) print ( mu , sigma ) # fit distribution dist = norm ( mu , sigma ) return dist # calculate the independent conditional probability def probability ( X , prior , dist1 , dist2 ) : return prior * dist1 . pdf ( X [ 0 ] ) * dist2 . pdf ( X [ 1 ] ) # generate 2d classification dataset X , y = make_blobs ( n_samples = 100 , centers = 2 , n_features = 2 , random_state = 1 ) # sort data into classes Xy0 = X [ y == 0 ] Xy1 = X [ y == 1 ] # calculate priors priory0 = len ( Xy0 ) / len ( X ) priory1 = len ( Xy1 ) / len ( X ) # create PDFs for y==0 distX1y0 = fit_distribution ( Xy0 [ : , 0 ] ) distX2y0 = fit_distribution ( Xy0 [ : , 1 ] ) # create PDFs for y==1 distX1y1 = fit_distribution ( Xy1 [ : , 0 ] ) distX2y1 = fit_distribution ( Xy1 [ : , 1 ] ) # classify one example Xsample , ysample = X [ 0 ] , y [ 0 ] py0 = probability ( Xsample , priory0 , distX1y0 , distX2y0 ) py1 = probability ( Xsample , priory1 , distX1y1 , distX2y1 ) print ( 'P(y=0 | %s) = %.3f' % ( Xsample , py0* 100 ) ) print ( 'P(y=1 | %s) = %.3f' % ( Xsample , py1* 100 ) ) print ( 'Truth: y=%d' % ysample )"
200;machinelearningmastery.com;https://machinelearningmastery.com/imbalanced-classification-with-python-7-day-mini-course/;2020-01-15;Imbalanced Classification With Python (7-Day Mini-Course);"# plot imbalanced classification problem

from collections import Counter

from sklearn . datasets import make_classification

from matplotlib import pyplot

from numpy import where

# define dataset

X , y = make_classification ( n_samples = 1000 , n_features = 2 , n_redundant = 0 , n_clusters_per_class = 1 , weights = [ 0.99 , 0.01 ] , flip_y = 0 )

# summarize class distribution

counter = Counter ( y )

print ( counter )

# scatter plot of examples by class label

for label , _ in counter . items ( ) :

row_ix = where ( y == label ) [ 0 ]

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( label ) )

pyplot . legend ( )"
201;machinelearningmastery.com;https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/;2019-12-31;Failure of Classification Accuracy for Imbalanced Class Distributions;"# create a dataset with a given class distribution

def get_dataset ( proportions ) :

# determine the number of classes

n_classes = len ( proportions )

# determine the number of examples to generate for each class

largest = max ( [ v for k , v in proportions . items ( ) ] )

n_samples = largest * n_classes

# create dataset

X , y = make_blobs ( n_samples = n_samples , centers = n_classes , n_features = 2 , random_state = 1 , cluster_std = 3 )

# collect the examples

X_list , y_list = list ( ) , list ( )

for k , v in proportions . items ( ) :

row_ix = where ( y == k ) [ 0 ]

selected = row_ix [ : v ]

X_list . append ( X [ selected , : ] )

y_list . append ( y [ selected ] )"
202;news.mit.edu;http://news.mit.edu/2020/charlotte-minsky-lyndie-mitchell-zollinger-named-gates-cambridge-scholars-0218;;Charlotte Minsky and Lyndie Mitchell Zollinger named 2020 Gates Cambridge Scholars;"MIT seniors Charlotte Minsky and Lyndie Mitchell Zollinger have won the prestigious Gates Cambridge Scholarship, which offers students an opportunity to pursue graduate study in the field of their choice at Cambridge University in England.

Minsky, from Greenfield, Massachusetts, is completing her bachelor’s degree in earth, atmospheric, and planetary sciences, as well as history. She had always thought her dual interest in science and the humanities were disparate until she joined the MIT and Slavery project, which illuminated for her the ways that science and technology can be tools for the structures of oppression. Minsky then realized that she could combine both science and history, and that the combined studies would allow her to struggle with the historical legacies of science. At Cambridge, she plans to read for an MPhil in history and philosophy of science before returning to the United States to earn a PhD in planetary science.

Regarding Minsky's work on the MIT and Slavery project, Professor Anne McCants notes, ""I was awed by the sophistication of the public presentation she gave of her research on the relationship between MIT and the economy of the post-Civil War reconstruction South, an event that was attended by all members of the MIT upper administration, as well as live-streamed for a global public audience. For someone so young, her clarity of thinking, personal confidence, and historical humility about the remaining questions were all quite extraordinary. I can honestly say that I had never seen anything like it before from a student, let alone one not even halfway through college.”

Minsky has proven herself equally adept at scientific research, and is currently working with Professor Ray Jayawardhana's group at Cornell University on testing a new method to characterize exoplanet atmospheres. She previously studied with Professor Julien de Wit (assistant professor in earth, atmospheric, and planetary sciences) to investigate the propagation of biases in exoplanet atmosphere models, as well as with Professor Benjamin Weiss and Research Scientist Mary Knapp, searching for the theorized Planet 9 by using archival radio data. She plans to continue similar work during her doctoral research.

Minsky is the current vice president for the Undergraduate Association (UA), and former chief of staff to the UA. She is the former president and co-founder of the Prison Education Initiative, and regularly teaches astronomy to inmates, as well as educating the MIT community about mass incarceration. She also served as the president of Queer West, a LGBTQ+ community and advocacy organization at MIT.

Mitchell Zollinger, from Sandy, Utah, was raised with a passion for learning, teaching, building, and medicine. After conducting research at the University of Utah’s Chemistry Department, she decided to come to MIT to study engineering. Mitchell Zollinger will graduate from MIT with a bachelor’s degree in mechanical engineering, and then pursue a doctorate in engineering at the University of Cambridge. During an unfortunate accident when a giant hamster wheel fell on top of her in one of her mechanical engineering classes, she realized the importance of a mechanical perspective on medical challenges. At Cambridge, she will develop mechanical models of the progression of traumatic brain injuries. This will provide clinicians with a range of patient-specific predicted outcomes to assist them in choosing the best treatment options, and will improve patients’ lives by saving vital time and reducing the risk of further brain damage.

Mitchell Zollinger’s work at Cambridge will build upon her summer research at the University of Auckland, where she worked to develop implantable sensors for the brain. Previously, she worked with Steven Gillmer of MIT Lincoln Laboratory, investigating the complexity of motions and required forces to open doors for people in wheelchairs. Her end goal was to create robotic assistive devices for people in wheelchairs who struggle with things like this on a day-to-day basis. “The most important thing about Lyndie’s research,” says Gillmer, “is she is doing it for the well-being of others.” She was also selected as one of only seven juniors to be a Pappalardo Apprentice.

Mitchell Zollinger has always been committed to encouraging women in STEM, as she herself was encouraged in the field by a female neighbor who had a doctorate in science. AT MIT, she has served as a residential tutor for the Women’s Technology Program in the Department of Mechanical Engineering, where she worked with high school girls to introduce and encourage them to pursue STEM fields. Mitchell Zollinger plans to continue similar initiatives through her future career as an academic in engineering.

Mitchell Zollinger led the effort to create the Addir Interfaith Engagement Association to expand the group’s efforts beyond conversation about diversity to promoting greater mutual respect and understanding across the Institute. She has also been pivotal to the establishment of the Office of Religious, Spiritual, and Ethical Life’s student advisory board.

Minsky and Mitchell Zollinger were advised in their applications by Kim Benard of the Office of Distinguished Fellowships team in Career Advising and Professional Development, who remarks “Charlotte and Lyndie are superb examples of an MIT education, combining compassion with creativity to create a better world. We are proud that they will be representing the Institute at Cambridge, and we are equally proud of the other students who interviewed.”

Established by the Bill and Melinda Gates Foundation in 2000, the Gates Cambridge Scholarship provides full funding for talented students from outside the United Kingdom to pursue postgraduate study in any subject at Cambridge University. The 2020 awards process was extremely competitive, with 28 ultimately chosen. Since the program’s inception in 2001, there have been 30 Gates Cambridge Scholars from MIT."
203;news.mit.edu;http://news.mit.edu/2020/gamma-radiation-found-ineffective-in-sterilizing-n95-masks-0410;;Gamma radiation found ineffective in sterilizing N95 masks;"The research described in this article has been published on a preprint server but has not yet been peer-reviewed by scientific or medical experts.

In mid-March, members of the Department of Nuclear Science and Engineering (NSE) joined forces with colleagues in Boston’s medical community to answer a question of critical importance during the Covid-19 pandemic: Can gamma irradiation sterilize disposable N95 masks without diminishing the masks’ effectiveness?

This type of personal protective equipment (PPE), which offers protection against infectious particles like coronavirus-laden aerosols, is in desperately short supply worldwide, and medical professionals in Covid-19 hotspots are already rationing the masks. Gamma radiation is commonly used to sterilize hospital foods and equipment surfaces, as well as much of the public’s food supply, and there has been significant interest in determining if it could allow N95 masks to be reused and address the expanding scarcity.

In a study uploaded on March 28 to medRχiv, the preprint server for health sciences, researchers announced their results: N95 masks subjected to cobalt-60 gamma irradiation for sterilization pass a qualitative fit test but lose a significant degree of filtration efficiency. This form of sterilization compromises the masks’ ability to protect medical providers from Covid-19.

The study, NSE’s first research effort related to the pandemic, also drew on the expertise of MIT’s Office of Environment, Health, and Safety.

“One of our students thought gamma irradiation might be a cool solution to a big problem, and I really wanted it to work,” says Michael Short, the Class of ’42 Associate Professor of Nuclear Science and Engineering, one of the study’s coauthors. “But we quickly recognized that the data went against the hypothesis.”

Team members believe these negative results nevertheless contribute to the larger effort to combat the pandemic. “There has never been a time when negative results are more significant,” notes study lead and co-author Avilash Cramer SM ’18, a fifth-year doctoral candidate in the Harvard-MIT Program in Health Sciences and Technology studying radiation physics. “Publishing as quickly as we can means that others working on the same problem can direct their energies in different directions.”

Fast-track research

While they may not have produced the desired outcome, the researchers nevertheless pulled off a study remarkable for its speed and multidisciplinary cooperation — a process inspired and shaped by the immediate threat of the Covid-19 pandemic. “The study took nine days from start to finish,” says Short. “It was the fastest I’ve ever done anything, by orders of magnitude.”

The dire reality of an N95 shortage in the United States sparked widespread concerns early in March. “It had already hit New York, and was on its way to Massachusetts, and President [L. Rafel] Reif wanted to know if we could do something to masks to permit their reuse,” recounts Short. “We looked into different methods, and noticed the idea of using gamma radiation was popping up in a lot of places.”

Cramer was losing sleep worrying about his classmates, medical residents at Boston-area hospitals already in the thick of treating Covid-19 patients. “After reading the literature, it was clear there wasn't a lot of good research out there regarding reusing masks,” he says. “The sky was falling in hospitals with equipment shortages everywhere, and while others had shown gamma rays could inactivate viruses, I wanted to demonstrate one way or the other if they damage the masks themselves.”

N95 masks are manufactured through a variety of proprietary processes using wool, glass particles, and plastics, with 1-2 percent copper and/or zinc. Viewed under a scanning electron microscope, these masks reveal a matrix of fibers with openings of approximately 1 micron. Because the filtering occurs through an electrostatic, rather than mechanical, process, a mask can repel or trap smaller incoming particles. This includes at least 95 percent of airborne particles 0.3 microns or larger in size, such as the airborne droplets that can convey the Covid-19 virus.

A call for multidisciplinary action

On March 11, Cramer emailed several contacts in the radiation physics community in search of a gamma irradiation source. Among the group was Short, who has some experience, among many things, in irradiating plastics. Cramer had worked with Short on previous research ventures, and was familiar with NSE from his time serving as a teaching assistant for an NSE class, Radiation Biophysics (22.055), taught by his PhD advisor, Rajiv Gupta, a physician at Massachusetts General Hospital and an associate professor of radiology at Harvard Medical School.

Short instantly responded to Cramer, offering the campus Cobalt-60 irradiation facility, a source of gamma radiation. “I had an exemption to work on campus and thought, let’s just do it: irradiate and sterilize the masks, then see if they can be used again,” says Short.

With support and guidance from Gupta, also a study co-author, Cramer paused his doctoral work (on low-cost radiology solutions for rural areas), and began writing up a research protocol and drafting additional researchers.

The experiment began on Saturday, March 14, and the first results emerged the next Thursday.

Short gathered the masks from his and a collaborator’s laboratory, keeping a handful for this study before donating the rest (a few hundred) to Beverly Hospital. In Building 6, Short and Mitchell Galanek of MIT Environmental Health and Safety placed the masks into the shielded ring of Cobalt-60, subjecting one group of masks to 10 kilograys (kGy) and another to 50 kGy of gamma radiation (A kilogray is a unit of ionizing radiation). One control group of masks was left unirradiated.

Short then biked the masks to Brigham and Women’s Hospital. There, resident and study co-author Sherry H. Yu, who had signed onto the study after receiving a single emailed invitation, carried out a series of qualitative fit tests. These tests, designed by the U.S. Occupational Safety and Health Administration, establish whether a mask fits securely to someone’s face and screens out potentially harmful aerosolized particles. Yu’s N95 mask-wearing guinea pig was Short himself.

“I spent three hours in a back room at the Brigham in the midst of Covid craziness trying to taste a nebulized sugar solution,” says Short. For this test, saccharin vapor is sprayed into a hood and collar assembly fitted over the head of a subject wearing an N95 mask. By moving their face from side to side and reading a passage, the subject simulates facial movements that might displace or detach the mask and render it less effective. If, after all these motions, a subject cannot taste the sweet mist, the N95 passes. All of Short’s gamma irradiated masks passed the qualitative fit test.

“We thought, Awesome, we’ve done it,” recalls Short. “But colleagues from the Greater Boston biomedical community told us the fit test wasn’t good enough — we needed to assess filter efficiency as well.”

Flawed filtering

Fortunately, the right kind of experimental setup existed just next door at MIT — in the laboratory of Ju Li, Battelle Energy Alliance Professor of Nuclear Science and Engineering and professor of materials science and engineering. Li and doctoral student Enze Tian (both study coauthors) signed on to shepherd the next phase of the study, using an apparatus that shoots sodium chloride particles of different sizes into the N95 masks. The device, normally used to test the protective properties of the Li lab’s masks against tiny metal fragments and nanoparticles, revealed the disappointing results.

“The sterilized masks lost two-thirds of their filtering efficiency, essentially turning N95 into N30 masks,” says Cramer. But why the deterioration?

“Our hypothesis is that ionizing radiation of whatever kind likely decharges the electrostatic filtration of the mask,” says Gupta. “The mechanical filtration of gauze can trap some particles, but radiation interferes with the electrostatic filter’s ability to repel or capture particles of 0.3 microns.”

Gupta is nevertheless pleased by the study’s results. “Even with lowered efficiency, these N95 masks are much better than the surgical masks we use,” he says. “Instead of throwing out N95 masks, they could be sterilized and used as N30 masks for the kind of procedures I do all day long.”

Cramer, who is continuing to explore other N95 mask sterilization methods, believes the study’s results serve a larger purpose: “Adding one more data point to the global understanding of how to clean devices is important — it’s the purest example of the scientific method I’ve ever had the fortune to be part of.”

“Every piece of our hastily assembled machine worked perfectly,” says Short. “We demonstrated that when a crisis hits, scientists can come together for the greater good and do what needs to happen.”"
204;machinelearningmastery.com;https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/;2019-01-10;How to Fix the Vanishing Gradients Problem Using the ReLU;"# deeper mlp with relu for the two circles classification problem with callback

from sklearn . datasets import make_circles

from sklearn . preprocessing import MinMaxScaler

from keras . layers import Dense

from keras . models import Sequential

from keras . optimizers import SGD

from keras . callbacks import TensorBoard

# generate 2d classification dataset

X , y = make_circles ( n_samples = 1000 , noise = 0.1 , random_state = 1 )

scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) )

X = scaler . fit_transform ( X )

# split into train and test

n_train = 500

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# define model

model = Sequential ( )

model . add ( Dense ( 5 , input_dim = 2 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = SGD ( lr = 0.01 , momentum = 0.9 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

# prepare callback

tb = TensorBoard ( histogram_freq = 1 , write_grads = True )

# fit model"
205;machinelearningmastery.com;https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/;2017-10-03;How to Use Word Embedding Layers for Deep Learning with Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64

from numpy import array from numpy import asarray from numpy import zeros from keras . preprocessing . text import Tokenizer from keras . preprocessing . sequence import pad_sequences from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers import Embedding # define documents docs = [ 'Well done!' , 'Good work' , 'Great effort' , 'nice work' , 'Excellent!' , 'Weak' , 'Poor effort!' , 'not good' , 'poor work' , 'Could have done better.' ] # define class labels labels = array ( [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] ) # prepare tokenizer t = Tokenizer ( ) t . fit_on_texts ( docs ) vocab_size = len ( t . word_index ) + 1 # integer encode the documents encoded_docs = t . texts_to_sequences ( docs ) print ( encoded_docs ) # pad documents to a max length of 4 words max_length = 4 padded_docs = pad_sequences ( encoded_docs , maxlen = max_length , padding = 'post' ) print ( padded_docs ) # load the whole embedding into memory embeddings_index = dict ( ) f = open ( '../glove_data/glove.6B/glove.6B.100d.txt' ) for line in f : values = line . split ( ) word = values [ 0 ] coefs = asarray ( values [ 1 : ] , dtype = 'float32' ) embeddings_index [ word ] = coefs f . close ( ) print ( 'Loaded %s word vectors.' % len ( embeddings_index ) ) # create a weight matrix for words in training docs embedding_matrix = zeros ( ( vocab_size , 100 ) ) for word , i in t . word_index . items ( ) : embedding_vector = embeddings_index . get ( word ) if embedding_vector is not None : embedding_matrix [ i ] = embedding_vector # define model model = Sequential ( ) e = Embedding ( vocab_size , 100 , weights = [ embedding_matrix ] , input_length = 4 , trainable = False ) model . add ( e ) model . add ( Flatten ( ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile the model model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) # summarize the model print ( model . summary ( ) ) # fit the model model . fit ( padded_docs , labels , epochs = 50 , verbose = 0 ) # evaluate the model loss , accuracy = model . evaluate ( padded_docs , labels , verbose = 0 ) print ( 'Accuracy: %f' % ( accuracy* 100 ) )"
206;towardsdatascience.com;https://towardsdatascience.com/tools-for-sharing-jupyter-notebooks-online-28c8d4ff821c?source=collection_home---4------1-----------------------;2020-04-17;Tools for Sharing Jupyter Notebooks Online;"Photo by Marvin Meyer on Unsplash

Tools for Sharing Jupyter Notebooks Online

Three tools for sharing python notebooks with non-programmers

I recently started helping out on a new data project set up by a friend of mine. The data side of the project is an analysis piece applying some natural language processing to text-based responses from a survey. I created a Github repository for the project and completed the analysis in a Jupyter Notebook.

As part of this project, I want to be able to share the notebooks with non-programmers who won’t necessarily be set up with or familiar with Github. This is a common problem that most data scientists face.

Jupyter Notebooks are a great tool for exploratory data analysis but it is pretty common to need to share this analysis with non-programmer stakeholders in a project. Fortunately, there are a number of tools available to host notebooks online for non-Github users.

In the following article, I am going to walk through how to use three of these tools and discuss the pros and cons of each."
207;machinelearningmastery.com;http://machinelearningmastery.com/estimate-performance-machine-learning-algorithms-weka/;2016-07-17;How To Estimate The Performance of Machine Learning Algorithms in Weka;"Tweet Share Share

Last Updated on August 22, 2019

The problem of predictive modeling is to create models that have good performance making predictions on new unseen data.

Therefore it is critically important to use robust techniques to train and evaluate your models on your available training data. The more reliable the estimate of the performance on your model, the further you can push the performance and be confident it will translate to the operational use of your model.

In this post you will discover the various different ways that you can estimate the performance of your machine learning models in Weka.

After reading this post you will know:

How to evaluate your model using the training dataset.

How to evaluate your model using a random train and test split.

How to evaluate your model using k-fold cross validation.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Model Evaluation Techniques

There are a number of model evaluation techniques that you can choose from, and the Weka machine learning workbench offers four of them, as follows:

Training Dataset

Prepare your model on the entire training dataset, then evaluate the model on the same dataset. This is generally problematic not least because a perfect algorithm could game this evaluation technique by simply memorizing (storing) all training patterns and achieve a perfect score, which would be misleading.

Supplied Test Set

Split your dataset manually using another program. Prepare your model on the entire training dataset and use the separate test set to evaluate the performance of the model. This is a good approach if you have a large dataset (many tens of thousands of instances).

Percentage Split

Randomly split your dataset into a training and a testing partitions each time you evaluate a model. This can give you a very quick estimate of performance and like using a supplied test set, is preferable only when you have a large dataset.

Cross Validation

Split the dataset into k-partitions or folds. Train a model on all of the partitions except one that is held out as the test set, then repeat this process creating k-different models and give each fold a chance of being held out as the test set. Then calculate the average performance of all k models. This is the gold standard for evaluating model performance, but has the cost of creating many more models.

You can see these techniques in the Weka Explorer on the “Classify” tab after you have loaded a dataset.

Which Test Option to Use

Given that there are four different test options to choose from, which one should you use?

Each test option has a time and place, summarized as follows:

Training Dataset : Only to be used when you have all of the data and you are interested in creating a descriptive rather than a predictive model. Because you have all of the data, you do not need to make new predictions. You are interested in creating a model to better understand the problem.

: Only to be used when you have all of the data and you are interested in creating a descriptive rather than a predictive model. Because you have all of the data, you do not need to make new predictions. You are interested in creating a model to better understand the problem. Supplied Test Set : When the data is very large, e.g. millions of records and you do not need all of it to train a model. Also useful when the test set has been defined by a third party.

: When the data is very large, e.g. millions of records and you do not need all of it to train a model. Also useful when the test set has been defined by a third party. Percentage Split : Excellent to use to get a quick idea of the performance of a model. Not to be used to make decisions, unless you have a very large dataset and are confident (e.g. you have tested) that the splits sufficiently describe the problem. A common split value is 66% to 34% for train and test sets respectively.

: Excellent to use to get a quick idea of the performance of a model. Not to be used to make decisions, unless you have a very large dataset and are confident (e.g. you have tested) that the splits sufficiently describe the problem. A common split value is 66% to 34% for train and test sets respectively. Cross Validation: The default. To be used when you are unsure. Generally provides a more accurate estimate of the performance than the other techniques. Not to be used when you have a very large data. Common values for k are 5 and 10, depending on the size of the dataset.

If in doubt, use k-fold cross validation where k is set to 10.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

What About The Final Model

Test options are concerned with estimating the performance of a model on unseen data.

This is an important concept to internalize. The goal of predictive modeling is to create a model that performs best in a situation that we do not fully understand, the future with new unknown data. We must use these powerful statistical techniques to best estimate the performance of the model in this situation.

That being said, once we have chosen a model, it must be finalized. None of these test options are used for this purpose.

The model must be trained on the entire training dataset and saved. This topic of model finalization is beyond the scope of this post.

Just note, that the performance of the finalized model does not need to be calculated, it is estimated using the test option techniques discussed above.

Performance Summary

The performance summary is provided in Weka when you evaluate a model.

In the “Classify” tab after you have evaluated an algorithm by clicking the “Start” button, the results are presented in the “Classifier output” pane.

This pane includes a lot of information, including:

The run information such as the algorithm and its configuration, the dataset and its properties as well as the test option

The details of the constructed model, if any.

The summary of the performance including a host of different measures.

Classification Performance Summary

When evaluating a machine learning algorithm on a classification problem, you are given a vast amount of performance information to digest.

This is because classification may be the most studied type of predictive modeling problem and there are so many different ways to think about the performance of classification algorithms.

There are three things to note in the performance summary for classification algorithms:

Classification accuracy. This the ratio of the number of correct predictions out of all predictions made, often presented as a percentage where 100% is the best an algorithm can achieve. If your data has unbalanced classes, you may want to look into the Kappa metric which presents the same information taking the class balance into account. Accuracy by class. Take note of the true-positive and false-positive rates for the predictions for each class which can be instructive of the class breakdown for the problem is uneven or there are more than two classes. This can help you interpret the results if predicting one class is more important than predicting another. Confusion matrix. A table showing the number of predictions for each class compared to the number of instances that actually belong to each class. This is very useful to get an overview of the types of mistakes the algorithm made.

Regression Performance Summary

When evaluating a machine learning algorithm on a regression problem, you are given a number of different performance measures to review.

Of note the performance summary for regression algorithms are two things:

Correlation Coefficient. This is how well the predictions are correlated or change with the actual output value. A value of 0 is the worst and a value of 1 is a perfectly correlated set of predictions. Root Mean Squared Error. This is the average amount of error made on the test set in the units of the output variable. This measure helps you get an idea on the amount a given prediction may be wrong on average.

Summary

In this post you discovered how to estimate the performance of your machine learning models on unseen data in Weka.

Specifically, you learned:

About the importance of estimating the performance of machine learning models on unseen data as the core problem in predictive modeling.

About the 4 different test options and when to use each, paying specific attention to train and test splits and k-fold cross validation.

About the performance summary for classification and regression problems and to which metrics to pay attention.

Do you have any questions about estimating model performance in Weka or about this post? Ask your questions in the comments and I will do my best to answer them.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
208;news.mit.edu;http://news.mit.edu/2020/events-postponed-canceled-covid-19-0309;;Events postponed or canceled as MIT responds to COVID-19;"MIT schools, departments, labs, centers, and offices have acted swiftly to postpone or cancel large events through May 15 in the wake of the Institute’s announcement last week of new policies regarding gatherings likely to attract 150 or more people.

To safeguard against COVID-19, and the spread of the 2019 novel coronavirus, many other MIT events have been modified both on campus and elsewhere, with increased opportunities offered for livestreaming.

The guidelines put forth last week have also now been expanded to include some large classes: The Institute will move classes with more than 150 students online, starting this week.

Impacts on classes and student travel

Following consultation with senior academic leadership and experts within MIT Medical, the Institute has suspended in-person meetings of classes with more than 150 students, effective tomorrow, Tuesday, March 10. The approximately 20 classes impacted by the decision will continue to be offered in virtual form.

“We are being guided by our medical professionals who are in close contact with state and national public health officials,” Ian Waitz, vice chancellor for undergraduate and graduate education, wrote today in a letter to deans and department heads. “They have advised us that while the risk to the community is low and there are no cases on campus as of now, we need to move quickly to help prevent the potential transmission of the disease and to be ready if and when it impacts our campus.”

“Our approach is to be aggressive, but to move forward in stages,” Waitz added, “while keeping in mind that some individual faculty and departments may be moving faster than others, that the level of comfort with remote teaching varies, and that some classes may translate better than others to alternative formats.”

As of now, midterm examinations will proceed as scheduled, but the plan for large courses is to run midterms in several rooms simultaneously so the number of students in each room remains well below 150. The Registrar’s Office is working on room scheduling strategies to best accommodate that approach.

The Institute has also decided that all MIT-sponsored student domestic travel of more than 100 miles will have to go through the Institute’s high-risk travel waiver process.

Impacts on undergraduate and graduate admissions

As shared in President L. Rafael Reif’s letter of last Thursday, MIT’s new policy on events will apply to Campus Preview Weekend, ordinarily an on-campus gathering for students admitted to the incoming first-year undergraduate class. In the coming weeks, the Admissions Office will be connecting with admitted students, current students, and campus partners to discuss what to do instead of a conventional CPW. For more information, please see: https://mitadmissions.org/blogs/entry/mits-covid-19-precautions-and-its-impact-on-admissions/

The Admissions Office will not host any programming for K-12 students, including admitted students and their families, between now and May 15, regardless of the size of the event. All scheduled admissions sessions and tours have been canceled between now and May 15, and MIT Admissions is canceling all scheduled admissions officer travel to domestic and international events in that time window.

Additionally, all graduate admissions visit days have been canceled, effective immediately. “Based upon reducing risk, we ask all departments to cancel all remaining graduate open houses and visit days, and to move to virtual formats,” Waitz says. “Many departments have already done this.”

Despite the cancellation of these formal events, the MIT campus currently remains open for visits by prospective students. However, in keeping with suggested best practices for public health, visitors from countries that the U.S. Centers for Disease Control and Prevention (CDC) finds have “widespread sustained (ongoing) transmission” of COVID-19 cannot visit campus until they have successfully completed 14 days of self-quarantine.

Impacts on major campus events

The MIT Excellence Awards and Collier Medal celebration, scheduled for this Thursday, March 12, has been postponed; a rescheduled date will be announced as soon as it is confirmed. The Excellence Awards and Collier Medal recognize the work of service, support, administrative, and sponsored research staff. The Excellence Awards acknowledge the extraordinary efforts made by members of the MIT community toward fulfilling the goals, values, and mission of the Institute. The Collier Medal is awarded to an individual or group exhibiting qualities such as a commitment to community service, kindness, selflessness, and generosity; it honors the memory of MIT Police Officer Sean Collier, who lost his life while protecting the MIT campus. A full list of this year’s honorees is available.

Career Advising and Professional Development is working on plans to change the format of the Spring Career Fair, previously scheduled for April 2, to a virtual career fair for a date to be announced in April. All other large-scale employer engagement events — such as career fairs, mixers, symposiums, and networking events — will also be canceled; adopt a virtual model; be postponed beyond May 15; or adopt other models that meet the new policies involving large events.

MIT is postponing the remaining two Climate Action Symposia, “MIT Climate Initiatives and the Role of Research Universities” and “Summing Up: Why Is the World Waiting?” — previously scheduled for April 2 and April 22, respectively. These symposia will be rescheduled; new dates will be announced on climatesymposia.mit.edu.

Solve at MIT on May 12-14 will be virtual. In addition to a livestream on this page, Solve will continue to bring together its cross-sector community via interactive online workshops and more. Participants can also contribute a solution or a donation to the Health Security and Pandemics Challenge.

Impacts on athletics and intercollegiate athletics events

The Department of Athletics, Physical Education and Recreation (DAPER) is taking steps to safeguard student-athletes, staff, and community members who utilize DAPER facilities for club sports, intramurals, and recreation. Unless otherwise announced, MIT’s intercollegiate athletics events will continue as scheduled. However, visiting teams are asked to bring only student-athletes and essential team personnel to events at MIT.

Additionally, DAPER has requested that only MIT students, faculty, and staff members attend upcoming home athletic events through May 15. All other spectators, including parents, are asked to watch events using DAPER’s video streaming service.

Other impacted events and activities

Discussions are ongoing about many additional events scheduled between now and May 15. The list below will be updated as more information becomes available. Among the affected events and activities announced so far:

Use of the pillars in Lobby 7 for community discussion is suspended for the rest of the spring semester, to minimize close contact and sharing of writing implements.

The Advanced Undergraduate Research Opportunities Program — better known as SuperUROP — has canceled the SuperUROP Showcase poster session, which had been scheduled for April 23, 2020. The SuperUROP teaching staff will be in touch with SuperUROP scholars about working remotely to fulfill the research and academic requirements for this program. SuperUROP has also canceled the program's closing reception, which had been scheduled for May 7, 2020. The SuperUROP logistics team will be in touch with SuperUROP scholars about how to obtain their certificates for completing the program. Questions? Email superurop-contact@mit.edu.

poster session, which had been scheduled for April 23, 2020. The SuperUROP teaching staff will be in touch with SuperUROP scholars about working remotely to fulfill the research and academic requirements for this program. SuperUROP has also canceled the program's closing reception, which had been scheduled for May 7, 2020. The SuperUROP logistics team will be in touch with SuperUROP scholars about how to obtain their certificates for completing the program. Questions? Email superurop-contact@mit.edu. SpaceTech 2020, scheduled for Wednesday, March 11, has been postponed until a later date. The all-day event, part of MIT Space Week, will highlight the future of space exploration by featuring lightning talks from current students; talks and panels from alumni; and an interactive guided tour along the Space Trail to visit Department of Aeronautics and Astronautics (AeroAstro) labs and ongoing research projects. Visit spacetech.mit.edu for the latest information.

scheduled for Wednesday, March 11, has been postponed until a later date. The all-day event, part of MIT Space Week, will highlight the future of space exploration by featuring lightning talks from current students; talks and panels from alumni; and an interactive guided tour along the Space Trail to visit Department of Aeronautics and Astronautics (AeroAstro) labs and ongoing research projects. Visit spacetech.mit.edu for the latest information. MIT Getfit has canceled both of its midpoint events originally scheduled for Wednesday, March 11. Organizers are working to contact participants with more information.

canceled both of its midpoint events originally scheduled for Wednesday, March 11. Organizers are working to contact participants with more information. The March 13 lecture titled “Fateful Triangle: How China Shaped US-India Relations During the Cold War,” by Tanvi Madan of the Brookings Institution, has been postponed. More information is available at http://southasianpolitics.net/.

by Tanvi Madan of the Brookings Institution, has been postponed. More information is available at http://southasianpolitics.net/. To the Moon to Stay Hackathon , scheduled for Saturday, March 14, has been postponed until a later date. MIT AeroAstro and the MIT Media Lab’s Space Exploration Initiative are partnering to design and build an experiment to go to the moon on board Blue Origin’s inaugural lunar mission. The goal of the hackathon is to bring the MIT community together to think about lunar missions and habitation through a variety of challenges. To receive updates, join their email list or visit tothemoon.mit.edu.

, scheduled for Saturday, March 14, has been postponed until a later date. MIT AeroAstro and the MIT Media Lab’s Space Exploration Initiative are partnering to design and build an experiment to go to the moon on board Blue Origin’s inaugural lunar mission. The goal of the hackathon is to bring the MIT community together to think about lunar missions and habitation through a variety of challenges. To receive updates, join their email list or visit tothemoon.mit.edu. The Koch Institute is limiting attendance at the SCIENCE with/in/sight: 2020 Visions event on March 17. This event is now for invited guests only.

All MIT Communications Forum events have been postponed until the fall. This includes Science Under Attack, originally scheduled for March 19, and David Thorburn’s presentation as part of the William Corbett Poetry Series, originally scheduled for April 8.

The MIT de Florez Award Competition , scheduled for April 15, will be conducted virtually. Additional information will be sent to the Mechanical Engineering community via email.

, scheduled for April 15, will be conducted virtually. Additional information will be sent to the Mechanical Engineering community via email. The Mechanical Engineering Graduate Student Gala , scheduled for April 19, has been canceled and will be rescheduled for the fall.

, scheduled for April 19, has been canceled and will be rescheduled for the fall. The Mechanical Engineering Student Awards Banquet , scheduled for May 15, has been canceled. Awards will be announced virtually.

, scheduled for May 15, has been canceled. Awards will be announced virtually. The Office of Engineering Outreach Programs (OEOP) has canceled its SEED Academy program through May 15. This includes the SEED Academy Spring Final Symposium on May 9. OEOP will continue to communicate with SEED Academy students and parents via email and through The Sprout newsletter to offer information on course, project, and engagement options.

The 2020 Brazil Conference at MIT and Harvard has been canceled. More information can be found at brazilconference.org.

has been canceled. More information can be found at brazilconference.org. The March 12 Starr Forum, titled “Russia’s Putin: From Silent Coup to Legal Dictatorship,” has been changed to a live webcast.

has been changed to a live webcast. The March 13 Myron Weiner Seminar on International Migration, titled “Future Aspirations Among Refugee Youth in Turkey Between Integration & Mobility,” has been canceled.

has been canceled. The MIT Sloan School of Management is canceling all international study tours and treks. Student conferences are either being cancelled or modified: The March 7 Robo-AI Exchange Conference , the March 13 New Space Age Conference , and the April 2 Golub Center for Finance and Policy discussion on equity market structure with the SEC are canceled. The March 13 ETA Summit and the April 17 Ops Sim Competition are proceeding, with virtualization. The March 16 Entrepreneurship and Innovation Alumni gathering in San Franciso is also canceled.

, the March 13 , and the April 2 on equity market structure with the SEC are canceled. The March 13 and the April 17 are proceeding, with virtualization. The March 16 in San Franciso is also canceled. The 2020 MIT Scholarship and UROP Brunch that was scheduled for April 4 has been canceled.

The MIT Campaign for a Better World event in Toronto, originally set for April 29, will be postponed.

The Program in Science, Technology, and Society’s Morison Lecture and Prize in Science, Technology, and Society, originally scheduled for April 14, 2020, 4 p.m.; E51-Wong Auditorium, has been rescheduled for Oct. 1, 2020.

originally scheduled for April 14, 2020, 4 p.m.; E51-Wong Auditorium, has been rescheduled for Oct. 1, 2020. The Women's and Gender Studies Program's Women Take the Reel Series film event,"" Warrior Women ,” scheduled for March 12 at 6:30 p.m., has been postponed until fall 2020.

,” scheduled for March 12 at 6:30 p.m., has been postponed until fall 2020. The MIT Graduate Alumni Gathering , scheduled for March 20–21 in Cambridge, has been postponed, with plans for rescheduling to a later date in 2021.

, scheduled for March 20–21 in Cambridge, has been postponed, with plans for rescheduling to a later date in 2021. The MIT Student Alumni Association’s Dinner with 12 Strangers event series, set to be held in Cambridge and Boston, has been cancelled for the spring semester.

with 12 Strangers event series, set to be held in Cambridge and Boston, has been cancelled for the spring semester. The Forum on the Future of Cities: Urban Climate hosted by the Senseable City Lab and the World Economic Forum, scheduled for April 6th, will be postponed until the Fall of 2020, with a date to be decided later.

hosted by the Senseable City Lab and the World Economic Forum, scheduled for April 6th, will be postponed until the Fall of 2020, with a date to be decided later. Digital Humanities @ MIT , scheduled for March 12, has been postponed until a date to be announced.

, scheduled for March 12, has been postponed until a date to be announced. The MacVicar Day symposium and reception have been canceled.

symposium and reception have been canceled. The MIT Statistics and Data Science Conference (SDSCon 2020) scheduled for April 3, has been postponed to fall 2020. More information can be found at sdscon.mit.edu.

(SDSCon 2020) scheduled for April 3, has been postponed to fall 2020. More information can be found at sdscon.mit.edu. J-PAL North America's Fourth Annual State and Local Innovation Initiative Convening: Charting the Next Decade of Evidence Generation in State and Local Governments has been postponed. The event was originally scheduled to take place March 19-20 at MIT and will now be rescheduled for November 2020.

has been postponed. The event was originally scheduled to take place March 19-20 at MIT and will now be rescheduled for November 2020. The Department of Electrical Engineering and Computer Science (EECS) has canceled the SuperUROP information session that had been scheduled for Wednesday, March 11, from 4-5 p.m. A video of the March 3 information session will be available shortly on the SuperUROP home page.

The Department of Electrical Engineering and Computer Science has canceled Masterworks, the April 23 EECS poster session showcasing research by current and recent master's students. There are no plans to reschedule or virtualize the event.

This list of events was last updated on March 11."
209;machinelearningmastery.com;http://machinelearningmastery.com/support-vector-machines-for-machine-learning/;2016-04-19;Support Vector Machines for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.

They were extremely popular around the time they were developed in the 1990s and continue to be the go-to method for a high-performing algorithm with little tuning.

In this post you will discover the Support Vector Machine (SVM) machine learning algorithm. After reading this post you will know:

How to disentangle the many names used to refer to support vector machines.

The representation used by SVM when the model is actually stored on disk.

How a learned SVM model representation can be used to make predictions for new data.

How to learn an SVM model from training data.

How to best prepare your data for the SVM algorithm.

Where you might look to get more information on SVM.

SVM is an exciting algorithm and the concepts are relatively simple. This post was written for developers with little or no background in statistics and linear algebra.

As such we will stay high-level in this description and focus on the specific implementation concerns. The question around why specific equations are used or how they were derived are not covered and you may want to dive deeper in the further reading section.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Maximal-Margin Classifier

The Maximal-Margin Classifier is a hypothetical classifier that best explains how SVM works in practice.

The numeric input variables (x) in your data (the columns) form an n-dimensional space. For example, if you had two input variables, this would form a two-dimensional space.

A hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions you can visualize this as a line and let’s assume that all of our input points can be completely separated by this line. For example:

B0 + (B1 * X1) + (B2 * X2) = 0

Where the coefficients (B1 and B2) that determine the slope of the line and the intercept (B0) are found by the learning algorithm, and X1 and X2 are the two input variables.

You can make classifications using this line. By plugging in input values into the line equation, you can calculate whether a new point is above or below the line.

Above the line, the equation returns a value greater than 0 and the point belongs to the first class (class 0).

Below the line, the equation returns a value less than 0 and the point belongs to the second class (class 1).

A value close to the line returns a value close to zero and the point may be difficult to classify.

If the magnitude of the value is large, the model may have more confidence in the prediction.

The distance between the line and the closest data points is referred to as the margin. The best or optimal line that can separate the two classes is the line that as the largest margin. This is called the Maximal-Margin hyperplane.

The margin is calculated as the perpendicular distance from the line to only the closest points. Only these points are relevant in defining the line and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane.

The hyperplane is learned from training data using an optimization procedure that maximizes the margin.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Soft Margin Classifier

In practice, real data is messy and cannot be separated perfectly with a hyperplane.

The constraint of maximizing the margin of the line that separates the classes must be relaxed. This is often called the soft margin classifier. This change allows some points in the training data to violate the separating line.

An additional set of coefficients are introduced that give the margin wiggle room in each dimension. These coefficients are sometimes called slack variables. This increases the complexity of the model as there are more parameters for the model to fit to the data to provide this complexity.

A tuning parameter is introduced called simply C that defines the magnitude of the wiggle allowed across all dimensions. The C parameters defines the amount of violation of the margin allowed. A C=0 is no violation and we are back to the inflexible Maximal-Margin Classifier described above. The larger the value of C the more violations of the hyperplane are permitted.

During the learning of the hyperplane from data, all training instances that lie within the distance of the margin will affect the placement of the hyperplane and are referred to as support vectors. And as C affects the number of instances that are allowed to fall within the margin, C influences the number of support vectors used by the model.

The smaller the value of C, the more sensitive the algorithm is to the training data (higher variance and lower bias).

The larger the value of C, the less sensitive the algorithm is to the training data (lower variance and higher bias).

Support Vector Machines (Kernels)

The SVM algorithm is implemented in practice using a kernel.

The learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra, which is out of the scope of this introduction to SVM.

A powerful insight is that the linear SVM can be rephrased using the inner product of any two given observations, rather than the observations themselves. The inner product between two vectors is the sum of the multiplication of each pair of input values.

For example, the inner product of the vectors [2, 3] and [5, 6] is 2*5 + 3*6 or 28.

The equation for making a prediction for a new input using the dot product between the input (x) and each support vector (xi) is calculated as follows:

f(x) = B0 + sum(ai * (x,xi))

This is an equation that involves calculating the inner products of a new input vector (x) with all support vectors in training data. The coefficients B0 and ai (for each input) must be estimated from the training data by the learning algorithm.

Linear Kernel SVM

The dot-product is called the kernel and can be re-written as:

K(x, xi) = sum(x * xi)

The kernel defines the similarity or a distance measure between new data and the support vectors. The dot product is the similarity measure used for linear SVM or a linear kernel because the distance is a linear combination of the inputs.

Other kernels can be used that transform the input space into higher dimensions such as a Polynomial Kernel and a Radial Kernel. This is called the Kernel Trick.

It is desirable to use more complex kernels as it allows lines to separate the classes that are curved or even more complex. This in turn can lead to more accurate classifiers.

Polynomial Kernel SVM

Instead of the dot-product, we can use a polynomial kernel, for example:

K(x,xi) = 1 + sum(x * xi)^d

Where the degree of the polynomial must be specified by hand to the learning algorithm. When d=1 this is the same as the linear kernel. The polynomial kernel allows for curved lines in the input space.

Radial Kernel SVM

Finally, we can also have a more complex radial kernel. For example:

K(x,xi) = exp(-gamma * sum((x – xi^2))

Where gamma is a parameter that must be specified to the learning algorithm. A good default value for gamma is 0.1, where gamma is often 0 < gamma < 1. The radial kernel is very local and can create complex regions within the feature space, like closed polygons in two-dimensional space.

How to Learn a SVM Model

The SVM model needs to be solved using an optimization procedure.

You can use a numerical optimization procedure to search for the coefficients of the hyperplane. This is inefficient and is not the approach used in widely used SVM implementations like LIBSVM. If implementing the algorithm as an exercise, you could use stochastic gradient descent.

There are specialized optimization procedures that re-formulate the optimization problem to be a Quadratic Programming problem. The most popular method for fitting SVM is the Sequential Minimal Optimization (SMO) method that is very efficient. It breaks the problem down into sub-problems that can be solved analytically (by calculating) rather than numerically (by searching or optimizing).

Data Preparation for SVM

This section lists some suggestions for how to best prepare your training data when learning an SVM model.

Numerical Inputs : SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables (one variable for each category).

: SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables (one variable for each category). Binary Classification: Basic SVM as described in this post is intended for binary (two-class) classification problems. Although, extensions have been developed for regression and multi-class classification.

Further Reading

Support Vector Machines are a huge area of study. There are numerous books and papers on the topic. This section lists some of the seminal and most useful results if you are looking to dive deeper into the background and theory of the technique.

Vladimir Vapnik, one of the inventors of the technique has two books that are considered seminal on the topic. They are very mathematical and also rigorous.

Any good book on machine learning will cover SVM, below are some of my favorites.

There are countless tutorials and journal articles on SVM. Below is a link to a seminal paper on SVM by Cortes and Vapnik and another to an excellent introductory tutorial.

Wikipedia provides some good (although dense) information on the topic:

Finally, there are a lot of posts on Q&A sites asking for simple explanations of SVM, below are two picks that you might find useful.

Summary

In this post you discovered the Support Vector Machine Algorithm for machine learning. You learned about:

The Maximal-Margin Classifier that provides a simple theoretical model for understanding SVM.

The Soft Margin Classifier which is a modification of the Maximal-Margin Classifier to relax the margin to handle noisy class boundaries in real data.

Support Vector Machines and how the learning algorithm can be reformulated as a dot-product kernel and how other kernels like Polynomial and Radial can be used.

How you can use numerical optimization to learn the hyperplane and that efficient implementations use an alternate optimization scheme called Sequential Minimal Optimization.

Do you have any questions about SVM or this post?

Ask in the comments and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
210;machinelearningmastery.com;https://machinelearningmastery.com/how-to-handle-big-p-little-n-p-n-in-machine-learning/;2020-04-14;How to Handle Big-p, Little-n (p >> n) in Machine Learning;"Tweet Share Share

What if I have more Columns than Rows in my dataset?

Machine learning datasets are often structured or tabular data comprised of rows and columns.

The columns that are fed as input to a model are called predictors or “p” and the rows are samples “n“. Most machine learning algorithms assume that there are many more samples than there are predictors, denoted as p << n.

Sometimes, this is not the case, and there are many more predictors than samples in the dataset, referred to as “big-p, little-n” and denoted as p >> n. These problems often require specialized data preparation and modeling algorithms to address them correctly.

In this tutorial, you will discover the challenge of big-p, little n or p >> n machine learning problems.

After completing this tutorial, you will know:

Most machine learning problems have many more samples than predictors and most machine learning algorithms make this assumption during the training process.

Some modeling problems have many more predictors than samples, referred to as p >> n.

Algorithms to explore when modeling machine learning datasets with more predictors than samples.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Predictors (p) and Samples (n)

Machine Learning Assumes p << n

How to Handle p >> n

Predictors (p) and Samples (n)

Consider a predictive modeling problem, such as classification or regression.

The dataset is structured data or tabular data, like what you might see in an Excel spreadsheet.

There are columns and rows. Most of the columns would be used as inputs to a model and one column would represent the output or variable to be predicted.

The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable—in this case, sales—is often called the response or dependent variable, and is typically denoted using the symbol Y.

— Page 15, An Introduction to Statistical Learning with Applications in R, 2017.

Each column represents a variable or one aspect of a sample. The columns that represent the inputs to the model are called predictors.

Each row represents one sample with values across each of the columns or features.

Predictors : Input columns of a dataset, also called input variables or features.

: Input columns of a dataset, also called input variables or features. Samples: Rows of a dataset, also called an observation, example, or instance.

It is common to describe a training dataset in machine learning in terms of the predictors and samples.

The number of predictors in a dataset is described using the term “p” and the number of samples in a dataset is described using the term “n” or sometimes “N“.

p : The number of predictors in a dataset.

: The number of predictors in a dataset. n: The number of samples in a dataset.

To make this concrete, let’s take a look at the iris flowers classification problem.

Below is a sample of the first five rows of this dataset.

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ... 1 2 3 4 5 6 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ...

This dataset has five columns and 150 rows.

The first four columns are inputs and the fifth column is the output, meaning that there are four predictors.

We would describe the iris flowers dataset as:

p=4, n=150.

Machine Learning Assumes p << n

It is almost always the case that the number of predictors (p) will be smaller than the number of samples (n).

Often much smaller.

We can summarize this expectation as p << n, where “<<” is a mathematical inequality that means “much less than.”

p << n: Typically we have fewer predictors than samples.

To demonstrate this, let’s look at a few more standard machine learning datasets:

Most machine learning algorithms operate based on the assumption that there are many more samples than predictors.

One way to think about predictors and samples is to take a geometrical perspective.

Consider a hypercube where the number of predictors (p) defines the number of dimensions of the hypercube. The volume of this hypercube is the scope of possible samples that could be drawn from the domain. The number of samples (n) are the actual samples drawn from the domain that you must use to model your predictive modeling problem.

This is a rationale for the axiom “get as much data as possible” in applied machine learning. It is a desire to gather a sufficiently representative sample of the p-dimensional problem domain.

As the number of dimensions (p) increases, the volume of the domain increases exponentially. This, in turn, requires more samples (n) from the domain to provide effective coverage of the domain for a learning algorithm. We don’t need full coverage of the domain, just what is likely to be observable.

This challenge of effectively sampling high-dimensional spaces is generally referred to as the curse of dimensionality.

Machine learning algorithms overcome the curse of dimensionality by making assumptions about the data and structure of the mapping function from inputs to outputs. They add a bias.

The fundamental reason for the curse of dimensionality is that high-dimensional functions have the potential to be much more complicated than low-dimensional ones, and that those complications are harder to discern. The only way to beat the curse is to incorporate knowledge about the data that is correct.

— Page 15, Pattern Classification, 2000.

Many machine learning algorithms that use distance measures and other local models (in feature space) often degrade in performance as the number of predictors is increased.

When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large.

— Page 168, An Introduction to Statistical Learning with Applications in R, 2017.

It is not always the case that the number of predictors is less than the number of samples.

How to Handle p >> n

Some predictive modeling problems have more predictors than samples by definition.

Often many more predictors than samples.

This is often described as “big-p, little-n,” “large-p, small-n,” or more compactly as “p >> n”, where the “>>” is a mathematical inequality operator that means “much more than.”

… prediction problems in which the number of features p is much larger than the number of observations N, often written p >> N.

— Page 649, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2016.

Consider this from a geometrical perspective.

Now, instead of having a domain with tens of dimensions (or fewer), the domain has many thousands of dimensions and only a few tens of samples from this space. We cannot expect to have anything like a representative sample of the domain.

Many examples of p >> n problems come from the field of medicine, where there is a small patient population and a large number of descriptive characteristics.

At the same time, applications have emerged in which the number of experimental units is comparatively small but the underlying dimension is massive; illustrative examples might include image analysis, microarray analysis, document classification, astronomy and atmospheric science.

— Statistical challenges of high-dimensional data, 2009.

A common example of a p >> n problem is gene expression arrays, where there may be thousands of genes (predictors) and only tens of samples.

Gene expression arrays typically have 50 to 100 samples and 5,000 to 20,000 variables (genes).

— Expression Arrays and the p >> n Problem, 2003.

Given that most machine learning algorithms assume many more samples than predictors, this introduces a challenge when modeling.

Specifically, the assumptions made by standard machine learning models may cause the models to behave unexpectedly, provide misleading results, or fail completely.

… models cannot be used “out of the box”, since the standard fitting algorithms all require p<n; in fact the usual rule of thumb is that there be five or ten times as many samples as variables.

— Expression Arrays and the p >> n Problem, 2003.

A major problem with p >> n problems when using machine learning models is overfitting the training dataset.

Given the lack of samples, most models are unable to generalize and instead learn the statistical noise in the training data. This makes the model perform well on the training dataset but perform poorly on new examples from the problem domain.

This is also a hard problem to diagnose, as the lack of samples does not allow for a test or validation dataset by which model overfitting can be evaluated. As such, it is common to use leave-one-out style cross-validation (LOOCV) when evaluating models on p >> n problems.

There are many ways to approach a p >> n type classification or regression problem.

Some examples include:

Ignore p and n

One approach is to ignore the p and n relationship and evaluate standard machine learning models.

This might be considered the baseline method by which any other more specialized interventions may be compared.

Feature Selection

Feature selection involves selecting a subset of predictors to use as input to predictive models.

Common techniques include filter methods that select features based on their statistical relationship to the target variable (e.g. correlation), and wrapper methods that select features based on their contribution to a model when predicting the target variable (e.g. RFE).

A suite of feature selection methods could be evaluated and compared, perhaps applied in an aggressive manner to dramatically reduce the number of input features to those determined to be most critical.

Feature selection is an important scientific requirement for a classifier when p is large.

— Page 658, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2016.

For more on feature selection see the tutorial:

Projection Methods

Projection methods create a lower-dimensionality representation of samples that preserves the relationships observed in the data.

They are often used for visualization, although the dimensionality reduction nature of the techniques may also make them useful as a data transform to reduce the number of predictors.

This might include techniques from linear algebra, such as SVD and PCA.

When p > N, the computations can be carried out in an N-dimensional space, rather than p, via the singular value decomposition …

— Page 659, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2016.

It might also include manifold learning algorithms often used for visualization such as t-SNE.

Regularized Algorithms

Standard machine learning algorithms may be adapted to use regularization during the training process.

This will penalize models based on the number of features used or weighting of features, encouraging the model to perform well and minimize the number of predictors used in the model.

This can act as a type of automatic feature selection during training and may involve augmenting existing models (e.g regularized linear regression and regularized logistic regression) or the use of specialized methods such as LARS and LASSO.

There is no best method and it is recommended to use controlled experiments to test a suite of different methods.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

Books

Articles

Summary

In this tutorial, you discovered the challenge of big-p, little n or p >> n machine learning problems.

Specifically, you learned:

Machine learning datasets can be described in terms of the number of predictors (p) and the number of samples (n).

Most machine learning problems have many more samples than predictors and most machine learning algorithms make this assumption during the training process.

Some modeling problems have many more predictors than samples, such as problems from medicine, referred to as p >> n, and may require the use of specialized algorithms.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
211;machinelearningmastery.com;http://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/;2016-09-15;Tune Learning Rate for Gradient Boosting with XGBoost in Python;"# XGBoost on Otto dataset, Tune learning_rate

from pandas import read_csv

from xgboost import XGBClassifier

from sklearn . model_selection import GridSearchCV

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import LabelEncoder

import matplotlib

matplotlib . use ( 'Agg' )

from matplotlib import pyplot

# load data

data = read_csv ( 'train.csv' )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 94 ]

y = dataset [ : , 94 ]

# encode string class values as integers

label_encoded_y = LabelEncoder ( ) . fit_transform ( y )

# grid search

model = XGBClassifier ( )

learning_rate = [ 0.0001 , 0.001 , 0.01 , 0.1 , 0.2 , 0.3 ]

param_grid = dict ( learning_rate = learning_rate )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 )

grid_search = GridSearchCV ( model , param_grid , scoring = ""neg_log_loss"" , n_jobs = - 1 , cv = kfold )

grid_result = grid_search . fit ( X , label_encoded_y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :

print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )

# plot

pyplot . errorbar ( learning_rate , means , yerr = stds )

pyplot . title ( ""XGBoost learning_rate vs Log Loss"" )

pyplot . xlabel ( 'learning_rate' )

pyplot . ylabel ( 'Log Loss' )"
212;machinelearningmastery.com;https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/;2018-03-01;How to Calculate Principal Component Analysis (PCA) from Scratch in Python;"from numpy import array

from numpy import mean

from numpy import cov

from numpy . linalg import eig

# define a matrix

A = array ( [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] ] )

print ( A )

# calculate the mean of each column

M = mean ( A . T , axis = 1 )

print ( M )

# center columns by subtracting column means

C = A - M

print ( C )

# calculate covariance matrix of centered matrix

V = cov ( C . T )

print ( V )

# eigendecomposition of covariance matrix

values , vectors = eig ( V )

print ( vectors )

print ( values )

# project data

P = vectors . T . dot ( C . T )"
213;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-autoregressive-forecasting-models-for-multi-step-air-pollution-time-series-forecasting/;2018-10-16;How to Develop Multi-Step Time Series Forecasting Models for Air Pollution;"# autoregression forecast with global impute strategy

from numpy import loadtxt

from numpy import nan

from numpy import isnan

from numpy import count_nonzero

from numpy import unique

from numpy import array

from numpy import nanmedian

from statsmodels . tsa . arima_model import ARIMA

from matplotlib import pyplot

from warnings import catch_warnings

from warnings import filterwarnings

# split the dataset by 'chunkID', return a list of chunks

def to_chunks ( values , chunk_ix = 0 ) :

chunks = list ( )

# get the unique chunk ids

chunk_ids = unique ( values [ : , chunk_ix ] )

# group rows by chunk id

for chunk_id in chunk_ids :

selection = values [ : , chunk_ix ] == chunk_id

chunks . append ( values [ selection , : ] )

return chunks

# return a list of relative forecast lead times

def get_lead_times ( ) :

return [ 1 , 2 , 3 , 4 , 5 , 10 , 17 , 24 , 48 , 72 ]

# interpolate series of hours (in place) in 24 hour time

def interpolate_hours ( hours ) :

# find the first hour

ix = - 1

for i in range ( len ( hours ) ) :

if not isnan ( hours [ i ] ) :

ix = i

break

# fill-forward

hour = hours [ ix ]

for i in range ( ix + 1 , len ( hours ) ) :

# increment hour

hour += 1

# check for a fill

if isnan ( hours [ i ] ) :

hours [ i ] = hour % 24

# fill-backward

hour = hours [ ix ]

for i in range ( ix - 1 , - 1 , - 1 ) :

# decrement hour

hour -= 1

# check for a fill

if isnan ( hours [ i ] ) :

hours [ i ] = hour % 24

# return true if the array has any non-nan values

def has_data ( data ) :

return count_nonzero ( isnan ( data ) ) < len ( data )

# impute missing data

def impute_missing ( train_chunks , rows , hours , series , col_ix ) :

# impute missing using the median value for hour in all series

imputed = list ( )

for i in range ( len ( series ) ) :

if isnan ( series [ i ] ) :

# collect all rows across all chunks for the hour

all_rows = list ( )

for rows in train_chunks :

[ all_rows . append ( row ) for row in rows [ rows [ : , 2 ] == hours [ i ] ] ]

# calculate the central tendency for target

all_rows = array ( all_rows )

# fill with median value

value = nanmedian ( all_rows [ : , col_ix ] )

if isnan ( value ) :

value = 0.0

imputed . append ( value )

else :

imputed . append ( series [ i ] )

return imputed

# layout a variable with breaks in the data for missing positions

def variable_to_series ( chunk_train , col_ix , n_steps = 5 * 24 ) :

# lay out whole series

data = [ nan for _ in range ( n_steps ) ]

# mark all available data

for i in range ( len ( chunk_train ) ) :

# get position in chunk

position = int ( chunk_train [ i , 1 ] - 1 )

# store data

data [ position ] = chunk_train [ i , col_ix ]

return data

# fit AR model and generate a forecast

def fit_and_forecast ( series ) :

# define the model

model = ARIMA ( series , order = ( 2 , 0 , 0 ) )

# return a nan forecast in case of exception

try :

# ignore statsmodels warnings

with catch_warnings ( ) :

filterwarnings ( ""ignore"" )

# fit the model

model_fit = model . fit ( disp = False )

# forecast 72 hours

yhat = model_fit . predict ( len ( series ) , len ( series ) + 72 )

# extract lead times

lead_times = array ( get_lead_times ( ) )

indices = lead_times - 1

return yhat [ indices ]

except :

return [ nan for _ in range ( len ( get_lead_times ( ) ) ) ]

# forecast all lead times for one variable

def forecast_variable ( hours , train_chunks , chunk_train , chunk_test , lead_times , target_ix ) :

# convert target number into column number

col_ix = 3 + target_ix

# check for no data

if not has_data ( chunk_train [ : , col_ix ] ) :

forecast = [ nan for _ in range ( len ( lead_times ) ) ]

return forecast

# get series

series = variable_to_series ( chunk_train , col_ix )

# impute

imputed = impute_missing ( train_chunks , chunk_train , hours , series , col_ix )

# fit AR model and forecast

forecast = fit_and_forecast ( imputed )

return forecast

# forecast for each chunk, returns [chunk][variable][time]

def forecast_chunks ( train_chunks , test_input ) :

lead_times = get_lead_times ( )

predictions = list ( )

# enumerate chunks to forecast

for i in range ( len ( train_chunks ) ) :

# prepare sequence of hours for the chunk

hours = variable_to_series ( train_chunks [ i ] , 2 )

# interpolate hours

interpolate_hours ( hours )

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 39 ) :

yhat = forecast_variable ( hours , train_chunks , train_chunks [ i ] , test_input [ i ] , lead_times , j )

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# convert the test dataset in chunks to [chunk][variable][time] format

def prepare_test_forecasts ( test_chunks ) :

predictions = list ( )

# enumerate chunks to forecast

for rows in test_chunks :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 3 , rows . shape [ 1 ] ) :

yhat = rows [ : , j ]

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# calculate the error between an actual and predicted value

def calculate_error ( actual , predicted ) :

# give the full actual value if predicted is nan

if isnan ( predicted ) :

return abs ( actual )

# calculate abs difference

return abs ( actual - predicted )

# evaluate a forecast in the format [chunk][variable][time]

def evaluate_forecasts ( predictions , testset ) :

lead_times = get_lead_times ( )

total_mae , times_mae = 0.0 , [ 0.0 for _ in range ( len ( lead_times ) ) ]

total_c , times_c = 0 , [ 0 for _ in range ( len ( lead_times ) ) ]

# enumerate test chunks

for i in range ( len ( test_chunks ) ) :

# convert to forecasts

actual = testset [ i ]

predicted = predictions [ i ]

# enumerate target variables

for j in range ( predicted . shape [ 0 ] ) :

# enumerate lead times

for k in range ( len ( lead_times ) ) :

# skip if actual in nan

if isnan ( actual [ j , k ] ) :

continue

# calculate error

error = calculate_error ( actual [ j , k ] , predicted [ j , k ] )

# update statistics

total_mae += error

times_mae [ k ] += error

total_c += 1

times_c [ k ] += 1

# normalize summed absolute errors

total_mae /= total_c

times_mae = [ times_mae [ i ] / times_c [ i ] for i in range ( len ( times_mae ) ) ]

return total_mae , times_mae

# summarize scores

def summarize_error ( name , total_mae , times_mae ) :

# print summary

lead_times = get_lead_times ( )

formatted = [ '+%d %.3f' % ( lead_times [ i ] , times_mae [ i ] ) for i in range ( len ( lead_times ) ) ]

s_scores = ', ' . join ( formatted )

print ( '%s: [%.3f MAE] %s' % ( name , total_mae , s_scores ) )

# plot summary

pyplot . plot ( [ str ( x ) for x in lead_times ] , times_mae , marker = '.' )

pyplot . show ( )

# load dataset

train = loadtxt ( 'AirQualityPrediction/naive_train.csv' , delimiter = ',' )

test = loadtxt ( 'AirQualityPrediction/naive_test.csv' , delimiter = ',' )

# group data by chunks

train_chunks = to_chunks ( train )

test_chunks = to_chunks ( test )

# forecast

test_input = [ rows [ : , : 3 ] for rows in test_chunks ]

forecast = forecast_chunks ( train_chunks , test_input )

# evaluate forecast

actual = prepare_test_forecasts ( test_chunks )

total_mae , times_mae = evaluate_forecasts ( forecast , actual )

# summarize forecast"
214;www.statnews.com;https://www.statnews.com/2020/04/16/blood-clots-coronavirus-tpa/;2020-04-16;Blood clots leave clinicians with clues about Covid-19 — but no proven treatments;"Blood clots in severe Covid-19 patients leave clinicians with clues about the illness — but no proven treatments

Doctors treating the sickest Covid-19 patients have zeroed in on a new phenomenon: Some people have developed widespread blood clots, their lungs peppered with tiny blockages that prevent oxygen from pumping into the bloodstream and body.

A number of doctors are now trying to blast those clots with tPA, or tissue plasminogen activator, an antithrombotic drug typically reserved for treating strokes and heart attacks. Other doctors are eyeing the blood thinner heparin as a potential way to prevent clotting before it starts.

Without a rigorous study, though, it’s impossible to know the potential risks or benefits of tPA, blood thinners, or other drugs — or what makes a difference. Until more robust research gets underway, the body of evidence now is a handful of case reports and anecdotal observations on the use of drugs to combat clots.

advertisement

“I can’t stress enough that it is important to have a controlled study to demonstrate that people who get this either do or don’t do better,” said Christopher Barrett, a senior surgical resident at Beth Israel Deaconess Medical Center, a research fellow at MIT and co-author of case reports recently published on blood clots in Covid-19 patients.

As with so much else about the Covid-19 response, health experts are learning about the symptom on the fly. Blood clots are common in patients who are immobilized, but they seem to be smaller and cause far more severe damage in some Covid-19 patients. Doctors have said they see patients with blood clots forming not only in their lungs, but also in blood vessels. Autopsies have also revealed blood clots in kidneys and other organs, which some experts say suggests an overwhelming immune system response to the virus that inflicts harm on the body.

advertisement

Physicians from the U.S., the Netherlands, and China have published a number of case reports in scientific journals about Covid-19 patients with a multitude of small blood clots. In one report, researchers in China said 7 out of 10 patients who died of Covid-19 had small blood clots throughout the bloodstream, compared to fewer than 1 in 100 people who survived. Some of the patients in those case reports received blood thinners or tPA, sometimes when there seemed to be nothing else to try. Some survived, some did not.

“This is a real-time learning experience,” said Clyde Yancy, chief of cardiology at Northwestern University Feinberg School of Medicine.

“I don’t think any of us can declare anything definitively, but we know from the best available data that about one-third of patients who have Covid-19 infections do in fact have evidence of thrombotic disease,” he added. Yancy said there is early-stage, preliminary evidence to suggest that a regimen of anti-coagulants used as a preventive tool could reduce the number of clotting episodes a patient experiences.

It still isn’t clear why the virus leads to these blood clots forming, or why patients’ bodies can’t break them up. It also isn’t clear how significant a role they play in a patient’s illness. Those questions will take time to answer, Barrett said.

But there remains a need for treatments that can buy time to help people fight the virus.

“It’s not necessarily the virus killing people, it’s the organ failure that happens as a result of the viral infection,” Barrett said. “If you can support people through their organ failure, … the immune system will eventually clear out the virus.”

The three patients in Barrett’s case reports, all of whom were on ventilators to help them breathe, initially did better when they were given tPA in what’s known as off-label use in salvage therapy. One of them died, one of them improved briefly, and one of them had a durable response, he said.

Barrett is part of a group awaiting approval from the Food and Drug Administration to move forward with a randomized clinical trial to determine what if any role tPA might play. The trial they hope to conduct at three hospitals in Colorado, one in Massachusetts, and one in New York will give people the drug when they are not as sick as the people in the case reports, who had exhausted all other treatments. Patients will be randomly assigned to receive the drug or a placebo; the trial will also test different dosing.

“We really need the data to prove or disprove that it’s working.” Hunter Moore, transplant surgery fellow at University of Colorado, Denver

“Until then, we’re kind of handicapped,” said Hunter Moore, a transplant surgery fellow at the University of Colorado, Denver, and a researcher working on the trial with Barrett. Now, he said, “it’s all based on off-label use and it’s kind of hearsay in terms of how it’s done. So we really need the data to prove or disprove that it’s working.”

Doctors around the country are already giving patients heparin or tPA. Many reached out to Moore and Barrett after reading their case reports, hoping to try tPA on their own patients. At Mount Sinai Hospital in New York, five patients were given tPA, with mixed results, according to an Associated Press story that sparked strong reactions among some. Former FDA Commissioner Scott Gottlieb has called for more comprehensive research on the subject — which Moore and Barrett’s proposed study could provide.

The drug tPA does carry its own risk. It’s typically given to stroke patients within hours of symptoms to reduce the risk of bleeding in the brain. But Moore pointed out that the risk of those bleeds for patients on tPA is lower than for Covid-19 patients who are placed on ECMO machines to improve oxygen levels in their blood.

Yancy of Northwestern said any studies on blood clots will contribute to the picture of how cardiovascular conditions heighten the danger of Covid-19 infections. That, too, could shed light on the disproportionate burden on African Americans, whose infection rate is threefold higher than other Americans’ and whose death rate is sixfold higher.

Risk factors for Covid-19 infection such as hypertension, diabetes, obesity, and preexisting cardiovascular disease — all of which are more common in African Americans — tip the scales toward more serious illness. Socioeconomic factors that make it harder for some people to work from home also likely play a part. Blood clotting may be one more key factor.

“The reason for the increased infection rate likely has very little to do with race [but] more to do with the life and living circumstances for African Americans,” Yancy said."
215;machinelearningmastery.com;http://machinelearningmastery.com/stochastic-gradient-boosting-xgboost-scikit-learn-python/;2016-09-18;Stochastic Gradient Boosting with XGBoost and scikit-learn in Python;"# XGBoost on Otto dataset, tune subsample

from pandas import read_csv

from xgboost import XGBClassifier

from sklearn . model_selection import GridSearchCV

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import LabelEncoder

import matplotlib

matplotlib . use ( 'Agg' )

from matplotlib import pyplot

# load data

data = read_csv ( 'train.csv' )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 94 ]

y = dataset [ : , 94 ]

# encode string class values as integers

label_encoded_y = LabelEncoder ( ) . fit_transform ( y )

# grid search

model = XGBClassifier ( )

subsample = [ 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 1.0 ]

param_grid = dict ( subsample = subsample )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 )

grid_search = GridSearchCV ( model , param_grid , scoring = ""neg_log_loss"" , n_jobs = - 1 , cv = kfold )

grid_result = grid_search . fit ( X , label_encoded_y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :

print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )

# plot

pyplot . errorbar ( subsample , means , yerr = stds )

pyplot . title ( ""XGBoost subsample vs Log Loss"" )

pyplot . xlabel ( 'subsample' )

pyplot . ylabel ( 'Log Loss' )"
216;machinelearningmastery.com;http://machinelearningmastery.com/best-tune-multithreading-support-xgboost-python/;2016-09-04;How to Best Tune Multithreading Support for XGBoost in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35

# Otto, parallel cross validation from pandas import read_csv from xgboost import XGBClassifier from sklearn . model_selection import StratifiedKFold from sklearn . model_selection import cross_val_score from sklearn . preprocessing import LabelEncoder import time # load data data = read_csv ( 'train.csv' ) dataset = data . values # split data into X and y X = dataset [ : , 0 : 94 ] y = dataset [ : , 94 ] # encode string class values as integers label_encoded_y = LabelEncoder ( ) . fit_transform ( y ) # prepare cross validation kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 ) # Single Thread XGBoost, Parallel Thread CV start = time . time ( ) model = XGBClassifier ( nthread = 1 ) results = cross_val_score ( model , X , label_encoded_y , cv = kfold , scoring = 'neg_log_loss' , n_jobs = - 1 ) elapsed = time . time ( ) - start print ( ""Single Thread XGBoost, Parallel Thread CV: %f"" % ( elapsed ) ) # Parallel Thread XGBoost, Single Thread CV start = time . time ( ) model = XGBClassifier ( nthread = - 1 ) results = cross_val_score ( model , X , label_encoded_y , cv = kfold , scoring = 'neg_log_loss' , n_jobs = 1 ) elapsed = time . time ( ) - start print ( ""Parallel Thread XGBoost, Single Thread CV: %f"" % ( elapsed ) ) # Parallel Thread XGBoost and CV start = time . time ( ) model = XGBClassifier ( nthread = - 1 ) results = cross_val_score ( model , X , label_encoded_y , cv = kfold , scoring = 'neg_log_loss' , n_jobs = - 1 ) elapsed = time . time ( ) - start print ( ""Parallel Thread XGBoost and CV: %f"" % ( elapsed ) )"
217;machinelearningmastery.com;http://machinelearningmastery.com/best-programming-language-for-machine-learning/;2014-05-09;Best Programming Language for Machine Learning;"Tweet Share Share

Last Updated on September 27, 2016

A question I get asked a lot is:

What is the best programming language for machine learning?

I’ve replied to this question many times now it’s about time to explore this further in a blog post.

Ultimately, the programming language you use for machine learning should consider your own requirements and predilections. No one can meaningfully address those concerns for you.

No one can meaningfully address those concerns for you.

What Languages Are Being Used

Before I give you my opinion, it is good to have a look around to see what languages and platforms are popular in self-selected communities of data analysis and machine learning professionals.

KDnuggets has had language polls forever. A recent poll is titled “What programming/statistics languages you used for an analytics / data mining / data science work in 2013“. The trends are almost identical to the previous year. The results suggest heavy use of R and Python and SQL for data access. SAS and MATLAB rank higher than I would have expected. I’d expect SAS accounts for larger corporate (Fortune 500) data analysis and MATLAB for engineering, research and student use.

Kaggle offer machine learning competitions and have polled their user base as to the tools and programming languages used by participants in competitions. They posted results in 2011 titled Kagglers’ Favorite Tools (also see the forum discussion). The results suggested the abundant use of R. The results also show good use of MATLAB and SAS with much lower Python representation. I can attest that I prefer R over Python for competition work. It just feels though it has more on offer in terms of data analysis and algorithm selection.

Ben Hamner, Kaggle Admin and author of the blog post above on the Kaggle blog goes into more detail on the options when it comes to programming languages for machine learning in a forum post titled “What tools do people generally use to solve problems“.

Ben comments that MATLAB/Octave is a good language for matrix operations and can be good when working with a well defined feature matrix. Python is fragmented by comprehensive and can be very slow unless you drop into C. He prefers Python when not working with a well defined feature matrix and uses Pandas and NLTK. Ben comments that “As a general rule, if it’s found to be interesting for statisticians, it’s been implemented in R” (well said). He also complains about the language itself being ugly and painful to work with. Finally, Ben comments on Julia that doesn’t have much to offer in the way of libraries but is his new favorite language. He comments that it has the conciseness of languages like MATLAB and Python with the speed of C.

Anthony Goldbloom, the CEO of Kaggle gave a presentation to the Bay Area R user group in 2011 on the popularity of R in Kaggle competitions titled Predictive modeling competitions: making data science a sport (see the powerpoint slides). The presentation slides give more detail on the use of programming languages and suggest an Other category that is as close to as large as large as the usage of R. It would be nice to have the raw data that was collected (why didn’t they release it to their own data community, seriously!?).

John Langford on his blog Hunch has an excellent article on the properties of a programming language to consider when working with machine learning algorithms titled “Programming Languages for Machine Learning Implementations“. He divides the properties into concerns of speed and the concerns of programability (programming ease). He points to powerful industry standard implementations of algorithms, all in C and comments that he has not used R or MATLAB (the post was written 8 years ago). Take some time and read some of the comments by academics and industry specialists alike. This is a deep and nuanced problem that really comes down to the specifics of the problem you are solving and the environment in which you are solving it.

Machine Learning Languages

I think of programming languages in the context of the machine learning activities I want to perform.

MATLAB/Octave

I think MATLAB is excellent for representing and working with matrices. As such, I think it’s an excellent language or platform to use when climbing into the linear algebra of a given method. I think it’s suited to learning about algorithms both superficially the first time around and deeply when you are trying to figure something out or go deep into the method. For example, it’s popular in university courses for beginners, like Andrew Ng’s Coursera Machine Learning course.

R

R is a workhorse for statistical analysis and by extension machine learning. Much talk is given to the learning curve, I didn’t really see the problem. It is the platform to use to understand and explore your data using statistical methods and graphs. It has an enormous number of machine learning algorithms, and advanced implementations too written by the developers of the algorithm.

I think you can explore, model and prototype with R. I think it suits one-off projects with an artifact like a set of predictions, report or research paper. For example, it is the most popular platform for machine learning competitors such as Kaggle.

Python

Python if a popular scientific language and a rising star for machine learning. I’d be surprised if it can take the data analysis mantle from R, but matrix handling in NumPy may challenge MATLAB and communication tools like IPython are very attractive and a step into the future of reproducibility.

I think the SciPy stack for machine learning and data analysis can be used for one-off projects (like papers), and frameworks like scikit-learn are mature enough to be used in production systems.

Java-family/C-family

Implementing a system that uses machine learning is an engineering challenge like any other. You need good design and developed requirements. Machine learning is algorithms, not magic. When it comes to serious production implementations, you need a robust library or you customize an implementation of the algorithm for your needs.

There are robust libraries, for example, Java has Weka and Mahout. Also, note that the deeper implementations of core algorithms like regression (LIBLINEAR) and SVM (LIBSVM) are written in C and leveraged by Python and other toolkits. I think you are serious you may prototype in R or Python, but you will implement in a heavier language for reasons such as execution speed and system reliability. For example, the backend of BigML is implemented in Clojure.

Other Concerns

Not a Programmer : If you are not a programmer (or not a confident programmer) I recommend playing machine learning via a GUI interface like Weka.

: If you are not a programmer (or not a confident programmer) I recommend playing machine learning via a GUI interface like Weka. One Language for Research and Ops : You may want to use the same language for prototyping and for production to reduce risk of not effectively transferring the results.

: You may want to use the same language for prototyping and for production to reduce risk of not effectively transferring the results. Pet Language: You may have a pet language of favorite language and want to stick to that. You can implement algorithms yourself or leverage libraries. Most languages have some form of machine learning package, however primitive.

The question of machine learning programming language is popular on blogs and question and answer sites. A few choice discussions include:

What programming language do you use for machine learning and data analysis why do you recommend it?

I’m keen to hear your thoughts, leave a comment."
218;machinelearningmastery.com;https://machinelearningmastery.com/understand-machine-learning-algorithms-by-implementing-them-from-scratch/;2015-08-27;Understand Machine Learning Algorithms By Implementing Them From Scratch;"Tweet Share Share

Last Updated on August 13, 2019

Implementing machine learning algorithms from scratch seems like a great way for a programmer to understand machine learning.

And maybe it is.

But there some downsides to this approach too.

In this post you will discover some great resources that you can use to implement machine learning algorithms from scratch.

You will also discover some of the limitations of this seemingly perfect approach.

Discover how to code ML algorithms from scratch including kNN, decision trees, neural nets, ensembles and much more in my new book, with full Python code and no fancy libraries.

Have you implemented a machine learning algorithm from scratch in an effort to learn about it Leave a comment, I’d love to hear about your experience.

Benefits of Implementing Machine Learning Algorithms From Scratch

I promote the idea of implementing machine learning algorithms from scratch.

I think you can learn a lot about how algorithms work. I also think that as a developer, it provides a bridge into learning the mathematical notations, descriptions and intuitions used in machine learning.

I’ve discussed the benefits of implementing algorithms from scratch before in the post “Benefits of Implementing Machine Learning Algorithms From Scratch“.

In the post I listed the benefits as:

the understanding you gain the starting point it provides the ownership of the algorithm and code it forces

Also in that post I comment how you can short-cut the process by leveraging existing tutorials and books. There is a wealth of good resources for getting started, but there are also stumbling blocks to watch out for.

In the next section I point out three books that you can follow to implement machine learning algorithms from scratch.

I’ve helped a lot of programmers get started in machine learning over the last few years. From my experience, I list 5 of the most common stumbling blocks that I see tripping up programmers and the tactics that you can use to over come them.

Finally, you will discover 3 quick tips to getting the most from code tutorials and going from a copy-paste programmer (if you happen to be one) to truly diving down the rabbit hole of machine learning algorithms.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Great Books You Can Use To Implement Algorithms

I have implemented a lot of algorithms from scratch, directly from research papers. It can be very difficult.

It is a much gentler start to follow someone else’s tutorial.

There are many excellent resources that you can use to get started implementing machine learning algorithms from scratch.

Perhaps the most authoritative are books that guide you through tutorials.

There are many benefits to starting with a book. For example:

Someone else has figured out the algorithm and how to turn it into code.

You can use it as a known working starting point for tinkering and experimentation.

Some great books that guide you through implementing machine learning algorithms step-by-step are:

Data Science from Scratch: First Principles with Python by Joel Grus

This truly is from scratch, working through visualization, stats, probability, working with data and then 12 or so different machine learning algorithms.

This is one of my favorite beginner machine learning books from this year.

Machine Learning: An Algorithmic Perspective by Stephen Marsland

This is the long awaited second edition to this popular book. This covers a large number of diverse machine learning algorithms with implementations.

I like that it gives a mix of mathematical description, pseudo code as well as working source code.

Machine Learning in Action by Peter Harrington

This book works through the 10 most popular machine learning algorithms providing case study problems and worked code examples in Python.

I like that there is a good effort to tie the code to the descriptions using numbering and arrows.

Did I miss a good book that provides programming tutorials for implementing machine learning algorithms from scratch?

Let me know in the comments.

5 Stumbling Blocks When Implementing Algorithms From Scratch (and how to overcome them)

Implementing machine learning algorithms from scratch using tutorials is a lot of fun.

But there can be stumbling blocks, and if you’re not careful, they may trip you up and kill your motivation.

In this section I want to point out the 5 most common stumbling blocks that I see and how to roll with them and not let them hold you up. I want you to get unstuck and plow on (or move on to another tutorial).

Some good general advice for avoiding the stumbling blocks below is to carefully check the reviews of books (or the comments on blog posts) before diving into a tutorial. You want to be sure that the code works and that you’re not wasting your time.

Another general tactic is to dive-in no matter what and figure out the parts that are not working and re-implement them yourself. This is a great hack to force understanding, but it’s probably not for the beginner and you may require a good technical reference close at hand.

Anyway, let’s dive into the 5 common stumbling blocks with machine learning from scratch tutorials:

1) The Code Does Not Work

The worst and perhaps most common stumbling block is that the code in the example does not work.

In fact, if you spend some time in the book reviews on Amazon for some texts or in the comments of big blog posts, it’s clear that this problem is more prevalent than you think.

How does this happen? A few reasons come to mind that might give you clues to applying your own fixes and carrying on:

The code never worked . This means that the book was published without being carefully edited. Not much you can do here other than perhaps getting into the mind of the author and trying to figure out what they meant. Maybe even try contacting the author or the publisher.

. This means that the book was published without being carefully edited. Not much you can do here other than perhaps getting into the mind of the author and trying to figure out what they meant. Maybe even try contacting the author or the publisher. The language has moved on . This can happen, especially if the post is old or the book has been in print for a long time. Two good examples are the version of Ruby moving from 1.x to 2.x and Python moving from 2.x to 3.x.

. This can happen, especially if the post is old or the book has been in print for a long time. Two good examples are the version of Ruby moving from 1.x to 2.x and Python moving from 2.x to 3.x. The third-party libraries have moved on . This is for those cases where the implementations were not totally from scratch and some utility libraries were used, such as for plotting. This is often not that bad. You can often just update the code to use the latest version of the library and modify the arguments to meet the API changes. It may even be possible to install an older version of the library (if there are few or no dependencies that you might break in your development environment).

. This is for those cases where the implementations were not totally from scratch and some utility libraries were used, such as for plotting. This is often not that bad. You can often just update the code to use the latest version of the library and modify the arguments to meet the API changes. It may even be possible to install an older version of the library (if there are few or no dependencies that you might break in your development environment). The dataset has moved on. This can happen if the data file is a URL and is no longer available (perhaps you can find the file elsewhere). It is much worse if the example is coded against a third-party API data source like Facebook or Twitter. These APIs can change a lot and quickly. Your best bet is to understand the most recent version of the API and adapt the code example, if possible.

A good general tactic if the code does not work is to look for the associated errata if it is a book, GitHub repository, code downloads or similar. Sometimes the problems have been fixed and are available on the book or author’s website. Some simple Googling should turn it up.

2) Poor Descriptions Of Code

I think the second worst stumbling block when implementing algorithms from scratch is when the descriptions provided with the code are bad.

These types of problems are particularly not good for a beginner, because you are trying your best to stay motivated and actually learn something from the exercise. All of that goes down in smoke if the code and text do not align.

I (perhaps kindly) call them “bad descriptions” because there may be many symptoms and causes. For example:

A mismatch between code and description . This may have been caused by the code and text being prepared at different times and not being correctly edited together. It may be something small like a variable name change or it may be whole function names or functions themselves.

. This may have been caused by the code and text being prepared at different times and not being correctly edited together. It may be something small like a variable name change or it may be whole function names or functions themselves. Missing explanations . Sometimes you are given large slabs of code that you are expected to figure out. This is frustrating, especially in a book where it’s page after page of code that would be easier to understand on the screen. If this is the case, you might be better off finding the online download for the code and working with it directly.

. Sometimes you are given large slabs of code that you are expected to figure out. This is frustrating, especially in a book where it’s page after page of code that would be easier to understand on the screen. If this is the case, you might be better off finding the online download for the code and working with it directly. Terse explanations. Sometimes you get explanations of the code, but they are too brief, like “uses information gain” or whatever. Frustrating! You still may have enough to research the term, but it would be much easier if the author had included an explanation in the context and relevant to the example.

A good general tactic is to look up description for the algorithm in other resources and try to map them onto the code you are working with. Essentially, try to build your own descriptions for the code.

This just might not be an option for a beginner and you may need to move on to another resource.

3) Code is not Idiomatic

We programmers can be pedantic about the “correct” use of our languages (e.g. Python code is not Pythonic). This is a good thing, it shows good attention to detail and best practices.

When sample code is not idiomatic to the language in which it is written it can be off putting. Sometimes it can be so distracting that the code can be unreadable.

There are many reasons that this may be the case, for example:

Port from another language . The sample code may be a port from another programming language. Such as FORTRAN in Java or C in Python. To a trained eye, this can be obvious.

. The sample code may be a port from another programming language. Such as FORTRAN in Java or C in Python. To a trained eye, this can be obvious. Author is learning the language . Sometimes the author may use a book or tutorial project to learn a language. This can be manifest by inconsistency throughout the code examples. This can be frustrating and even distracting when examples are verbose making poor use of language features and API.

. Sometimes the author may use a book or tutorial project to learn a language. This can be manifest by inconsistency throughout the code examples. This can be frustrating and even distracting when examples are verbose making poor use of language features and API. Author has not used the language professionally. This can be more subtle to spot and can be manifest by the use of esoteric language features and APIs. This can be confusing when you have to research or decode the strange code.

If idiomatic code is deeply important to you, these stumbling blocks could be an opportunity. You could port the code from the “Java-Python” hybrid (or whatever) to a pure Pythonic implementation.

In so doing, you would gain a deeper understanding for the algorithm and more ownership over the code.

4) Code is not Connected to the Math

A good code example or tutorial will provide a bridge from the mathematical description to the code.

This is important because it allows you to travel across and start to build an intuition for the notation and the concise mathematical descriptions.

There problem is, sometimes this bridge may be broken or missing completely.

Errors in the math. This is insidious for the beginner that is already straining to build connections from the math to the code. Incorrect math can mislead or worse consume vast amounts of time with no pay off. Knowing that it is possible, is a good start.

This is insidious for the beginner that is already straining to build connections from the math to the code. Incorrect math can mislead or worse consume vast amounts of time with no pay off. Knowing that it is possible, is a good start. Terse mathematical description . Equations may be littered around the sample code, leaving it to you to figure out what it is and how it relates to the code. You have few options, you could just treat it as a math free example and refer to a different more complete reference text, or you could put in effort to relate the math to the code yourself. This is more likely by authors that are not familiar with the mathematical description of the algorithm and seemingly drop it in as an after thought.

. Equations may be littered around the sample code, leaving it to you to figure out what it is and how it relates to the code. You have few options, you could just treat it as a math free example and refer to a different more complete reference text, or you could put in effort to relate the math to the code yourself. This is more likely by authors that are not familiar with the mathematical description of the algorithm and seemingly drop it in as an after thought. Missing mathematics. Some references are math free, by design. In this case you may need to find your own reference text and build the bridge yourself. This is probably not for beginners, but it is a skill well worth investing the time into.

A beginner might want to stick with code and ignore the math, to build confidence and momentum. Later, it will pay to invest in a high-quality reference text and start relating the code to the math.

You want to get good at relating the algebra to standard code constructs and build an intuition for the process involved. It’s an applied skill. You need to put in the work and practice.

5) Incomplete Code Listing

We saw in 2) that you can have no descriptions and long listings of code. This problem can be inverted where you don’t have enough code. This is the case when the code listing is incomplete.

I am a big believer in complete code listings. I think the code listing should give you everything you need to give a “complete” and working implementation, even if it is the simplest possible case.

You can build on a simple case, you can’t run an incomplete example. You have to put in work and tie it all together.

Some reasons that this stumbling block may be the case, are:

Elaborate descriptions . Verbose writing can be a sign of incomplete thinking. Not always, but sometimes. If something is not well understood there may be an implicit attempt to cover it up with a wash of words. If there is no code at all, you could take it as a challenge to design the algorithm from the description and corroborate it from other descriptions and resources.

. Verbose writing can be a sign of incomplete thinking. Not always, but sometimes. If something is not well understood there may be an implicit attempt to cover it up with a wash of words. If there is no code at all, you could take it as a challenge to design the algorithm from the description and corroborate it from other descriptions and resources. Code snippets . Concepts may be elaborately described then demonstrated with a small code snippet. This can help to closely tie the concept to the code snippet, but it requires a lot of work on your behalf to tie it all together into a working system.

. Concepts may be elaborately described then demonstrated with a small code snippet. This can help to closely tie the concept to the code snippet, but it requires a lot of work on your behalf to tie it all together into a working system. No sample output. A key aspect often missing from code examples is a sample output. If present, this can give you an unambiguous idea of what to expect when you run it. Without a sample output, it’s a total guess.

In some situations, having to tie code together yourself might present an interesting challenge. Again, not suitable for the beginner, but perhaps a fun exercise later once you have some algorithms under your belt.

3 Tips to Get The Most From Implementing Algorithms

You may implement a fair number of algorithms. Once you do a few you may do a few more and before you know it, you’ve built your own little library of algorithms that you understand intimately.

In this section I wan to give you 3 quick tips that you can use to get the most out of your experiences implementing machine learning algorithms.

Add advanced features. Take your working code example and build on it. If the tutorial is any good, it will list ideas for extension. If not, you can research some yourself. List a number of candidate extensions to the algorithm and implement them, one-by-one. This will force you to at least understand the code enough to make the modification. Adapt to another problem. Run the algorithm on a different dataset. Fix any issues if it breaks. Go further and adapt the implementation to a different problem. If the code example was two-class classification, update it for multi-class classification or regression. Visualize algorithm behavior. I find plotting algorithm performance and behavior in real-time a very valuable learning tool, even today. You can start out by plotting at the epoch-level (all algorithms are iterative at some level) accuracy on the test and training datasets. From there, you can pick out algorithm specific visualizations, like the 2D grids of a self-organizing map, the coefficients on a time series in regression, and a voronoi tessellation for a k-Nearest Neighbors algorithm.

I think these tips will allow you to go a lot further than the tutorials and code examples.

This last point especially will give you deep insights into algorithm behavior that few practitioners take the time to acquire.

Your Action Step

This is a long post and you have learned how to get the most from implementing machine learning algorithms from scratch.

Importantly, you have learned about the most common stumbling blocks, some framings for how they might come about and some tactics you can use to turn them into opportunities.

Your next step is obvious: start implementing algorithms from scratch.

Unsure of where to start?

Start right here, I have a gentle tutorial for implementing the k-Nearest Neighbors algorithm in Python.

Still unsure?

Pick up a copy of Data Science from Scratch: First Principles with Python. You won’t regret it.

Share your experiences

Leave a comment and let me know about your experiences implementing machine learning algorithms from scratch.

Discover How to Code Algorithms From Scratch! No Libraries, Just Python Code. ...with step-by-step tutorials on real-world datasets Discover how in my new Ebook:

Machine Learning Algorithms From Scratch It covers 18 tutorials with all the code for 12 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Stochastic Gradient Descent and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
219;machinelearningmastery.com;http://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/;2016-04-07;Classification And Regression Trees for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Decision Trees are an important type of algorithm for predictive modeling machine learning.

The classical decision tree algorithms have been around for decades and modern variations like random forest are among the most powerful techniques available.

In this post you will discover the humble decision tree algorithm known by it’s more modern name CART which stands for Classification And Regression Trees. After reading this post, you will know:

The many names used to describe the CART algorithm for machine learning.

The representation used by learned CART models that is actually stored on disk.

How a CART model can be learned from training data.

How a learned CART model can be used to make predictions on unseen data.

Additional resources that you can use to learn more about CART and related algorithms.

If you have taken an algorithms and data structures course, it might be hard to hold you back from implementing this simple and powerful algorithm. And from there, you’re a small step away from your own implementation of Random Forests.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Update Aug 2017: Fixed a typo that indicated that Gini is the count of instances for a class, should have been the proportion of instances. Also updated to show Gini weighting for evaluating the split in addition to calculating purity for child nodes.

Decision Trees

Classification and Regression Trees or CART for short is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.

Classically, this algorithm is referred to as “decision trees”, but on some platforms like R they are referred to by the more modern term CART.

The CART algorithm provides a foundation for important algorithms like bagged decision trees, random forest and boosted decision trees.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

CART Model Representation

The representation for the CART model is a binary tree.

This is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).

The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.

Given a dataset with two inputs (x) of height in centimeters and weight in kilograms the output of sex as male or female, below is a crude example of a binary decision tree (completely fictitious for demonstration purposes only).

The tree can be stored to file as a graph or a set of rules. For example, below is the above decision tree as a set of rules.

If Height > 180 cm Then Male If Height <= 180 cm AND Weight > 80 kg Then Male If Height <= 180 cm AND Weight <= 80 kg Then Female Make Predictions With CART Models 1 2 3 4 If Height > 180 cm Then Male If Height <= 180 cm AND Weight > 80 kg Then Male If Height <= 180 cm AND Weight <= 80 kg Then Female Make Predictions With CART Models

With the binary tree representation of the CART model described above, making predictions is relatively straightforward.

Given a new input, the tree is traversed by evaluating the specific input started at the root node of the tree.

A learned binary tree is actually a partitioning of the input space. You can think of each input variable as a dimension on a p-dimensional space. The decision tree split this up into rectangles (when p=2 input variables) or some kind of hyper-rectangles with more inputs.

New data is filtered through the tree and lands in one of the rectangles and the output value for that rectangle is the prediction made by the model. This gives you some feeling for the type of decisions that a CART model is capable of making, e.g. boxy decision boundaries.

For example, given the input of [height = 160 cm, weight = 65 kg], we would traverse the above tree as follows:

Height > 180 cm: No Weight > 80 kg: No Therefore: Female 1 2 3 Height > 180 cm: No Weight > 80 kg: No Therefore: Female

Learn a CART Model From Data

Creating a CART model involves selecting input variables and split points on those variables until a suitable tree is constructed.

The selection of which input variable to use and the specific split or cut-point is chosen using a greedy algorithm to minimize a cost function. Tree construction ends using a predefined stopping criterion, such as a minimum number of training instances assigned to each leaf node of the tree.

Greedy Splitting

Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is used to divide the space called recursive binary splitting.

This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function. The split with the best cost (lowest cost because we minimize cost) is selected.

All input variables and all possible split points are evaluated and chosen in a greedy manner (e.g. the very best split point is chosen each time).

For regression predictive modeling problems the cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle:

sum(y – prediction)^2

Where y is the output for the training sample and prediction is the predicted output for the rectangle.

For classification the Gini index function is used which provides an indication of how “pure” the leaf nodes are (how mixed the training data assigned to each node is).

G = sum(pk * (1 – pk))

Where G is the Gini index over all classes, pk are the proportion of training instances with class k in the rectangle of interest. A node that has all classes of the same type (perfect class purity) will have G=0, where as a G that has a 50-50 split of classes for a binary classification problem (worst purity) will have a G=0.5.

For a binary classification problem, this can be re-written as:

G = 2 * p1 * p2

or

G = 1 – (p1^2 + p2^2)

The Gini index calculation for each node is weighted by the total number of instances in the parent node. The Gini score for a chosen split point in a binary classification problem is therefore calculated as follows:

G = ((1 – (g1_1^2 + g1_2^2)) * (ng1/n)) + ((1 – (g2_1^2 + g2_2^2)) * (ng2/n))

Where G is the Gini index for the split point, g1_1 is the proportion of instances in group 1 for class 1, g1_2 for class 2, g2_1 for group 2 and class 1, g2_2 group 2 class 2, ng1 and ng2 are the total number of instances in group 1 and 2 and n are the total number of instances we are trying to group from the parent node.

Stopping Criterion

The recursive binary splitting procedure described above needs to know when to stop splitting as it works its way down the tree with the training data.

The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.

The count of training members is tuned to the dataset, e.g. 5 or 10. It defines how specific to the training data the tree will be. Too specific (e.g. a count of 1) and the tree will overfit the training data and likely have poor performance on the test set.

Pruning The Tree

The stopping criterion is important as it strongly influences the performance of your tree. You can use pruning after learning your tree to further lift performance.

The complexity of a decision tree is defined as the number of splits in the tree. Simpler trees are preferred. They are easy to understand (you can print them out and show them to subject matter experts), and they are less likely to overfit your data.

The fastest and simplest pruning method is to work through each leaf node in the tree and evaluate the effect of removing it using a hold-out test set. Leaf nodes are removed only if it results in a drop in the overall cost function on the entire test set. You stop removing nodes when no further improvements can be made.

More sophisticated pruning methods can be used such as cost complexity pruning (also called weakest link pruning) where a learning parameter (alpha) is used to weigh whether nodes can be removed based on the size of the sub-tree.

Data Preparation for CART

CART does not require any special data preparation other than a good representation of the problem.

Further Reading

This section lists some resources that you can refer to if you are looking to go deeper with CART.

Below are some good machine learning texts that describe the CART algorithm from a machine learning perspective.

Summary

In this post you have discovered the Classification And Regression Trees (CART) for machine learning. You learned:

The classical name Decision Tree and the more Modern name CART for the algorithm.

The representation used for CART is a binary tree.

Predictions are made with CART by traversing the binary tree given a new input record.

The tree is learned using a greedy algorithm on the training data to pick splits in the tree.

Stopping criteria define how much tree learns and pruning can be used to improve a learned tree.

Do you have any questions about CART or this post?

Ask in the comments and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
220;news.mit.edu;http://news.mit.edu/2020/instrument-may-enable-mail-testing-detect-heavy-metals-water-0225;;Instrument may enable mail-in testing to detect heavy metals in water;"Lead, arsenic, and other heavy metals are increasingly present in water systems around the world due to human activities, such as pesticide use and, more recently, the inadequate disposal of electronic waste. Chronic exposure to even trace levels of these contaminants, at concentrations of parts per billion, can cause debilitating health conditions in pregnant women, children, and other vulnerable populations.

Monitoring water for heavy metals is a formidable task, however, particularly for resource-constrained regions where workers must collect many liters of water and chemically preserve samples before transporting them to distant laboratories for analysis.

To simplify the monitoring process, MIT researchers have developed an approach called SEPSTAT, for solid-phase extraction, preservation, storage, transportation, and analysis of trace contaminants. The method is based on a small, user-friendly device the team developed, which absorbs trace contaminants in water and preserves them in a dry state so the samples can be easily dropped in the mail and shipped to a laboratory for further analysis.



A whisk-like device lined with small pockets filled with gold polymer beads, fits inside a typical sampling bottle, and can be twirled to pick up any metal contaminants in water.

The device resembles a small, flexible propeller, or whisk, which fits inside a typical sampling bottle. When twirled inside the bottle for several minutes, the instrument can absorb most of the trace contaminants in the water sample. A user can either air-dry the device or blot it with a piece of paper, then flatten it and mail it in an envelope to a laboratory, where scientists can dip it in a solution of acid to remove the contaminants and collect them for further analysis in the lab.

“We initially designed this for use in India, but it’s taught me a lot about our own water issues and trace contaminants in the United States,” says device designer Emily Hanhauser, a graduate student in MIT’s Department of Mechanical Engineering. “For instance, someone who has heard about the water crisis in Flint, Michigan, who now wants to know what’s in their water, might one day order something like this online, do the test themselves, and send it to a lab.”

Hanhauser and her colleagues recently published their results in the journal Environmental Science and Technology. Her MIT co-authors are Chintan Vaishnav of the Tata Center for Technology and Design and the MIT Sloan School of Management; John Hart, associate professor of mechanical engineering; and Rohit Karnik, professor of mechanical engineering and associate department head for education, along with Michael Bono of Boston University.

From teabags to whisks

The team originally set out to understand the water monitoring infrastructure in India. Millions of water samples are collected by workers at local laboratories all around the country, which are equipped to perform basic water quality analysis. However, to analyze trace contaminants, workers at these local labs need to chemically preserve large numbers of water samples and transport the vessels, often over hundreds of kilometers, to state capitals, where centralized labs have facilities to properly analyze trace contaminants.

“If you’re collecting a lot of these samples and trying to bring them to a lab, it’s pretty onerous work, and there is a significant transportation barrier,” Hanhauser says.



After the device is pulled out and dried, it can preserve any metal contaminants that it has picked up, for long periods of time. The device can be flattened and mailed to a lab, where the contaminants can be further analyzed.

In looking to streamline the logistics of water monitoring, she and her colleagues wondered whether they could bypass the need to transport the water, and instead transport the contaminants by themselves, in a dry state.

They eventually found inspiration in dry blood spotting, a simple technique that involves pricking a person’s finger and collecting a drop of blood on a card of cellulose. When dried, the chemicals in the blood are stable and preserved, and the cards can be mailed off for further analysis, avoiding the need to preserve and ship large volumes of blood.

The team started thinking of a similar collection system for heavy metals, and looked through the literature for materials that could both absorb trace contaminants from water and keep them stable when dry.

They eventually settled on ion-exchange resins, a class of material that comes in the form of small polymer beads, several hundreds of microns wide. These beads contain groups of molecules bound to a hydrogen ion. When dipped in water, the hydrogen comes off and can be exchanged with another ion, such as a heavy metal cation, that takes hydrogen’s place on the bead. In this way, the beads can absorb heavy metals and other trace contaminants from water.

The researchers then looked for ways to immerse the beads in water, and first considered a teabag-like design. They filled a mesh-like pocket with beads and dunked it in water they spiked with heavy metals. They found, though, that it took days for the beads to adequately absorb the contaminants if they simply left the teabag in the water. When they stirred the teabag around, turbulence sped the process somewhat, but it still took far too long for the beads, packed into one large teabag, to absorb the contaminants.

Ultimately, Hanhauser found that a handheld stirring design worked best to take up metal contaminants in water within a reasonable amount of time. The device is made from a polymer mesh cut into several propeller-like panels. Within each panel, Hanhauser hand-stitched small pockets, which she filled with polymer beads. She then stitched each panel around a polymer stick to resemble a sort of egg beater or whisk.

Testing the waters

The researchers fabricated several of the devices, then tested them on samples of natural water collected around Boston, including the Charles and Mystic rivers. They spiked the samples with various heavy metal contaminants, such as lead, copper, nickel, and cadmium, then stuck a device in the bottle of each sample, and twirled it around by hand to catch and absorb the contaminants. They then placed the devices on a counter to dry overnight.

To recover the contaminants from the device, they dipped the device in hydrochloric acid. The hydrogen in the solution effectively knocks away any ions attached to the polymer beads, including heavy metals, which can then be collected and analyzed with instruments such as mass spectrometers.

The researchers found that by stirring the device in the water sample, the device was able to absorb and preserve about 94 percent of the metal contaminants in each sample. In their recent trials, they found they could still detect the contaminants and predict their concentrations in the original water samples, with an accuracy range of 10 to 20 percent, even after storing the device in a dry state for up to two years.

With a cost of less than $2, the researchers believe that the device could facilitate transport of samples to centralized laboratories, collection and preservation of samples for future analysis, and acquisition of water quality data in a centralized manner, which, in turn, could help to identify sources of contamination, guide policies, and enable improved water quality management.

The researchers have now partnered with a company in India, in hopes of commercializing the device. Together, their project was recently chosen as one of 26 proposals out of more than 950 to be funded by the Indian government under its Atal New India Challenge program.

This research was funded, in part, by the MIT Abdul Latif Jameel Water and Food Systems Lab, the MIT Tata Center, and the National Science Foundation."
221;machinelearningmastery.com;http://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/;2016-04-24;Boosting and AdaBoost for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers.

In this post you will discover the AdaBoost Ensemble method for machine learning. After reading this post, you will know:

What the boosting ensemble method is and generally how it works.

How to learn to boost decision trees using the AdaBoost algorithm.

How to make predictions using the learned AdaBoost model.

How to best prepare your data for use with the AdaBoost algorithm

This post was written for developers and assumes no background in statistics or mathematics. The post focuses on how the algorithm works and how to use it for predictive modeling problems. If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Boosting Ensemble Method

Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.

This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.

AdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting.

Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Learning An AdaBoost Model From Data

AdaBoost is best used to boost the performance of decision trees on binary classification problems.

AdaBoost was originally called AdaBoost.M1 by the authors of the technique Freund and Schapire. More recently it may be referred to as discrete AdaBoost because it is used for classification rather than regression.

AdaBoost can be used to boost the performance of any machine learning algorithm. It is best used with weak learners. These are models that achieve accuracy just above random chance on a classification problem.

The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps.

Each instance in the training dataset is weighted. The initial weight is set to:

weight(xi) = 1/n

Where xi is the i’th training instance and n is the number of training instances.

How To Train One Model

A weak classifier (decision stump) is prepared on the training data using the weighted samples. Only binary (two-class) classification problems are supported, so each decision stump makes one decision on one input variable and outputs a +1.0 or -1.0 value for the first or second class value.

The misclassification rate is calculated for the trained model. Traditionally, this is calculated as:

error = (correct – N) / N

Where error is the misclassification rate, correct are the number of training instance predicted correctly by the model and N is the total number of training instances. For example, if the model predicted 78 of 100 training instances correctly the error or misclassification rate would be (78-100)/100 or 0.22.

This is modified to use the weighting of the training instances:

error = sum(w(i) * terror(i)) / sum(w)

Which is the weighted sum of the misclassification rate, where w is the weight for training instance i and terror is the prediction error for training instance i which is 1 if misclassified and 0 if correctly classified.

For example, if we had 3 training instances with the weights 0.01, 0.5 and 0.2. The predicted values were -1, -1 and -1, and the actual output variables in the instances were -1, 1 and -1, then the terrors would be 0, 1, and 0. The misclassification rate would be calculated as:

error = (0.01*0 + 0.5*1 + 0.2*0) / (0.01 + 0.5 + 0.2)

or

error = 0.704

A stage value is calculated for the trained model which provides a weighting for any predictions that the model makes. The stage value for a trained model is calculated as follows:

stage = ln((1-error) / error)

Where stage is the stage value used to weight predictions from the model, ln() is the natural logarithm and error is the misclassification error for the model. The effect of the stage weight is that more accurate models have more weight or contribution to the final prediction.

The training weights are updated giving more weight to incorrectly predicted instances, and less weight to correctly predicted instances.

For example, the weight of one training instance (w) is updated using:

w = w * exp(stage * terror)

Where w is the weight for a specific training instance, exp() is the numerical constant e or Euler’s number raised to a power, stage is the misclassification rate for the weak classifier and terror is the error the weak classifier made predicting the output variable for the training instance, evaluated as:

terror = 0 if(y == p), otherwise 1

Where y is the output variable for the training instance and p is the prediction from the weak learner.

This has the effect of not changing the weight if the training instance was classified correctly and making the weight slightly larger if the weak learner misclassified the instance.

AdaBoost Ensemble

Weak models are added sequentially, trained using the weighted training data.

The process continues until a pre-set number of weak learners have been created (a user parameter) or no further improvement can be made on the training dataset.

Once completed, you are left with a pool of weak learners each with a stage value.

Making Predictions with AdaBoost

Predictions are made by calculating the weighted average of the weak classifiers.

For a new input instance, each weak learner calculates a predicted value as either +1.0 or -1.0. The predicted values are weighted by each weak learners stage value. The prediction for the ensemble model is taken as a the sum of the weighted predictions. If the sum is positive, then the first class is predicted, if negative the second class is predicted.

For example, 5 weak classifiers may predict the values 1.0, 1.0, -1.0, 1.0, -1.0. From a majority vote, it looks like the model will predict a value of 1.0 or the first class. These same 5 weak classifiers may have the stage values 0.2, 0.5, 0.8, 0.2 and 0.9 respectively. Calculating the weighted sum of these predictions results in an output of -0.8, which would be an ensemble prediction of -1.0 or the second class.

Data Preparation for AdaBoost

This section lists some heuristics for best preparing your data for AdaBoost.

Quality Data : Because the ensemble method continues to attempt to correct misclassifications in the training data, you need to be careful that the training data is of a high-quality.

: Because the ensemble method continues to attempt to correct misclassifications in the training data, you need to be careful that the training data is of a high-quality. Outliers : Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. These could be removed from the training dataset.

: Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. These could be removed from the training dataset. Noisy Data: Noisy data, specifically noise in the output variable can be problematic. If possible, attempt to isolate and clean these from your training dataset.

Further Reading

Below are some machine learning texts that describe AdaBoost from a machine learning perspective.

Below are some seminal and good overview research articles on the method that may be useful if you are looking to dive deeper into the theoretical underpinnings of the method:

Summary

In this post you discovered the Boosting ensemble method for machine learning. You learned about:

Boosting and how it is a general technique that keeps adding weak learners to correct classification errors.

AdaBoost as the first successful boosting algorithm for binary classification problems.

Learning the AdaBoost model by weighting training instances and the weak learners themselves.

Predicting with AdaBoost by weighting predictions from weak learners.

Where to look for more theoretical background on the AdaBoost algorithm.

If you have any questions about this post or the Boosting or the AdaBoost algorithm ask in the comments and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
222;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-deep-learning-models-for-univariate-time-series-forecasting/;2018-10-28;How to Develop Deep Learning Models for Univariate Time Series Forecasting;"# evaluate convlstm

from math import sqrt

from numpy import array

from numpy import mean

from numpy import std

from pandas import DataFrame

from pandas import concat

from pandas import read_csv

from sklearn . metrics import mean_squared_error

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import ConvLSTM2D

from matplotlib import pyplot

# split a univariate dataset into train/test sets

def train_test_split ( data , n_test ) :

return data [ : - n_test ] , data [ - n_test : ]

# transform list into supervised learning format

def series_to_supervised ( data , n_in = 1 , n_out = 1 ) :

df = DataFrame ( data )

cols = list ( )

# input sequence (t-n, ... t-1)

for i in range ( n_in , 0 , - 1 ) :

cols . append ( df . shift ( i ) )

# forecast sequence (t, t+1, ... t+n)

for i in range ( 0 , n_out ) :

cols . append ( df . shift ( - i ) )

# put it all together

agg = concat ( cols , axis = 1 )

# drop rows with NaN values

agg . dropna ( inplace = True )

return agg . values

# root mean squared error or rmse

def measure_rmse ( actual , predicted ) :

return sqrt ( mean_squared_error ( actual , predicted ) )

# difference dataset

def difference ( data , interval ) :

return [ data [ i ] - data [ i - interval ] for i in range ( interval , len ( data ) ) ]

# fit a model

def model_fit ( train , config ) :

# unpack config

n_seq , n_steps , n_filters , n_kernel , n_nodes , n_epochs , n_batch = config

n_input = n_seq * n_steps

# prepare data

data = series_to_supervised ( train , n_in = n_input )

train_x , train_y = data [ : , : - 1 ] , data [ : , - 1 ]

train_x = train_x . reshape ( ( train_x . shape [ 0 ] , n_seq , 1 , n_steps , 1 ) )

# define model

model = Sequential ( )

model . add ( ConvLSTM2D ( filters = n_filters , kernel_size = ( 1 , n_kernel ) , activation = 'relu' , input_shape = ( n_seq , 1 , n_steps , 1 ) ) )

model . add ( Flatten ( ) )

model . add ( Dense ( n_nodes , activation = 'relu' ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mse' , optimizer = 'adam' )

# fit

model . fit ( train_x , train_y , epochs = n_epochs , batch_size = n_batch , verbose = 0 )

return model

# forecast with a pre-fit model

def model_predict ( model , history , config ) :

# unpack config

n_seq , n_steps , _ , _ , _ , _ , _ = config

n_input = n_seq * n_steps

# prepare data

x_input = array ( history [ - n_input : ] ) . reshape ( ( 1 , n_seq , 1 , n_steps , 1 ) )

# forecast

yhat = model . predict ( x_input , verbose = 0 )

return yhat [ 0 ]

# walk-forward validation for univariate data

def walk_forward_validation ( data , n_test , cfg ) :

predictions = list ( )

# split dataset

train , test = train_test_split ( data , n_test )

# fit model

model = model_fit ( train , cfg )

# seed history with training dataset

history = [ x for x in train ]

# step over each time-step in the test set

for i in range ( len ( test ) ) :

# fit model and make forecast for history

yhat = model_predict ( model , history , cfg )

# store forecast in list of predictions

predictions . append ( yhat )

# add actual observation to history for the next loop

history . append ( test [ i ] )

# estimate prediction error

error = measure_rmse ( test , predictions )

print ( ' > %.3f' % error )

return error

# repeat evaluation of a config

def repeat_evaluate ( data , config , n_test , n_repeats = 30 ) :

# fit and evaluate the model n times

scores = [ walk_forward_validation ( data , n_test , config ) for _ in range ( n_repeats ) ]

return scores

# summarize model performance

def summarize_scores ( name , scores ) :

# print a summary

scores_m , score_std = mean ( scores ) , std ( scores )

print ( '%s: %.3f RMSE (+/- %.3f)' % ( name , scores_m , score_std ) )

# box and whisker plot

pyplot . boxplot ( scores )

pyplot . show ( )

series = read_csv ( 'monthly-car-sales.csv' , header = 0 , index_col = 0 )

data = series . values

# data split

n_test = 12

# define config

config = [ 3 , 12 , 256 , 3 , 200 , 200 , 100 ]

# grid search

scores = repeat_evaluate ( data , config , n_test )

# summarize scores"
223;machinelearningmastery.com;http://machinelearningmastery.com/how-to-run-your-first-classifier-in-weka/;2014-02-16;How to Run Your First Classifier in Weka;"Tweet Share Share

Last Updated on August 22, 2019

Weka makes learning applied machine learning easy, efficient, and fun. It is a GUI tool that allows you to load datasets, run algorithms and design and run experiments with results statistically robust enough to publish.

I recommend Weka to beginners in machine learning because it lets them focus on learning the process of applied machine learning rather than getting bogged down by the mathematics and the programming — those can come later.

In this post, I want to show you how easy it is to load a dataset, run an advanced classification algorithm and review the results.

If you follow along, you will have machine learning results in under 5 minutes, and the knowledge and confidence to go ahead and try more datasets and more algorithms.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

1. Download Weka and Install

Visit the Weka Download page and locate a version of Weka suitable for your computer (Windows, Mac, or Linux).

Weka requires Java. You may already have Java installed and if not, there are versions of Weka listed on the download page (for Windows) that include Java and will install it for you. I’m on a Mac myself, and like everything else on Mac, Weka just works out of the box.

If you are interested in machine learning, then I know you can figure out how to download and install software into your own computer. If you need help installing Weka, see the following post that provides step-by-step instructions:

2. Start Weka

Start Weka. This may involve finding it in program launcher or double clicking on the weka.jar file. This will start the Weka GUI Chooser.

The Weka GUI Chooser lets you choose one of the Explorer, Experimenter, KnowledgeExplorer and the Simple CLI (command line interface).

Click the “Explorer” button to launch the Weka Explorer.

This GUI lets you load datasets and run classification algorithms. It also provides other features, like data filtering, clustering, association rule extraction, and visualization, but we won’t be using these features right now.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

3. Open the data/iris.arff Dataset

Click the “Open file…” button to open a data set and double click on the “data” directory.

Weka provides a number of small common machine learning datasets that you can use to practice on.

Select the “iris.arff” file to load the Iris dataset.

The Iris Flower dataset is a famous dataset from statistics and is heavily borrowed by researchers in machine learning. It contains 150 instances (rows) and 4 attributes (columns) and a class attribute for the species of iris flower (one of setosa, versicolor, and virginica). You can read more about Iris flower dataset on Wikipedia.

4. Select and Run an Algorithm

Now that you have loaded a dataset, it’s time to choose a machine learning algorithm to model the problem and make predictions.

Click the “Classify” tab. This is the area for running algorithms against a loaded dataset in Weka.

You will note that the “ZeroR” algorithm is selected by default.

Click the “Start” button to run this algorithm.

The ZeroR algorithm selects the majority class in the dataset (all three species of iris are equally present in the data, so it picks the first one: setosa) and uses that to make all predictions. This is the baseline for the dataset and the measure by which all algorithms can be compared. The result is 33%, as expected (3 classes, each equally represented, assigning one of the three to each prediction results in 33% classification accuracy).

You will also note that the test options selects Cross Validation by default with 10 folds. This means that the dataset is split into 10 parts: the first 9 are used to train the algorithm, and the 10th is used to assess the algorithm. This process is repeated, allowing each of the 10 parts of the split dataset a chance to be the held-out test set. You can read more about cross validation here.

The ZeroR algorithm is important, but boring.

Click the “Choose” button in the “Classifier” section and click on “trees” and click on the “J48” algorithm.

This is an implementation of the C4.8 algorithm in Java (“J” for Java, 48 for C4.8, hence the J48 name) and is a minor extension to the famous C4.5 algorithm. You can read more about the C4.5 algorithm here.

Click the “Start” button to run the algorithm.

5. Review Results

After running the J48 algorithm, you can note the results in the “Classifier output” section.

The algorithm was run with 10-fold cross-validation: this means it was given an opportunity to make a prediction for each instance of the dataset (with different training folds) and the presented result is a summary of those predictions.

Firstly, note the Classification Accuracy. You can see that the model achieved a result of 144/150 correct or 96%, which seems a lot better than the baseline of 33%.

Secondly, look at the Confusion Matrix. You can see a table of actual classes compared to predicted classes and you can see that there was 1 error where an Iris-setosa was classified as an Iris-versicolor, 2 cases where Iris-virginica was classified as an Iris-versicolor, and 3 cases where an Iris-versicolor was classified as an Iris-setosa (a total of 6 errors). This table can help to explain the accuracy achieved by the algorithm.

Summary

In this post, you loaded your first dataset and ran your first machine learning algorithm (an implementation of the C4.8 algorithm) in Weka. The ZeroR algorithm doesn’t really count: it’s just a useful baseline.

You now know how to load the datasets that are provided with Weka and how to run algorithms: go forth and try different algorithms and see what you come up with.

Leave a note in the comments if you can achieve better than 96% accuracy on the Iris dataset.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
224;machinelearningmastery.com;http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/;2016-05-31;How To Compare Machine Learning Algorithms in Python with scikit-learn;"# Compare Algorithms

import pandas

import matplotlib . pyplot as plt

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

from sklearn . tree import DecisionTreeClassifier

from sklearn . neighbors import KNeighborsClassifier

from sklearn . discriminant_analysis import LinearDiscriminantAnalysis

from sklearn . naive_bayes import GaussianNB

from sklearn . svm import SVC

# load dataset

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

# prepare configuration for cross validation test harness

seed = 7

# prepare models

models = [ ]

models . append ( ( 'LR' , LogisticRegression ( ) ) )

models . append ( ( 'LDA' , LinearDiscriminantAnalysis ( ) ) )

models . append ( ( 'KNN' , KNeighborsClassifier ( ) ) )

models . append ( ( 'CART' , DecisionTreeClassifier ( ) ) )

models . append ( ( 'NB' , GaussianNB ( ) ) )

models . append ( ( 'SVM' , SVC ( ) ) )

# evaluate each model in turn

results = [ ]

names = [ ]

scoring = 'accuracy'

for name , model in models :

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

cv_results = model_selection . cross_val_score ( model , X , Y , cv = kfold , scoring = scoring )

results . append ( cv_results )

names . append ( name )

msg = ""%s: %f (%f)"" % ( name , cv_results . mean ( ) , cv_results . std ( ) )

print ( msg )

# boxplot algorithm comparison

fig = plt . figure ( )

fig . suptitle ( 'Algorithm Comparison' )

ax = fig . add_subplot ( 111 )

plt . boxplot ( results )

ax . set_xticklabels ( names )"
225;machinelearningmastery.com;https://machinelearningmastery.com/how-to-calculate-joint-marginal-and-conditional-probability/;2019-09-29;How to Develop an Intuition for Joint, Marginal, and Conditional Probability;"Tweet Share Share

Last Updated on December 6, 2019

Probability for a single random variable is straight forward, although it can become complicated when considering two or more variables.

With just two variables, we may be interested in the probability of two simultaneous events, called joint probability: the probability of one event given the occurrence of another event called the conditional probability, or just the probability of an event regardless of other variables, called the marginal probability.

These types of probability are easy to define but the intuition behind their meaning can take some time to sink in, requiring some worked examples that can be tinkered with.

In this tutorial, you will discover the intuitions behind calculating the joint, marginal, and conditional probability.

After completing this tutorial, you will know:

How to calculate joint, marginal, and conditional probability for independent random variables.

How to collect observations from joint random variables and construct a joint probability table.

How to calculate joint, marginal, and conditional probability from a joint probability table.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Update Oct/2019 : Fixed some minor typos, thanks Minel and jmy.

: Fixed some minor typos, thanks Minel and jmy. Update Dec/2019: Fixed typo in the description of the joint probability table.

Tutorial Overview

This tutorial is divided into three parts; they are:

Joint, Marginal, and Conditional Probabilities Probabilities of Rolling Two Dice Probabilities of Weather in Two Cities

Joint, Marginal, and Conditional Probabilities

Calculating probability is relatively straight forward when working with a single random variable.

It gets more interesting when considering two or more random variables, as we often do in many real world circumstances.

There are three main types of probabilities that we may be interested in calculating when working with two (or more) random variables.

Briefly, they are:

Joint Probability . The probability of simultaneous events.

. The probability of simultaneous events. Marginal Probability . The probability of an event irrespective of the other variables.

. The probability of an event irrespective of the other variables. Conditional Probability. The probability of events given the presence of other events.

The meaning and calculation of these different types of probabilities vary depending on whether the two random variables are independent (simpler) or dependent (more complicated).

We will explore how to calculate and interpret these three types of probability with worked examples.

In the next section, we will look at the independent rolls of two dice, and in the following section, we will look at the occurrence of weather events of two geographically close cities.

Probabilities of Rolling Two Dice

A good starting point for exploring joint and marginal probabilities is to consider independent random variables as the calculations are very simple.

The roll of a fair die gives a one in six (1/6) or 0.166 (16.666%) probability of a number 1 to 6 coming up.

P(dice1=1) = 1/6

P(dice1=2) = 1/6

P(dice1=3) = 1/6

P(dice1=4) = 1/6

P(dice1=5) = 1/6

P(dice1=6) = 1/6

If we roll a second die, we get the same probability of each value on that die. Each event for a die has an equal probability and the rolls of dice1 and dice2 do not affect each other.

P(dice1={1,2,3,4,5,6}) = 1.0

P(dice2={1,2,3,4,5,6}) = 1.0

First, we can calculate the probability of rolling an even number for dice1 as the sum of the probabilities of rolling a 2, 4, or 6, for example:

P(dice1={2, 4, 6}) = P(dice1=2) + P(dice1=4) + P(dice1=6)

P(dice1={2, 4, 6}) = 1/6 + 1/6 + 1/6

This is 0.5 or 50% as we might intuitively expect.

Now, we might consider the joint probability of rolling an even number with both dice simultaneously. The joint probability for independent random variables is calculated as follows:

P(A and B) = P(A) * P(B)

This is calculated as the probability of rolling an even number for dice1 multiplied by the probability of rolling an even number for dice2. The probability of the first event constrains the probability of the second event.

P(dice1={2, 4, 6} and dice2={2, 4, 6}) = P(dice1={2, 4, 6}) * P(dice2={2, 4, 6})

We know that the probability of rolling an even number of each die is 0.5, therefore the probability of rolling two even numbers is 3/6 or 0.5. Plugging that in, we get: 0.5 * 0.5 (0.25) or 25%.

Another way to look at this is to consider that rolling one die gives 6 combinations. Rolling two dice together gives 6 combinations for dice2 for each of the 6 combinations of dice1 or (6×6) 36 combinations. A total of 3 of the 6 combinations of dice1 will be even, and 3 of the 6 combinations of those will be even. That gives (3×3) 9 out of the 36 combinations as an even number of each die or (9/36 = 0.25) 25%.

Tip: If you are ever in doubt of your probability calculations when working with independent variables with discrete events, think in terms of combinations and things will make sense again.

We can construct a table of the joint probabilities based on our knowledge of the domain. The complete table is listed below with dice1 across the top (x-axis) and dice2 along the side (y-axis). The joint probabilities of each event for a given cell are calculated using the joint probability formula, e.g. 0.166 * 0.166 or 0.027 or about 2.777%.

1 2 3 4 5 6 1 0.027 0.027 0.027 0.027 0.027 0.027 2 0.027 0.027 0.027 0.027 0.027 0.027 3 0.027 0.027 0.027 0.027 0.027 0.027 4 0.027 0.027 0.027 0.027 0.027 0.027 5 0.027 0.027 0.027 0.027 0.027 0.027 6 0.027 0.027 0.027 0.027 0.027 0.027 1 2 3 4 5 6 7 1 2 3 4 5 6 1 0.027 0.027 0.027 0.027 0.027 0.027 2 0.027 0.027 0.027 0.027 0.027 0.027 3 0.027 0.027 0.027 0.027 0.027 0.027 4 0.027 0.027 0.027 0.027 0.027 0.027 5 0.027 0.027 0.027 0.027 0.027 0.027 6 0.027 0.027 0.027 0.027 0.027 0.027

This table captures the joint probability distribution of the events of the two random variables, dice1 and dice2. It is pretty boring, but we can use it to sharpen our understanding of joint and marginal probability of independent variables.

For example, the joint probability of rolling a 2 with dice1 and a 2 with dice2 can be read from the table directly as 2.777%. We can explore more elaborate cases, such as rolling a 2 with dice1 and rolling an odd number with dice2.

This can be read off as summing the values in the second column for rolling a 2 with dice1 and the first, third, and fifth rows for rolling an odd number with dice2.

P(dice1=2, dice2={1,3,5}) = 0.027 + 0.027 + 0.027

This comes out to be about 0.083, or about 8.333%.

We can also use this table to calculate the marginal probability. This is calculated as the sum of an entire column of probabilities for dice1 or a row of probabilities for dice2.

For example, we can calculate the marginal probability of rolling a 6 with dice2 as the sum of probabilities across the final row of the table. This comes out to be about 0.166 or 16.666% as we may intuitively expect.

Importantly, if we sum the probabilities for all cells in the table, it must equal 1.0. Additionally, if we sum the probabilities for each row, then the sum of these sums must equal 1.0. The same if we sum the probabilities in each column, then the sum of these sums too must equal 1.0. This is a requirement for a table of joint probabilities.

Because the events are independent, there is nothing special needed to calculate conditional probabilities.

P(A given B) = P(A)

For example, the probability of rolling a 2 with dice1 is the same regardless of what was rolled with dice2.

P(dice1=2 given dice2=6) = P(dice1=2)

In this way, conditional probability does not have a useful meaning for independent random variables.

Developing a table of joint probabilities is a helpful tool for better understanding how to calculate and explore the joint and marginal probabilities.

In the next section, let’s look at a more complicated example with dependent random variables.

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Probabilities of Weather in Two Cities

We can develop an intuition for joint and marginal probabilities using a table of joint probabilities of events for two dependent random variables.

Consider the situation where there are two cities, city1 and city2. The cities are close enough that they are generally affected by the same weather, yet they are far enough apart that they do not get identical weather.

We can consider discrete weather classifications for these cities on a given day, such as sunny, cloudy, and rainy. When it is sunny in city1, it is usually sunny in city2, but not always. As such, there is a dependency between the weather in the two cities.

Now, let’s explore the different types of probability.

Data Collection

First, we can record the observed weather in each city over twenty days.

For example, on day 1, what was the weather in each, day 2, and so on.

Day | City1 | City2 1 Sunny Sunny 2 Sunny Cloudy 3 ... 1 2 3 4 5 Day | City1 | City2 1 Sunny Sunny 2 Sunny Cloudy 3 ...

The complete table of results is omitted for brevity, we will make up totals later.

We can then calculate the sum of the total number of paired events that were observed.

For example, the total number of times it was sunny in city1 and sunny in city2, the total number of times it was sunny in city1 and cloudy in city2, and so on.

City 1 | City 2 | Total sunny sunny 6/20 sunny cloudy 1/20 sunny rainy 0/20 ... 1 2 3 4 5 City 1 | City 2 | Total sunny sunny 6/20 sunny cloudy 1/20 sunny rainy 0/20 ...

Again, the complete table is omitted for brevity, we will make up totals later.

This data provides the basis for exploring the probability of weather events in the two cities.

Joint Probabilities

First, we might be interested in the probability of weather events in each city.

We can create a table that contains the probabilities of the paired or joint weather events.

The table below summarizes the probability of each discrete weather for the two cities, with city1 defined across the top (x-axis) and city2 defined along the side (y-axis).

Sunny | Cloudy | Rainy Sunny 6/20 2/20 0/20 Cloudy 1/20 5/20 2/20 Rainy 0/20 1/20 3/20 1 2 3 4 Sunny | Cloudy | Rainy Sunny 6/20 2/20 0/20 Cloudy 1/20 5/20 2/20 Rainy 0/20 1/20 3/20

A cell in the table describes the joint probability of an event in each city, and together, the probabilities in the table summarize the joint probability distribution of weather events for the two cities.

The sum of the joint probabilities for all cells in the table must equal 1.0.

We can calculate the joint probability for the weather in two cities. For example, we would expect the joint probability of it being sunny in both cities at the same time as being high. This can be stated formally as:

P(city1=sunny and city2=sunny)

Or more compactly:

P(sunny, sunny)

We can read this off the table directly as 6/20 or 0.3 or 30%. A relatively high probability.

We can take this a step further and consider the probability of it not being rainy in the first city but having rain in the second city. We could state this as:

P(city1=sunny or cloudy and city2=rainy)

Again, we can calculate this directly from the table. Firstly, P(sunny,rainy) is 0/20 and P(cloudy,rainy) is 1/20. We can then add these probabilities together to give 1/20 or 0.05 or 5%. It can happen, but it is not likely.

The table also gives an idea of the marginal distribution of events. For example, we might be interested in the probability of a sunny day in city1, regardless of what happens in city2. This can be read from the table by summing the probabilities for city1 for sunny, e.g the first column of probabilities:

P(city1=sunny) = P(city1=sunny, city2=sunny) + P(city1=sunny, city2=cloudy) + P(city1=sunny, city2=rainy)

Or

P(city1=sunny) = 6/20 + 1/20 + 0/20

P(city1=sunny) = 7/20

Therefore, the marginal probability of a sunny day in city1 is 0.35 or 35%.

We can do the same thing for city2 by calculating the marginal probability of an event across some or all probabilities in a row. For example, the probability of a rainy day in city2 would be calculated as the sum of probabilities along the bottom row of the table:

P(city2=rainy) = 0/20 + 1/20 + 3/20

P(city2=rainy) = 4/20

Therefore, the marginal probability of a rainy day in city2 is 0.2 or 20%.

The marginal probabilities are often interesting and useful, and it is a good idea to update the table of joint probabilities to include them; for example:

Sunny | Cloudy | Rainy | Marginal Sunny 6/20 2/20 0/20 8/20 Cloudy 1/20 5/20 2/20 8/20 Rainy 0/20 1/20 3/20 4/20 Marginal 7/20 8/20 5/20 20/20 1 2 3 4 5 Sunny | Cloudy | Rainy | Marginal Sunny 6/20 2/20 0/20 8/20 Cloudy 1/20 5/20 2/20 8/20 Rainy 0/20 1/20 3/20 4/20 Marginal 7/20 8/20 5/20 20/20

Conditional Probabilities

We might be interested in the probability of a weather event given the occurrence of a weather event in another city.

This is called the conditional probability and can be calculated using the joint and marginal probabilities.

P(A given B) = P(A and B) / P(B)

For example, we might be interested in the probability of it being sunny in city1, given that it is sunny in city2.

This can be stated formally as:

P(city1=sunny given city2=sunny) = P(city1=sunny and city2=sunny) / P(city2=sunny)

We can fill in the joint and marginal probabilities from the table in the previous section; for example:

P(city1=sunny given city2=sunny) = 6/20 / 8/20

P(city1=sunny given city2=sunny) = 0.3 / 0.4

This comes out to be 0.75 or 75%, which is intuitive. We would expect that if it is sunny in city2 that city1 should also be sunny most of the time.

This is different from the joint probability of it being sunny in both cities on a given day which has a lower probability of 30%.

It makes more sense if we consider it from the perspective of the number of combinations. We have more information in this conditional case, therefore we don’t have to calculate the probability across all 20 days. Specifically, we are assuming it is sunny in city2, which dramatically reduces the number of days from 20 to 8. A total of 6 of those days that were sunny in city2 were also sunny in city1, giving the fraction 6/8 or (0.75) 75%.

All of this can be read from the table of joint probabilities.

An important aspect of conditional probability that is often misunderstood is that it is not reversible.

P(A given B) != P(B given A)

That is the probability of it being sunny in city1 given that it is sunny in city2 is not the same as the probability of it being sunny in city2 given that it is sunny in city1.

P(city1=sunny given city2=sunny) != P(city2=sunny given city1=sunny)

In this case, the probability of it being sunny in city2 given that it is sunny in city1 is sunny is calculated as follows:

P(city2=sunny given city1=sunny) = P(city2=sunny and city1=sunny) / P(city1=sunny)

P(city2=sunny given city1=sunny) = 6/20 / 7/20

P(city2=sunny given city1=sunny) = 0.3 / 0.35

P(city2=sunny given city1=sunny) = 0.857

In this case, it is higher, at about 85.714%.

We can also use the conditional probability to calculate the joint probability.

P(A and B) = P(A given B) * P(B)

For example, if all we know is the conditional probability of sunny in city2 given city1 and the marginal probability of city2, we can calculate the joint probability as:

P(city1=sunny and city2=sunny) = P(city2=sunny given city1=sunny) * P(city1=sunny)

P(city1=sunny and city2=sunny) = 0.857 * 0.35

P(city1=sunny and city2=sunny) = 0.3

This gives 0.3 or 30% as we expected.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Articles

Summary

In this tutorial, you discovered the intuitions behind calculating the joint, marginal, and conditional probability.

Specifically, you learned:

How to calculate joint, marginal, and conditional probability for independent random variables.

How to collect observations from joint random variables and construct a joint probability table.

How to calculate joint, marginal, and conditional probability from a joint probability table.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Probability for Machine Learning! Develop Your Understanding of Probability ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Probability for Machine Learning It provides self-study tutorials and end-to-end projects on:

Bayes Theorem, Bayesian Optimization, Distributions, Maximum Likelihood, Cross-Entropy, Calibrating Models

and much more... Finally Harness Uncertainty in Your Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
226;towardsdatascience.com;https://towardsdatascience.com/work-smarter-not-harder-when-building-neural-networks-6f4aa7c5ee61?source=collection_home---4------0-----------------------;2020-04-17;Work smarter, not harder when building Neural Networks;"Work smarter, not harder when building Neural Networks

Using a simple example to illuminate design principles for neural networks

A fundamental technique in applied mathematics is to find a change of coordinates that converts a difficult or impossible problem into a simpler one. Perhaps my favorite example of this are the equations for a circle. If we write down the equation of the unit circle in Cartesian coordinates we can express the geometric concept of a circle as an implicit function:

This gets even worse when we try to get an explicit expression by solving for y.

Now we have to piece this simple function together from two branches of the square root (top and bottom half of the circle). Beyond aesthetics using this representation of a circle can prevent us from seeing the geometrical simplicity inherent in a perfect circle. By contrast, if we represent this same mathematical object using polar coordinates, then the unit circle becomes very simple to work with.

Polar coordinates for a point.

In polar coordinates our unit circle takes the simple form r=1, θ ∈ [0,2π). So in polar coordinates a circle takes the form of a rectangle (height=1, width=2π). Cartesian coordinates are the natural coordinates for rectangles and polar coordinates are the natural coordinates for circles. In this article I will apply this line of thinking to constructing simple neural networks, and build a illustrative example of the importance of choosing the correct coordinates.

Artificial Neural Networks for Curve Fitting

If you are reading this article then you have probably heard of Artificial Neural Networks (ANN). We will focus on the simplest type of ANN’s called a feed-forward network in this discussion. Briefly, ANN’s are just networks(functions) built by chaining together simple nonlinear functions (layers).

Feed-forward neural network

By chaining (many) of these simple layers together, the universal approximation theorem tells us we can represent any (nice) function as a ANN with finite depth and width. In the starting polar coordinates example this is like saying that we can write every point in the (x,y) plane in polar coordinates (no holes). This is great and a necessary property to ensure we don’t miss anything when we change coordinates, but it doesn’t tell us two things:

How to find the actual coordinates.

If that is actually a good way of representing the information.

For ANNs the “coordinates” are the parameters of the neural network (the weights and biases for each layer in the neural network). Instead of having a quick mathematical formula to find those coordinates, for neural networks we find them using optimization to minimize a loss function. The loss function measures the distance between our specified function and the training data.

Let’s look at a simple example of this to build an approximation to a sine function using a neural network.

Fitting a sin function using a Neural Network

Let’s apply this powerful function approximation approach to build a neural network version of the simple function sin(x). I will be using the fantastic neural network library Flux written in Julia. This package provides a very simple and powerful framework for building neural networks. It also adds very little extra syntax to build neural networks and lets us focus on the fundamental building blocks. I’m just going to include code snippets in the article. For the full code check out this github repo.

The below code constructs a simple network with two hidden layers using a standard nonlinear function tanh.



ann = Chain(Dense(1,20,tanh),Dense(20,20,tanh),Dense(20,1));



We can visualize this neural network as:

A simple feed-forward neural network for learning the sin function

The above diagram shows we have a single input and output value with two layers in-between. These layers are called hidden because they are invisible if you are only tracking the input and outputs. Recall that this diagram is used to represent a certain family of functions, were a particular member is specified by fixing the parameters.

The functional form of the above neural network. The parameters are given by the two weight matrices W and the bias vectors b. The C parameter gives the intercept of the linear output layer.

Alright, so we are hoping that we can approximately represent the function sin(x) by some member of this family of functions. To try and find the member that is closest to this we should minimize a loss function using some training data.

function loss(x, y)

pred=ann(x)

loss=Flux.mse(ann(x), y)

#loss+=0.1*sum(l1,params(ann)) #l1 reg

return loss

end @epochs 3000 Flux.train!(loss,params(ann), data, ADAM())

After fitting our neural network we can see how well it does for the training data, and a set of test data.

A feed-forward neural network with a tanh non-linearity fit for 3000 epochs to the training data shown as blue circles. The test data is shown as green crosses.

Clearly this neural network does a poor job of capturing the fundamental periodic nature of the signal. This is highlighted by the exceptionally poor generalization to the test data (green). This isn’t just my network either.

What went wrong? The universal approximation theorem, so often cited, tells us that we can represent this function using a feed-forward neural network of finite depth and width. However, as I often rediscover-finite can still be extremely large. Just because some set of weights and biases exist that can be used to build this function out of the typical neural network components doesn’t mean we can easily find them.

We could try to apply brute-force and build larger networks or train for longer, etc. However, a very simple change will get us a nearly perfect approximation in a fraction of the time.

A better idea

Given this data we can see that it is periodic. Right now we haven’t included that piece of information in our neural network design at all. Instead of using a tanh nonlinear function, lets use a sin function. This way the output from our first layer will be periodic.

The below diagram shows the idea for a better approximation network for this system.

ANN with sin nonlinearity.

Written out in functional form we family of functions we are using takes the form:

Where Q is the number of neurons in the hidden layer. This is very easy to do in Flux:

Q = 20;

ann = Chain(Dense(1,Q,sin),Dense(Q,1));

Let’s fit this new model are see if we can get a better model.

A better neural network basis

This is a much better model, as it has captured the periodic nature. However, notice that we are still slightly off both in the training and the test data. This is because our model is actually still has some extra degrees of freedom it doesn’t really need. We could solve this by applying some regularization to the parameters, but in this case we can turn to some pretty cool mathematics for a solution.

Jean-Baptiste Joseph Fourier 1768–1830

The functional form of the neural network above is very close to the form for a Fourier Series. Fourier series are a basis (set of coordinates) which can be used to describe all nice functions over a finite domain [a,b]. If a function is periodic then a description over a finite domain can be extended to the whole real line. Therefore, Fourier series are often used for periodic functions.

Fourier series take the form:

The only difference between our neural network and the Fourier series is that our neural network allows the weights W to vary, whereas Fourier only has the A coefficients and the bias (b) terms which can be chosen to fit the function.

In the Flux library it is easy enough to remove those weight parameters in the first layer from the trainable set of parameters. By setting them at integer values we can actually create a neural network which IS a Fourier series.

# Fourier Series ANN Q = 20;

ann = Chain(Dense(1,Q,sin),Dense(Q,1)); function loss(x, y)

pred=ann(x)

loss=Flux.mse(ann(x), y)

return loss

end opt = ADAM() ps=params(ann) for j=1:20

ann[1].W[j]=j

end

delete!(ps, ann[1].W) #Make the first layers weights fixed @epochs 3000 Flux.train!(loss,ps, data, opt)

Here is the fit using a sin(x) function and the Fourier ANN:

Fourier ANN approach generalizes well.

If you are wondering this approach generalizes very well for other more complicated periodic functions as well. Consider a periodic signal with some higher harmonics mixed in.

Second example of a Fourier series neural network for a more complex periodic signal.

Some takeaway lessons

This simple example provides some important insights into how ANN’s work and how we can improve our models.

The design of your neural networks can have huge effects on the results. Specific knowledge of the problem and the domain can have huge impacts. Ensure that everything you know about the problem is being conveyed to the neural network. Here we saw that telling the function to produce a periodic signal did much more that increasing the network size or training time could do. Along the lines of the last lesson, domain knowledge matters. Applied mathematicians and physicists have been working on the function approximation problem for a very long time. Fourier series were discovered in the 1820’s to solve heat conduction problems. These techniques may be worth your time to learn, even if you don’t care about the application. Calculating Fourier series using a ANN is very silly. One of the top ten algorithms of the last century was invented for exactly this sort of calculation. The point is to show how a small change to incorporate more domain knowledge can hugely improve results.

For more about building smarter neural networks I would recommend checking out Nathan Kutz’s work."
227;machinelearningmastery.com;https://machinelearningmastery.com/how-to-use-test-time-augmentation-to-improve-model-performance-for-image-classification/;2019-04-14;How to Use Test-Time Augmentation to Make Better Predictions;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104

# cnn model for the cifar10 problem with test-time augmentation import numpy from numpy import argmax from numpy import mean from numpy import std from numpy import expand_dims from sklearn . metrics import accuracy_score from keras . datasets . cifar10 import load_data from keras . utils import to_categorical from keras . preprocessing . image import ImageDataGenerator from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . layers import BatchNormalization # load and return the cifar10 dataset ready for modeling def load_dataset ( ) : # load dataset ( trainX , trainY ) , ( testX , testY ) = load_data ( ) # normalize pixel values trainX = trainX . astype ( 'float32' ) / 255 testX = testX . astype ( 'float32' ) / 255 # one hot encode target values trainY = to_categorical ( trainY ) testY = to_categorical ( testY ) return trainX , trainY , testX , testY # define the cnn model for the cifar10 dataset def define_model ( ) : # define model model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' , kernel_initializer = 'he_uniform' , input_shape = ( 32 , 32 , 3 ) ) ) model . add ( BatchNormalization ( ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' , kernel_initializer = 'he_uniform' ) ) model . add ( BatchNormalization ( ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( BatchNormalization ( ) ) model . add ( Dense ( 10 , activation = 'softmax' ) ) # compile model model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] ) return model # make a prediction using test-time augmentation def tta_prediction ( datagen , model , image , n_examples ) : # convert image into dataset samples = expand_dims ( image , 0 ) # prepare iterator it = datagen . flow ( samples , batch_size = n_examples ) # make predictions for each augmented image yhats = model . predict_generator ( it , steps = n_examples , verbose = 0 ) # sum across predictions summed = numpy . sum ( yhats , axis = 0 ) # argmax across classes return argmax ( summed ) # evaluate a model on a dataset using test-time augmentation def tta_evaluate_model ( model , testX , testY ) : # configure image data augmentation datagen = ImageDataGenerator ( horizontal_flip = True ) # define the number of augmented images to generate per test set image n_examples_per_image = 7 yhats = list ( ) for i in range ( len ( testX ) ) : # make augmented prediction yhat = tta_prediction ( datagen , model , testX [ i ] , n_examples_per_image ) # store for evaluation yhats . append ( yhat ) # calculate accuracy testY_labels = argmax ( testY , axis = 1 ) acc = accuracy_score ( testY_labels , yhats ) return acc # fit and evaluate a defined model def evaluate_model ( model , trainX , trainY , testX , testY ) : # fit model model . fit ( trainX , trainY , epochs = 3 , batch_size = 128 , verbose = 0 ) # evaluate model using tta acc = tta_evaluate_model ( model , testX , testY ) return acc # repeatedly evaluate model, return distribution of scores def repeated_evaluation ( trainX , trainY , testX , testY , repeats = 10 ) : scores = list ( ) for _ in range ( repeats ) : # define model model = define_model ( ) # fit and evaluate model accuracy = evaluate_model ( model , trainX , trainY , testX , testY ) # store score scores . append ( accuracy ) print ( '> %.3f' % accuracy ) return scores # load dataset trainX , trainY , testX , testY = load_dataset ( ) # evaluate model scores = repeated_evaluation ( trainX , trainY , testX , testY ) # summarize result print ( 'Accuracy: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) )"
228;machinelearningmastery.com;http://machinelearningmastery.com/5-step-life-cycle-neural-network-models-keras/;2016-08-10;5 Step Life-Cycle for Neural Network Models in Keras;"# Sample Multilayer Perceptron Neural Network in Keras

from keras . models import Sequential

from keras . layers import Dense

import numpy

# load and prepare the dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# 1. define the network

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# 2. compile the network

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# 3. fit the network

history = model . fit ( X , Y , epochs = 100 , batch_size = 10 )

# 4. evaluate the network

loss , accuracy = model . evaluate ( X , Y )

print ( ""

Loss: %.2f, Accuracy: %.2f%%"" % ( loss , accuracy* 100 ) )

# 5. make predictions

probabilities = model . predict ( X )

predictions = [ float ( round ( x ) ) for x in probabilities ]

accuracy = numpy . mean ( predictions == Y )"
229;machinelearningmastery.com;https://machinelearningmastery.com/why-learn-linear-algebra-for-machine-learning/;2018-01-28;5 Reasons to Learn Linear Algebra for Machine Learning;"Tweet Share Share

Last Updated on August 9, 2019

Why Learn Linear Algebra for Machine Learning?

Linear algebra is a field of mathematics that could be called the mathematics of data.

It is undeniably a pillar of the field of machine learning, and many recommend it as a prerequisite subject to study prior to getting started in machine learning. This is misleading advice, as linear algebra makes more sense to a practitioner once they have a context of the applied machine learning process in which to interpret it.

In this post, you will discover why machine learning practitioners should study linear algebra to improve their skills and capabilities as practitioners.

After reading this post, you will know:

Not everyone should learn linear algebra, that it depends where you are in your process of learning machine learning.

5 Reasons why a deeper understanding of linear algebra is required for intermediate machine learning practitioners.

Where to get started once you are motivated to begin your journey into the field of linear algebra.

Discover vectors, matrices, tensors, matrix types, matrix factorization, PCA, SVD and much more in my new book, with 19 step-by-step tutorials and full source code.

Let’s get started.

Reasons to NOT Learn Linear Algebra

Before we go through the reasons that you should learn linear algebra, let’s start off by taking a small look at the reason why you should not.

I think you should not study linear algebra if you are just getting started with applied machine learning.

It’s not required . Having an appreciation for the abstract operations that underly some machine learning algorithms is not required in order to use machine learning as a tool to solve problems.

. Having an appreciation for the abstract operations that underly some machine learning algorithms is not required in order to use machine learning as a tool to solve problems. It’s slow . Taking months to years to study an entire related field before machine learning will delay you achieving your goals of being able to work through predictive modeling problems.

. Taking months to years to study an entire related field before machine learning will delay you achieving your goals of being able to work through predictive modeling problems. It’s a huge field. Not all of linear algebra is relevant to theoretical machine learning, let alone applied machine learning.

I recommend a breadth-first approach to getting started in applied machine learning.

I call this approach a results-first approach. It is where you start by learning and practicing the steps for working through a predictive modeling problem end-to-end (e.g. how to get results) with a tool (such as scikit-learn and Pandas in Python).

This process then provides the skeleton and context for progressively deepening your knowledge, such as how algorithms work and eventually the math that underlies them.

After you know how to work through a predictive modeling problem, let’s look at why you should deepen your understanding of linear algebra.

Need help with Linear Algebra for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. You Need to Learn Linear Algebra Notation

You need to be able to read and write vector and matrix notation.

Algorithms are described in books, papers and on websites using vector and matrix notation.

Linear algebra is the mathematics of data and the notation allows you to describe operations on data precisely with specific operators.

You need to be able to read and write this notation. This skill will allow you to:

Read descriptions of existing algorithms in textbooks.

Interpret and implement descriptions of new methods in research papers.

Concisely describe your own methods to other practitioners.

Further, programming languages such as Python offer efficient ways of implementing linear algebra notation directly.

An understanding of the notation and how it is realized in your language or library will allow for shorter and perhaps more efficient implementations of machine learning algorithms.

2. You Need to Learn Linear Algebra Arithmetic

In partnership with the notation of linear algebra are the arithmetic operations performed.

You need to know how to add, subtract, and multiply scalars, vectors, and matrices.

A challenge for newcomers to the field of linear algebra are operations such as matrix multiplication and tensor multiplication that are not implemented as the direct multiplication of the elements of these structures, and at first glance appear nonintuitive.

Again, most if not all of these operations are implemented efficiently and provided via API calls in modern linear algebra libraries.

An understanding of how vector and matrix operations are implemented is required as a part of being able to effectively read and write matrix notation.

3. You Need to Learn Linear Algebra for Statistics

You must learn linear algebra in order to be able to learn statistics. Especially multivariate statistics.

Statistics and data analysis are another pillar field of mathematics to support machine learning. They are primarily concerned with describing and understanding data. As the mathematics of data, linear algebra has left its fingerprint on many related fields of mathematics, including statistics.

In order to be able to read and interpret statistics, you must learn the notation and operations of linear algebra.

Modern statistics uses both the notation and tools of linear algebra to describe the tools and techniques of statistical methods. From vectors for the means and variances of data, to covariance matrices that describe the relationships between multiple Gaussian variables.

The results of some collaborations between the two fields are also staple machine learning methods, such as the Principal Component Analysis, or PCA for short, used for data reduction.

4. You Need to Learn Matrix Factorization

Building on notation and arithmetic is the idea of matrix factorization, also called matrix decomposition.

You need to know how to factorize a matrix and what it means.

Matrix factorization is a key tool in linear algebra and used widely as an element of many more complex operations in both linear algebra (such as the matrix inverse) and machine learning (least squares).

Further, there are a range of different matrix factorization methods, each with different strengths and capabilities, some of which you may recognize as “machine learning” methods, such as Singular-Value Decomposition, or SVD for short, for data reduction.

In order to read and interpret higher-order matrix operations, you must understand matrix factorization.

5. You Need to Learn Linear Least Squares

You need to know how to use matrix factorization to solve linear least squares.

Linear algebra was originally developed to solve systems of linear equations. These are cases where there are more equations than there are unknown variables (e.g. coefficients). As a result, they are challenging to solve arithmetically because there is no single solution as there is no line or plane can fit the data without some error.

Problems of this type can be framed as the minimization of squared error, called least squares, and can be recast in the language of linear algebra, called linear least squares.

Linear least squares problems can be solved efficiently on computers using matrix operations such as matrix factorization.

Least squares is most known for its role in the solution to linear regression models, but also plays a wider role in a range of machine learning algorithms.

In order to understand and interpret these algorithms, you must understand how to use matrix factorization methods to solve least squares problems.

One More Reason

If I could give one more reason, it would be: because it is fun.

Seriously.

Learning linear algebra, at least the way I teach it with practical examples and executable code, is a lot fun. Once you can see how the operations work on real data, it is hard to avoid developing a strong intuition for the methods.

Do you have more reasons why it is critical for an intermediate machine learning practitioner to learn linear algebra?

Let me know if the comments below.

Where to Start in Linear Algebra?

Perhaps now you are motivated to take a step into the field of linear algebra.

I would caution you to not take a straight course on linear algebra. It is a big field, and not all of it will be relevant or applicable to you as a machine learning practitioner, at least not in the beginning.

I would recommend a staggered approach and starting with the following areas of linear algebra that are relevant to machine learning.

Vector and Matrix Notation.

Vector and Matrix Arithmetic.

Multivariate Statistics.

Matrix Factorization.

Linear Least Squares.

I would consider this the minimum linear algebra required to be an effective machine learning practitioner.

You can go deeper and learn how the operations were derived, which in turn may deepen your understanding and effectiveness in some aspects of applied machine learning, but it may be beyond the point of diminishing returns for most practitioners, at least in terms of the day-to-day activities of the average machine learning practitioner.

Summary

In this post, you discovered why, as a machine learning practitioner, you should deepen your understanding of linear algebra.

Specifically, you learned:

Not everyone should learn linear algebra, that it depends where you are in your process of learning machine learning.

5 Reasons why a deeper understanding of linear algebra is required for intermediate machine learning practitioners.

Where to get started once you are motivated to begin your journey into the field of linear algebra.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Linear Algebra for Machine Learning! Develop a working understand of linear algebra ...by writing lines of code in python Discover how in my new Ebook:

Linear Algebra for Machine Learning It provides self-study tutorials on topics like:

Vector Norms, Matrix Multiplication, Tensors, Eigendecomposition, SVD, PCA and much more... Finally Understand the Mathematics of Data Skip the Academics. Just Results. See What's Inside"
230;machinelearningmastery.com;https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/;2019-09-26;A Gentle Introduction to Joint, Marginal, and Conditional Probability;"Tweet Share Share

Last Updated on November 22, 2019

Probability quantifies the uncertainty of the outcomes of a random variable.

It is relatively easy to understand and compute the probability for a single variable. Nevertheless, in machine learning, we often have many random variables that interact in often complex and unknown ways.

There are specific techniques that can be used to quantify the probability for multiple random variables, such as the joint, marginal, and conditional probability. These techniques provide the basis for a probabilistic understanding of fitting a predictive model to data.

In this post, you will discover a gentle introduction to joint, marginal, and conditional probability for multiple random variables.

After reading this post, you will know:

Joint probability is the probability of two events occurring simultaneously.

Marginal probability is the probability of an event irrespective of the outcome of another variable.

Conditional probability is the probability of one event occurring in the presence of a second event.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Update Oct/2019 : Fixed minor typo, thanks Anna.

: Fixed minor typo, thanks Anna. Update Nov/2019: Described the symmetrical calculation of joint probability.

Overview

This tutorial is divided into three parts; they are:

Probability of One Random Variable Probability of Multiple Random Variables Probability of Independence and Exclusivity

Probability of One Random Variable

Probability quantifies the likelihood of an event.

Specifically, it quantifies how likely a specific outcome is for a random variable, such as the flip of a coin, the roll of a dice, or drawing a playing card from a deck.

Probability gives a measure of how likely it is for something to happen.

— Page 57, Probability: For the Enthusiastic Beginner, 2016.

For a random variable x, P(x) is a function that assigns a probability to all values of x.

Probability Density of x = P(x)

The probability of a specific event A for a random variable x is denoted as P(x=A), or simply as P(A).

Probability of Event A = P(A)

Probability is calculated as the number of desired outcomes divided by the total possible outcomes, in the case where all outcomes are equally likely.

Probability = (number of desired outcomes) / (total number of possible outcomes)

This is intuitive if we think about a discrete random variable such as the roll of a die. For example, the probability of a die rolling a 5 is calculated as one outcome of rolling a 5 (1) divided by the total number of discrete outcomes (6) or 1/6 or about 0.1666 or about 16.666%.

The sum of the probabilities of all outcomes must equal one. If not, we do not have valid probabilities.

Sum of the Probabilities for All Outcomes = 1.0.

The probability of an impossible outcome is zero. For example, it is impossible to roll a 7 with a standard six-sided die.

Probability of Impossible Outcome = 0.0

The probability of a certain outcome is one. For example, it is certain that a value between 1 and 6 will occur when rolling a six-sided die.

Probability of Certain Outcome = 1.0

The probability of an event not occurring, called the complement.

This can be calculated by one minus the probability of the event, or 1 – P(A). For example, the probability of not rolling a 5 would be 1 – P(5) or 1 – 0.166 or about 0.833 or about 83.333%.

Probability of Not Event A = 1 – P(A)

Now that we are familiar with the probability of one random variable, let’s consider probability for multiple random variables.

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Probability of Multiple Random Variables

In machine learning, we are likely to work with many random variables.

For example, given a table of data, such as in excel, each row represents a separate observation or event, and each column represents a separate random variable.

Variables may be either discrete, meaning that they take on a finite set of values, or continuous, meaning they take on a real or numerical value.

As such, we are interested in the probability across two or more random variables.

This is complicated as there are many ways that random variables can interact, which, in turn, impacts their probabilities.

This can be simplified by reducing the discussion to just two random variables (X, Y), although the principles generalize to multiple variables.

And further, to discuss the probability of just two events, one for each variable (X=A, Y=B), although we could just as easily be discussing groups of events for each variable.

Therefore, we will introduce the probability of multiple random variables as the probability of event A and event B, which in shorthand is X=A and Y=B.

We assume that the two variables are related or dependent in some way.

As such, there are three main types of probability we might want to consider; they are:

Joint Probability : Probability of events A and B.

: Probability of events A and B. Marginal Probability : Probability of event X=A given variable Y.

: Probability of event X=A given variable Y. Conditional Probability: Probability of event A given event B.

These types of probability form the basis of much of predictive modeling with problems such as classification and regression. For example:

The probability of a row of data is the joint probability across each input variable.

The probability of a specific value of one input variable is the marginal probability across the values of the other input variables.

The predictive model itself is an estimate of the conditional probability of an output given an input example.

Joint, marginal, and conditional probability are foundational in machine learning.

Let’s take a closer look at each in turn.

Joint Probability of Two Variables

We may be interested in the probability of two simultaneous events, e.g. the outcomes of two different random variables.

The probability of two (or more) events is called the joint probability. The joint probability of two or more random variables is referred to as the joint probability distribution.

For example, the joint probability of event A and event B is written formally as:

P(A and B)

The “and” or conjunction is denoted using the upside down capital “U” operator “^” or sometimes a comma “,”.

P(A ^ B)

P(A, B)

The joint probability for events A and B is calculated the probability of event A given event B multiplied by the probability of event B.

This can be stated formally as follows:

P(A and B) = P(A given B) * P(B)

The calculation of the joint probability is sometimes called the fundamental rule of probability or the “product rule” of probability or the “chain rule” of probability.

Here, P(A given B) is the probability of event A given that event B has occurred, called the conditional probability, described below.

The joint probability is symmetrical, meaning that P(A and B) is the same as P(B and A). The calculation using the conditional probability is also symmetrical, for example:

P(A and B) = P(A given B) * P(B) = P(B given A) * P(A)

Marginal Probability

We may be interested in the probability of an event for one random variable, irrespective of the outcome of another random variable.

For example, the probability of X=A for all outcomes of Y.

The probability of one event in the presence of all (or a subset of) outcomes of the other random variable is called the marginal probability or the marginal distribution. The marginal probability of one random variable in the presence of additional random variables is referred to as the marginal probability distribution.

It is called the marginal probability because if all outcomes and probabilities for the two variables were laid out together in a table (X as columns, Y as rows), then the marginal probability of one variable (X) would be the sum of probabilities for the other variable (Y rows) on the margin of the table.

There is no special notation for the marginal probability; it is just the sum or union over all the probabilities of all events for the second variable for a given fixed event for the first variable.

P(X=A) = sum P(X=A, Y=yi) for all y

This is another important foundational rule in probability, referred to as the “sum rule.”

The marginal probability is different from the conditional probability (described next) because it considers the union of all events for the second variable rather than the probability of a single event.

Conditional Probability

We may be interested in the probability of an event given the occurrence of another event.

The probability of one event given the occurrence of another event is called the conditional probability. The conditional probability of one to one or more random variables is referred to as the conditional probability distribution.

For example, the conditional probability of event A given event B is written formally as:

P(A given B)

The “given” is denoted using the pipe “|” operator; for example:

P(A | B)

The conditional probability for events A given event B is calculated as follows:

P(A given B) = P(A and B) / P(B)

This calculation assumes that the probability of event B is not zero, e.g. is not impossible.

The notion of event A given event B does not mean that event B has occurred (e.g. is certain); instead, it is the probability of event A occurring after or in the presence of event B for a given trial.

Probability of Independence and Exclusivity

When considering multiple random variables, it is possible that they do not interact.

We may know or assume that two variables are not dependent upon each other instead are independent.

Alternately, the variables may interact but their events may not occur simultaneously, referred to as exclusivity.

We will take a closer look at the probability of multiple random variables under these circumstances in this section.

Independence

If one variable is not dependent on a second variable, this is called independence or statistical independence.

This has an impact on calculating the probabilities of the two variables.

For example, we may be interested in the joint probability of independent events A and B, which is the same as the probability of A and the probability of B.

Probabilities are combined using multiplication, therefore the joint probability of independent events is calculated as the probability of event A multiplied by the probability of event B.

This can be stated formally as follows:

Joint Probability: P(A and B) = P(A) * P(B)

As we might intuit, the marginal probability for an event for an independent random variable is simply the probability of the event.

It is the idea of probability of a single random variable that are familiar with:

Marginal Probability: P(A)

We refer to the marginal probability of an independent probability as simply the probability.

Similarly, the conditional probability of A given B when the variables are independent is simply the probability of A as the probability of B has no effect. For example:

Conditional Probability: P(A given B) = P(A)

We may be familiar with the notion of statistical independence from sampling. This assumes that one sample is unaffected by prior samples and does not affect future samples.

Many machine learning algorithms assume that samples from a domain are independent to each other and come from the same probability distribution, referred to as independent and identically distributed, or i.i.d. for short.

Exclusivity

If the occurrence of one event excludes the occurrence of other events, then the events are said to be mutually exclusive.

The probability of the events are said to be disjoint, meaning that they cannot interact, are strictly independent.

If the probability of event A is mutually exclusive with event B, then the joint probability of event A and event B is zero.

P(A and B) = 0.0

Instead, the probability of an outcome can be described as event A or event B, stated formally as follows:

P(A or B) = P(A) + P(B)

The “or” is also called a union and is denoted as a capital “U” letter; for example:

P(A or B) = P(A U B)

If the events are not mutually exclusive, we may be interested in the outcome of either event.

The probability of non-mutually exclusive events is calculated as the probability of event A and the probability of event B minus the probability of both events occurring simultaneously.

This can be stated formally as follows:

P(A or B) = P(A) + P(B) – P(A and B)

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Articles

Summary

In this post, you discovered a gentle introduction to joint, marginal, and conditional probability for multiple random variables.

Specifically, you learned:

Joint probability is the probability of two events occurring simultaneously.

Marginal probability is the probability of an event irrespective of the outcome of another variable.

Conditional probability is the probability of one event occurring in the presence of a second event.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Probability for Machine Learning! Develop Your Understanding of Probability ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Probability for Machine Learning It provides self-study tutorials and end-to-end projects on:

Bayes Theorem, Bayesian Optimization, Distributions, Maximum Likelihood, Cross-Entropy, Calibrating Models

and much more... Finally Harness Uncertainty in Your Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
231;news.mit.edu;http://news.mit.edu/2020/algorithm-eye-visibility-helps-pilots-alaska-0106;;An algorithm with an eye for visibility helps pilots in Alaska;"More than three-quarters of Alaskan communities have no access to highways or roads. In these remote regions, small aircraft are a town's bus, ambulance, and food delivery — the only means of getting people and things in and out.

As routine as daily flight may be, it can be dangerous. These small (or general aviation) aircraft are typically flown visually, by a pilot looking out the cockpit windows. If sudden storms or fog appears, a pilot might not be able to see a runway, nearby aircraft, or rising terrain. In 2018, the Federal Aviation Administration (FAA) reported 95 aviation accidents in Alaska, including several fatal crashes that occurred in remote regions where poor visibility may have played a role.

""General aviation pilots in Alaska need to be aware of the forecasted conditions during pre-flight planning, but also of any rapidly changing conditions during flight,"" says Michael Matthews, a meteorologist at MIT Lincoln Laboratory. ""There are certain rules, like you can't fly with less than three miles of visibility. If it is worse, pilots need to fly on instruments, but they need to be certified for that.""

Pilots check current or forecasted weather conditions before they fly, but a lack of automated weather observation stations throughout the Alaskan bush makes it hard to know exactly what to expect. To help, the FAA recently installed 221 web cameras near runways and mountain passes. Pilots can look at the image feeds online to plan their route. Still, it's difficult to go through what could be hundreds of images and estimate just how far one can see.

So, Matthews has been working with the FAA to turn these web cameras into visibility sensors. He has developed an algorithm, called Visibility Estimation through Image Analytics (VEIA), that uses a camera's image feed to automatically determine the area's visibility. These estimates can then be shared among forecasters and with pilots online in real-time.

Trained eyes

In concept, the VEIA algorithm determines visibility the same way humans do. It looks for stationary ""edges."" For human observers, these edges are landmarks of known distances from an airfield, such as a tower or mountain top. They're trained to interpret how well they can see each marker compared to on a clear, sunny day.

Likewise, the algorithm is first taught what edges look like in clear conditions. The system looks at the past 10 days' worth of imagery, an optimal timeframe because any shorter timeframe could be skewed by bad weather and any longer could be affected by seasonal changes, according to Matthews. Using these 10-day images, the system creates a composite ""clear"" image. This image becomes the reference to which a current image is compared.

To run a comparison, an edge-detection algorithm (called a Sobel filter) is applied to both the reference and current image. This algorithm identifies edges that are persistent — the horizon, buildings, mountain sides — and removes fleeting edges like cars and clouds. Then, the system compares the overall edge strengths and generates a ratio. The ratio is converted into visibility in miles.

Developing an algorithm that works well across images from any web camera was challenging, Matthews says. Based on where they are placed, some cameras might have a view of 100 miles and others just 100 feet. Other problems stemmed from permanent objects that were very close to the camera and dominated the view, such as a large antenna. The algorithm had to be designed to look past these near objects.

""If you're an observer on Mount Washington, you have a trained eye to look for very specific things to get a visibility estimate. Say, the ski lifts on Attitash Mountain, and so on. We didn't want to make an algorithm that is trained so specifically; we wanted this same algorithm to apply anywhere and across all types of edges,"" Matthews says.

To validate its estimates, the VEIA algorithm was tested against data from Automated Surface Observing Stations (ASOS). These stations, of which there are close to 50 in Alaska, are outfitted with sensors that can estimate visibility each hour. The VEIA algorithm, which provides estimates every 10 minutes, was more than 90 percent accurate in detecting low-visibility conditions when compared to co-located ASOS data.

Informed pilots

The FAA plans to test the VEIA algorithm in summer 2020 on an experimental website. During the test period, pilots can visit the experimental website to see real-time visibility estimates alongside the camera imagery itself.

""Furthermore, the VEIA estimates can be ingested into weather prediction models to improve the forecasts,"" says Jenny Colavito, who is the ceiling and visibility research project lead at the FAA. ""All of this leads to keeping pilots better informed of weather conditions so that they can avoid flying into hazards.""

The FAA is looking into using weather cameras in other regions, starting in Hawaii. ""Like Alaska, Hawaii has extreme terrain and weather conditions that can change rapidly. I anticipate that the VEIA algorithm will be utilized along with the weather cameras in Hawaii to provide as much information to pilots as possible,"" Colavito adds. One of the key advantages of VEIA is that it requires no specialized sensors to do its job, just the image feed from the web cams.

Matthews recently accepted an R&D 100 Award for the algorithm, named one of the world's 100 most innovative products developed in 2019. As a researcher in air traffic management for 28 years, he is thrilled to have achieved this honor.

""Some mountain passes in Alaska are like highways, especially in the summertime, with the number of people flying. You can find countless stories of terrible crashes, people just doing everyday things — a family on their way to a volleyball game,"" Matthews reflects. ""I hope that VEIA might help people go about their lives safer.""

DISTRIBUTION STATEMENT A: Approved for public release. Distribution is unlimited. This material is based upon work supported by the Federal Aviation Administration under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Federal Aviation Administration."
232;machinelearningmastery.com;https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/#comments;2020-04-09;Stacking Ensemble Machine Learning With Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65

# compare ensemble to each baseline classifier from numpy import mean from numpy import std from sklearn . datasets import make_classification from sklearn . model_selection import cross_val_score from sklearn . model_selection import RepeatedStratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . neighbors import KNeighborsClassifier from sklearn . tree import DecisionTreeClassifier from sklearn . svm import SVC from sklearn . naive_bayes import GaussianNB from sklearn . ensemble import StackingClassifier from matplotlib import pyplot # get the dataset def get_dataset ( ) : X , y = make_classification ( n_samples = 1000 , n_features = 20 , n_informative = 15 , n_redundant = 5 , random_state = 1 ) return X , y # get a stacking ensemble of models def get_stacking ( ) : # define the base models level0 = list ( ) level0 . append ( ( 'lr' , LogisticRegression ( ) ) ) level0 . append ( ( 'knn' , KNeighborsClassifier ( ) ) ) level0 . append ( ( 'cart' , DecisionTreeClassifier ( ) ) ) level0 . append ( ( 'svm' , SVC ( ) ) ) level0 . append ( ( 'bayes' , GaussianNB ( ) ) ) # define meta learner model level1 = LogisticRegression ( ) # define the stacking ensemble model = StackingClassifier ( estimators = level0 , final_estimator = level1 , cv = 5 ) return model # get a list of models to evaluate def get_models ( ) : models = dict ( ) models [ 'lr' ] = LogisticRegression ( ) models [ 'knn' ] = KNeighborsClassifier ( ) models [ 'cart' ] = DecisionTreeClassifier ( ) models [ 'svm' ] = SVC ( ) models [ 'bayes' ] = GaussianNB ( ) models [ 'stacking' ] = get_stacking ( ) return models # evaluate a give model using cross-validation def evaluate_model ( model ) : cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 ) scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' ) return scores # define dataset X , y = get_dataset ( ) # get the models to evaluate models = get_models ( ) # evaluate the models and store results results , names = list ( ) , list ( ) for name , model in models . items ( ) : scores = evaluate_model ( model ) results . append ( scores ) names . append ( name ) print ( '>%s %.3f (%.3f)' % ( name , mean ( scores ) , std ( scores ) ) ) # plot model performance for comparison pyplot . boxplot ( results , labels = names , showmeans = True ) pyplot . show ( )"
233;news.mit.edu;http://news.mit.edu/2019/first-laser-ultrasound-images-humans-1219;;Researchers produce first laser ultrasound images of humans;"For most people, getting an ultrasound is a relatively easy procedure: As a technician gently presses a probe against a patient’s skin, sound waves generated by the probe travel through the skin, bouncing off muscle, fat, and other soft tissues before reflecting back to the probe, which detects and translates the waves into an image of what lies beneath.

Conventional ultrasound doesn’t expose patients to harmful radiation as X-ray and CT scanners do, and it’s generally noninvasive. But it does require contact with a patient’s body, and as such, may be limiting in situations where clinicians might want to image patients who don’t tolerate the probe well, such as babies, burn victims, or other patients with sensitive skin. Furthermore, ultrasound probe contact induces significant image variability, which is a major challenge in modern ultrasound imaging.

Now, MIT engineers have come up with an alternative to conventional ultrasound that doesn’t require contact with the body to see inside a patient. The new laser ultrasound technique leverages an eye- and skin-safe laser system to remotely image the inside of a person. When trained on a patient’s skin, one laser remotely generates sound waves that bounce through the body. A second laser remotely detects the reflected waves, which researchers then translate into an image similar to conventional ultrasound.

In a paper published today by Nature in the journal Light: Science and Applications, the team reports generating the first laser ultrasound images in humans. The researchers scanned the forearms of several volunteers and observed common tissue features such as muscle, fat, and bone, down to about 6 centimeters below the skin. These images, comparable to conventional ultrasound, were produced using remote lasers focused on a volunteer from half a meter away.

“We’re at the beginning of what we could do with laser ultrasound,” says Brian W. Anthony, a principal research scientist in MIT’s Department of Mechanical Engineering and Institute for Medical Engineering and Science (IMES), a senior author on the paper. “Imagine we get to a point where we can do everything ultrasound can do now, but at a distance. This gives you a whole new way of seeing organs inside the body and determining properties of deep tissue, without making contact with the patient.”

Early concepts for noncontact laser ultrasound for medical imaging originated from a Lincoln Laboratory program established by Rob Haupt of the Active Optical Systems Group and Chuck Wynn of the Advanced Capabilities and Technologies Group, who are co-authors on the new paper along with Matthew Johnson. From there, the research grew via collaboration with Anthony and his students, Xiang (Shawn) Zhang, who is now an MIT postdoc and is the paper’s first author, and recent doctoral graduate Jonathan Fincke, who is also a co-author. The project combined the Lincoln Laboratory researchers’ expertise in laser and optical systems with the Anthony group's experience with advanced ultrasound systems and medical image reconstruction.

Yelling into a canyon — with a flashlight

In recent years, researchers have explored laser-based methods in ultrasound excitation in a field known as photoacoustics. Instead of directly sending sound waves into the body, the idea is to send in light, in the form of a pulsed laser tuned at a particular wavelength, that penetrates the skin and is absorbed by blood vessels.

The blood vessels rapidly expand and relax — instantly heated by a laser pulse then rapidly cooled by the body back to their original size — only to be struck again by another light pulse. The resulting mechanical vibrations generate sound waves that travel back up, where they can be detected by transducers placed on the skin and translated into a photoacoustic image.

While photoacoustics uses lasers to remotely probe internal structures, the technique still requires a detector in direct contact with the body in order to pick up the sound waves. What’s more, light can only travel a short distance into the skin before fading away. As a result, other researchers have used photoacoustics to image blood vessels just beneath the skin, but not much deeper.

Since sound waves travel further into the body than light, Zhang, Anthony, and their colleagues looked for a way to convert a laser beam’s light into sound waves at the surface of the skin, in order to image deeper in the body.

Based on their research, the team selected 1,550-nanometer lasers, a wavelength which is highly absorbed by water (and is eye- and skin-safe with a large safety margin). As skin is essentially composed of water, the team reasoned that it should efficiently absorb this light, and heat up and expand in response. As it oscillates back to its normal state, the skin itself should produce sound waves that propagate through the body.

The researchers tested this idea with a laser setup, using one pulsed laser set at 1,550 nanometers to generate sound waves, and a second continuous laser, tuned to the same wavelength, to remotely detect reflected sound waves. This second laser is a sensitive motion detector that measures vibrations on the skin surface caused by the sound waves bouncing off muscle, fat, and other tissues. Skin surface motion, generated by the reflected sound waves, causes a change in the laser’s frequency, which can be measured. By mechanically scanning the lasers over the body, scientists can acquire data at different locations and generate an image of the region.

“It’s like we’re constantly yelling into the Grand Canyon while walking along the wall and listening at different locations,” Anthony says. “That then gives you enough data to figure out the geometry of all the things inside that the waves bounced against — and the yelling is done with a flashlight.”

In-home imaging

The researchers first used the new setup to image metal objects embedded in a gelatin mold roughly resembling skin’s water content. They imaged the same gelatin using a commercial ultrasound probe and found both images were encouragingly similar. They moved on to image excised animal tissue — in this case, pig skin — where they found laser ultrasound could distinguish subtler features, such as the boundary between muscle, fat, and bone.

Finally, the team carried out the first laser ultrasound experiments in humans, using a protocol that was approved by the MIT Committee on the Use of Humans as Experimental Subjects. After scanning the forearms of several healthy volunteers, the researchers produced the first fully noncontact laser ultrasound images of a human. The fat, muscle, and tissue boundaries are clearly visible and comparable to images generated using commercial, contact-based ultrasound probes.

The researchers plan to improve their technique, and they are looking for ways to boost the system’s performance to resolve fine features in the tissue. They are also looking to hone the detection laser’s capabilities. Further down the road, they hope to miniaturize the laser setup, so that laser ultrasound might one day be deployed as a portable device.

“I can imagine a scenario where you’re able to do this in the home,” Anthony says. “When I get up in the morning, I can get an image of my thyroid or arteries, and can have in-home physiological imaging inside of my body. You could imagine deploying this in the ambient environment to get an understanding of your internal state.”

This research was supported in part by the MIT Lincoln Laboratory Biomedical Line Program for the United States Air Force and by the U.S. Army Medical Research and Material Command's Military Operational Medicine Research Program."
234;towardsdatascience.com;https://towardsdatascience.com/how-we-have-beaten-the-crypto-market-using-machine-learning-a45e8a7dbdcd?source=collection_home---4------1-----------------------;2020-04-18;How We Managed to Beat the Crypto Market Using Machine Learning;"We lived in 5-star hotels, worked on trading bots and gambled in local casinos for a break. It was a surreal experience that completely changed my career. Upon leaving, I was confident that I’d take on beating the market myself, but years have passed, and I haven’t got into it.

While that experience was inspiring, it was also quite demotivating. The only person I knew who managed to beat the market was clearly out of my league, both intellectually and psychologically. He had a brilliant mind and an outstanding ability to handle stress. I had severe doubts about whether I was good enough.

Later on, I started building a company, and while hiring software developers and data scientists, I always had this potential project in mind. At some point, we seemingly got the right people, but it was too hard to justify the risk. Imagine that you’re a senior data scientist at Google making half a million a year. What are the chances you can quit your job and beat the market significantly enough to make it worth it? I would not take this risk. Similarly, we were doing some generously compensated client work and the idea of getting our top talent busy with an at least year-long experiment that would most likely fail… was simply scary.

What changed my mind was one of our projects, a liquidity provider. It was growing from primitive markets, got more and more clients among crypto derivative exchanges and was being used under increasingly competitive requirements. We hit the limits of our ability to keep the profit above zero — as our old approach was about not doing anything stupid rather than doing something particularly smart. We decided to move the goalpost from not losing money to beating the market as a way to push ourselves to the next level. Even if we failed (which was highly probable), the insights we would get during this process would most likely improve our liquidity provider.

We started building a team. We were lucky enough to have access to a mature trading infrastructure, so we could focus on data science. The core of the team were our two most experienced data scientists in time series (predictions) and clustering (market regimes) and a data engineer who helped with data processing, backtesting and tooling. Also, I was responsible for research and management. Overall, we put together a pretty well-balanced team, and it was time to get our hands dirty.

It was intense. We treated this project as a kind of intellectual war. We did not discuss or plan it; it just naturally worked out this way. It felt like this attitude was the only way to overcome the enormous gap that was separating us from the people who can consistently beat the market. We worked at full speed; the drive was going through the roof. It was R&D in its pure form: exhausting in the most exciting way possible.

The first two months were fun. We got improvement after improvement. It seemed like we were getting so close to profitability... But then we hit a plateau, and over the next two months we made next to zero progress, and there was a growing feeling of discouragement. Psychologically it was a very tough period. Can we do this? Aren’t we wasting our time?

There were several reasons behind that plateau. First, the initial progress was mostly driven by the publicly available information, and we didn’t notice how fast we got into the territory where you’re mostly on your own. Second, the further you go, the more complex and time-consuming ideas/experiments get. Third, due to overconfidence caused by the initial progress, we spread our resources too thin by working on too many things.

It was clear we needed to shake things up, so we added more structure, switched from creative chaos to a more mature and sustainable process with more granular tasks that gave us doping in the form of regular progress. Even if sometimes it was a little artificial, the motivation coming out of it made it worth it.

And we definitely got lucky. I recall at least three occasions when we got a breakthrough out of nowhere. For example, at some point, we struggled with detecting trends under specific market conditions. Among other things, we were playing with an idea that was mentioned between the lines in a hardly relevant white paper. It turned out to be a game changer. We haven’t seen that idea anywhere else, so the chances of finding it were slim to none. We would probably have eventually discovered it ourselves, but it would have taken longer, and we may have needed to cancel the project by that time. When margins are that thin, luck becomes crucial.

After about six and a half months of hard work, we saw the first consistently profitable version. Last week, almost three months later, it has reached $100k in profit. It felt like we finished the first milestone, so I wanted to sit down and put all the thoughts together.

Below, I will cover some more technical topics in detail.

Market making

On the surface, market making is a “get rich fast” strategy. Let’s take crypto derivative exchanges as an example. There you get a 0.025% rebate (negative commission) for each trade. Using the most naive market making strategy, on Bitmex alone you can make two to three thousand trades a day, which is about 50–70% a day just in rebates. Sounds good, right? But don’t hurry to buy a Lamborghini: market making is a place where dreams come to die, thanks to adverse selection (when your buy orders get executed while the market is going down and the other way around) and inventory risk (when the price jumps against your position)."
235;machinelearningmastery.com;https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/;2017-10-31;Gentle Introduction to Statistical Language Modeling and Neural Language Models;"Tweet Share Share

Last Updated on August 7, 2019

Language modeling is central to many important natural language processing tasks.

Recently, neural-network-based language models have demonstrated better performance than classical methods both standalone and as part of more challenging natural language processing tasks.

In this post, you will discover language modeling for natural language processing.

After reading this post, you will know:

Why language modeling is critical to addressing tasks in natural language processing.

What a language model is and some examples of where they are used.

How neural networks can be used for language modeling.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Updated Jun/2019: Added links to step-by-step language model tutorials.

Overview

This post is divided into 3 parts; they are:

Problem of Modeling Language Statistical Language Modeling Neural Language Models

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Problem of Modeling Language

Formal languages, like programming languages, can be fully specified.

All the reserved words can be defined and the valid ways that they can be used can be precisely defined.

We cannot do this with natural language. Natural languages are not designed; they emerge, and therefore there is no formal specification.

There may be formal rules for parts of the language, and heuristics, but natural language that does not confirm is often used. Natural languages involve vast numbers of terms that can be used in ways that introduce all kinds of ambiguities, yet can still be understood by other humans.

Further, languages change, word usages change: it is a moving target.

Nevertheless, linguists try to specify the language with formal grammars and structures. It can be done, but it is very difficult and the results can be fragile.

An alternative approach to specifying the model of the language is to learn it from examples.

2. Statistical Language Modeling

Statistical Language Modeling, or Language Modeling and LM for short, is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede it.

Language modeling is the task of assigning a probability to sentences in a language. […] Besides assigning a probability to each sequence of words, the language models also assigns a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words

— Page 105, Neural Network Methods in Natural Language Processing, 2017.

A language model learns the probability of word occurrence based on examples of text. Simpler models may look at a context of a short sequence of words, whereas larger models may work at the level of sentences or paragraphs. Most commonly, language models operate at the level of words.

The notion of a language model is inherently probabilistic. A language model is a function that puts a probability measure over strings drawn from some vocabulary.

— Page 238, An Introduction to Information Retrieval, 2008.

A language model can be developed and used standalone, such as to generate new sequences of text that appear to have come from the corpus.

Language modeling is a root problem for a large range of natural language processing tasks. More practically, language models are used on the front-end or back-end of a more sophisticated model for a task that requires language understanding.

… language modeling is a crucial component in real-world applications such as machine-translation and automatic speech recognition, […] For these reasons, language modeling plays a central role in natural-language processing, AI, and machine-learning research.

— Page 105, Neural Network Methods in Natural Language Processing, 2017.

A good example is speech recognition, where audio data is used as an input to the model and the output requires a language model that interprets the input signal and recognizes each new word within the context of the words already recognized.

Speech recognition is principally concerned with the problem of transcribing the speech signal as a sequence of words. […] From this point of view, speech is assumed to be a generated by a language model which provides estimates of Pr(w) for all word strings w independently of the observed signal […] THe goal of speech recognition is to find the most likely word sequence given the observed acoustic signal.

— Pages 205-206, The Oxford Handbook of Computational Linguistics, 2005.

Similarly, language models are used to generate text in many similar natural language processing tasks, for example:

Optical Character Recognition

Handwriting Recognition.

Machine Translation.

Spelling Correction.

Image Captioning.

Text Summarization

And much more.

Language modeling is the art of determining the probability of a sequence of words. This is useful in a large variety of areas including speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction

— A Bit of Progress in Language Modeling, 2001.

Developing better language models often results in models that perform better on their intended natural language processing task. This is the motivation for developing better and more accurate language models.

[language models] have played a key role in traditional NLP tasks such as speech recognition, machine translation, or text summarization. Often (although not always), training better language models improves the underlying metrics of the downstream task (such as word error rate for speech recognition, or BLEU score for translation), which makes the task of training better LMs valuable by itself.

— Exploring the Limits of Language Modeling, 2016.

3. Neural Language Models

Recently, the use of neural networks in the development of language models has become very popular, to the point that it may now be the preferred approach.

The use of neural networks in language modeling is often called Neural Language Modeling, or NLM for short.

Neural network approaches are achieving better results than classical methods both on standalone language models and when models are incorporated into larger models on challenging tasks like speech recognition and machine translation.

A key reason for the leaps in improved performance may be the method’s ability to generalize.

Nonlinear neural network models solve some of the shortcomings of traditional language models: they allow conditioning on increasingly large context sizes with only a linear increase in the number of parameters, they alleviate the need for manually designing backoff orders, and they support generalization across different contexts.

— Page 109, Neural Network Methods in Natural Language Processing, 2017.

Specifically, a word embedding is adopted that uses a real-valued vector to represent each word in a project vector space. This learned representation of words based on their usage allows words with a similar meaning to have a similar representation.

Neural Language Models (NLM) address the n-gram data sparsity issue through parameterization of words as vectors (word embeddings) and using them as inputs to a neural network. The parameters are learned as part of the training process. Word embeddings obtained through NLMs exhibit the property whereby semantically close words are likewise close in the induced vector space.

— Character-Aware Neural Language Model, 2015.

This generalization is something that the representation used in classical statistical language models can not easily achieve.

“True generalization” is difficult to obtain in a discrete word indice space, since there is no obvious relation between the word indices.

— Connectionist language modeling for large vocabulary continuous speech recognition, 2002.

Further, the distributed representation approach allows the embedding representation to scale better with the size of the vocabulary. Classical methods that have one discrete representation per word fight the curse of dimensionality with larger and larger vocabularies of words that result in longer and more sparse representations.

The neural network approach to language modeling can be described using the three following model properties, taken from “A Neural Probabilistic Language Model“, 2003.

Associate each word in the vocabulary with a distributed word feature vector. Express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence. Learn simultaneously the word feature vector and the parameters of the probability function.

This represents a relatively simple model where both the representation and probabilistic model are learned together directly from raw text data.

Recently, the neural based approaches have started to and then consistently started to outperform the classical statistical approaches.

We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.

— Recurrent neural network based language model, 2010.

Initially, feed-forward neural network models were used to introduce the approach.

More recently, recurrent neural networks and then networks with a long-term memory like the Long Short-Term Memory network, or LSTM, allow the models to learn the relevant context over much longer input sequences than the simpler feed-forward networks.

[an RNN language model] provides further generalization: instead of considering just several preceding words, neurons with input from recurrent connections are assumed to represent short term memory. The model learns itself from the data how to represent memory. While shallow feedforward neural networks (those with just one hidden layer) can only cluster similar words, recurrent neural network (which can be considered as a deep architecture) can perform clustering of similar histories. This allows for instance efficient representation of patterns with variable length.

— Extensions of recurrent neural network language model, 2011.

Recently, researchers have been seeking the limits of these language models. In the paper “Exploring the Limits of Language Modeling“, evaluating language models over large datasets, such as the corpus of one million words, the authors find that LSTM-based neural language models out-perform the classical methods.

… we have shown that RNN LMs can be trained on large amounts of data, and outperform competing models including carefully tuned N-grams.

— Exploring the Limits of Language Modeling, 2016.

Further, they propose some heuristics for developing high-performing neural language models in general:

Size matters . The best models were the largest models, specifically number of memory units.

. The best models were the largest models, specifically number of memory units. Regularization matters . Use of regularization like dropout on input connections improves results.

. Use of regularization like dropout on input connections improves results. CNNs vs Embeddings . Character-level Convolutional Neural Network (CNN) models can be used on the front-end instead of word embeddings, achieving similar and sometimes better results.

. Character-level Convolutional Neural Network (CNN) models can be used on the front-end instead of word embeddings, achieving similar and sometimes better results. Ensembles matter. Combining the prediction from multiple models can offer large improvements in model performance.

Language Model Tutorials

This section lists some step-by-step tutorials for developing deep learning neural network language models.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Books

Papers

Articles

Summary

In this post, you discovered language modeling for natural language processing tasks.

Specifically, you learned:

That natural language is not formally specified and requires the use of statistical models to learn from examples.

That statistical language models are central to many challenging natural language processing tasks.

That state-of-the-art results are achieved using neural language models, specifically those with word embeddings and recurrent neural network algorithms.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside"
236;news.mit.edu;http://news.mit.edu/2020/mit-entrepreneur-innovation-covid-19-0402;;MIT’s entrepreneurial ecosystem steps up to the challenge of Covid-19;"Innovation and entrepreneurship aren’t easy. New companies are forced to make due with minimal resources. Decisions must be made in the face of great uncertainty. Conditions change rapidly.

Perhaps unsurprisingly then, MIT’s I&E community has stepped up to the unforeseen challenges of the Covid-19 pandemic. Groups from many corners of the Institute are adapting to the myriad disruptions brought on by the emergency and spearheading efforts to help the people most affected.

At a time when most students would be on spring break, many were collaborating on projects and participating in hacking workshops to respond to Covid-19. And as faculty and staff develop new curricula and support structures, they’re focusing on the needs of their students with the same devotion entrepreneurs must focus on their customers.

Above all, members of the MIT community have treated the challenges presented by Covid-19 as opportunities to help. Perhaps nowhere is that more apparent than the Covid-19 Rapid Innovation Dashboard, which was just a rough idea as recently as March 16, but is now a bustling hub of MIT’s Covid-19-related activities. Projects on the dashboard include an initiative to help low-income K-12 students with school shutdowns, an effort leveraging behavioral science to reduce the spread of misinformation about the virus, and multiple projects aimed at improving access to ventilators.

People following those projects would hardly suspect the participants have been uprooted from their lives and forced to radically change the way they work.

“We never would’ve wished this on anybody, but I feel like we’re ready for it,” says Bill Aulet, the managing director of the Martin Trust Center for MIT Entrepreneurship and a professor of the practice at MIT’s Sloan School of Management. “Working in an environment of great change, if you’re a great entrepreneur, is playing to your strengths. I think the students will rise to the occasion, and that’s what we’re seeing now.”

The Rapid Innovation Dashboard

In the second week of March, as the global consequences of Covid-19’s spread were becoming apparent, members of the MIT Innovation Initiative began getting contacted by members of the MIT community looking for ways to help.

Most people wanted information on the various grassroots projects that had sprouted up around campus to address disruptions related to the spread of the virus. Some people were looking for ways to promote their projects and get support.

MITii’s team began brainstorming ways to help fill in those gaps, officially beginning work on the dashboard the week of March 16 — the same time staff members began working remotely.

“From ideation to whiteboarding, to concept, to iteration, to launch, we did it all in real time, and we went from idea to standing the dashboard up in four days,” MITii executive director Gene Keselman says. “It was beautiful for all of us innovation nerds.”

The site launched on March 19 with six projects. Today there are 50 live projects on the site and counting. Some of them deal with mechanical or scientific problems, like the aforementioned efforts to improve access to ventilators, while others are more data-focused, like an initiative to track the spread of the virus at the county level. Still others are oriented toward wellness, like a collection of MIT-related coloring pages for destressing.

“A lot of the things we’re seeing are data-driven, creative-driven projects to get people involved and get them feeling like they’re making an impact,” Keselman says.

The current dashboard is version 1.0 of an ongoing project that will continue to evolve based on the community’s needs. Down the line, the MITii team is considering ways to better connect the MIT community with investors looking to fund projects related to the virus.

“This is going to be a long term problem, and even when we go back to the office, issues will persist, we’ll be dealing with things that are the runoff from Covid-19,” Keselman says. “There will be an opportunity to keep this thing going to solve all kinds of second- or third-order problems.”

Overcoming adversity

The dashboard is just one example of how different entrepreneurial organizations on campus are stepping up to the challenges of Covid-19. The Trust Center is encouraging students to leverage its Orbit app, to get help from entrepreneurs in residence, engage with other members of MIT’s entrepreneurial community, and navigate MIT’s myriad entrepreneurial resources. And in response to Covid-19, the Trust Center launched the Antifragile Entrepreneurship Speaker Series to provide thought leadership to students.

“We’ve revitalized our speaker series,” Aulet says. “We used to fly people in, but now we can have anyone. They’re sitting at home, they’re bored, and we can have more interaction than we did before. We try to create antifragile humans, and antifragile humans excel in times like this.”

MIT D-Lab, where hands-on learning and common makerspaces are central to operations, is just one example of an area where faculty members are taking this opportunity to try new ways of managing projects and rethinking their curriculum.

“We’re in a real brainstorming phase right now, in the best sense of the word — throwing out all the wild ideas that come to us, and entertaining anything as we decide how to move forward,” Libby Hsu, a lecturer and academic program manager at D-Lab, told MIT News the week before MIT classes resumed. “We’re getting ready to ship materials and tools to students at their homes. We’re studying how to use Zoom to facilitate project work student teams have already put in. We’re realistically re-assessing what deliverables we could ask of students to help D-Lab staff prototype things for them here on campus, perhaps later in the semester or over the summer.”

Other entrepreneurial groups on campus, like the Venture Mentoring Service, MIT Sandbox, and the Legatum Center, are similarly adopting virtualized support mechanisms.

On March 5, MIT Solve, which uses social impact challenges to tackle the world’s biggest problems, launched a new Global Challenge seeking innovations around the prevention, detection, and response of Covid-19. The winning team will receive a $10,000 grant to further develop their solution.

The students themselves, of course, are also organizing initiatives. In addition to the student-led projects in the Rapid Innovation Dashboard, student finalists in this year's MIT IDEAS Social Innovation Challenge have leveraged their entrepreneurial experience to help address equipment shortages and assist communities in fulfilling changing food and shelter needs. And a new community initiative, the MIT COVID-19 Challenge, held its first virtual “ideathon” this past weekend, with another major event April 3-5.

Indeed, Keselman could’ve been talking about any group on campus when he said of his team at MITii, “We feel like we lived an entire lifetime in just the last week.”

The early efforts may not have been the way many participants expected to spend their spring break, but in the entrepreneurial world, new challenges are par for the course.

“Being knocked out of your homeostasis is a good thing and a bad thing, and it’s an entrepreneur’s job to make it more of a good thing than a bad thing,” Aulet says. “I think we’ll come out of this utilizing technology to have more efficient, more effective, more inclusive engagements. Is this disrupting the entrepreneurial ecosystem? Absolutely. Should we come out of it stronger? Absolutely.”"
237;news.mit.edu;http://news.mit.edu/2020/stimulus-jonathan-parker-economic-recovery-0330;;3 Questions: Jonathan Parker on building an economic recovery;"The Covid-19 pandemic is a public health crisis with enormous economic implications: As much of the U.S. reduces daily activity in spring 2020, unemployment is already surging and experts are forecasting major drops in GDP during the second quarter of the year. U.S. Congress has also just passed a $2 trillion aid package for individuals and businesses.

To assess the current state of the economy, MIT News contacted Jonathan Parker, the Robert C. Merton Professor of Finance at the MIT Sloan School of Management. Among his other areas of research, Parker is a leading expert in understanding how U.S. citizens use stimulus payments from the government, and how big an impact such efforts make on GDP and the macroeconomy.

Q: What are the particular effects of the Covid-19 pandemic on the economy, and how should economic policy be used to respond?

A: Unlike in the typical recession, the main responsibility of our government today is not directly economic policy. First and foremost, we have to focus on winning the medical war against the virus. This not only saves lives, but is also the best way to help the economy. However, the war hasn’t gone well at this point, and for good public health reasons we have shut down large parts of our economy. People are not going to work, producing goods, and earning income, and people are avoiding the types of consumption that would put them in crowded places. So, there is going to be a huge collapse in GDP and national income.

Q: The U.S. Congress just passed a $2 trillion aid package to help compensate for the drastic economic slowdown. To what extent can such policy measures maintain incomes?

A: There is no way for us to make up the lost income, because we have lost it by not producing the goods and services that earn it. That said, we can transfer money to people so that the most vulnerable people don’t lose access completely to the goods and services that we do have. And that is part of what House and Senate leaders have just done in passing the recent relief package. The bill includes what are now being called “stimulus payments” to send around $1,200 out to American households. [The package also includes enhanced unemployment insurance for many people, as well as other aid for people adversely affected by the shutdown.]

While this is called stimulus, it is better thought of as disaster insurance for now. We don’t want the economy stimulated. People should be staying home. But the hardest hit need to be able to pay bills and eat. Ideally, we would freeze time for the period when we are isolating, to limit the spread of the virus and allow the government to catch up with the production of virus-wartime medical supplies like ventilators and masks and test kits, so that we can move from isolating all of us to isolating only the sick. And then having frozen time, we would restart the economy where we were before. Sending out checks to people allows those at the bottom of the income and wealth distribution to survive this freeze, and is part of restarting where we left off.

Q: Don’t we need to give significant funds to businesses for the same reason?

A: No, and yes. Starting with “no,” we don’t have to give funds to large firms, or even make them favorable loans. In the American economic system, when large companies that are profitable in the long run go bankrupt, they continue to operate and employ Americans, and emerge from bankruptcy sometimes stronger than before. This happened for General Motors in the financial crisis, and American Airlines operated for years in bankruptcy. For large companies, bankruptcy is only about the division of profits between stockholders and bondholders, not about whether the company continues to operate, so loans and transfers to large corporations almost exclusively benefit the stockholders.

U.S. stocks are owned by the very wealthiest people all over the world, and I think it is a mistake for the stimulus program to be transferring money from taxpayers to the world’s wealthiest people right now (or any time). The parts of the $2 trillion bill that are for supporting large firms are incorrectly fighting the last war. In 2008, the government supported banks because they were all threatened and, like Lehman Brothers, they cannot survive bankruptcy. So, this aspect of the current legislation is a mistake.

But there is an important answer of “yes,” also. First, in crisis times, there is a large increase in the demand for money and safe money-like assets so that financial markets can function. The Federal Reserve is tasked with providing the money and money-like assets that are appropriate with the demands of businesses, and it is doing this nicely. This type of support makes the taxpayer money, so it’s a win-win situation, not a bailout. Of course, this legislation also has the Treasury involved and is supporting private bond markets, and while this can also help, we have to look more closely at what is and is not a subsidy from taxpayers to stockholders of big firms, rather than an aid to the economy.

Second, small businesses need help to survive this crisis. Small businesses do not survive bankruptcy. While many will be able to renegotiate leases and bank loans and so forth, many others will not. Thus, I am highly supportive of the parts of this bill that provide somewhat-subsidized loans to small businesses to keep them operational through the economic hard times. Again, we want to be able to restart the economy when the virus threat is contained, and to do that, we want our small businesses to also be able to restart and thrive."
238;news.mit.edu;http://news.mit.edu/2020/refrigerator-super-cools-molecules-nanokelvin-temperatures-0408;;New “refrigerator” super-cools molecules to nanokelvin temperatures;"For years, scientists have looked for ways to cool molecules down to ultracold temperatures, at which point the molecules should slow to a crawl, allowing scientists to precisely control their quantum behavior. This could enable researchers to use molecules as complex bits for quantum computing, tuning individual molecules like tiny knobs to carry out multiple streams of calculations at a time.

While scientists have super-cooled atoms, doing the same for molecules, which are more complex in their behavior and structure, has proven to be a much bigger challenge.

Now MIT physicists have found a way to cool molecules of sodium lithium down to 200 billionths of a Kelvin, just a hair above absolute zero. They did so by applying a technique called collisional cooling, in which they immersed molecules of cold sodium lithium in a cloud of even colder sodium atoms. The ultracold atoms acted as a refrigerant to cool the molecules even further.

Collisional cooling is a standard technique used to cool down atoms using other, colder atoms. And for more than a decade, researchers have attempted to supercool a number of different molecules using collisional cooling, only to find that when molecules collided with atoms, they exchanged energy in such a way that the molecules were heated or destroyed in the process, called “bad” collisions.

In their own experiments, the MIT researchers found that if sodium lithium molecules and sodium atoms were made to spin in the same way, they could avoid self-destructing, and instead engaged in “good” collisions, where the atoms took away the molecules’ energy, in the form of heat. The team used precise control of magnetic fields and an intricate system of lasers to choreograph the spin and the rotational motion of the molecules. As result, the atom-molecule mixture had a high ratio of good-to-bad collisions and was cooled down from 2 microkelvins to 220 nanokelvins.

“Collisional cooling has been the workhorse for cooling atoms,” adds Nobel Prize laureate Wolfgang Ketterle, the John D. Arthur professor of physics at MIT. “I wasn’t convinced that our scheme would work, but since we didn’t know for sure, we had to try it. We know now that it works for cooling sodium lithium molecules. Whether it will work for other classes of molecules remains to be seen.”

Their findings, published today in the journal Nature, mark the first time researchers have successfully used collisional cooling to cool molecules down to nanokelvin temperatures.

Ketterle’s coauthors on the paper are lead author Hyungmok Son, a graduate student in Harvard University’s Department of Physics, along with MIT physics graduate student Juliana Park, and Alan Jamison, a professor of physics and member of the Institute for Quantum Computing at the University of Waterloo and visiting scientist in MIT’s Research Laboratory of Electronics.

Reaching ultralow temperatures

In the past, scientists found that when they tried to cool molecules down to ultracold temperatures by surrounding them with even colder atoms, the particles collided such that the atoms imparted extra energy or rotation to the molecules, sending them flying out of the trap, or self-destructing all together by chemical reactions.

The MIT researchers wondered whether molecules and atoms, having the same spin, could avoid this effect, and remain ultracold and stable as a result. They looked to test their idea with sodium lithium, a “diatomic” molecule that Ketterle’s group experiments with regularly, consisting of one lithium and one sodium atom.

“Sodium lithium molecules are quite different from other molecules people have tried,” Jamison says. “Many folks expected those differences would make cooling even less likely to work. However, we had a feeling these differences could be an advantage instead of a detriment.”

The researchers fine-tuned a system of more than 20 laser beams and various magnetic fields to trap and cool atoms of sodium and lithium in a vacuum chamber, down to about 2 microkelvins — a temperature Son says is optimal for the atoms to bond together as sodium lithium molecules.

Once the researchers were able to produce enough molecules, they shone laser beams of specific frequencies and polarizations to control the quantum state of the molecules and carefully tuned microwave fields to make atoms spin in the same way as the molecules. “Then we make the refrigerator colder and colder,” says Son, referring to the sodium atoms that surround the cloud of the newly formed molecules. “We lower the power of the trapping laser, making the optical trap looser and looser, which brings the temperature of sodium atoms down, and further cools the molecules, to 200 billionths of a kelvin.”

The group observed that the molecules were able to remain at these ultracold temperatures for up to one second. “In our world, a second is very long,” Ketterle says. “What you want to do with these molecules is quantum computation and exploring new materials, which all can be done in small fractions of a second.”

If the team can get sodium lithium molecules to be about five times colder than what they have so far achieved, they will have reached a so-called quantum degenerate regime where individual molecules become indistinguishable and their collective behavior is controlled by quantum mechanics. Son and his colleagues have some ideas for how to achieve this, which will involve months of work in optimizing their setup, as well as acquiring a new laser to integrate into their setup.

“Our work will lead to discussion in our community why collisional cooling has worked for us but not for others,” Son says “Perhaps we will soon have predictions how other molecules could be cooled in this way.”

This research was funded, in part, by the National Science Foundation, NASA, and the Samsung Scholarship."
239;news.mit.edu;http://news.mit.edu/2019/hello-world-hello-mit;;Hello, World. Hello, MIT.;"MIT will celebrate the launch of the new MIT Stephen A. Schwarzman College of Computing with three days of event programming that will explore the future of computing education and research, and offer insights from leading experts in computer science, artificial intelligence (AI), education, ethics, and more. Events take place Tuesday, Feb. 26 – Thursday, Feb. 28, with most activities in the Kresge Auditorium. Reporters are invited to attend.

The celebration opens with a range of student-centered experiences across campus, including a panel discussion on Tuesday with pioneering women working in the field of computing. It continues on Wednesday with a full-day academic symposium followed by an evening fireside chat with MIT’s recipients of the Turing Award, often described as the “Nobel Prize of computing.”

Those activities build up to a Thursday program designed to engage a broad audience. Massachusetts Governor Charlie Baker will kick off Thursday’s morning program, featuring TED-style talks from MIT faculty and remarks by Alphabet’s Eric Schmidt, as well as a panel of entrepreneurs and investors on computing for the marketplace. The afternoon features a Q&A session with MIT President L. Rafael Reif and Stephen A. Schwarzman, the chairman, CEO, and co-founder of Blackstone; a discussion led by New York Times columnist and author Tom Friedman with former U.S. Secretary of State Henry Kissinger; and a panel discussion on “Computing for the People” chaired by Prof. Melissa Nobles, dean of MIT’s School of Humanities, Arts, and Social Sciences, and moderated by Friedman.

Announced in October 2018, the MIT Stephen A. Schwarzman College of Computing was established as part of MIT’s $1 billion commitment to advancing the study of computing and AI and its ethical and societal implications, while also integrating computer science education with every academic discipline at MIT. The new college is slated to open in September 2019 and will serve as an interdisciplinary hub for work in computer science, AI, data science, and related fields.

Additional details about the MIT Stephen A. Schwarzman College of Computing launch celebration are available at: https://helloworld.mit.edu. The celebration theme – “Hello, World. Hello, MIT.” – is a nod to a tradition in computer science of using “Hello, World” when testing a new program.

MEDIA RSVP:

Journalists interested in attending are encouraged to email Abby Abazorius at abbya@mit.edu or expertrequests@mit.edu for more information or to RSVP."
240;news.mit.edu;http://news.mit.edu/2020/titans-missing-river-deltas-and-earthly-climate-connection-samuel-birch-0408;;Titan’s missing river deltas and an Earthly climate connection;"“I’ll never forget the moment when I first saw new Cassini data come down from Titan’s surface,” says Samuel Birch. “I was in awe at witnessing this brand new, never-seen-before bit of our solar system.”

Birch explores and models the evolution of the surfaces of planets, moons, and small bodies in the outer solar system, including Saturn’s largest moon, Titan, and the Comet 67P/Churyumov-Gerasimenko — two very different, icy worlds investigated by the spacecraft Cassini and Rosetta. He joins MIT this summer as one of eight recipients of the 2020 Heising-Simons Foundation 51 Pegasi b Fellows bridging planetary science and astronomy, accelerating our understanding of planetary system formation and evolution, and advance new technologies for detecting Earthlike worlds.

Over the years, the Heising-Simons Foundation has generously supported a growing cohort of exoplanet researchers at MIT, including Jason Dittmann, Ian Wong, Ben Rackham, Clara Sousa-Silva, and now Samuel Birch, a research associate from Cornell University. In the coming three years, with support networks, mentorship from MIT Department of Earth, Atmospheric and Planetary Science (EAPS) members like Professor Taylor Perron and Research Scientist Jason Soderblom, and a grant of up to $375,000, Birch will have the space and time to fully explore ideas, deciphering what the surfaces of those objects tell us about their climatological past and potential habitability. He’ll also develop and operate related spacecraft missions and mission concepts that seek to study edges of our solar system.

“I like to think of myself as an explorer of the outer solar system, trying to figure what is shaping the weird landscapes on these icy worlds,” Birch says.

Not quite familiar territory

As scientists learn more about the geophysics of Saturn’s moon Titan, their findings motivate newer and bigger questions that extend to Earth and other planetary bodies, highlighting the need for its continued study. “Titan’s surface is perhaps the most intriguing in our solar system, as there are rivers and seas of liquid methane and sand dunes made of organic plastics — all the result of a dense, nitrogen-dominated atmosphere,” says Birch. With a salty liquid water ocean beneath the surface, and an icy exterior sculpted by rivers, seas, and waves, Titan’s hydrologic cycle is similar to Earth’s. However, when its coastal rivers meet the lakes and sea, they seem to be missing deltas at their ends, Birch says. This may be because deltas like those on Earth do not form (or rarely form) because of differences in materials, dynamics, and coastal conditions. Alternately, their characteristics and representation in Cassini datasets may make them difficult to identify.

To solve this mystery, Birch and MIT researchers will investigate deltaic and river dynamics, using a combination of theoretical, experimental, and numerical modeling, atmospheric simulations, and a re-evaluation of Cassini data for evidence of the resulting landforms. This suite of studies will help them understand what a delta “looks” like and map their distribution, which may unveil a record of Titan’s climate history and reveal how liquid methane has molded its landscapes.

“If we can understand the reasons for the stark differences between Earth and Titan — and with it, the fate of all the mass eroded by Titan’s rivers,” Birch says, “we have the chance to really advance our knowledge of the history of erosion, sea-level, and climate change on Titan.”

Life extensions

This work inherently informs the study of fundamental Earthlike surface processes related to climate and the search for life beyond Earth. Since Titan lacks the complex interplay of diverse physical and chemical processes of Earth’s biosphere — like active tectonics, variable bedrock lithologies, diverse climate zones, vegetation, and (as far as we know) organisms — the moon serves as a natural laboratory for studying the effects of sea-level change on shoreline, river, and delta evolution. Additionally, scientists target deltas because of their high astrobiological potential for harboring life, like those on Mars. Analogous, active environments like Titan’s offer promise for the upcoming Dragonfly mission — when a nuclear-powered, dual-quadcopter will explore the moon, and perhaps these valuable spots.

In the long run, Birch would like to parlay the skills he cultivates here to develop his own research group and continue to participate in missions that address key questions regarding the evolution of planetary surfaces. “I am extremely honored by this opportunity and that the community and the Heising-Simons Foundation value my work … I am fortunate that the mentors I will have at MIT are some of the best in the field,” Birch says, acknowledging the support of his collaborators and advisor, and welcoming the challenge and rewards that the future research will bring. “It is a fantastic opportunity and can’t wait to see what we can all discover on Titan and elsewhere!”

The Heising-Simons Foundation is a family foundation based in Los Altos and San Francisco, California. The foundation works with its many partners to advance sustainable solutions in climate and clean energy, enable groundbreaking research in science, enhance the education of our youngest learners, and support human rights for all people. In addition to Birch, other fellows selected in this year’s cohort will join their host institutions: Elizabeth Bailey at the University of California at Santa Cruz; Ashley Baker and Kimberly Moore at Caltech; Emilie Dunham at the University of California at Los Angeles; Emily First and Eileen Gonzales at Cornell University; and Benjamin Tofflemire at University of Texas at Austin."
241;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-cyclegan-models-from-scratch-with-keras/;2019-08-06;How to Implement CycleGAN Models From Scratch With Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141

# example of defining composite models for training cyclegan generators from keras . optimizers import Adam from keras . models import Model from keras . models import Sequential from keras . models import Input from keras . layers import Conv2D from keras . layers import Conv2DTranspose from keras . layers import Activation from keras . layers import LeakyReLU from keras . initializers import RandomNormal from keras . layers import Concatenate from keras_contrib . layers . normalization . instancenormalization import InstanceNormalization from keras . utils . vis_utils import plot_model # define the discriminator model def define_discriminator ( image_shape ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # source image input in_image = Input ( shape = image_shape ) # C64 d = Conv2D ( 64 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( in_image ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C128 d = Conv2D ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C256 d = Conv2D ( 256 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C512 d = Conv2D ( 512 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # second last output layer d = Conv2D ( 512 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # patch output patch_out = Conv2D ( 1 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) # define model model = Model ( in_image , patch_out ) # compile model model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.0002 , beta_1 = 0.5 ) , loss_weights = [ 0.5 ] ) return model # generator a resnet block def resnet_block ( n_filters , input_layer ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # first layer convolutional layer g = Conv2D ( n_filters , ( 3 , 3 ) , padding = 'same' , kernel_initializer = init ) ( input_layer ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # second convolutional layer g = Conv2D ( n_filters , ( 3 , 3 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) # concatenate merge channel-wise with input layer g = Concatenate ( ) ( [ g , input_layer ] ) return g # define the standalone generator model def define_generator ( image_shape , n_resnet = 9 ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # image input in_image = Input ( shape = image_shape ) # c7s1-64 g = Conv2D ( 64 , ( 7 , 7 ) , padding = 'same' , kernel_initializer = init ) ( in_image ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # d128 g = Conv2D ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # d256 g = Conv2D ( 256 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # R256 for _ in range ( n_resnet ) : g = resnet_block ( 256 , g ) # u128 g = Conv2DTranspose ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # u64 g = Conv2DTranspose ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # c7s1-3 g = Conv2D ( 3 , ( 7 , 7 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) out_image = Activation ( 'tanh' ) ( g ) # define model model = Model ( in_image , out_image ) return model # define a composite model for updating generators by adversarial and cycle loss def define_composite_model ( g_model_1 , d_model , g_model_2 , image_shape ) : # ensure the model we're updating is trainable g_model_1 . trainable = True # mark discriminator as not trainable d_model . trainable = False # mark other generator model as not trainable g_model_2 . trainable = False # discriminator element input_gen = Input ( shape = image_shape ) gen1_out = g_model_1 ( input_gen ) output_d = d_model ( gen1_out ) # identity element input_id = Input ( shape = image_shape ) output_id = g_model_1 ( input_id ) # forward cycle output_f = g_model_2 ( gen1_out ) # backward cycle gen2_out = g_model_2 ( input_id ) output_b = g_model_1 ( gen2_out ) # define model graph model = Model ( [ input_gen , input_id ] , [ output_d , output_id , output_f , output_b ] ) # define optimization algorithm configuration opt = Adam ( lr = 0.0002 , beta_1 = 0.5 ) # compile model with weighting of least squares loss and L1 loss model . compile ( loss = [ 'mse' , 'mae' , 'mae' , 'mae' ] , loss_weights = [ 1 , 5 , 10 , 10 ] , optimizer = opt ) return model # input shape image_shape = ( 256 , 256 , 3 ) # generator: A -> B g_model_AtoB = define_generator ( image_shape ) # generator: B -> A g_model_BtoA = define_generator ( image_shape ) # discriminator: A -> [real/fake] d_model_A = define_discriminator ( image_shape ) # discriminator: B -> [real/fake] d_model_B = define_discriminator ( image_shape ) # composite: A -> B -> [real/fake, A] c_model_AtoB = define_composite_model ( g_model_AtoB , d_model_B , g_model_BtoA , image_shape ) # composite: B -> A -> [real/fake, B] c_model_BtoA = define_composite_model ( g_model_BtoA , d_model_A , g_model_AtoB , image_shape )"
242;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-an-intuition-for-probability-with-worked-examples/;2019-10-01;How to Develop an Intuition for Probability With Worked Examples;"Tweet Share Share

Last Updated on November 1, 2019

Probability calculations are frustratingly unintuitive.

Our brains are too eager to take shortcuts and get the wrong answer, instead of thinking through a problem and calculating the probability correctly.

To make this issue obvious and aid in developing intuition, it can be useful to work through classical problems from applied probability. These problems, such as the birthday problem, boy or girl problem, and the Monty Hall problem trick us with the incorrect intuitive answer and require a careful application of the rules of marginal, conditional, and joint probability in order to arrive at the correct solution.

In this post, you will discover how to develop an intuition for probability by working through classical thought-provoking problems.

After reading this post, you will know:

How to solve the birthday problem by multiplying probabilities together.

How to solve the boy or girl problem using conditional probability.

How to solve the Monty Hall problem using joint probability.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Birthday Problem Boy or Girl Problem Monty Hall Problem

Birthday Problem

A classic example of applied probability involves calculating the probability of two people having the same birthday.

It is a classic example because the result does not match our intuition. As such, it is sometimes called the birthday paradox.

The problem can be generally stated as:

Problem: How many people are required so that any two people in the group have the same birthday with at least a 50-50 chance?

There are no tricks to this problem; it involves simply calculating the marginal probability.

It is assumed that the probability of a randomly selected person having a birthday on any given day of the year (excluding leap years) is uniformly distributed across the days of the year, e.g. 1/365 or about 0.273%.

Our intuition might leap to an answer and assume that we might need at least as many people as there are days in the year, e.g. 365. Our intuition likely fails because we are thinking about ourselves and other people matching our own birthday. That is, we are thinking about how many people are needed for another person born on the same day as you. That is a different question.

Instead, to calculate the solution, we can think about comparing pairs of people within a group and the probability of a given pair being born on the same day. This unlocks the calculation required.

The number of pairwise comparisons within a group (excluding comparing each person with themselves) is calculated as follows:

comparisons = n * (n – 1) / 2

For example, if we have a group of five people, we would be doing 10 pairwise comparisons among the group to check if they have the same birthday, which is more opportunity for a hit than we might expect. Importantly, the number of comparisons within the group increases exponentially with the size of the group.

One more step is required. It is easier to calculate the inverse of the problem. That is, the probability that two people in a group do not have the same birthday. We can then invert the final result to give the desired probability, for example:

p(2 in n people have the same birthday) = 1 – p(2 in n people do not have the same birthday)

We can see why calculating the probability of non-matching birthdays is easy with an example with a small group, in this case, three people.

People can be added to the group one-by-one. Each time a person is added to the group, it decreases the number of available days where there is no birthday in the year, decreasing the number of available days by one. For example 365 days, 364 days, etc.

Additionally, the probability of a non-match for a given additional person added to the group must be combined with the prior calculated probabilities before it. For example P(n=2) * P(n=3), etc.

This gives the following, calculating the probability of no matching birthdays with a group size of three:

P(n=3) = 365/365 * 364/365 * 363/365

P(n=3) = 99.18%

Inverting this gives about 0.820% of a matching birthday among a group of three people.

Stepping through this, the first person has a birthday, which reduces the number of candidate days for the rest of the group from 365 to 364 unused days (i.e. days without a birthday). For the second person, we calculate the probability of a conflicting birthday as 364 safe days from 365 days in the year or about a (364/365) 99.72% probability of not having the same birthday. We now subtract the second person’s birthday from the number of available days to give 363. The probability of the third person of not having a matching birthday is then given as 363/365 multiplied by the prior probability to give about 99.18%

This calculation can get tedious for large groups, therefore we might want to automate it.

The example below calculates the probabilities for group sizes from two to 30.

# example of the birthday problem # define group size n = 30 # number of days in the year days = 365 # calculate probability for different group sizes p = 1.0 for i in range(1, n): av = days - i p *= av / days print('n=%d, %d/%d, p=%.3f 1-p=%.3f' % (i+1, av, days, p*100, (1-p)*100)) 1 2 3 4 5 6 7 8 9 10 11 # example of the birthday problem # define group size n = 30 # number of days in the year days = 365 # calculate probability for different group sizes p = 1.0 for i in range ( 1 , n ) : av = days - i p * = av / days print ( 'n=%d, %d/%d, p=%.3f 1-p=%.3f' % ( i + 1 , av , days , p* 100 , ( 1 - p ) * 100 ) )

Running the example first prints the group size, then the available days divided by the total days in the year, then the probability of no matching birthdays in the group followed by the complement or the probability of two people having a birthday in the group.

n=2, 364/365, p=99.726 1-p=0.274 n=3, 363/365, p=99.180 1-p=0.820 n=4, 362/365, p=98.364 1-p=1.636 n=5, 361/365, p=97.286 1-p=2.714 n=6, 360/365, p=95.954 1-p=4.046 n=7, 359/365, p=94.376 1-p=5.624 n=8, 358/365, p=92.566 1-p=7.434 n=9, 357/365, p=90.538 1-p=9.462 n=10, 356/365, p=88.305 1-p=11.695 n=11, 355/365, p=85.886 1-p=14.114 n=12, 354/365, p=83.298 1-p=16.702 n=13, 353/365, p=80.559 1-p=19.441 n=14, 352/365, p=77.690 1-p=22.310 n=15, 351/365, p=74.710 1-p=25.290 n=16, 350/365, p=71.640 1-p=28.360 n=17, 349/365, p=68.499 1-p=31.501 n=18, 348/365, p=65.309 1-p=34.691 n=19, 347/365, p=62.088 1-p=37.912 n=20, 346/365, p=58.856 1-p=41.144 n=21, 345/365, p=55.631 1-p=44.369 n=22, 344/365, p=52.430 1-p=47.570 n=23, 343/365, p=49.270 1-p=50.730 n=24, 342/365, p=46.166 1-p=53.834 n=25, 341/365, p=43.130 1-p=56.870 n=26, 340/365, p=40.176 1-p=59.824 n=27, 339/365, p=37.314 1-p=62.686 n=28, 338/365, p=34.554 1-p=65.446 n=29, 337/365, p=31.903 1-p=68.097 n=30, 336/365, p=29.368 1-p=70.632 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 n=2, 364/365, p=99.726 1-p=0.274 n=3, 363/365, p=99.180 1-p=0.820 n=4, 362/365, p=98.364 1-p=1.636 n=5, 361/365, p=97.286 1-p=2.714 n=6, 360/365, p=95.954 1-p=4.046 n=7, 359/365, p=94.376 1-p=5.624 n=8, 358/365, p=92.566 1-p=7.434 n=9, 357/365, p=90.538 1-p=9.462 n=10, 356/365, p=88.305 1-p=11.695 n=11, 355/365, p=85.886 1-p=14.114 n=12, 354/365, p=83.298 1-p=16.702 n=13, 353/365, p=80.559 1-p=19.441 n=14, 352/365, p=77.690 1-p=22.310 n=15, 351/365, p=74.710 1-p=25.290 n=16, 350/365, p=71.640 1-p=28.360 n=17, 349/365, p=68.499 1-p=31.501 n=18, 348/365, p=65.309 1-p=34.691 n=19, 347/365, p=62.088 1-p=37.912 n=20, 346/365, p=58.856 1-p=41.144 n=21, 345/365, p=55.631 1-p=44.369 n=22, 344/365, p=52.430 1-p=47.570 n=23, 343/365, p=49.270 1-p=50.730 n=24, 342/365, p=46.166 1-p=53.834 n=25, 341/365, p=43.130 1-p=56.870 n=26, 340/365, p=40.176 1-p=59.824 n=27, 339/365, p=37.314 1-p=62.686 n=28, 338/365, p=34.554 1-p=65.446 n=29, 337/365, p=31.903 1-p=68.097 n=30, 336/365, p=29.368 1-p=70.632

The result is surprising, showing that only 23 people are required to give more than a 50% chance of two people having a birthday on the same day.

More surprising is that with 30 people, this increases to a 70% probability. It’s surprising because 20 to 30 people is about the average class size in school, a number of people for which we all have an intuition (if we attended school).

If the group size is increased to around 60 people, then the probability of two people in the group having the same birthday is above 99%!

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Boy or Girl Problem

Another classic example of applied probability is the case of calculating the probability of whether a baby is a boy or girl.

The probability of whether a given baby is a boy or a girl with no additional information is 50%. This may or may not be true in reality, but let’s assume it for the case of this useful illustration of probability.

As soon as more information is included, the probability calculation changes, and this trips up even people versed in math and probability.

A popular example is called the “two-child problem” that involves being given information about a family with two children and estimating the sex of one child. If the problem is not stated precisely, it can lead to misunderstanding, and in turn, two different ways of calculating the probability. This is the challenge of using natural language instead of notation, and in this case is referred to as the “boy or girl paradox.”

Let’s look at two precisely stated examples.

Case 1: A woman has two children and the oldest is a boy. What is the probability of this woman having two sons?

Our intuition suggests that the probability that the other child is a boy is 0.5 or 50%. Alternately, our intuition might suggest the probability of a family with two boys is 1/4 (e.g. a probability of 0.25) for the four possible combinations of boys and girls for a two-child family.

We can explore this by enumerating all possible combinations that include the information given:

Younger Child | Older Child | Conditional Probability Girl Boy 1/2 Boy Boy 1/2 (*) Girl Girl 0 (impossible) Boy Girl 0 (impossible) 1 2 3 4 5 Younger Child | Older Child | Conditional Probability Girl Boy 1/2 Boy Boy 1/2 (*) Girl Girl 0 (impossible) Boy Girl 0 (impossible)

There would be four outcomes, but the information given reduces the domain to 2 possible outcomes (older child is a boy).

Indeed, only one of the two outcomes can be boy-boy, therefore the probability is 1/2 or (0.5) 50%.

Let’s look at a second very similar case.

Case 2: A woman has two children and one of them is a boy. What is the probability of this woman having two sons?

Our intuition leaps to the same conclusion. At least mine did.

And this would be incorrect.

For example, 1/2 for a boy as the second child being a boy. Another leap might be 1/4 for the case of boy-boy out of all possible cases of having two children.

To find out why, again, let’s enumerate all possible combinations:

Younger Child | Older Child | Conditional Probability Girl Boy 1/3 Boy Boy 1/3 (*) Boy Girl 1/3 Girl Girl 0 (impossible) 1 2 3 4 5 Younger Child | Older Child | Conditional Probability Girl Boy 1/3 Boy Boy 1/3 (*) Boy Girl 1/3 Girl Girl 0 (impossible)

There would be four outcomes, but the information given reduces the domain to three possible outcomes (one child is a boy). One of the three cases is boy-boy, therefore the probability is 1/3 or about 33.33%.

We have more information in Case 1, which allows us to narrow down the domain of possible outcomes and give a result that matches our intuition.

Case 2 looks very similar, but in fact, it includes less information. We have no idea as to whether the older or younger child is a boy, therefore the domain of possible outcomes is larger, resulting in a non-intuitive answer.

These are both problems in conditional probability and we can solve them using the conditional probability formula, rather than enumerating examples.

P (A | B) = P(A and B) / P(B)

The trick is in how the problem is stated.

The outcomes that we are interested in are a sequence, not a single birth event. We are interested in a boy-boy outcome given some information.

First, let’s state a table of all possible sequences regardless of what information is given, e.g. the unconditional probabilities:

Younger Child | Older Child | Unconditional Probability Girl Boy 1/4 Boy Boy 1/4 Girl Girl 1/4 Boy Girl 1/4 1 2 3 4 5 Younger Child | Older Child | Unconditional Probability Girl Boy 1/4 Boy Boy 1/4 Girl Girl 1/4 Boy Girl 1/4

We can calculate the conditional probabilities using the table of unconditional probabilities.

In case 1, we know that the oldest child, or second part of the outcome, is a boy, therefore we can state the problem as follows:

P(boy-boy | {boy-boy or girl-boy})

We can calculate the conditional probability as follows:

= P(boy-boy and {boy-boy or girl-boy}) / P({boy-boy or girl-boy})

= P(boy-boy) / P({boy-boy or girl-boy})

= 1/4 / 2/4

= 0.25 / 0.5

= 0.5

In case 2, we know one child is a boy, but not whether it is the older or younger child; therefore, we can state the problem as follows:

P(boy-boy | {boy-boy or girl-boy or boy-girl})

We can calculate the conditional probability as follows:

= P(boy-boy and {boy-boy or girl-boy or boy-girl}) / P({boy-boy or girl-boy or boy-girl})

= 1/4 / 3/4

= 0.25 / 0.75

= 0.333

This is a useful illustration of how we might overcome our incorrect intuitions and achieve the correct answer by first enumerating the possible cases, and second by calculating the conditional probability directly.

Monty Hall Problem

A final classical problem in applied probability is called the game show problem, or the Monty Hall problem.

It is based on a real game show called “Let’s Make a Deal” and named for the host of the show.

The problem can be described generally as follows:

Problem: The contestant is given a choice of three doors. Behind one is a car, behind the other two are goats. Once a door is chosen, the host, who knows where the car is, opens another door, which has a goat, and asks the contestant if they wish to keep their choice or change to the other unopened door.

It is another classical problem because the solution is not intuitive and in the past has caused great confusion and debate.

Intuition for the problem says that there is a 1 in 3 or 33% chance of picking the car initially, and this becomes 1/2 or 50% once the host opens a door to reveal a goat.

This is incorrect.

We can start by enumerating all combinations and listing the unconditional probabilities. Assume the three doors and the user randomly selects a door, e.g. door 1.

Door 1 | Door 2 | Door 3 | Unconditional Probability Goat Goat Car 1/3 Goat Car Goat 1/3 Car Goat Goat 1/3 1 2 3 4 Door 1 | Door 2 | Door 3 | Unconditional Probability Goat Goat Car 1/3 Goat Car Goat 1/3 Car Goat Goat 1/3

At this stage, there is a 1/3 probability of a car, matching our intuition so far.

Then, the host opens another door with a goat, in this case, door 2.

The opened door was not selected randomly; instead, it was selected with information about where the car is not.

Our intuition suggests we remove the second case from the table and update the probability to 1/2 for each remaining case.

This is incorrect and is the cause of the error.

We can summarize our intuitive conditional probabilities for this scenario as follows:

Door 1 | Door 2 | Door 3 | Uncon. | Cond. Goat Goat Car 1/3 1/2 Goat Car Goat 1/3 0 Car Goat Goat 1/3 1/2 1 2 3 4 Door 1 | Door 2 | Door 3 | Uncon. | Cond. Goat Goat Car 1/3 1/2 Goat Car Goat 1/3 0 Car Goat Goat 1/3 1/2

This would be correct if the contestant did not make a choice before the host opened a door, e.g. if the host opening a door was independent.

The trick comes because the contestant made a choice before the host opened a door and this is useful information. It means the host could not open the chosen door (door1) or open a door with a car behind it. The host’s choice was dependent upon the first choice of the contestant and then constrained.

Instead, we must calculate the probability of switching or not switching, regardless of which door the host opens.

Let’s look at a table of outcomes given the choice of door 1 and staying or switching.

Door 1 | Door 2 | Door 3 | Stay | Switch Goat Goat Car Goat Car Goat Car Goat Goat Car Car Goat Goat Car Goat 1 2 3 4 Door 1 | Door 2 | Door 3 | Stay | Switch Goat Goat Car Goat Car Goat Car Goat Goat Car Car Goat Goat Car Goat

We can see that 2/3 cases of switching result in winning a car (first two rows), and that 1/3 gives the car if we stay (final row).

The contestant has a 2/3 or 66.66% probability of winning the car if they switch.

They should always switch.

We have solved it by enumerating and counting.

Another approach to solving this problem is to calculate the joint probability of the host opening doors to test the stay-versus-switch decision under both cases, in order to maximize the probability of the desired outcome.

For example, given that the contestant has chosen door 1, we can calculate the probability of the host opening door 3 if door 1 has the car as follows:

P(door1=car and door3=open) = 1/3 * 1/2

= 0.333 * 0.5

= 0.166

We can then calculate the joint probability of door 2 having the car and the host opening door 3. This is different because if door 2 contains the car, the host can only open door 3; it has a probability of 1.0, a certainty.

P(door2=car and door3=open) = 1/3 * 1

= 0.333 * 1.0

= 0.333

Having chosen door 1 and the host opening door 3, the probability is higher that the car is behind door 2 (about 33%) than door 1 (about 16%). We should switch.

In this case, we should switch to door 2.

Alternately, we can model the choice of the host opening door 2, which has the same structure of probabilities:

P(door1=car and door2=open) = 0.166

P(door3=car and door2=open) = 0.333

Again, having chosen door 1 and the host opening door 2, the probability is higher that the car is behind door 3 (about 33%) than door 1 (about 16%). We should switch.

If we are seeking to maximize these probabilities, then the best strategy is to switch.

Again, in this example, we have seen how we can overcome our faulty intuitions and solve the problem both by enumerating the cases and by using conditional probability.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Articles

Summary

In this post, you discovered how to develop an intuition for probability by working through classical thought-provoking problems.

Specifically, you learned:

How to solve the birthday problem by multiplying probabilities together.

How to solve the boy or girl problem using conditional probability.

How to solve the Monty Hall problem using joint probability.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Probability for Machine Learning! Develop Your Understanding of Probability ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Probability for Machine Learning It provides self-study tutorials and end-to-end projects on:

Bayes Theorem, Bayesian Optimization, Distributions, Maximum Likelihood, Cross-Entropy, Calibrating Models

and much more... Finally Harness Uncertainty in Your Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
243;machinelearningmastery.com;http://machinelearningmastery.com/python-growing-platform-applied-machine-learning/;2016-10-31;Python is the Growing Platform for Applied Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

You should pick the right tool for the job.

The specific predictive modeling problem that you are working on should dictate the specific programming language, libraries and even machine learning algorithms to use.

But, what if you are just getting started and looking for a platform to learn and practice machine learning?

In this post, you will discover that Python is the growing platform for applied machine learning, likely to outpace and topple R in terms of adoption and perhaps capability.

After reading this post you will know:

That search volume for Python machine learning is growing fast and has already outpaced R.

That the percentage of Python machine learning jobs is growing and has already outpaced R.

That Python is used by nearly 50% of polled practitioners and growing.

Discover how to prepare data with pandas, fit and evaluate models with scikit-learn, and more in my new book, with 16 step-by-step tutorials, 3 projects, and full python code.

Let’s get started.

Python for Machine Learning is Growing

Let’s look at 3 areas where we can see Python for machine learning growing:

Search Volume. Job Ads. Professional Tool Usage.

Python Machine Learning Search Volume is Growing

Search volume is probably indicative of students, engineers and other practitioners searching for information to get started or go deeper into the topic.

Google provides a tool called Google Trends that gives insight into the search volume of keywords over time.

We can investigate the growth of “Python machine learning” from 2004 to 2016 (the last 12 years). Below is a graph of the change in search volume for this period:

We can see that the trend upward started in Perhaps 2012 with a steeper rise starting in 2015, likely boosted by Python Deep Learning tools like TensorFlow.

We can also contrast this to the search volume for R machine learning and we can see that from about the middle of 2015, Python machine learning has been beating out R.

Blue denotes “Python Machine Learning” and red denotes “R Machine Learning”.

Python Machine Learning Jobs are Growing

Indeed is a job search website and like Google trends, they show the volume of job ads that match keywords.

We can investigate the demand for “python machine learning jobs” for the last 4 years.

We can see time along the x-axis and the percentage of job postings that match the keyword. The graph shows almost linear growth from 2012 to 2015 with a hockey-stick like increase in 2016.

We can also compare the job ads for python and R.

Blue shows “Python machine learning” and orange shows “R machine learning”.

We see a more pronounced story compared to Google search volume. The percentage of job ads available to indeed.com shows that demand for Python machine learning skills has been dominating R machine learning skills since at least 2012 with the gap only widening in recent years.

KDNuggets Survey Results: More People Using Python for Machine Learning

We can get some insight into the tools used by machine learning practitioners by reviewing the results for the KDnuggets Software Poll Results.

Here’s a quote from the 2016 results:

R remains the leading tool, with 49% share, but Python grows faster and almost catches up to R.

— Gregory Piatetsky

The poll tracks the tools used by machine learning and data science professionals, where a participant can select more than one tool (which is the norm I would expect)

Here is the growth of Python for machine learning over the last 4 years:

2016 45.8% 2015 30.3% 2014 19.5% 2013 13.3% 1 2 3 4 2016 45.8% 2015 30.3% 2014 19.5% 2013 13.3%

Below is a plot of this growth.

We can see a near linear growth trend where Python s used by just under 50% of profesionals in 2016.

It is important to note that the number of participants in the poll has also grown from many hundreds to thousands in recent years and participants are self-selected.

What is interesting is that scikit-learn also appears separately on the poll, accounting for 17.2%.

For more information see: KDnuggets 2016 Software Poll Results.

O’Reilly Survey Results: More People Using Python for Machine Learning

O’Reilly performs an annual Data Science Salary Survey.

They collect a lot of data from professional data scientists and machine learning practitioners and present the results in very nice reports. For example, here is the 2016 Data Science Salary Survey report [View the PDF Report].

The survey tracks tool usage of practitioners, and as with the KDNuggets data.

Quoting from the key findings from the 2016 report, we can see that Python plays an important role in data science salary.

Python and Spark are among the tools that contribute most to salary.

— Page 1, 2016 Data Science Salary Survey report.

Reviewing the survey results, we can see a similar growth trend in use of the use of the Python ecosystem for machine learning over the last 4 years.

2016 54% 2015 51% 2014 42% (interpreted from graph) 2013 40% 1 2 3 4 2016 54% 2015 51% 2014 42% (interpreted from graph) 2013 40%

Again, we can plot this growth.

It’s interesting that the 2016 results are very similar to those from the KDNuggets poll.

Quotes

You can find quotes to support any position on the Internet.

Take quotes with a grain of salt. Nevertheless, quotes can be insightful, raising and supporting points.

Let’s first take a look at some cherry-picked quotes from news sites and blogs about the growth of Python for machine learning.

News Quotes

Python has emerged over the past few years as a leader in data science programming. While there are still plenty of folks using R, SPSS, Julia or several other popular languages, Python’s growing popularity in the field is evident in the growth of its data science libraries.

— Katharine Jarmul, Introduction To Data Science: How To “big Data” With Python, Dataconomy

Our research shows that Python is one of the most popular languages for data science analyses, in use by more than one-third (36%) of organizations.

— Dave Menninger, Big Data Grows Up at Strata+Hadoop World 2016, SmartDataCollective

… the last few years have seen a proliferation of cutting-edge, commercially usable machine learning frameworks, including the highly successful scikit-learn Python library and well-publicized releases of libraries like Tensorflow by Google and CNTK by Microsoft Research.

— Josh Schwartz, Machine Learning Is No Longer Just for Experts, Harvard Business Review

Note that scikit-learn, TensorFlow and CNTK are all Python machine learning libraries.

Python is versatile, simple, easier to learn, and powerful because of its usefulness in a variety of contexts, some of which have nothing to do with data science. R is a specialized environment that looks to optimize for data analysis, but which is harder to learn. You’ll get paid more if you stick it out with R rather than working with Python

— Roger Huang, Data science sexiness: Your guide to Python and R, and which one is best, TheNextWeb

Quora Quotes

Below are some cherry picked quotes regarding the use of Python for machine learning taken from Quora questions.

Python if a popular scientific language and a rising star for machine learning. I’d be surprised if it can take the data analysis mantle from R, but matrix handling in NumPy may challenge MATLAB and communication tools like IPython are very attractive and a step into the future of reproducibility. I think the SciPy stack for machine learning and data analysis can be used for one-off projects (like papers), and frameworks like scikit-learn may be mature enough to be used in production systems.

— Aswath Muralidharan, Production Engineer. In response to the Quora question “What are the top 5 programming languages for Machine Learning?”

I’d also recommend Python as it is a fantastic all-round programming language that is incredibly useful for drafting code fragments and exploring data (with the IPython shell), great for documenting steps and results in the analytical process chain (IPython Notebook), has a huge selection of libraries for almost any machine learning objective and can even be optimized for production system implementation. In my opinions there are languages that are superior to Python in any of these categories – but none of them offers this versatility.

— Benedikt Koehler, Founder & CEO DataLion. In response to the Quora question “What is the best language to use while learning machine learning for the first time?”

[…] It is because the language can make a productive environment for people that just want to get something done quickly. It is fairly easy to wrap C libraries, and C++ is doable. This gives Python access to a wide range of existing code. Also the language doesn’t get in the way when it comes time to implement things. In many ways it makes coding “fun again” for a wide range of tasks.

— Shawn Masters, VP of Engineering. In response to the Quora question “Will Python become as popular as Java, given that Python is used in Machine Learning?”

In my opinion, Python truly dominates this category. A quick search of almost any artificial intelligence, machine learning, NLP, or data analytics topic, plus ‘Python’, will return examples of useful, actively maintained libraries.

— Ryan Hill, programmer. In response to the Quora question “Which programming language has the best repository of machine learning libraries?”

Summary

In this post, you discovered that Python is the growing platform for applied machine learning.

Specifically, you learned that:

The number of people interested in Python for machine learning is larger than R and is growing.

The number of jobs posted for Python machine learning skills is larger than R and growing.

The number of polled data science professionals that use Python is growing year over year.

Has this influenced your decision to get started with the

Python ecosystem for machine learning?

Share your thoughts in the comments below.

Discover Fast Machine Learning in Python! Develop Your Own Models in Minutes ...with just a few lines of scikit-learn code Learn how in my new Ebook:

Machine Learning Mastery With Python Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, modeling, tuning, and much more... Finally Bring Machine Learning To

Your Own Projects Skip the Academics. Just Results. See What's Inside"
244;machinelearningmastery.com;http://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/;2016-08-30;Feature Importance and Feature Selection With XGBoost in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37

# use feature importance for feature selection from numpy import loadtxt from numpy import sort from xgboost import XGBClassifier from sklearn . model_selection import train_test_split from sklearn . metrics import accuracy_score from sklearn . feature_selection import SelectFromModel # load data dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" ) # split data into X and y X = dataset [ : , 0 : 8 ] Y = dataset [ : , 8 ] # split data into train and test sets X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.33 , random_state = 7 ) # fit model on all training data model = XGBClassifier ( ) model . fit ( X_train , y_train ) # make predictions for test data and evaluate y_pred = model . predict ( X_test ) predictions = [ round ( value ) for value in y_pred ] accuracy = accuracy_score ( y_test , predictions ) print ( ""Accuracy: %.2f%%"" % ( accuracy * 100.0 ) ) # Fit model using each importance as a threshold thresholds = sort ( model . feature_importances_ ) for thresh in thresholds : # select features using threshold selection = SelectFromModel ( model , threshold = thresh , prefit = True ) select_X_train = selection . transform ( X_train ) # train model selection_model = XGBClassifier ( ) selection_model . fit ( select_X_train , y_train ) # eval model select_X_test = selection . transform ( X_test ) y_pred = selection_model . predict ( select_X_test ) predictions = [ round ( value ) for value in y_pred ] accuracy = accuracy_score ( y_test , predictions ) print ( ""Thresh=%.3f, n=%d, Accuracy: %.2f%%"" % ( thresh , select_X_train . shape [ 1 ] , accuracy* 100.0 ) )"
245;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/;2019-05-19;How to Develop a Deep CNN for Multi-Label Classification of Photos;"Tweet Share Share

Last Updated on January 10, 2020

The Planet dataset has become a standard computer vision benchmark that involves multi-label classification or tagging the contents satellite photos of Amazon tropical rainforest.

The dataset was the basis of a data science competition on the Kaggle website and was effectively solved. Nevertheless, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch.

This includes how to develop a robust test harness for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.

In this tutorial, you will discover how to develop a convolutional neural network to classify satellite photos of the Amazon tropical rainforest.

After completing this tutorial, you will know:

How to load and prepare satellite photos of the Amazon tropical rainforest for modeling.

How to develop a convolutional neural network for photo classification from scratch and improve model performance.

How to develop a final model and use it to make ad hoc predictions on new data.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Update Sept/2019 : Provided more guidance in the explanation for downloading the dataset.

: Provided more guidance in the explanation for downloading the dataset. Update Oct/2019: Updated for Keras 2.3.0 and TensorFlow 2.0.0.

Tutorial Overview

This tutorial is divided into seven parts; they are:

Introduction to the Planet Dataset How to Prepare Data for Modeling Model Evaluation Measure How to Evaluate a Baseline Model How to Improve Model Performance How to use Transfer Learning How to Finalize the Model and Make Predictions

Introduction to the Planet Dataset

The “Planet: Understanding the Amazon from Space” competition was held on Kaggle in 2017.

The competition involved classifying small squares of satellite images taken from space of the Amazon rainforest in Brazil in terms of 17 classes, such as “agriculture“, “clear“, and “water“. Given the name of the competition, the dataset is often referred to simply as the “Planet dataset“.

The color images were provided in both TIFF and JPEG format with the size 256×256 pixels. A total of 40,779 images were provided in the training dataset and 40,669 images were provided in the test set for which predictions were required.

The problem is an example of a multi-label image classification task, where one or more class labels must be predicted for each label. This is different from multi-class classification, where each image is assigned one from among many classes.

The multiple class labels were provided for each image in the training dataset with an accompanying file that mapped the image filename to the string class labels.

The competition was run for approximately four months (April to July in 2017) and a total of 938 teams participated, generating much discussion around the use of data preparation, data augmentation, and the use of convolutional neural networks.

The competition was won by a competitor named “bestfitting” with a public leaderboard F-beta score of 0.93398 on 66% of the test dataset and a private leaderboard F-beta score of 0.93317 on 34% of the test dataset. His approach was described in the post “Planet: Understanding the Amazon from Space, 1st Place Winner’s Interview” and involved a pipeline and ensemble of a large number of models, mostly convolutional neural networks with transfer learning.

It was a challenging competition, although the dataset remains freely available (if you have a Kaggle account), and provides a good benchmark problem for practicing image classification with convolutional neural networks for aerial and satellite datasets.

As such, it is routine to achieve an F-beta score of greater than 80 with a manually designed convolutional neural network and an F-beta score 89+ using transfer learning on this task.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Prepare Data for Modeling

The first step is to download the dataset.

In order to download the data files, you must have a Kaggle account. If you do not have a Kaggle account, you can create one here: Kaggle Homepage.

The dataset can be downloaded from the Planet Data page. This page lists all of the files provided for the competition, although we do not need to download all of the files.

Before you can download the dataset, you must click the “Join Competition” button. You may need to agree to the competition rules, then the dataset will be available for download.

To download a given file, click the small icon of the download button that appears next to the file (on the right) when you hover over it with the mouse, as seen in the picture below.

The specific files required for this tutorial are as follows:

train-jpg.tar.7z (600MB)

(600MB) train_v2.csv.zip (159KB)

Note: the jpeg zip files might be listed without the .7z extension. If so, download the .tar versions.

Once you have downloaded the dataset files, you must unzip them. The .zip files for the CSV files can be unzipped using your favorite unzipping program.

The .7z files that contain the JPEG images can also be unzipped using your favorite unzipping program. If this is a new zip format for you, you may need additional software, such as “The Unarchiver” software on MacOS, or p7zip on many platforms.

For example, on the command line on most POSIX-based workstations the .7z files can be decompressed using the p7zip and tar files as follows:

7z x test-jpg.tar.7z tar -xvf test-jpg.tar 7z x train-jpg.tar.7z tar -xvf train-jpg.tar 1 2 3 4 7z x test-jpg.tar.7z tar -xvf test-jpg.tar 7z x train-jpg.tar.7z tar -xvf train-jpg.tar

Once unzipped, you will now have a CSV file and a directory in your current working directory, as follows:

train-jpg/ train_v2.csv 1 2 train-jpg/ train_v2.csv

Inspecting the folder, you will see many jpeg files.

Inspecting the train_v2.csv file, you will see a mapping of jpeg files in the training dataset (train-jpg/) and their mapping to class labels separated by a space for each; for example:

image_name,tags train_0,haze primary train_1,agriculture clear primary water train_2,clear primary train_3,clear primary train_4,agriculture clear habitation primary road ... 1 2 3 4 5 6 7 image_name,tags train_0,haze primary train_1,agriculture clear primary water train_2,clear primary train_3,clear primary train_4,agriculture clear habitation primary road ...

The dataset must be prepared before modeling.

There are at least two approaches we could explore; they are: an in-memory approach and a progressive loading approach.

The dataset could be prepared with the intent of loading the entire training dataset into memory when fitting models. This will require a machine with sufficient RAM to hold all of the images (e.g. 32GB or 64GB of RAM), such as an Amazon EC2 instance, although training models will be significantly faster.

Alternately, the dataset could be loaded as-needed during training, batch by batch. This would require developing a data generator. Training models would be significantly slower, but training could be performed on workstations with less RAM (e.g. 8GB or 16GB).

In this tutorial, we will use the former approach. As such, I strongly encourage you to run the tutorial on an Amazon EC2 instance with sufficient RAM and access to a GPUs, such as the affordable p3.2xlarge instance on the Deep Learning AMI (Amazon Linux) AMI, which costs approximately $3 USD per hour. For a step-by-step tutorial on how to set up an Amazon EC2 instance for deep learning, see the post:

If using an EC2 instance is not an option for you, then I will give hints below on how to further reduce the size of the training dataset so that it will fit into memory on your workstation so that you can complete this tutorial.

Visualize Dataset

The first step is to inspect some of the images in the training dataset.

We can do this by loading some images and plotting multiple images in one figure using Matplotlib.

The complete example is listed below.

# plot the first 9 images in the planet dataset from matplotlib import pyplot from matplotlib.image import imread # define location of dataset folder = 'train-jpg/' # plot first few images for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # define filename filename = folder + 'train_' + str(i) + '.jpg' # load image pixels image = imread(filename) # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # plot the first 9 images in the planet dataset from matplotlib import pyplot from matplotlib . image import imread # define location of dataset folder = 'train-jpg/' # plot first few images for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # define filename filename = folder + 'train_' + str ( i ) + '.jpg' # load image pixels image = imread ( filename ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example creates a figure that plots the first nine images in the training dataset.

We can see that the images are indeed satellite photos of the rain forest. Some show significant haze, others show show trees, roads, or rivers and other structures.

The plots suggests that modeling may benefit from data augmentation as well as simple techniques to make the features in the images more visible.

Create Mappings

The next step involves understanding the labels that may be assigned to each image.

We can load the CSV mapping file for the training dataset (train_v2.csv) directly using the read_csv() Pandas function.

The complete example is listed below.

# load and summarize the mapping file for the planet dataset from pandas import read_csv # load file as CSV filename = 'train_v2.csv' mapping_csv = read_csv(filename) # summarize properties print(mapping_csv.shape) print(mapping_csv[:10]) 1 2 3 4 5 6 7 8 # load and summarize the mapping file for the planet dataset from pandas import read_csv # load file as CSV filename = 'train_v2.csv' mapping_csv = read_csv ( filename ) # summarize properties print ( mapping_csv . shape ) print ( mapping_csv [ : 10 ] )

Running the example first summarizes the shape of the training dataset. We can see that there are indeed 40,479 training images known to the mapping file.

Next, the first 10 rows of the file are summarized. We can see that the second column of the file contains a space-separated list of tags to assign to each image.

(40479, 2) image_name tags 0 train_0 haze primary 1 train_1 agriculture clear primary water 2 train_2 clear primary 3 train_3 clear primary 4 train_4 agriculture clear habitation primary road 5 train_5 haze primary water 6 train_6 agriculture clear cultivation primary water 7 train_7 haze primary 8 train_8 agriculture clear cultivation primary 9 train_9 agriculture clear cultivation primary road 1 2 3 4 5 6 7 8 9 10 11 12 13 (40479, 2) image_name tags 0 train_0 haze primary 1 train_1 agriculture clear primary water 2 train_2 clear primary 3 train_3 clear primary 4 train_4 agriculture clear habitation primary road 5 train_5 haze primary water 6 train_6 agriculture clear cultivation primary water 7 train_7 haze primary 8 train_8 agriculture clear cultivation primary 9 train_9 agriculture clear cultivation primary road

We will need the set of all known tags to be assigned to images, as well as a unique and consistent integer to apply to each tag. This is so that we can develop a target vector for each image with a one hot encoding, e.g. a vector with all zeros and a one at the index for each tag applied to the image.

This can be achieved by looping through each row in the “tags” column, splitting the tags by space, and storing them in a set. We will then have a set of all known tags. For example:

# create a set of labels labels = set() for i in range(len(mapping_csv)): # convert spaced separated tags into an array of tags tags = mapping_csv['tags'][i].split(' ') # add tags to the set of known labels labels.update(tags) 1 2 3 4 5 6 7 # create a set of labels labels = set ( ) for i in range ( len ( mapping_csv ) ) : # convert spaced separated tags into an array of tags tags = mapping_csv [ 'tags' ] [ i ] . split ( ' ' ) # add tags to the set of known labels labels . update ( tags )

This can then be ordered alphabetically and each tag assigned an integer based on this alphabetic rank.

This will mean that the same tag will always be assigned the same integer for consistency.

# convert set of labels to a list to list labels = list(labels) # order set alphabetically labels.sort() 1 2 3 4 # convert set of labels to a list to list labels = list ( labels ) # order set alphabetically labels . sort ( )

We can create a dictionary that maps tags to integers so that we can encode the training dataset for modeling.

We can also create a dictionary with the reverse mapping from integers to string tag values, so later when the model makes a prediction, we can turn it into something readable.

# dict that maps labels to integers, and the reverse labels_map = {labels[i]:i for i in range(len(labels))} inv_labels_map = {i:labels[i] for i in range(len(labels))} 1 2 3 # dict that maps labels to integers, and the reverse labels_map = { labels [ i ] : i for i in range ( len ( labels ) ) } inv_labels_map = { i : labels [ i ] for i in range ( len ( labels ) ) }

We can tie all of this together into a convenience function called create_tag_mapping() that will take the loaded DataFrame containing the train_v2.csv data and return a mapping and inverse mapping dictionaries.

# create a mapping of tags to integers given the loaded mapping file def create_tag_mapping(mapping_csv): # create a set of all known tags labels = set() for i in range(len(mapping_csv)): # convert spaced separated tags into an array of tags tags = mapping_csv['tags'][i].split(' ') # add tags to the set of known labels labels.update(tags) # convert set of labels to a list to list labels = list(labels) # order set alphabetically labels.sort() # dict that maps labels to integers, and the reverse labels_map = {labels[i]:i for i in range(len(labels))} inv_labels_map = {i:labels[i] for i in range(len(labels))} return labels_map, inv_labels_map 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # create a mapping of tags to integers given the loaded mapping file def create_tag_mapping ( mapping_csv ) : # create a set of all known tags labels = set ( ) for i in range ( len ( mapping_csv ) ) : # convert spaced separated tags into an array of tags tags = mapping_csv [ 'tags' ] [ i ] . split ( ' ' ) # add tags to the set of known labels labels . update ( tags ) # convert set of labels to a list to list labels = list ( labels ) # order set alphabetically labels . sort ( ) # dict that maps labels to integers, and the reverse labels_map = { labels [ i ] : i for i in range ( len ( labels ) ) } inv_labels_map = { i : labels [ i ] for i in range ( len ( labels ) ) } return labels_map , inv_labels_map

We can test out this function to see how many and what tags we have to work with; the complete example is listed below.

# create a mapping of tags to integers from pandas import read_csv # create a mapping of tags to integers given the loaded mapping file def create_tag_mapping(mapping_csv): # create a set of all known tags labels = set() for i in range(len(mapping_csv)): # convert spaced separated tags into an array of tags tags = mapping_csv['tags'][i].split(' ') # add tags to the set of known labels labels.update(tags) # convert set of labels to a list to list labels = list(labels) # order set alphabetically labels.sort() # dict that maps labels to integers, and the reverse labels_map = {labels[i]:i for i in range(len(labels))} inv_labels_map = {i:labels[i] for i in range(len(labels))} return labels_map, inv_labels_map # load file as CSV filename = 'train_v2.csv' mapping_csv = read_csv(filename) # create a mapping of tags to integers mapping, inv_mapping = create_tag_mapping(mapping_csv) print(len(mapping)) print(mapping) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # create a mapping of tags to integers from pandas import read_csv # create a mapping of tags to integers given the loaded mapping file def create_tag_mapping ( mapping_csv ) : # create a set of all known tags labels = set ( ) for i in range ( len ( mapping_csv ) ) : # convert spaced separated tags into an array of tags tags = mapping_csv [ 'tags' ] [ i ] . split ( ' ' ) # add tags to the set of known labels labels . update ( tags ) # convert set of labels to a list to list labels = list ( labels ) # order set alphabetically labels . sort ( ) # dict that maps labels to integers, and the reverse labels_map = { labels [ i ] : i for i in range ( len ( labels ) ) } inv_labels_map = { i : labels [ i ] for i in range ( len ( labels ) ) } return labels_map , inv_labels_map # load file as CSV filename = 'train_v2.csv' mapping_csv = read_csv ( filename ) # create a mapping of tags to integers mapping , inv_mapping = create_tag_mapping ( mapping_csv ) print ( len ( mapping ) ) print ( mapping )

Running the example, we can see that we have a total of 17 tags in the dataset.

We can also see the mapping dictionary where each tag is assigned a consistent and unique integer. The tags appear to be sensible descriptions of the types of features we may see in a given satellite image.

It might be interesting as a further extension to explore the distribution of tags across images to see if their assignment or use in the training dataset is balanced or imbalanced. This could give further insight into how difficult the prediction problem may be.

17 {'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16} 1 2 3 17 {'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}

We also need a mapping of training set filenames to the tags for the image.

This is a simple dictionary with the filename of the image as the key and the list of tags as the value.

The create_file_mapping() below implements this, also taking the loaded DataFrame as an argument and returning the mapping with the tag value for each filename stored as a list.

# create a mapping of filename to tags def create_file_mapping(mapping_csv): mapping = dict() for i in range(len(mapping_csv)): name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i] mapping[name] = tags.split(' ') return mapping 1 2 3 4 5 6 7 # create a mapping of filename to tags def create_file_mapping ( mapping_csv ) : mapping = dict ( ) for i in range ( len ( mapping_csv ) ) : name , tags = mapping_csv [ 'image_name' ] [ i ] , mapping_csv [ 'tags' ] [ i ] mapping [ name ] = tags . split ( ' ' ) return mapping

We can now prepare the image component of the dataset.

Create In-Memory Dataset

We need to be able to load the JPEG images into memory.

This can be achieved by enumerating all files in the train-jpg/ folder. Keras provides a simple API to load an image from file via the load_img() function and to cover it to a NumPy array via the img_to_array() function.

As part of loading an image, we can force the size to be smaller to save memory and speed up training. In this case, we will halve the size of the image from 256×256 to 128×128. We will also store the pixel values as an unsigned 8-bit integer (e.g. values between 0 and 255).

# load image photo = load_img(filename, target_size=(128,128)) # convert to numpy array photo = img_to_array(photo, dtype='uint8') 1 2 3 4 # load image photo = load_img ( filename , target_size = ( 128 , 128 ) ) # convert to numpy array photo = img_to_array ( photo , dtype = 'uint8' )

The photo will represent an input to the model, but we require an output for the photo.

We can then retrieve the tags for the loaded image using the filename without the extension using the prepared filename-to-tags mapping prepared with the create_file_mapping() function developed in the previous section.

# get tags tags = file_mapping(filename[:-4]) 1 2 # get tags tags = file_mapping ( filename [ : - 4 ] )

We need to one hot encode the tags for the image. This means that we will require a 17-element vector with a 1 value for each tag present. We can get the index of where to place the 1 values from the mapping of tags to integers created via the create_tag_mapping() function developed in the previous section.

The one_hot_encode() function below implements this, given a list of tags for an image and the mapping of tags to integers as arguments, and it will return a 17 element NumPy array that describes a one hot encoding of the tags for one photo.

# create a one hot encoding for one list of tags def one_hot_encode(tags, mapping): # create empty vector encoding = zeros(len(mapping), dtype='uint8') # mark 1 for each tag in the vector for tag in tags: encoding[mapping[tag]] = 1 return encoding 1 2 3 4 5 6 7 8 # create a one hot encoding for one list of tags def one_hot_encode ( tags , mapping ) : # create empty vector encoding = zeros ( len ( mapping ) , dtype = 'uint8' ) # mark 1 for each tag in the vector for tag in tags : encoding [ mapping [ tag ] ] = 1 return encoding

We can now load the input (photos) and output (one hot encoded vector) elements for the entire training dataset.

The load_dataset() function below implements this given the path to the JPEG images, the mapping of files to tags, and the mapping of tags to integers as inputs; it will return NumPy arrays for the X and y elements for modeling.

# load all images into memory def load_dataset(path, file_mapping, tag_mapping): photos, targets = list(), list() # enumerate files in the directory for filename in listdir(folder): # load image photo = load_img(path + filename, target_size=(128,128)) # convert to numpy array photo = img_to_array(photo, dtype='uint8') # get tags tags = file_mapping[filename[:-4]] # one hot encode tags target = one_hot_encode(tags, tag_mapping) # store photos.append(photo) targets.append(target) X = asarray(photos, dtype='uint8') y = asarray(targets, dtype='uint8') return X, y 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # load all images into memory def load_dataset ( path , file_mapping , tag_mapping ) : photos , targets = list ( ) , list ( ) # enumerate files in the directory for filename in listdir ( folder ) : # load image photo = load_img ( path + filename , target_size = ( 128 , 128 ) ) # convert to numpy array photo = img_to_array ( photo , dtype = 'uint8' ) # get tags tags = file_mapping [ filename [ : - 4 ] ] # one hot encode tags target = one_hot_encode ( tags , tag_mapping ) # store photos . append ( photo ) targets . append ( target ) X = asarray ( photos , dtype = 'uint8' ) y = asarray ( targets , dtype = 'uint8' ) return X , y

Note: this will load the entire training dataset into memory and may require at least 128x128x3 x 40,479 images x 8 bits, or about 2 GB RAM just to hold the loaded photos.

If you run out of memory here, or later when modeling (when pixels are 16 or 32 bits), try reducing the size of the loaded photos to 32×32 and/or stop the loop after loading 20,000 photographs.

Once loaded, we can save these NumPy arrays to file for later use.

We could use the save() or savez() NumPy functions to save the arrays direction. Instead, we will use the savez_compressed() NumPy function to save both arrays in one function call in a compressed format, saving a few more megabytes. Loading the arrays of smaller images will be significantly faster than loading the raw JPEG images each time during modeling.

# save both arrays to one file in compressed format savez_compressed('planet_data.npz', X, y) 1 2 # save both arrays to one file in compressed format savez_compressed ( 'planet_data.npz' , X , y )

We can tie all of this together and prepare the Planet dataset for in-memory modeling and save it to a new single file for fast loading later.

The complete example is listed below.

# load and prepare planet dataset and save to file from os import listdir from numpy import zeros from numpy import asarray from numpy import savez_compressed from pandas import read_csv from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array # create a mapping of tags to integers given the loaded mapping file def create_tag_mapping(mapping_csv): # create a set of all known tags labels = set() for i in range(len(mapping_csv)): # convert spaced separated tags into an array of tags tags = mapping_csv['tags'][i].split(' ') # add tags to the set of known labels labels.update(tags) # convert set of labels to a list to list labels = list(labels) # order set alphabetically labels.sort() # dict that maps labels to integers, and the reverse labels_map = {labels[i]:i for i in range(len(labels))} inv_labels_map = {i:labels[i] for i in range(len(labels))} return labels_map, inv_labels_map # create a mapping of filename to a list of tags def create_file_mapping(mapping_csv): mapping = dict() for i in range(len(mapping_csv)): name, tags = mapping_csv['image_name'][i], mapping_csv['tags'][i] mapping[name] = tags.split(' ') return mapping # create a one hot encoding for one list of tags def one_hot_encode(tags, mapping): # create empty vector encoding = zeros(len(mapping), dtype='uint8') # mark 1 for each tag in the vector for tag in tags: encoding[mapping[tag]] = 1 return encoding # load all images into memory def load_dataset(path, file_mapping, tag_mapping): photos, targets = list(), list() # enumerate files in the directory for filename in listdir(folder): # load image photo = load_img(path + filename, target_size=(128,128)) # convert to numpy array photo = img_to_array(photo, dtype='uint8') # get tags tags = file_mapping[filename[:-4]] # one hot encode tags target = one_hot_encode(tags, tag_mapping) # store photos.append(photo) targets.append(target) X = asarray(photos, dtype='uint8') y = asarray(targets, dtype='uint8') return X, y # load the mapping file filename = 'train_v2.csv' mapping_csv = read_csv(filename) # create a mapping of tags to integers tag_mapping, _ = create_tag_mapping(mapping_csv) # create a mapping of filenames to tag lists file_mapping = create_file_mapping(mapping_csv) # load the jpeg images folder = 'train-jpg/' X, y = load_dataset(folder, file_mapping, tag_mapping) print(X.shape, y.shape) # save both arrays to one file in compressed format savez_compressed('planet_data.npz', X, y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 # load and prepare planet dataset and save to file from os import listdir from numpy import zeros from numpy import asarray from numpy import savez_compressed from pandas import read_csv from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array # create a mapping of tags to integers given the loaded mapping file def create_tag_mapping ( mapping_csv ) : # create a set of all known tags labels = set ( ) for i in range ( len ( mapping_csv ) ) : # convert spaced separated tags into an array of tags tags = mapping_csv [ 'tags' ] [ i ] . split ( ' ' ) # add tags to the set of known labels labels . update ( tags ) # convert set of labels to a list to list labels = list ( labels ) # order set alphabetically labels . sort ( ) # dict that maps labels to integers, and the reverse labels_map = { labels [ i ] : i for i in range ( len ( labels ) ) } inv_labels_map = { i : labels [ i ] for i in range ( len ( labels ) ) } return labels_map , inv_labels_map # create a mapping of filename to a list of tags def create_file_mapping ( mapping_csv ) : mapping = dict ( ) for i in range ( len ( mapping_csv ) ) : name , tags = mapping_csv [ 'image_name' ] [ i ] , mapping_csv [ 'tags' ] [ i ] mapping [ name ] = tags . split ( ' ' ) return mapping # create a one hot encoding for one list of tags def one_hot_encode ( tags , mapping ) : # create empty vector encoding = zeros ( len ( mapping ) , dtype = 'uint8' ) # mark 1 for each tag in the vector for tag in tags : encoding [ mapping [ tag ] ] = 1 return encoding # load all images into memory def load_dataset ( path , file_mapping , tag_mapping ) : photos , targets = list ( ) , list ( ) # enumerate files in the directory for filename in listdir ( folder ) : # load image photo = load_img ( path + filename , target_size = ( 128 , 128 ) ) # convert to numpy array photo = img_to_array ( photo , dtype = 'uint8' ) # get tags tags = file_mapping [ filename [ : - 4 ] ] # one hot encode tags target = one_hot_encode ( tags , tag_mapping ) # store photos . append ( photo ) targets . append ( target ) X = asarray ( photos , dtype = 'uint8' ) y = asarray ( targets , dtype = 'uint8' ) return X , y # load the mapping file filename = 'train_v2.csv' mapping_csv = read_csv ( filename ) # create a mapping of tags to integers tag_mapping , _ = create_tag_mapping ( mapping_csv ) # create a mapping of filenames to tag lists file_mapping = create_file_mapping ( mapping_csv ) # load the jpeg images folder = 'train-jpg/' X , y = load_dataset ( folder , file_mapping , tag_mapping ) print ( X . shape , y . shape ) # save both arrays to one file in compressed format savez_compressed ( 'planet_data.npz' , X , y )

Running the example first loads the entire dataset and summarizes the shape. We can confirm that the input samples (X) are 128×128 color images and that the output samples are 17-element vectors.

At the end of the run, a single file ‘planet_data.npz‘ is saved containing the dataset that is approximately 1.2 gigabytes in size, saving about 700 megabytes due to compression.

(40479, 128, 128, 3) (40479, 17) 1 (40479, 128, 128, 3) (40479, 17)

The dataset can be loaded easily later using the load() NumPy function, as follows:

# load prepared planet dataset from numpy import load data = load('planet_data.npz') X, y = data['arr_0'], data['arr_1'] print('Loaded: ', X.shape, y.shape) 1 2 3 4 5 # load prepared planet dataset from numpy import load data = load ( 'planet_data.npz' ) X , y = data [ 'arr_0' ] , data [ 'arr_1' ] print ( 'Loaded: ' , X . shape , y . shape )

Running this small example confirms that the dataset is correctly loaded.

Loaded: (40479, 128, 128, 3) (40479, 17) 1 Loaded: (40479, 128, 128, 3) (40479, 17)

Model Evaluation Measure

Before we start modeling, we must select a performance metric.

Classification accuracy is often appropriate for binary classification tasks with a balanced number of examples in each class.

In this case, we are working neither with a binary or multi-class classification task; instead, it is a multi-label classification task and the number of labels are not balanced, with some used more heavily than others.

As such, the Kaggle competition organizes chose the F-beta metric, specifically the F2 score. This is a metric that is related to the F1 score (also called F-measure).

The F1 score calculates the average of the recall and the precision. You may remember that the precision and recall are calculated as follows:

precision = true positives / (true positives + false positives) recall = true positives / (true positives + false negatives) 1 2 precision = true positives / (true positives + false positives) recall = true positives / (true positives + false negatives)

Precision describes how good a model is at predicting the positive class. Recall describes how good the model is at predicting the positive class when the actual outcome is positive.

The F1 is the mean of these two scores, specifically the harmonic mean instead of the arithmetic mean because the values are proportions. F1 is preferred over accuracy when evaluating the performance of a model on an imbalanced dataset, with a value between 0 and 1 for worst and best possible scores.

F1 = 2 x (precision x recall) / (precision + recall) 1 F1 = 2 x (precision x recall) / (precision + recall)

The F-beta metric is a generalization of F1 that allows a term called beta to be introduced that weights how important recall is compared to precision when calculating the mean

F-Beta = (1 + Beta^2) x (precision x recall) / (Beta^2 x precision + recall) 1 F-Beta = (1 + Beta^2) x (precision x recall) / (Beta^2 x precision + recall)

A common value of beta is two, and this was the value used in the competition, where recall valued twice as highly as precision. This is often referred to as the F2 score.

The idea of a positive and negative class only makes sense for a binary classification problem. As we are predicting multiple classes, the idea of positive and negative and related terms are calculated for each class in a one vs. rest manner, then averaged across each class.

The scikit-learn library provides an implementation of F-beta via the fbeta_score() function. We can call this function to evaluate a set of predictions and specify a beta value of 2 and the “average” argument set to “samples“.

score = fbeta_score(y_true, y_pred, 2, average='samples') 1 score = fbeta_score ( y_true , y_pred , 2 , average = 'samples' )

For example, we can test this on our prepared dataset.

We can split our loaded dataset into separate train and test datasets that we can use to train and evaluate models on this problem. This can be achieved using the train_test_split() and specifying a ‘random_state‘ argument so that the same data split is given each time the code is run.

We will use 70% for the training set and 30% for the test set.

trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1) 1 trainX , testX , trainY , testY = train_test_split ( X , y , test_size = 0.3 , random_state = 1 )

The load_dataset() function below implements this by loading the saved dataset, splitting it into train and test components, and returning them ready for use.

# load train and test dataset def load_dataset(): # load dataset data = load('planet_data.npz') X, y = data['arr_0'], data['arr_1'] # separate into train and test datasets trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1) print(trainX.shape, trainY.shape, testX.shape, testY.shape) return trainX, trainY, testX, testY 1 2 3 4 5 6 7 8 9 # load train and test dataset def load_dataset ( ) : # load dataset data = load ( 'planet_data.npz' ) X , y = data [ 'arr_0' ] , data [ 'arr_1' ] # separate into train and test datasets trainX , testX , trainY , testY = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) print ( trainX . shape , trainY . shape , testX . shape , testY . shape ) return trainX , trainY , testX , testY

We can then make a prediction of all classes or all 1 values in the one hot encoded vectors.

# make all one predictions train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])]) test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])]) 1 2 3 # make all one predictions train_yhat = asarray ( [ ones ( trainY . shape [ 1 ] ) for _ in range ( trainY . shape [ 0 ] ) ] ) test_yhat = asarray ( [ ones ( testY . shape [ 1 ] ) for _ in range ( testY . shape [ 0 ] ) ] )

The predictions can then be evaluated using the scikit-learn fbeta_score() function with the true values in the train and test dataset.

train_score = fbeta_score(trainY, train_yhat, 2, average='samples') test_score = fbeta_score(testY, test_yhat, 2, average='samples') 1 2 train_score = fbeta_score ( trainY , train_yhat , 2 , average = 'samples' ) test_score = fbeta_score ( testY , test_yhat , 2 , average = 'samples' )

Tying this together, the complete example is listed below.

# test f-beta score from numpy import load from numpy import ones from numpy import asarray from sklearn.model_selection import train_test_split from sklearn.metrics import fbeta_score # load train and test dataset def load_dataset(): # load dataset data = load('planet_data.npz') X, y = data['arr_0'], data['arr_1'] # separate into train and test datasets trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1) print(trainX.shape, trainY.shape, testX.shape, testY.shape) return trainX, trainY, testX, testY # load dataset trainX, trainY, testX, testY = load_dataset() # make all one predictions train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])]) test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])]) # evaluate predictions train_score = fbeta_score(trainY, train_yhat, 2, average='samples') test_score = fbeta_score(testY, test_yhat, 2, average='samples') print('All Ones: train=%.3f, test=%.3f' % (train_score, test_score)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # test f-beta score from numpy import load from numpy import ones from numpy import asarray from sklearn . model_selection import train_test_split from sklearn . metrics import fbeta_score # load train and test dataset def load_dataset ( ) : # load dataset data = load ( 'planet_data.npz' ) X , y = data [ 'arr_0' ] , data [ 'arr_1' ] # separate into train and test datasets trainX , testX , trainY , testY = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) print ( trainX . shape , trainY . shape , testX . shape , testY . shape ) return trainX , trainY , testX , testY # load dataset trainX , trainY , testX , testY = load_dataset ( ) # make all one predictions train_yhat = asarray ( [ ones ( trainY . shape [ 1 ] ) for _ in range ( trainY . shape [ 0 ] ) ] ) test_yhat = asarray ( [ ones ( testY . shape [ 1 ] ) for _ in range ( testY . shape [ 0 ] ) ] ) # evaluate predictions train_score = fbeta_score ( trainY , train_yhat , 2 , average = 'samples' ) test_score = fbeta_score ( testY , test_yhat , 2 , average = 'samples' ) print ( 'All Ones: train=%.3f, test=%.3f' % ( train_score , test_score ) )

Running this example first loads the prepared dataset, then splits it into train and test sets and the shape of the prepared datasets is reported. We can see that we have a little more than 28,000 examples in the training dataset and a little more than 12,000 examples in the test set.

Next, the all-one predictions are prepared and then evaluated and the scores are reported. We can see that an all ones prediction for both datasets results in a score of about 0.48.

(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) All Ones: train=0.484, test=0.483 1 2 (28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) All Ones: train=0.484, test=0.483

We will require a version of the F-beta score calculation in Keras to use as a metric.

Keras used to support this metric for binary classification problems (2 classes) prior to version 2.0 of the library; we can see the code for this older version here: metrics.py. This code can be used as the basis for defining a new metric function that can be used with Keras. A version of this function is also proposed in a Kaggle kernel titled “F-beta score for Keras“. This new function is listed below.

from keras import backend # calculate fbeta score for multi-class/label classification def fbeta(y_true, y_pred, beta=2): # clip predictions y_pred = backend.clip(y_pred, 0, 1) # calculate elements tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1) fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1) fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1) # calculate precision p = tp / (tp + fp + backend.epsilon()) # calculate recall r = tp / (tp + fn + backend.epsilon()) # calculate fbeta, averaged across each class bb = beta ** 2 fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon())) return fbeta_score 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from keras import backend # calculate fbeta score for multi-class/label classification def fbeta ( y_true , y_pred , beta = 2 ) : # clip predictions y_pred = backend . clip ( y_pred , 0 , 1 ) # calculate elements tp = backend . sum ( backend . round ( backend . clip ( y_true * y_pred , 0 , 1 ) ) , axis = 1 ) fp = backend . sum ( backend . round ( backend . clip ( y_pred - y_true , 0 , 1 ) ) , axis = 1 ) fn = backend . sum ( backend . round ( backend . clip ( y_true - y_pred , 0 , 1 ) ) , axis = 1 ) # calculate precision p = tp / ( tp + fp + backend . epsilon ( ) ) # calculate recall r = tp / ( tp + fn + backend . epsilon ( ) ) # calculate fbeta, averaged across each class bb = beta * * 2 fbeta_score = backend . mean ( ( 1 + bb ) * ( p * r ) / ( bb * p + r + backend . epsilon ( ) ) ) return fbeta_score

It can be used when compiling a model in Keras, specified via the metrics argument; for example:

... model.compile(... metrics=[fbeta]) 1 2 . . . model . compile ( . . . metrics = [ fbeta ] )

We can test this new function and compare results to the scikit-learn function as follows.

# compare f-beta score between sklearn and keras from numpy import load from numpy import ones from numpy import asarray from sklearn.model_selection import train_test_split from sklearn.metrics import fbeta_score from keras import backend # load train and test dataset def load_dataset(): # load dataset data = load('planet_data.npz') X, y = data['arr_0'], data['arr_1'] # separate into train and test datasets trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1) print(trainX.shape, trainY.shape, testX.shape, testY.shape) return trainX, trainY, testX, testY # calculate fbeta score for multi-class/label classification def fbeta(y_true, y_pred, beta=2): # clip predictions y_pred = backend.clip(y_pred, 0, 1) # calculate elements tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1) fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1) fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1) # calculate precision p = tp / (tp + fp + backend.epsilon()) # calculate recall r = tp / (tp + fn + backend.epsilon()) # calculate fbeta, averaged across each class bb = beta ** 2 fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon())) return fbeta_score # load dataset trainX, trainY, testX, testY = load_dataset() # make all one predictions train_yhat = asarray([ones(trainY.shape[1]) for _ in range(trainY.shape[0])]) test_yhat = asarray([ones(testY.shape[1]) for _ in range(testY.shape[0])]) # evaluate predictions with sklearn train_score = fbeta_score(trainY, train_yhat, 2, average='samples') test_score = fbeta_score(testY, test_yhat, 2, average='samples') print('All Ones (sklearn): train=%.3f, test=%.3f' % (train_score, test_score)) # evaluate predictions with keras train_score = fbeta(backend.variable(trainY), backend.variable(train_yhat)) test_score = fbeta(backend.variable(testY), backend.variable(test_yhat)) print('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # compare f-beta score between sklearn and keras from numpy import load from numpy import ones from numpy import asarray from sklearn . model_selection import train_test_split from sklearn . metrics import fbeta_score from keras import backend # load train and test dataset def load_dataset ( ) : # load dataset data = load ( 'planet_data.npz' ) X , y = data [ 'arr_0' ] , data [ 'arr_1' ] # separate into train and test datasets trainX , testX , trainY , testY = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) print ( trainX . shape , trainY . shape , testX . shape , testY . shape ) return trainX , trainY , testX , testY # calculate fbeta score for multi-class/label classification def fbeta ( y_true , y_pred , beta = 2 ) : # clip predictions y_pred = backend . clip ( y_pred , 0 , 1 ) # calculate elements tp = backend . sum ( backend . round ( backend . clip ( y_true * y_pred , 0 , 1 ) ) , axis = 1 ) fp = backend . sum ( backend . round ( backend . clip ( y_pred - y_true , 0 , 1 ) ) , axis = 1 ) fn = backend . sum ( backend . round ( backend . clip ( y_true - y_pred , 0 , 1 ) ) , axis = 1 ) # calculate precision p = tp / ( tp + fp + backend . epsilon ( ) ) # calculate recall r = tp / ( tp + fn + backend . epsilon ( ) ) # calculate fbeta, averaged across each class bb = beta * * 2 fbeta_score = backend . mean ( ( 1 + bb ) * ( p * r ) / ( bb * p + r + backend . epsilon ( ) ) ) return fbeta_score # load dataset trainX , trainY , testX , testY = load_dataset ( ) # make all one predictions train_yhat = asarray ( [ ones ( trainY . shape [ 1 ] ) for _ in range ( trainY . shape [ 0 ] ) ] ) test_yhat = asarray ( [ ones ( testY . shape [ 1 ] ) for _ in range ( testY . shape [ 0 ] ) ] ) # evaluate predictions with sklearn train_score = fbeta_score ( trainY , train_yhat , 2 , average = 'samples' ) test_score = fbeta_score ( testY , test_yhat , 2 , average = 'samples' ) print ( 'All Ones (sklearn): train=%.3f, test=%.3f' % ( train_score , test_score ) ) # evaluate predictions with keras train_score = fbeta ( backend . variable ( trainY ) , backend . variable ( train_yhat ) ) test_score = fbeta ( backend . variable ( testY ) , backend . variable ( test_yhat ) ) print ( 'All Ones (keras): train=%.3f, test=%.3f' % ( train_score , test_score ) )

Running the example loads the datasets as before, and in this case, the F-beta is calculated using both scikit-learn and Keras. We can see that both functions achieve the same result.

(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) All Ones (sklearn): train=0.484, test=0.483 All Ones (keras): train=0.484, test=0.483 1 2 3 (28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) All Ones (sklearn): train=0.484, test=0.483 All Ones (keras): train=0.484, test=0.483

We can use the score of 0.483 on the test set as a naive forecast to which all models in the subsequent sections can be compared to determine if they are skillful or not.

How to Evaluate a Baseline Model

We are now ready to develop and evaluate a baseline convolutional neural network model for the prepared planet dataset.

We will design a baseline model with a VGG-type structure. That is blocks of convolutional layers with small 3×3 filters followed by a max pooling layer, with this pattern repeating with a doubling in the number of filters with each block added.

Specifically, each block will have two convolutional layers with 3×3 filters, ReLU activation and He weight initialization with same padding, ensuring the output feature maps have the same width and height. These will be followed by a max pooling layer with a 3×3 kernel. Three of these blocks will be used with 32, 64 and 128 filters respectively.

model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(128, 128, 3))) model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) 1 2 3 4 5 6 7 8 9 10 model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 128 , 128 , 3 ) ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

The output of the final pooling layer will be flattened and fed to a fully connected layer for interpretation then finally to an output layer for prediction.

The model must produce a 17-element vector with a prediction between 0 and 1 for each output class.

model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(17, activation='sigmoid')) 1 2 3 model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 17 , activation = 'sigmoid' ) )

If this were a multi-class classification problem, we would use a softmax activation function and the categorical cross entropy loss function. This would not be appropriate for multi-label classification, as we expect the model to output multiple 1 values, not a single 1 value. In this case, we will use the sigmoid activation function in the output layer and optimize the binary cross entropy loss function.

The model will be optimized with mini-batch stochastic gradient descent with a conservative learning rate of 0.01 and a momentum of 0.9, and the model will keep track of the “fbeta” metric during training.

# compile model opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta]) 1 2 3 # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ fbeta ] )

The define_model() function below ties all of this together and parameterized the shape of the input and output, in case you want to experiment by changing these values or reuse the code on another dataset.

The function will return a model ready to be fit on the planet dataset.

# define cnn model def define_model(in_shape=(128, 128, 3), out_shape=17): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape)) model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(out_shape, activation='sigmoid')) # compile model opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta]) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # define cnn model def define_model ( in_shape = ( 128 , 128 , 3 ) , out_shape = 17 ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = in_shape ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( out_shape , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ fbeta ] ) return model

The choice of this model as the baseline model is somewhat arbitrary. You may want to explore with other baseline models that have fewer layers or different learning rates.

We can use the load_dataset() function developed in the previous section to load the dataset and split it into train and test sets for fitting and evaluating a defined model.

The pixel values will be normalized before fitting the model. We will achieve this by defining an ImageDataGenerator instance and specify the rescale argument as 1.0/255.0. This will normalize pixel values per batch to 32-bit floating point values, which might be more memory efficient than rescaling all of the pixel values at once in memory.

# create data generator datagen = ImageDataGenerator(rescale=1.0/255.0) 1 2 # create data generator datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 )

We can create iterators from this data generator for both the train and test sets, and in this case, we will use the relatively large batch size of 128 images to accelerate learning.

# prepare iterators train_it = datagen.flow(trainX, trainY, batch_size=128) test_it = datagen.flow(testX, testY, batch_size=128) 1 2 3 # prepare iterators train_it = datagen . flow ( trainX , trainY , batch_size = 128 ) test_it = datagen . flow ( testX , testY , batch_size = 128 )

The defined model can then be fit using the train iterator, and the test iterator can be used to evaluate the test dataset at the end of each epoch. The model will be fit for 50 epochs.

# fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0) 1 2 3 # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 50 , verbose = 0 )

Once fit, we can calculate the final loss and F-beta scores on the test dataset to estimate the skill of the model.

# evaluate model loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta)) 1 2 3 # evaluate model loss , fbeta = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> loss=%.3f, fbeta=%.3f' % ( loss , fbeta ) )

The fit_generator() function called to fit the model returns a dictionary containing the loss and F-beta scores recorded each epoch on the train and test dataset. We can create a plot of these traces that can provide insight into the learning dynamics of the model.

The summarize_diagnostics() function will create a figure from this recorded history data with one plot showing loss and another the F-beta scores for the model at the end of each training epoch on the train dataset (blue lines) and test dataset (orange lines).

The created figure is saved to a PNG file with the same filename as the script with a “_plot.png” extension. This allows the same test harness to be used with multiple different script files for different model configurations, saving the learning curves in separate files along the way.

# plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Fbeta') pyplot.plot(history.history['fbeta'], color='blue', label='train') pyplot.plot(history.history['val_fbeta'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Fbeta' ) pyplot . plot ( history . history [ 'fbeta' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_fbeta' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( )

We can tie this together and define a function run_test_harness() to drive the test harness, including the loading and preparation of the data as well as definition, fit, and evaluation of the model.

# run the test harness for evaluating a model def run_test_harness(): # load dataset trainX, trainY, testX, testY = load_dataset() # create data generator datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = datagen.flow(trainX, trainY, batch_size=128) test_it = datagen.flow(testX, testY, batch_size=128) # define model model = define_model() # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0) # evaluate model loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta)) # learning curves summarize_diagnostics(history) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # run the test harness for evaluating a model def run_test_harness ( ) : # load dataset trainX , trainY , testX , testY = load_dataset ( ) # create data generator datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = datagen . flow ( trainX , trainY , batch_size = 128 ) test_it = datagen . flow ( testX , testY , batch_size = 128 ) # define model model = define_model ( ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 50 , verbose = 0 ) # evaluate model loss , fbeta = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> loss=%.3f, fbeta=%.3f' % ( loss , fbeta ) ) # learning curves summarize_diagnostics ( history )

The complete example of evaluating a baseline model on the planet dataset is listed below.

# baseline model for the planet dataset import sys from numpy import load from matplotlib import pyplot from sklearn.model_selection import train_test_split from keras import backend from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Conv2D from keras.layers import MaxPooling2D from keras.layers import Dense from keras.layers import Flatten from keras.optimizers import SGD # load train and test dataset def load_dataset(): # load dataset data = load('planet_data.npz') X, y = data['arr_0'], data['arr_1'] # separate into train and test datasets trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1) print(trainX.shape, trainY.shape, testX.shape, testY.shape) return trainX, trainY, testX, testY # calculate fbeta score for multi-class/label classification def fbeta(y_true, y_pred, beta=2): # clip predictions y_pred = backend.clip(y_pred, 0, 1) # calculate elements tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1) fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1) fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1) # calculate precision p = tp / (tp + fp + backend.epsilon()) # calculate recall r = tp / (tp + fn + backend.epsilon()) # calculate fbeta, averaged across each class bb = beta ** 2 fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon())) return fbeta_score # define cnn model def define_model(in_shape=(128, 128, 3), out_shape=17): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape)) model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(out_shape, activation='sigmoid')) # compile model opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta]) return model # plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Fbeta') pyplot.plot(history.history['fbeta'], color='blue', label='train') pyplot.plot(history.history['val_fbeta'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() # run the test harness for evaluating a model def run_test_harness(): # load dataset trainX, trainY, testX, testY = load_dataset() # create data generator datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = datagen.flow(trainX, trainY, batch_size=128) test_it = datagen.flow(testX, testY, batch_size=128) # define model model = define_model() # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0) # evaluate model loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta)) # learning curves summarize_diagnostics(history) # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # baseline model for the planet dataset import sys from numpy import load from matplotlib import pyplot from sklearn . model_selection import train_test_split from keras import backend from keras . preprocessing . image import ImageDataGenerator from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . optimizers import SGD # load train and test dataset def load_dataset ( ) : # load dataset data = load ( 'planet_data.npz' ) X , y = data [ 'arr_0' ] , data [ 'arr_1' ] # separate into train and test datasets trainX , testX , trainY , testY = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) print ( trainX . shape , trainY . shape , testX . shape , testY . shape ) return trainX , trainY , testX , testY # calculate fbeta score for multi-class/label classification def fbeta ( y_true , y_pred , beta = 2 ) : # clip predictions y_pred = backend . clip ( y_pred , 0 , 1 ) # calculate elements tp = backend . sum ( backend . round ( backend . clip ( y_true * y_pred , 0 , 1 ) ) , axis = 1 ) fp = backend . sum ( backend . round ( backend . clip ( y_pred - y_true , 0 , 1 ) ) , axis = 1 ) fn = backend . sum ( backend . round ( backend . clip ( y_true - y_pred , 0 , 1 ) ) , axis = 1 ) # calculate precision p = tp / ( tp + fp + backend . epsilon ( ) ) # calculate recall r = tp / ( tp + fn + backend . epsilon ( ) ) # calculate fbeta, averaged across each class bb = beta * * 2 fbeta_score = backend . mean ( ( 1 + bb ) * ( p * r ) / ( bb * p + r + backend . epsilon ( ) ) ) return fbeta_score # define cnn model def define_model ( in_shape = ( 128 , 128 , 3 ) , out_shape = 17 ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = in_shape ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( out_shape , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ fbeta ] ) return model # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Fbeta' ) pyplot . plot ( history . history [ 'fbeta' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_fbeta' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( ) # run the test harness for evaluating a model def run_test_harness ( ) : # load dataset trainX , trainY , testX , testY = load_dataset ( ) # create data generator datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = datagen . flow ( trainX , trainY , batch_size = 128 ) test_it = datagen . flow ( testX , testY , batch_size = 128 ) # define model model = define_model ( ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 50 , verbose = 0 ) # evaluate model loss , fbeta = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> loss=%.3f, fbeta=%.3f' % ( loss , fbeta ) ) # learning curves summarize_diagnostics ( history ) # entry point, run the test harness run_test_harness ( )

Running the example first loads the dataset and splits it into train and test sets. The shape of the input and output elements of each of the train and test datasets is printed, confirming that the same data split was performed as before.

The model is fit and evaluated, and an F-beta score for the final model on the test dataset is reported.

Your specific results may vary given the stochastic nature of the learning algorithm.

In this case, the baseline model achieved an F-beta score of about 0.831, which is quite a bit better than the naive score of 0.483 reported in the previous section. This suggests that the baseline model is skillful.

(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) > loss=0.470, fbeta=0.831 1 2 (28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) > loss=0.470, fbeta=0.831

A figure is also created and saved to file showing plots of the learning curves for the model on the train and test sets with regard to both loss and F-beta.

In this case, the plot of the loss learning curves suggests that the model has overfit the training dataset, perhaps around epoch 20 out of 50, although the overfitting has not seemingly negatively impacted the performance of the model on the test dataset with regard to the F-beta score.

Now that we have a baseline model for the dataset, we have a strong basis for experimentation and improvement.

We will explore some ideas for improving the performance of the model in the next section.

How to Improve Model Performance

In the previous section, we defined a baseline model that can be used as the basis for improvement on the planet dataset.

The model achieved a reasonable F-beta score, although the learning curves suggested that the model had overfit the training dataset. Two common approaches to explore to address overfitting are dropout regularization and data augmentation. Both have the effect of disrupting and slowing down the learning process, specifically the rate that the model improves over training epochs.

We will explore both of these methods in this section. Given that we expect the rate of learning to be slowed, we give the model more time to learn by increasing the number of training epochs from 50 to 200.

Dropout Regularization

Dropout regularization is a computationally cheap way to regularize a deep neural network.

Dropout works by probabilistically removing, or “dropping out,” inputs to a layer, which may be input variables in the data sample or activations from a previous layer. It has the effect of simulating a large number of networks with very different network structure and, in turn, making nodes in the network generally more robust to the inputs.

For more information on dropout, see the post:

Typically, a small amount of dropout can be applied after each VGG block, with more dropout applied to the fully connected layers near the output layer of the model.

Below is the define_model() function for an updated version of the baseline model with the addition of Dropout. In this case, a dropout of 20% is applied after each VGG block, with a larger dropout rate of 50% applied after the fully connected layer in the classifier part of the model.

# define cnn model def define_model(in_shape=(128, 128, 3), out_shape=17): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape)) model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dropout(0.5)) model.add(Dense(out_shape, activation='sigmoid')) # compile model opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta]) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # define cnn model def define_model ( in_shape = ( 128 , 128 , 3 ) , out_shape = 17 ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = in_shape ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dropout ( 0.5 ) ) model . add ( Dense ( out_shape , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ fbeta ] ) return model

The full code listing of the baseline model with the addition of dropout on the planet dataset is listed below for completeness.

# baseline model with dropout on the planet dataset import sys from numpy import load from matplotlib import pyplot from sklearn.model_selection import train_test_split from keras import backend from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Conv2D from keras.layers import MaxPooling2D from keras.layers import Dense from keras.layers import Flatten from keras.layers import Dropout from keras.optimizers import SGD # load train and test dataset def load_dataset(): # load dataset data = load('planet_data.npz') X, y = data['arr_0'], data['arr_1'] # separate into train and test datasets trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1) print(trainX.shape, trainY.shape, testX.shape, testY.shape) return trainX, trainY, testX, testY # calculate fbeta score for multi-class/label classification def fbeta(y_true, y_pred, beta=2): # clip predictions y_pred = backend.clip(y_pred, 0, 1) # calculate elements tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1) fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1) fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1) # calculate precision p = tp / (tp + fp + backend.epsilon()) # calculate recall r = tp / (tp + fn + backend.epsilon()) # calculate fbeta, averaged across each class bb = beta ** 2 fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon())) return fbeta_score # define cnn model def define_model(in_shape=(128, 128, 3), out_shape=17): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape)) model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dropout(0.5)) model.add(Dense(out_shape, activation='sigmoid')) # compile model opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta]) return model # plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Fbeta') pyplot.plot(history.history['fbeta'], color='blue', label='train') pyplot.plot(history.history['val_fbeta'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() # run the test harness for evaluating a model def run_test_harness(): # load dataset trainX, trainY, testX, testY = load_dataset() # create data generator datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = datagen.flow(trainX, trainY, batch_size=128) test_it = datagen.flow(testX, testY, batch_size=128) # define model model = define_model() # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=200, verbose=0) # evaluate model loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta)) # learning curves summarize_diagnostics(history) # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 # baseline model with dropout on the planet dataset import sys from numpy import load from matplotlib import pyplot from sklearn . model_selection import train_test_split from keras import backend from keras . preprocessing . image import ImageDataGenerator from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . layers import Dropout from keras . optimizers import SGD # load train and test dataset def load_dataset ( ) : # load dataset data = load ( 'planet_data.npz' ) X , y = data [ 'arr_0' ] , data [ 'arr_1' ] # separate into train and test datasets trainX , testX , trainY , testY = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) print ( trainX . shape , trainY . shape , testX . shape , testY . shape ) return trainX , trainY , testX , testY # calculate fbeta score for multi-class/label classification def fbeta ( y_true , y_pred , beta = 2 ) : # clip predictions y_pred = backend . clip ( y_pred , 0 , 1 ) # calculate elements tp = backend . sum ( backend . round ( backend . clip ( y_true * y_pred , 0 , 1 ) ) , axis = 1 ) fp = backend . sum ( backend . round ( backend . clip ( y_pred - y_true , 0 , 1 ) ) , axis = 1 ) fn = backend . sum ( backend . round ( backend . clip ( y_true - y_pred , 0 , 1 ) ) , axis = 1 ) # calculate precision p = tp / ( tp + fp + backend . epsilon ( ) ) # calculate recall r = tp / ( tp + fn + backend . epsilon ( ) ) # calculate fbeta, averaged across each class bb = beta * * 2 fbeta_score = backend . mean ( ( 1 + bb ) * ( p * r ) / ( bb * p + r + backend . epsilon ( ) ) ) return fbeta_score # define cnn model def define_model ( in_shape = ( 128 , 128 , 3 ) , out_shape = 17 ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = in_shape ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dropout ( 0.5 ) ) model . add ( Dense ( out_shape , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ fbeta ] ) return model # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Fbeta' ) pyplot . plot ( history . history [ 'fbeta' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_fbeta' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( ) # run the test harness for evaluating a model def run_test_harness ( ) : # load dataset trainX , trainY , testX , testY = load_dataset ( ) # create data generator datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = datagen . flow ( trainX , trainY , batch_size = 128 ) test_it = datagen . flow ( testX , testY , batch_size = 128 ) # define model model = define_model ( ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 200 , verbose = 0 ) # evaluate model loss , fbeta = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> loss=%.3f, fbeta=%.3f' % ( loss , fbeta ) ) # learning curves summarize_diagnostics ( history ) # entry point, run the test harness run_test_harness ( )

Running the example first fits the model, then reports the model performance on the hold out test dataset.

Your specific results may vary given the stochastic nature of the learning algorithm.

In this case, we can see a small lift in model performance from an F-beta score of about 0.831 for the baseline model to about 0.859 with the addition of dropout.

(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) > loss=0.190, fbeta=0.859 1 2 (28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) > loss=0.190, fbeta=0.859

Reviewing the learning curves, we can see that dropout has had some effect on the rate of improvement of the model on both the train and test sets.

Overfitting has been reduced or delayed, although performance may begin to stall towards the middle of the run, around epoch 100.

The results suggest that further regularization may be required. This could be achieved by a larger dropout rate and/or perhaps the addition of weight decay. Additionally, the batch size could be decreased and the learning rate decreased, both of which may further slow the rate of improvement by the model, perhaps with a positive effect on reducing the overfitting of the training dataset.

Image Data Augmentation

Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.

Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.

Data augmentation can also act as a regularization technique, adding noise to the training data and encouraging the model to learn the same features, invariant to their position in the input.

Small changes to the input photos of the satellite photos might be useful for this problem, such as horizontal flips, vertical flips, rotations, zooms, and perhaps more. These augmentations can be specified as arguments to the ImageDataGenerator instance, used for the training dataset. The augmentations should not be used for the test dataset, as we wish to evaluate the performance of the model on the unmodified photographs.

This requires that we have a separate ImageDataGenerator instance for the train and test dataset, then iterators for the train and test sets created from the respective data generators. For example:

# create data generator train_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90) test_datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = train_datagen.flow(trainX, trainY, batch_size=128) test_it = test_datagen.flow(testX, testY, batch_size=128) 1 2 3 4 5 6 # create data generator train_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 , horizontal_flip = True , vertical_flip = True , rotation_range = 90 ) test_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = train_datagen . flow ( trainX , trainY , batch_size = 128 ) test_it = test_datagen . flow ( testX , testY , batch_size = 128 )

In this case, photos in the training dataset will be augmented with random horizontal and vertical flips as well as random rotations of up to 90 degrees. Photos in both the train and test steps will have their pixel values scaled in the same way as we did for the baseline model.

The full code listing of the baseline model with training data augmentation for the planet dataset is listed below for completeness.

# baseline model with data augmentation for the planet dataset import sys from numpy import load from matplotlib import pyplot from sklearn.model_selection import train_test_split from keras import backend from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Conv2D from keras.layers import MaxPooling2D from keras.layers import Dense from keras.layers import Flatten from keras.optimizers import SGD # load train and test dataset def load_dataset(): # load dataset data = load('planet_data.npz') X, y = data['arr_0'], data['arr_1'] # separate into train and test datasets trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1) print(trainX.shape, trainY.shape, testX.shape, testY.shape) return trainX, trainY, testX, testY # calculate fbeta score for multi-class/label classification def fbeta(y_true, y_pred, beta=2): # clip predictions y_pred = backend.clip(y_pred, 0, 1) # calculate elements tp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1) fp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1) fn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1) # calculate precision p = tp / (tp + fp + backend.epsilon()) # calculate recall r = tp / (tp + fn + backend.epsilon()) # calculate fbeta, averaged across each class bb = beta ** 2 fbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon())) return fbeta_score # define cnn model def define_model(in_shape=(128, 128, 3), out_shape=17): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape)) model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) model.add(Dense(out_shape, activation='sigmoid')) # compile model opt = SGD(lr=0.01, momentum=0.9) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta]) return model # plot diagnostic learning curves def summarize_diagnostics(history): # plot loss pyplot.subplot(211) pyplot.title('Cross Entropy Loss') pyplot.plot(history.history['loss'], color='blue', label='train') pyplot.plot(history.history['val_loss'], color='orange', label='test') # plot accuracy pyplot.subplot(212) pyplot.title('Fbeta') pyplot.plot(history.history['fbeta'], color='blue', label='train') pyplot.plot(history.history['val_fbeta'], color='orange', label='test') # save plot to file filename = sys.argv[0].split('/')[-1] pyplot.savefig(filename + '_plot.png') pyplot.close() # run the test harness for evaluating a model def run_test_harness(): # load dataset trainX, trainY, testX, testY = load_dataset() # create data generator train_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, vertical_flip=True, rotation_range=90) test_datagen = ImageDataGenerator(rescale=1.0/255.0) # prepare iterators train_it = train_datagen.flow(trainX, trainY, batch_size=128) test_it = test_datagen.flow(testX, testY, batch_size=128) # define model model = define_model() # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=200, verbose=0) # evaluate model loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0) print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta)) # learning curves summarize_diagnostics(history) # entry point, run the test harness run_test_harness() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 # baseline model with data augmentation for the planet dataset import sys from numpy import load from matplotlib import pyplot from sklearn . model_selection import train_test_split from keras import backend from keras . preprocessing . image import ImageDataGenerator from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . optimizers import SGD # load train and test dataset def load_dataset ( ) : # load dataset data = load ( 'planet_data.npz' ) X , y = data [ 'arr_0' ] , data [ 'arr_1' ] # separate into train and test datasets trainX , testX , trainY , testY = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) print ( trainX . shape , trainY . shape , testX . shape , testY . shape ) return trainX , trainY , testX , testY # calculate fbeta score for multi-class/label classification def fbeta ( y_true , y_pred , beta = 2 ) : # clip predictions y_pred = backend . clip ( y_pred , 0 , 1 ) # calculate elements tp = backend . sum ( backend . round ( backend . clip ( y_true * y_pred , 0 , 1 ) ) , axis = 1 ) fp = backend . sum ( backend . round ( backend . clip ( y_pred - y_true , 0 , 1 ) ) , axis = 1 ) fn = backend . sum ( backend . round ( backend . clip ( y_true - y_pred , 0 , 1 ) ) , axis = 1 ) # calculate precision p = tp / ( tp + fp + backend . epsilon ( ) ) # calculate recall r = tp / ( tp + fn + backend . epsilon ( ) ) # calculate fbeta, averaged across each class bb = beta * * 2 fbeta_score = backend . mean ( ( 1 + bb ) * ( p * r ) / ( bb * p + r + backend . epsilon ( ) ) ) return fbeta_score # define cnn model def define_model ( in_shape = ( 128 , 128 , 3 ) , out_shape = 17 ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = in_shape ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( out_shape , activation = 'sigmoid' ) ) # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( optimizer = opt , loss = 'binary_crossentropy' , metrics = [ fbeta ] ) return model # plot diagnostic learning curves def summarize_diagnostics ( history ) : # plot loss pyplot . subplot ( 211 ) pyplot . title ( 'Cross Entropy Loss' ) pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' ) # plot accuracy pyplot . subplot ( 212 ) pyplot . title ( 'Fbeta' ) pyplot . plot ( history . history [ 'fbeta' ] , color = 'blue' , label = 'train' ) pyplot . plot ( history . history [ 'val_fbeta' ] , color = 'orange' , label = 'test' ) # save plot to file filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ] pyplot . savefig ( filename + '_plot.png' ) pyplot . close ( ) # run the test harness for evaluating a model def run_test_harness ( ) : # load dataset trainX , trainY , testX , testY = load_dataset ( ) # create data generator train_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 , horizontal_flip = True , vertical_flip = True , rotation_range = 90 ) test_datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 ) # prepare iterators train_it = train_datagen . flow ( trainX , trainY , batch_size = 128 ) test_it = test_datagen . flow ( testX , testY , batch_size = 128 ) # define model model = define_model ( ) # fit model history = model . fit_generator ( train_it , steps_per_epoch = len ( train_it ) , validation_data = test_it , validation_steps = len ( test_it ) , epochs = 200 , verbose = 0 ) # evaluate model loss , fbeta = model . evaluate_generator ( test_it , steps = len ( test_it ) , verbose = 0 ) print ( '> loss=%.3f, fbeta=%.3f' % ( loss , fbeta ) ) # learning curves summarize_diagnostics ( history ) # entry point, run the test harness run_test_harness ( )

Running the example first fits the model, then reports the model performance on the hold out test dataset.

Your specific results may vary given the stochastic nature of the learning algorithm.

In this case, we can see a lift in performance of about 0.06 from an F-beta score of about 0.831 for the baseline model to a score of about 0.882 for the baseline model with simple data augmentation. This is a large improvement, larger than we saw with dropout.

(28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) > loss=0.103, fbeta=0.882 1 2 (28335, 128, 128, 3) (28335, 17) (12144, 128, 128, 3) (12144, 17) > loss=0.103, fbeta=0.882

Reviewing the learning curves, we can see that the overfitting has been dramatically impacted. Learning continues well past 100 epochs, although may show signs of leveling out towards the end of the run. The results suggest that further augmentation or other types of regularization added to this configuration may be helpful.

It may be interesting to explore additional image augmentations that may further encourage the learning of features invariant to their position in the input, such as zooms and shifts.

Discussion

We have explored two different improvements to the baseline model.

The results can be summarized below, although we must assume some variance in these results given the stochastic nature of the algorithm:

Baseline + Dropout Regularization : 0.859

: 0.859 Baseline + Data Augmentation: 0.882

As suspected, the addition of regularization techniques slows the progression of the learning algorithms and reduces overfitting, resulting in improved performance on the holdout dataset. It is likely that the combination of both approaches with a further increase in the number of training epochs will result in further improvements. That is, the combination of both dropout with data augmentation.

This is just the beginning of the types of improvements that can be explored on this dataset. In addition to tweaks to the regularization methods described, other regularization methods could be explored such as weight decay and early stopping.

It may be worth exploring changes to the learning algorithm, such as changes to the learning rate, use of a learning rate schedule, or an adaptive learning rate such as Adam.

Alternate model architectures may also be worth exploring. The chosen baseline model is expected to offer more capacity than may be required for this problem and a smaller model may faster to train and in turn could result in better performance.

How to Use Transfer Learning

Transfer learning involves using all or parts of a model trained on a related task.

Keras provides a range of pre-trained models that can be loaded and used wholly or partially via the Keras Applications API.

A useful model for transfer learning is one of the VGG models, such as VGG-16 with 16 layers that, at the time it was developed, achieved top results on the ImageNet photo classification challenge.

The model is comprised of two main parts: the feature extractor part of the model that is made up of VGG blocks, and the classifier part of the model that is made up of fully connected layers and the output layer.

We can use the feature extraction part of the model and add a new classifier part of the model that is tailored to the planets dataset. Specifically, we can hold the weights of all of the convolutional layers fixed during training and only train new fully connected layers that will learn to interpret the features extracted from the model and make a suite of binary classifications.

This can be achieved by loading the VGG-16 model, removing the fully connected layers from the output-end of the mode"
246;machinelearningmastery.com;http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/;2014-04-15;A Gentle Introduction to Scikit-Learn;"# Sample Decision Tree Classifier

from sklearn import datasets

from sklearn import metrics

from sklearn . tree import DecisionTreeClassifier

# load the iris datasets

dataset = datasets . load_iris ( )

# fit a CART model to the data

model = DecisionTreeClassifier ( )

model . fit ( dataset . data , dataset . target )

print ( model )

# make predictions

expected = dataset . target

predicted = model . predict ( dataset . data )

# summarize the fit of the model

print ( metrics . classification_report ( expected , predicted ) )"
247;towardsdatascience.com;https://towardsdatascience.com/serial-dependence-in-binary-sequences-409c5e8f54d0?source=collection_home---4------2-----------------------;2020-04-17;Serial dependence in binary sequences;"In this blog post, I am going to investigate the serial (aka temporal) dependence phenomenon in random binary sequences.

Random binary sequences are sequences of zeros and ones generated by random processes. Primary outputs generated by most random number generators are binary. Binary sequences often encode the occurrence of random events:

Extreme returns on financial markets.

Failures of machines, servers,…

Value-at-Risk exceedance indicators in financial market risk models.

In many of those situations, it is necessary to ensure that the tracked events occur independently of each other. For example, the occurrence of an incident in a production system should not make the system more incident prone. For that, we need to take a close look at the dependence structure of the binary sequence under consideration.

Outline

In this blog post, I am going to develop and test a scoring method for quantifying the dependence strength in random binary sequences. The Meixner Dependency Score is easy to implement and is based on the orthogonal polynomials associated with Geometric distribution.

After reading through this blog post, you will know:

How the problem of serial dependency measurement can be formulated in statistical terms. How to derive the Meixner polynomials from the Geometric distribution. How to calculate the Meixner Dependency Score for a waiting times sequence to quantify the dependence strength. How to test dependency scoring methods using simple Monte-Carlo experiments involving Markov Chains with a known dependency structure.

Statistical formulation of the problem

Given a sequence of random variables X = (X[0], X[1], …, X[n]) taking values in the set {0, 1} and a probability p∈(0,1), I would like to investigate the serial dependencies between the elements of X. This investigation should be based on a sample x drawn from X, i.e. a finite sequence of zeros and ones (x[0], x[1],..., x[n]). This is a very difficult and deep problem, and in this blog post I am going to focus on collecting evidence to support or reject the following two basic hypotheses:

The random variables X[i] have the distribution Ber(p) (Bernoulli distribution with the success probability p). The random variables X[i] are independent.

If we assume that the elements of x are independent samples from a fixed Bernoulli random variable X[0], then we can estimate the probability p by calculating the average of x[0], x[1], …, x[n]. This is because the expectation of X[0] is 𝔼 X[0] = p.

To set an appropriate context for the serial dependence detection problem, let me remark that random binary sequences x are often associated with discreet observations of a system with two states. We say that “an event” occurred at time i, if x[0] = 1.

How can we check whether our events occur independently of each other and gather evidence that there is no serial dependence between the event times?

We often tend to see serial dependence in situations where there is none. Typical examples are the “winning or lucky streaks” experienced by gamblers in casinos.

To rigorously investigate the question of serial dependence in the sequence X based on the observation x, we can look at the distribution of waiting times between the events. For example the sequence

[0, 1, 1, 0, 0, 1, 0, 0]

yields the following sequence of waiting times

[1, 3].

Note that the waiting time calculation discards the initial and trailing zeros in the event sequence x.

To formally define the waiting time sequence y = (y[1],…,y[m]) based on x, consider the sequence of indices I=(i[1],i[2],…), such that for each i∈ I we have x[i] =1. We set

For an i.i.d. sequence of Bernoulli random variables, the sequence of waiting times consists of i.i.d. random variables with the geometric distribution. Let’s have a closer look at its properties.

The Geometric Distribution

The probability mass function (PMF) of the geometric distribution with parameter p is given by

The PMF of the geometric distribution with the parameter p=0.1 looks as follows.

The probability mass function of the geometric distribution with the parameter p=0.1

One way of test whether a waiting times sequence follows the geometric distribution is to look at the orthogonal polynomials generated by that distribution.

A family of orthogonal polynomials is intimately tied to every probability distribution μ on ℝ. For any such distribution, we can define a scalar product between (square-integrable) real-valued functions f and g as

where Y is a random variable with distribution μ. For a geometric distribution, the above scalar product takes the form

We say that the functions f and g are orthogonal (with respect to μ) if and only if ⟨f,g⟩ = 0.

Finally, a sequence of polynomials (q[i]) with i≥ 0 is called orthogonal, if and only if, ⟨q[k], q[i]⟩ = 0 for all k ≠ i, and each q[i] has degree i. As a consequence, we have 𝔼 q[i](Y) = 0 for i>0. This is the tool I am going to use to check whether a given Y follows a geometric distribution.

The family of orthogonal polynomials M = (M[i](y)) corresponding to the geometric distribution is a special case of the Meixner family derived from the negative binomial distribution (a generalization of the geometric distribution). The members of the Meixner family satisfy the following handy recursive relation:

with M[1](y) = 1 and M[-1](y) = 0. This relation is used to calculate the sequence M. Also, note that M[k] depends on the value of the parameter p.

In the companion python source code, the function meixner_poly_eval is used to evaluate Meixner polynomials up to a given degree on a given set of points. I used this function to plot the graphs of these polynomials up to degree 55.

Graphs of Meixner orthogonal polynomials with p=0.1 up to degree 55

As mentioned above, for every polynomial M[k](y) in M the equation

holds, if and only if Y∈ Geom(p) and k>0.

This relation can be used to test whether a given sample of waiting times belongs to the geometric distribution with parameter p. We are going to estimate the expectations 𝔼 M[k](Y) and use the estimates as scores: Values close to zero can be interpreted as evidence speaking for geometric distribution with parameter p. If the values are far from zero, this is a sign that we need to revisit our assumptions and possibly discard the i.i.d. hypothesis about the original event sequence. Note that significant deviations from zero can also occur if the events are independent but the true probability p’ differs significantly the putative p. This can easily be tested as described earlier.

Meixner Dependency Score

In order to arrive at a single number (a score) that quantifies the degree of serial dependence in the sense developed above, we will define the Meixner dependency score (MDS) as a mean of expectation estimates for the first k Meixner polynomials evaluated at a sample of waiting times y=(y[1],..,y[m]):

The idea of using orthogonal polynomials to test the dependence structure assumptions in binary sequences has its roots in the method of moments inference and was developed in the financial literature to backtest Value-at-Risk models. In this blog post I adapt the approach outlined in Candelon et al. See the References section below.

Monte-Carlo study

To find out whether the procedure devised above has a chance to work in practice, I am going to test it using synthetically generated data. This type of a testing procedure is commonly known as Monte-Carlo (MC) study. It doesn’t replace tests with real-world data, but it can help evaluate a statistical method in a controlled environment.

The experiment we are going to perform consists of the following steps:

Generate a binary sequence using a model with a known serial dependence structure. Estimate the expectations 𝔼 M[k](X) for k=1,…,10. Repeat the steps 1 and 2 over a large number of independent trials and visualize the aggregated results.

Let’s start with a simple case of simulating an i.i.d. sequence of Bernoulli random variables with success probability p[0]. This can be used as a sanity check and to test whether our implementation of the procedure is correct.

For the following experiment we set p[0] = 0.05 and simulate 5000 samples from X = (X[1],.., X[1000]).

For a model with a simple form of serial dependence, we select the probabilities 0 < p[1] < p[0] < p[2] < 1 and set the distributions of random variables in the sequence X = (X[0], X[1], X[2], …) as follows. Let X[0] be Bernoulli with success probability p[0]. The distributions of X[i] for i>0 are conditional on X[i-1]:

In order to compare this model with an i.i.d. Bernoulli model with the success

probability p[0], we need to set p[1] and p[2] such that the unconditional

distributions of X[i] are Ber(p[0]).

In other words, for a given 0 < p[1] < p[0] < 1, we are looking for a p[2]∈ (0, 1), such that the random binary sequence defined above satisfies P(X[i] = 1) = p[0].

This binary sequence can be represented as a simple Markov chain with the state space {0, 1}, the transition probability matrix P given by

and the initial distribution λ = (1-p[0], p[0]). The marginal distribution of X[i] is given by

This basic result can be found in any book about Markov Chains (see the References section below). The task of finding a p[2] such that P(X[i] = 1) is as close to p[0] as possible, can be easily solved with an off-the-shelf optimization algorithm such as BFGS.

For example, the above procedure quickly yields p[2] = 0.62 for p[1] = 0.02 and p[0] = 0.05.

Monte-Carlo study results

Let’s test the dependency scoring algorithm for the following values p[1] and p[2]. An MC evaluation of the Meixner polynomials leads to the following Meixner dependency scores:

+------+------+-------+

| p1 | p2 | MDS |

+------+------+-------+

| 0.5, | 0.5 | 0.083 |

| 0.4 | 0.24 | 0.194 |

| 0.3 | 0.43 | 0.382 |

| 0.2 | 0.62 | 0.583 |

+------+------+-------+

Again, 5000 simulations of samples of length 1000 yield the following histograms for the expectation estimates.

As we can see, the histograms are shifted slightly to the left.

All the results and plots presented in this blog post have been generated using the following Python script: meixner.py

Closing remarks

In this blog post, we learned about the concept of serial dependence in binary sequences. We implemented a serial dependence detection method, the Meixner Dependency Score, based on the Meixner orthogonal polynomials and tested its performance using a simple Markov chain model.

An important financial use case for dependency testing is the analysis of the Value-at-Risk exceedance event sequence generated by a market risk model. Market Risk models are widely used in the banking industry for regulatory capital requirement calculations and in the asset management industry for portfolio construction and risk management.

The Value-At-Risk (Var) is a quantile of the loss distribution for a financial asset price over a fixed time horizon. For example, an investor holding a long position in gold may be interested in the 95% VaR over a one-day time horizon for the gold price in USD. For a well-calibrated VaR model, the extreme losses on 5% of the business days observed over an investment period will exceed the 95% VaR score. The VaR exceedance events must not only occur with the expected frequency but must also be independent of each other. Proper calibration of the market risk model that brings the dependency of the exceedance events to a low level is typically much more difficult than simple calibration of the exceedance frequency. But this is especially important in the times of financial distress, because underestimating the risk leads inevitably to underhedging, and, possibly, to more severe losses.

References"
248;machinelearningmastery.com;https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/;2018-12-23;How to Create a Bagging Ensemble of Deep Learning Models in Keras;"# bagging mlp ensemble on blobs dataset

from sklearn . datasets import make_blobs

from sklearn . utils import resample

from sklearn . metrics import accuracy_score

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Dense

from matplotlib import pyplot

from numpy import mean

from numpy import std

import numpy

from numpy import array

from numpy import argmax

# evaluate a single mlp model

def evaluate_model ( trainX , trainy , testX , testy ) :

# encode targets

trainy_enc = to_categorical ( trainy )

testy_enc = to_categorical ( testy )

# define model

model = Sequential ( )

model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' ) )

model . add ( Dense ( 3 , activation = 'softmax' ) )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( trainX , trainy_enc , epochs = 50 , verbose = 0 )

# evaluate the model

_ , test_acc = model . evaluate ( testX , testy_enc , verbose = 0 )

return model , test_acc

# make an ensemble prediction for multi-class classification

def ensemble_predictions ( members , testX ) :

# make predictions

yhats = [ model . predict ( testX ) for model in members ]

yhats = array ( yhats )

# sum across ensemble members

summed = numpy . sum ( yhats , axis = 0 )

# argmax across classes

result = argmax ( summed , axis = 1 )

return result

# evaluate a specific number of members in an ensemble

def evaluate_n_members ( members , n_members , testX , testy ) :

# select a subset of members

subset = members [ : n_members ]

# make prediction

yhat = ensemble_predictions ( subset , testX )

# calculate accuracy

return accuracy_score ( testy , yhat )

# generate 2d classification dataset

dataX , datay = make_blobs ( n_samples = 55000 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 )

X , newX = dataX [ : 5000 , : ] , dataX [ 5000 : , : ]

y , newy = datay [ : 5000 ] , datay [ 5000 : ]

# multiple train-test splits

n_splits = 10

scores , members = list ( ) , list ( )

for _ in range ( n_splits ) :

# select indexes

ix = [ i for i in range ( len ( X ) ) ]

train_ix = resample ( ix , replace = True , n_samples = 4500 )

test_ix = [ x for x in ix if x not in train_ix ]

# select data

trainX , trainy = X [ train_ix ] , y [ train_ix ]

testX , testy = X [ test_ix ] , y [ test_ix ]

# evaluate model

model , test_acc = evaluate_model ( trainX , trainy , testX , testy )

print ( '>%.3f' % test_acc )

scores . append ( test_acc )

members . append ( model )

# summarize expected performance

print ( 'Estimated Accuracy %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) )

# evaluate different numbers of ensembles on hold out set

single_scores , ensemble_scores = list ( ) , list ( )

for i in range ( 1 , n_splits + 1 ) :

ensemble_score = evaluate_n_members ( members , i , newX , newy )

newy_enc = to_categorical ( newy )

_ , single_score = members [ i - 1 ] . evaluate ( newX , newy_enc , verbose = 0 )

print ( '> %d: single=%.3f, ensemble=%.3f' % ( i , single_score , ensemble_score ) )

ensemble_scores . append ( ensemble_score )

single_scores . append ( single_score )

# plot score vs number of ensemble members

print ( 'Accuracy %.3f (%.3f)' % ( mean ( single_scores ) , std ( single_scores ) ) )

x_axis = [ i for i in range ( 1 , n_splits + 1 ) ]

pyplot . plot ( x_axis , single_scores , marker = 'o' , linestyle = 'None' )

pyplot . plot ( x_axis , ensemble_scores , marker = 'o' )"
249;machinelearningmastery.com;http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/;2014-09-25;Discover Feature Engineering, How to Engineer Features and How to Get Good at It;"Tweet Share Share

Last Updated on December 27, 2019

Feature engineering is an informal topic, but one that is absolutely known and agreed to be key to success in applied machine learning.

In creating this guide I went wide and deep and synthesized all of the material I could.

You will discover what feature engineering is, what problem it solves, why it matters, how to engineer features, who is doing it well and where you can go to learn more and get good at it.

If you read one article on feature engineering, I want it to be this one.

feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books, but it is absolutely vital to ML success. […] Much of the success of machine learning is actually success in engineering features that a learner can understand.

— Scott Locklin, in “Neglected machine learning ideas”

Problem that Feature Engineering Solves

When your goal is to get the best possible results from a predictive model, you need to get the most from what you have.

This includes getting the best results from the algorithms you are using. It also involves getting the most out of the data for your algorithms to work with.

How do you get the most out of your data for predictive modeling?

This is the problem that the process and practice of feature engineering solves.

Actually the success of all Machine Learning algorithms depends on how you present the data.

— Mohammad Pezeshki, answer to “What are some general tips on feature selection and engineering that every data scientist should know?”

Importance of Feature Engineering

The features in your data will directly influence the predictive models you use and the results you can achieve.

You can say that: the better the features that you prepare and choose, the better the results you will achieve. It is true, but it also misleading.

The results you achieve are a factor of the model you choose, the data you have available and the features you prepared. Even your framing of the problem and objective measures you’re using to estimate accuracy play a part. Your results are dependent on many inter-dependent properties.

You need great features that describe the structures inherent in your data.

Better features means flexibility.

You can choose “the wrong models” (less than optimal) and still get good results. Most models can pick up on good structure in data. The flexibility of good features will allow you to use less complex models that are faster to run, easier to understand and easier to maintain. This is very desirable.

Better features means simpler models.

With well engineered features, you can choose “the wrong parameters” (less than optimal) and still get good results, for much the same reasons. You do not need to work as hard to pick the right models and the most optimized parameters.

With good features, you are closer to the underlying problem and a representation of all the data you have available and could use to best characterize that underlying problem.

Better features means better results.

The algorithms we used are very standard for Kagglers. […] We spent most of our efforts in feature engineering.

— Xavier Conort, on “Q&A with Xavier Conort” on winning the Flight Quest challenge on Kaggle

What is Feature Engineering?

Here is how I define feature engineering:

Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.

You can see the dependencies in this definition:

The performance measures you’ve chosen (RMSE? AUC?)

The framing of the problem (classification? regression?)

The predictive models you’re using (SVM?)

The raw data you have selected and prepared (samples? formatting? cleaning?)

feature engineering is manually designing what the input x’s should be

— Tomasz Malisiewicz, answer to “What is feature engineering?”

Feature Engineering is a Representation Problem

Machine learning algorithms learn a solution to a problem from sample data.

In this context, feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?

It’s deep. Doing well in machine learning, even in artificial intelligence in general comes back to representation problems. It’s hard stuff, perhaps unknowable (or at best intractable) to know the best representation to use, a priori.

you have to turn your inputs into things the algorithm can understand

— Shayne Miel, answer to “What is the intuitive explanation of feature engineering in machine learning?”

Feature Engineering is an Art

It is an art like engineering is an art, like programming is an art, like medicine is an art.

There are well defined procedures that are methodical, provable and understood.

The data is a variable and is different every time. You get good at deciding which procedures to use and when, by practice. By empirical apprenticeship. Like engineering, like programming, like medicine, like machine learning in general.

Mastery of feature engineering comes with hands on practice, and study of what others that are doing well are practicing.

…some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.

— Pedro Domingos, in “A Few Useful Things to Know about Machine Learning” (PDF)

Sub-Problems of Feature Engineering

It is common to think of feature engineering as one thing.

For example, for a long time for me, feature engineering was feature construction.

I would think to myself “I’m doing feature engineering now” and I would pursue the question “How can I decompose or aggregate raw data to better describe the underlying problem?” The goal was right, but the approach was one of many.

In this section we look at these many approaches and the specific sub-problems that they are intended to address. Each could be an in depth article of their own as they are large and important areas of practice and study.

Feature: An attribute useful for your modeling task

Let’s start with data and what is a feature.

Tabular data is described in terms of observations or instances (rows) that are made up of variables or attributes (columns). An attribute could be a feature.

The idea of a feature, separate from an attribute, makes more sense in the context of a problem. A feature is an attribute that is useful or meaningful to your problem. It is an important part of an observation for learning about the structure of the problem that is being modeled.

I use “meaningful” to discriminate attributes from features. Some might not. I think there is no such thing as a non-meaningful feature. If a feature has no impact on the problem, it is not part of the problem.

In computer vision, an image is an observation, but a feature could be a line in the image. In natural language processing, a document or a tweet could be an observation, and a phrase or word count could be a feature. In speech recognition, an utterance could be an observation, but a feature might be a single word or phoneme.

Feature Importance: An estimate of the usefulness of a feature

You can objectively estimate the usefulness of features.

This can be helpful as a pre-cursor to selecting features. Features are allocated scores and can then be ranked by their scores. Those features with the highest scores can be selected for inclusion in the training dataset, whereas those remaining can be ignored.

Feature importance scores can also provide you with information that you can use to extract or construct new features, similar but different to those that have been estimated to be useful.

A feature may be important if it is highly correlated with the dependent variable (the thing being predicted). Correlation coefficients and other univariate (each attribute is considered independently) methods are common methods.

More complex predictive modeling algorithms perform feature importance and selection internally while constructing their model. Some examples include MARS, Random Forest and Gradient Boosted Machines. These models can also report on the variable importance determined during the model preparation process.

Feature Extraction: The automatic construction of new features from raw data

Some observations are far too voluminous in their raw state to be modeled by predictive modeling algorithms directly.

Common examples include image, audio, and textual data, but could just as easily include tabular data with millions of attributes.

Feature extraction is a process of automatically reducing the dimensionality of these types of observations into a much smaller set that can be modelled.

For tabular data, this might include projection methods like Principal Component Analysis and unsupervised clustering methods. For image data, this might include line or edge detection. Depending on the domain, image, video and audio observations lend themselves to many of the same types of DSP methods.

Key to feature extraction is that the methods are automatic (although may need to be designed and constructed from simpler methods) and solve the problem of unmanageably high dimensional data, most typically used for analog observations stored in digital formats.

Feature Selection: From many features to a few that are useful

Not all features are created equal.

Those attributes that are irrelevant to the problem need to be removed. There will be some features that will be more important than others to the model accuracy. There will also be features that will be redundant in the context of other features.

Feature selection addresses these problems by automatically selecting a subset that are most useful to the problem.

Feature selection algorithms may use a scoring method to rank and choose features, such as correlation or other feature importance methods.

More advanced methods may search subsets of features by trial and error, creating and evaluating models automatically in pursuit of the objectively most predictive sub-group of features.

There are also methods that bake in feature selection or get it as a side effect of the model. Stepwise regression is an example of an algorithm that automatically performs feature selection as part of the model construction process.

Regularization methods like LASSO and ridge regression may also be considered algorithms with feature selection baked in, as they actively seek to remove or discount the contribution of features as part of the model building process.

Read more in the post: An Introduction to Feature Selection.

Feature Construction: The manual construction of new features from raw data

The best results come down to you, the practitioner, crafting the features.

Feature importance and selection can inform you about the objective utility of features, but those features have to come from somewhere.

You need to manually create them. This requires spending a lot of time with actual sample data (not aggregates) and thinking about the underlying form of the problem, structures in the data and how best to expose them to predictive modeling algorithms.

With tabular data, it often means a mixture of aggregating or combining features to create new features, and decomposing or splitting features to create new features.

With textual data, it often means devising document or context specific indicators relevant to the problem. With image data, it can often mean enormous amounts of time prescribing automatic filters to pick out relevant structures.

This is the part of feature engineering that is often talked the most about as an artform, the part that is attributed the importance and signalled as the differentiator in competitive machine learning.

It is manual, it is slow, it requires lots of human brain power, and it makes a big difference.

Feature engineering and feature selection are not mutually exclusive. They are both useful. I’d say feature engineering is more important though, especially because you can’t really automate it.

— Robert Neuhaus, answer to “Which do you think improves accuracy more, feature selection or feature engineering?”

Feature Learning: The automatic identification and use of features in raw data

Can we avoid the manual load of prescribing how to construct or extract features from raw data?

Representation learning or feature learning is an effort towards this goal.

Modern deep learning methods are achieving some success in this area, such as autoencoders and restricted Boltzmann machines. They have been shown to automatically and in a unsupervised or semi-supervised way, learn abstract representations of features (a compressed form), that in turn have supported state-of-the-art results in domains such as speech recognition, image classification, object recognition and other areas.

We do not have automatic feature extraction or construction, yet, and we will probably never have automatic feature engineering.

The abstract representations are prepared automatically, but you cannot understand and leverage what has been learned, other than in a black-box manner. They cannot (yet, or easily) inform you and the process on how to create more similar and different features like those that are doing well, on a given problem or on similar problems in the future. The acquired skill is trapped.

Nevertheless, it’s fascinating, exciting and an important and modern part of feature engineering.

Process of Feature Engineering

Feature engineering is best understood in the broader process of applied machine learning.

You need this context.

Process of Machine Learning

The process of applied machine learning (for lack of a better name) that in a broad brush sense involves lots of activities. Up front is problem definition, next is data selection and preparation, in the middle is model preparation, evaluation and tuning and at the end is the presentation of results.

Process descriptions like data mining and KDD help to better understand the tasks and subtasks. You can pick and choose and phrase the process the way you like. I’ve talked a lot about this before.

A picture relevant to our discussion on feature engineering is the front-middle of this process. It might look something like the following:

(tasks before here…) Select Data: Integrate data, de-normalize it into a dataset, collect it together. Preprocess Data: Format it, clean it, sample it so you can work with it. Transform Data: Feature Engineer happens here. Model Data: Create models, evaluate them and tune them. (tasks after here…)

The traditional idea of “Transforming Data” from a raw state to a state suitable for modeling is where feature engineering fits in. Transform data and feature engineering may in fact be synonyms.

This picture helps in a few ways.

You can see that before feature engineering, we are munging out data into a format we can even look at, and just before that we are collating and denormalizing data from databases into some kind of central picture.

We can, and should go back through these steps as we identify new perspectives on the data.

For example, we may have an attribute that is an aggregate field, like a sum. Rather than a single sum, we may decide to create features to describe the quantity by time interval, such as season. We need to step backward in the process through Preprocessing and even Selecting data to get access to the “real raw data” and create this feature.

We can see that feature engineering is followed by modeling.

It suggests a strong interaction with modeling, reminding us of the interplay of devising features and testing them against the coalface of our test harness and final performance measures.

This also suggests we may need to leave the data in a form suitable for the chosen modeling algorithm, such as normalize or standardize the features as a final step. This sounds like a preprocessing step, it probably is, but it helps us consider what types of finishing touches are needed to the data before effective modeling.

Iterative Process of Feature Engineering

Knowing where feature engineering fits into the context of the process of applied machine learning highlights that it does not standalone.

It is an iterative process that interplays with data selection and model evaluation, again and again, until we run out of time on our problem.

The process might look as follows:

Brainstorm features: Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal. Devise features: Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two. Select features: Use different feature importance scorings and feature selection methods to prepare one or more “views” for your models to operate upon. Evaluate models: Estimate model accuracy on unseen data using the chosen features.

You need a well defined problem so that you know when to stop this process and move on to trying other models, other model configurations, ensembles of models, and so on. There are gains to be had later in the pipeline once you plateau on ideas or the accuracy delta.

You need a well considered and designed test harness for objectively estimating model skill on unseen data. It will be the only measure you have of your feature engineering process, and you must trust it not to waste your time.

General Examples of Feature Engineering

Let’s make the concepts of feature engineering more concrete.

In this section we will consider tabular data like that you might have in an excel spreadsheet. We will look at some examples of manual feature construction that you might like to consider on your own problems.

When I hear “feature engineering is critically important”, this is the type of feature engineering I think of. It is the most common form that I am familiar with and practice.

Which of these is best? You cannot know before hand. You must try them and evaluate the results to achieve on your algorithm and performance measures.

Decompose Categorical Attributes

Imagine you have a categorical attribute, like “Item_Color” that can be Red, Blue or Unknown.

Unknown may be special, but to a model, it looks like just another colour choice. It might be beneficial to better expose this information.

You could create a new binary feature called “Has_Color” and assign it a value of “1” when an item has a color and “0” when the color is unknown.

Going a step further, you could create a binary feature for each value that Item_Color has. This would be three binary attributes: Is_Red, Is_Blue and Is_Unknown.

These additional features could be used instead of the Item_Color feature (if you wanted to try a simpler linear model) or in addition to it (if you wanted to get more out of something like a decision tree).

Decompose a Date-Time

A date-time contains a lot of information that can be difficult for a model to take advantage of in it’s native form, such as ISO 8601 (i.e. 2014-09-20T20:45:40Z).

If you suspect there are relationships between times and other attributes, you can decompose a date-time into constituent parts that may allow models to discover and exploit these relationships.

For example, you may suspect that there is a relationship between the time of day and other attributes.

You could create a new numerical feature called Hour_of_Day for the hour that might help a regression model.

You could create a new ordinal feature called Part_Of_Day with 4 values Morning, Midday, Afternoon, Night with whatever hour boundaries you think are relevant. This might be useful for a decision tree.

You can use similar approaches to pick out time of week relationships, time of month relationships and various structures of seasonality across a year.

Date-times are rich in structure and if you suspect there is time dependence in your data, take your time and tease them out.

Reframe Numerical Quantities

Your data is very likely to contain quantities, which can be reframed to better expose relevant structures. This may be a transform into a new unit or the decomposition of a rate into time and amount components.

You may have a quantity like a weight, distance or timing. A linear transform may be useful to regression and other scale dependent methods.

For example, you may have Item_Weight in grams, with a value like 6289. You could create a new feature with this quantity in kilograms as 6.289 or rounded kilograms like 6. If the domain is shipping data, perhaps kilograms is sufficient or more useful (less noisy) a precision for Item_Weight.

The Item_Weight could be split into two features: Item_Weight_Kilograms and Item_Weight_Remainder_Grams, with example values of 6 and 289 respectively.

There may be domain knowledge that items with a weight above 4 incur a higher taxation rate. That magic domain number could be used to create a new binary feature Item_Above_4kg with a value of “1” for our example of 6289 grams.

You may also have a quantity stored as a rate or an aggregate quantity for an interval. For example, Num_Customer_Purchases aggregated over a year.

In this case you may want to go back to the data collection step and create new features in addition to this aggregate and try to expose more temporal structure in the purchases, like perhaps seasonality. For example, the following new binary features could be created: Purchases_Summer, Purchases_Fall, Purchases_Winter and Purchases_Spring.

Concrete Examples of Feature Engineering

A great place to study examples of feature engineering is in the results from competitive machine learning.

Competitions typically use data from a real-world problem domain. A write-up of methods and approach is required at the end of a competition. These write-ups give valuable insight into effective real-world machine learning processes and methods.

In this section we touch on a few examples of interesting and notable post-competition write-ups that focus on feature engineering.

Predicting Student Test Performance in KDD Cup 2010

The KDD Cup is a machine learning competition held for attendees of the ACM Special Interest Group on Knowledge Discovery and Data Mining conferences, each year.

In 2010, the focus of the competition was the problem of modeling how students learn. A corpus of student results on algebraic problems was provided to be used to predict those students’ future performance.

The winner of the competition were a group of students and academics at the National Taiwan University. Their approach is described in the paper “Feature Engineering and Classifier Ensemble for KDD Cup 2010”.

The paper credits feature engineering as a key method in winning. Feature engineering simplified the structure of the problem at the expense of creating millions of binary features. The simple structure allowed the team to use highly performant but very simple linear methods to achieve the winning predictive model.

The paper provides details of how specific temporal and other non-linearities in the problem structure were reduced to simple composite binary indicators.

This is an extreme and instructive example of what is possible with simple attribute decomposition.

Predicting Patient Admittance in the Heritage Health Prize

The heritage health prize was a 3 million dollar prize awarded to the team who could best predict which patients would be admitted to hospital within the next year.

The prize had milestone awards each year where the top teams would be awarded a prize and their processes and methods made public.

I remember reading the papers released at the first of the three milestones and being impressed with the amount of feature engineering involved.

Specifically, the paper “Round 1 Milestone Prize: How We Did It – Team Market Makers” by Phil Brierley, David Vogel and Randy Axelrod. Most competitions involve vast amounts of feature engineering, but it struck me how clearly this paper made the point.

The paper provides both tables of attributes and SQL required to construct the attributes.

The paper gives some great real-world examples of feature engineering by simple decomposition. There are a lot of counts, mins, maxes, lots of binary attributes, and discretized numerical attributes. Very simple methods used to great effect.

More Resources on Feature Engineering

We have covered a lot of ground in this article and I hope you have a much greater appreciation of what feature engineering is, where it fits in, and how to do it.

This is really the start of your journey. You need to practice feature engineering and you need to study great practitioners of feature engineering.

This section provides some resources that might help you on your journey.

Books

I cannot find any books or book chapters on the topic.

There are however some great books on feature extraction. If you are working with digital representations of analog observations like images, video, sound or text, you might like to dive deeper into some feature extraction literature.

There are also lots of books on feature selection. If you are working to reduce your features by removing those that are redundant or irrelevant, dive deeper into feature selection.

Papers and Slides

It is a hard topic to find papers on.

Again, there are plenty of papers of feature extraction and chapters in books of feature selection, but not much of feature engineering. Also feature engineering has a meaning in software engineering as well, one that is not relevant to our discussion.

Here are some generally relevant papers:

Here are some generally relevant and interesting slides:

Links

There blog posts here and there. The most useful links are tutorials that work through a problem and clearly articulate the intentional feature engineering.

Below are some generally interesting links:

Videos

There are a few videos on the topic of feature engineering. The best by far is titled “Feature Engineering” by Ryan Baker. It’s short (9 minutes or so) and I recommend watching it for some good practical tips.

If you think I missed a key concept or resource, please leave a comment.

Update 2015: I notice that there is now a Wikipedia article on Feature Engineering and it copies large chunks of this post. Oh well."
250;news.mit.edu;http://news.mit.edu/2020/data-feminism-catherine-dignazio-0309;;The elephant in the server room;"Suppose you would like to know mortality rates for women during childbirth, by country, around the world. Where would you look? One option is the WomanStats Project, the website of an academic research effort investigating the links between the security and activities of nation-states, and the security of the women who live in them.

The project, founded in 2001, meets a need by patching together data from around the world. Many countries are indifferent to collecting statistics about women’s lives. But even where countries try harder to gather data, there are clear challenges to arriving at useful numbers — whether it comes to women’s physical security, property rights, and government participation, among many other issues.

For instance: In some countries, violations of women’s rights may be reported more regularly than in other places. That means a more responsive legal system may create the appearance of greater problems, when it provides relatively more support for women. The WomanStats Project notes many such complications.

Thus the WomanStats Project offers some answers — for example, Australia, Canada, and much of Western Europe have low childbirth mortality rates — while also showing what the challenges are to taking numbers at face value. This, according to MIT professor Catherine D’Ignazio, makes the site unusual, and valuable.

“The data never speak for themselves,” says D’Ignazio, referring to the general problem of finding reliable numbers about women’s lives. “There are always humans and institutions speaking for the data, and different people have their own agendas. The data are never innocent.”

Now D’Ignazio, an assistant professor in MIT’s Department of Urban Studies and Planning, has taken a deeper look at this issue in a new book, co-authored with Lauren Klein, an associate professor of English and quantitative theory and methods at Emory University. In the book, “Data Feminism,” published this month by the MIT Press, the authors use the lens of intersectional feminism to scrutinize how data science reflects the social structures it emerges from.

“Intersectional feminism examines unequal power,” write D’Ignazio and Klein, in the book’s introduction. “And in our contemporary world, data is power too. Because the power of data is wielded unjustly, it must be challenged and changed.”

The 4 percent problem

To see a clear case of power relations generating biased data, D’Ignazio and Klein note, consider research led by MIT’s own Joy Buolamwini, who as a graduate student in a class studying facial-recognition programs, observed that the software in question could not “see” her face. Buolamwini found that for the facial-recognition system in question, the software was based on a set of faces which were 78 percent male and 84 percent white; only 4 percent were female and dark-skinned, like herself.

Subsequent media coverage of Buolamwini’s work, D’Ignazio and Klein write, contained “a hint of shock.” But the results were probably less surprising to those who are not white males, they think.

“If the past is racist, oppressive, sexist, and biased, and that’s your training data, that is what you are tuning for,” D’Ignazio says.

Or consider another example, from tech giant Amazon, which tested an automated system that used AI to sort through promising CVs sent in by job applicants. One problem: Because a high percentage of company employees were men, the algorithm favored men’s names, other things being equal.

“They thought this would help [the] process, but of course what it does is train the AI [system] to be biased toward women, because they themselves have not hired that many women,” D’Ignazio observes.

To Amazon’s credit, it did recognize the problem. Moreover, D’Ignazio notes, this kind of issue is a problem that can be addressed. “Some of the technologies can be reformed with a more participatory process, or better training data. … If we agree that’s a good goal, one path forward is to adjust your training set and include more people of color, more women.”

“Who’s on the team? Who had the idea? Who’s benefiting?”

Still, the question of who participates in data science is, as the authors write, “the elephant in the server room.” As of 2011, only 26 percent of all undergraduates receiving computer science degrees in the U.S. were women. That is not only a low figure, but actually a decline from past levels: In 1985, 37 percent of computer science graduates were women, the highest mark on record.

As a result of the lack of diversity in the field, D’Ignazio and Klein believe, many data projects are radically limited in their ability to see all facets of the complex social situations they purport to measure.

“We want to try to tune people in to these kinds of power relationships and why they matter deeply,” D’Ignazio says. “Who’s on the team? Who had the idea? Who’s benefiting from the project? Who’s potentially harmed by the project?”

In all, D’Ignazio and Klein outline seven principles of data feminism, from examining and challenging power, to rethinking binary systems and hierarchies, and embracing pluralism. (Those statistics about gender and computer science graduates are limited, they note, by only using the “male” and “female” categories, thus excluding people who identify in different terms.)

People interested in data feminism, the authors state, should also “value multiple forms of knowledge,” including firsthand knowledge that may lead us to question seemingly official data. Also, they should always consider the context in which data are generated, and “make labor visible” when it comes to data science. This last principle, the researchers note, speaks to the problem that even when women and other excluded people contribute to data projects, they often receive less credit for their work.

For all the book’s critique of existing systems, programs, and practices, D’Ignazio and Klein are also careful to include examples of positive, successful efforts, such as the WomanStats project, which has grown and thrived over two decades.

“For people who are data people but are new to feminism, we want to provide them with a very accessible introduction, and give them concepts and tools they can use in their practice,” D’Ignazio says. “We’re not imagining that people already have feminism in their toolkit. On the other hand, we are trying to speak to folks who are very tuned in to feminism or social justice principles, and highlight for them the ways data science is both problematic, but can be marshalled in the service of justice.”"
251;machinelearningmastery.com;http://machinelearningmastery.com/applied-machine-learning-weka-mini-course/;2016-08-14;Weka Machine Learning Mini-Course;"Tweet Share Share

Last Updated on August 22, 2019

Become A Machine Learning Practitioner in 14-Days

Machine learning is a fascinating study, but how do you actually use it on your own problems?

You may be confused as to how best prepare your data for machine learning, which algorithms to use or how to choose one model over another.

In this post you will discover a 14-part crash course into applied machine learning using the Weka platform without a single mathematical equation or line of programming code.

After completing this mini course:

You will know how to work through a dataset end-to-end and deliver a set of predictions or a high-performance model.

You will know your way around the Weka machine learning workbench including how to explore algorithms and design controlled experiments.

You will know how to create multiple views of your problem, evaluate multiple algorithms and use statistics to choose the best performing model for your own predictive modeling problems.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

(Tip: You might want to print or bookmark this page so that you can refer back to it later)

Who Is This Mini-Course For?

Before we get started, let’s make sure you are in the right place. The list below provides some general guidelines as to who this course was designed for.

Don’t panic if you don’t match these points exactly, you might just need to brush up in one area or another to keep up.

You are a developer that knows a little machine learning.

This means you know about some of the basics of machine learning like cross validation, some algorithms and the bias-variance trade-off. It does not mean that you are a machine learning PhD, just that you know the landmarks or know where to look them up.

This mini-course is not a textbook on machine learning.

It will take you from a developer that knows a little machine learning to a developer who can use the Weka platform to work through a dataset from beginning to end and deliver a set of predictions or a high performance model.

Mini-Course Overview (what to expect)

This mini-course is divided into 14 parts.

Each lesson was designed to take you about 30 minutes. You might finish some much sooner and for others you may choose to go deeper and spend more time.

You can complete each part as quickly or as slowly as you like. A comfortable schedule may be to complete one lesson per day over a two week period. Highly recommended.

The topics you will cover over the next 14 lessons are as follows:

Lesson 01 : Download and Install Weka.

: Download and Install Weka. Lesson 02 : Load Standard Machine Learning Datasets.

: Load Standard Machine Learning Datasets. Lesson 03 : Descriptive Stats and Visualization.

: Descriptive Stats and Visualization. Lesson 04 : Rescale Your Data.

: Rescale Your Data. Lesson 05 : Perform Feature Selection on Your Data.

: Perform Feature Selection on Your Data. Lesson 06 : Machine Learning Algorithms in Weka.

: Machine Learning Algorithms in Weka. Lesson 07 : Estimate Model Performance.

: Estimate Model Performance. Lesson 08 : Baseline Performance On Your Data.

: Baseline Performance On Your Data. Lesson 09 : Classification Algorithms.

: Classification Algorithms. Lesson 10 : Regression Algorithms.

: Regression Algorithms. Lesson 11 : Ensemble Algorithms.

: Ensemble Algorithms. Lesson 12 : Compare the Performance of Algorithms.

: Compare the Performance of Algorithms. Lesson 13 : Tune Algorithm Parameters.

: Tune Algorithm Parameters. Lesson 14: Save Your Model.

This is going to be a lot of fun.

You’re going to have to do some work though, a little reading, a little tinkering in Weka. You want to get started in applied machine learning right?

(Tip: All of the answers these lessons can be found on this blog, use the search feature)

Any questions at all, please post in the comments below.

Share your results in the comments.

Hang in there, don’t give up!

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Lesson 01: Download and Install Weka

The first thing to do is install the Weka software on your workstation.

Weka is free open source software. It is written in Java and can run on any platform that supports Java, including:

Windows.

Mac OS X.

Linux.

You can download Weka as standalone software or as a version bundled with Java.

If you do not already have Java installed on your system, I recommend downloading and installing a version bundled with Java.

Your task for this lesson is to visit the Weka download page, download and install Weka on your workstation.

Lesson 02: Load Standard Machine Learning Datasets

Now that you have Weka installed, you need to load data.

Weka is designed to load data in a native format called ARFF. It is a modified CSV format that includes additional information about the types of each attribute (column).

Your Weka installation includes a subdirectory with a number of standard machine learning datasets in ARFF format ready for you to load.

Weka also supports loading data from raw CSV files as well as a database and converts the data to ARFF as needed.

In this lesson you will load a standard dataset in the Weka Explorer.

Start Weka (click on the bird icon), this will start the Weka GUI Chooser. Click the “Explorer” button, this will open the Weka Explorer interface. Click the “Open file…” button and navigate to the data/ directory in your Weka installation and load the diabetes.arff dataset.

Note, if you do not have a data/ directory in your Weka installation, or you cannot find it, download the .zip version of Weka from the Weka download webpage, unzip it and access the data/ directory.

You have just loaded your first dataset in Weka.

Try loading some of the other datasets in the data/ directory.

Try downloading a raw CSV file from the UCI Machine Learning repository and loading it in Weka.

Lesson 03: Descriptive Stats and Visualization

Once you can load data in Weka, it is important to take a look at it.

Weka lets you review descriptive statistics calculated from your data. It also provides visualization tools.

In this lesson you will use Weka to learn more about your data.

Open the Weka GUI Chooser. Open the Weka Explorer. Load the data/diabetes.arff dataset. Click on different attributes in the “Attributes” list and review the details in the “Selected attribute” pane. Click the “Visualize All” button to review all attribute distributions. Click the “Visualize” tab and review the scatter plot matrix for all attributes.

Get comfortable reviewing the details for different attributes in the “Preprocess” tab and tuning the scatter plot matrix in the “Visualize” tab.

Lesson 04: Rescale Your Data

Raw data is often not suitable for modeling.

Often you can improve the performance of your machine learning models by rescaling attributes.

In this lesson you will learn how to use data filters in Weka to rescale your data. You will normalize all of the attributes for a dataset, rescaling them to the consistent range of 0-to-1.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Choose” button in the “Filter” pane and select unsupervised.attribute.Normalize. Click the “Apply” button.

Review the details for each attribute in the “Selected attribute” pane and note the change to the scale.

Explore using other data filters such as the Standardize filter.

Explore configuring filters by clicking on the name of the loaded filter and changing it’s parameters.

Test out saving modified datasets for later use by clicking the “Save…” button on the “Preprocess” tab.

Lesson 05: Perform Feature Selection on Your Data

Not all of the attributes in your dataset may be relevant to the attribute you want to predict.

You can use feature selection to identify those attributes that are most relevant to your output variable.

In this lesson you will get familiar with using different feature selection methods.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Select attributes” tab. Click the “Choose” button in the “Attribute Evaluator” pane and select the “CorrelationAttributeEval”. You will be presented with a dialog asking you to change to the “Ranker” search method, needed when using this feature selection method. Click the “Yes” button. Click the “Start” button to run the feature selection method.

Review the output in the “Attribute selection output” pane and note the correlation scores for each attribute, the larger numbers indicating the more relevant features.

Explore other feature selection methods such as the use of information gain (entropy).

Explore selecting features to removal from your dataset in the “Preprocess” tab and the “Remove” button.

Lesson 06: Machine Learning Algorithms in Weka

A key benefit of the Weka workbench is the large number of machine learning algorithms it provides.

You need to know your way around machine learning algorithms.

In this lesson you will take a closer look at machine learning algorithms in Weka.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Classify” tab. Click the “Choose” button and note the different groupings for algorithms. Click the name of the selected algorithm to configure it. Click the “More” button on the configuration window to learn more about the implementation. Click the “Capabilities” button on the configuration window to learn more about how it can be used. Note the “Open” and “Save” buttons on the window where different configurations can be saved and loaded. Hover on a configuration parameter and note the tooltip help. Click the “Start” button to run an algorithm.

Browse the algorithms available. Note that some algorithms are unavailable given whether your dataset is a classification (predict a category) or regression (predict a real value) type problem.

Explore and learn more about the various algorithms available in Weka.

Get confidence choosing and configuring algorithms.

Lesson 07: Estimate Model Performance

Now that you know how to choose and configure different algorithms, you need to know how to evaluate the performance of an algorithm.

In this lesson you are going to learn about the different ways to evaluate the performance of an algorithm in Weka.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Classify” tab.

The “Test options” pane lists the various different techniques that you can use to evaluate the performance of an algorithm.

The gold standard is 10-fold “Cross Validation”. This is selected by default. For a small dataset, the number of folds can be adjusted from 10 to 5 or even 3.

If your dataset is very large and you want to evaluate algorithms quickly, you can use the “Percentage split” option. By default, this option will train on 66% of your dataset and use the remaining 34% to evaluate the performance of your model.

Alternately, if you have a separate file containing a validation dataset, you can evaluate your model on that by selecting the “Supplied test set” option. Your model will be trained on the entire training dataset and evaluated on the separate dataset.

Finally, you can evaluate the performance of your model on the whole training dataset. This is useful if you are more interested in a descriptive than a predictive model.

Click the “Start” button to run a given algorithm with your chosen test option.

Experiment with different Test options.

Further refine the test options in the configuration provided by clicking the “More options…” button.

Lesson 08: Baseline Performance On Your Data

When you start evaluating multiple machine learning algorithms on your dataset, you need a baseline for comparison.

A baseline result gives you a point of reference to know whether the results for a given algorithm are good or bad, and by how much.

In this lesson you will learn about the ZeroR algorithm that you can use as a baseline for classification and regression algorithms.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Classify” tab. The ZeroR algorithm is chosen by default. Click the “Start” button.

This will run the ZeroR algorithm using 10-fold cross validation on your dataset.

The ZeroR algorithm also called the Zero Rule is an algorithm that you can use to calculate a baseline of performance for all algorithms on your dataset. It is the “worst” result and any algorithm that shows a better performance has some skill on your problem.

On a classification algorithm, the ZeroR algorithm will always predict the most abundant category. If the dataset has an equal number of classes, it will predict the first category value.

On the diabetes dataset, this results in a classification accuracy of 65%.

For regression problems, the ZeroR algorithm will always predict the mean output value.

Experiment with the ZeroR algorithm on a range of different datasets. It is the algorithm you should always run first before all others to develop a baseline.

Lesson 09: Tour of Classification Algorithms

Weka provides a large number of classification algorithms.

In this lesson you will discover 5 top classification algorithms that you can use on your classification problems.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Classify” tab. Click the “Choose” button.

5 Top algorithms that you can use for classification include:

Logistic Regression (functions.Logistic).

Naive Bayes (bayes.NaiveBayes).

k-Nearest Neighbors (lazy.IBk).

Classification and Regression Trees (trees.REPTree).

Support Vector Machines (functions.SMO).

Experiment with each of these top algorithms.

Try them out on different classification datasets, such as those with two classes and those with more.

Lesson 10: Tour of Regression Algorithms

Classification algorithms is Weka’s specialty, but many of these algorithms can be used for regression.

Regression is the prediction of a real valued outcome (like a dollar amount), different from classification that predicts a category (like “dog” or “cat”).

In this lesson you will discover 5 top regression algorithms that you can use on your regression problems.

You can download a suite of standard regression machine learning datasets from the Weka dataset download webpage. Download the datasets-numeric.jar archive of regression problems, titled:

“A jar file containing 37 regression problems, obtained from various sources”

Use your favorite unzip program to unzip the .jar file and you will have a new directory called numeric/ containing 37 regression problems that you can work with.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/housing.arff dataset. Click the “Classify” tab. Click the “Choose” button.

5 Top algorithms that you can use for regression include:

Linear Regression (functions.LinearRegression).

Support Vector Regression (functions.SMOReg).

k-Nearest Neighbors (lazy.IBk).

Classification and Regression Trees (trees.REPTree).

Artificial Neural Network (functions.MultilayerPerceptron).

Experiment with each of these top algorithms.

Try them out on different regression datasets.

Lesson 11: Tour of Ensemble Algorithms

Weka is very easy to use and this may be its biggest advantage over other platforms.

In addition to this, Weka provides a large suite of ensemble machine learning algorithms and this may be Weka’s second big advantage over other platforms.

It is worth spending your time to get good at using Weka’s ensemble algorithms. In this lesson you will discover 5 top ensemble machine learning algorithms that you can use.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Classify” tab. Click the “Choose” button.

5 Top ensemble algorithms that you can use include:

Bagging (meta.Bagging).

Random Forest (trees.RandomForest).

AdaBoost (meta.AdaBoost).

Voting (meta.Voting).

Stacking (meta.Stacking).

Experiment with each of these top algorithms.

Most of these ensemble methods let you choose the sub-models. Experiment using different combinations of sub-models. Combinations of techniques that work in very different ways and produce different predictions often result in better performance.

Try them out on different classification and regression datasets.

Lesson 12: Compare the Performance of Algorithms

Weka provides a different tool specifically designed for comparing algorithms called the Weka Experiment Environment.

The Weka Experiment Environment allows you to design and execute controlled experiments with machine learning algorithms and then analyze the results.

In this lesson you will design your first experiment in Weka and discover how to use the Weka Experiment Environment to compare the performance of machine learning algorithms.

Open the “Weka Chooser GUI”. Click the “Experimenter” button to open the “Weka Experiment Environment”. Click the “New” button. Click the “Add new…” button in the “Datasets” pane and select “data/diabetes.arff”. Click the “Add new…” button in the “Algorithms” pane and add “ZeroR” and “IBk”. Click the “Run” tab and click the “Start” button. Click the “Analyse” tab and click the “Experiment” button and then the “Perform test” button.

You just designed, executed and analysed the results of your first controlled experiment in Weka.

You compared the ZeroR algorithm to the IBk algorithm with default configuration on the diabetes dataset.

The results show that IBK has a higher classification accuracy than ZeroR and that this difference is statistically significant (the little “v” character next to the result).

Expand the experiment and add more algorithms and rerun the experiment.

Change the “Test base” on the “Analyse” tab to change which set of results is taken as the reference for comparison to the other results.

Lesson 13: Tune Algorithm Parameters

To get the most out of a machine learning algorithm you must tune the parameters of the method to your problem.

You cannot know how to best do this before hand, therefore you must try out lots of different parameters.

The Weka Experiment Environment allows you to design controlled experiments to compare the results of different algorithm parameters and whether the differences are statistically significant.

In this lesson you are going to design an experiment to compare the parameters of the k-Nearest Neighbors algorithm.

Open the “Weka Chooser GUI”. Click the “Experimenter” button to open the “Weka Experiment Environment” Click the “New” button. Click the “Add new…” button in the “Datasets” pane and select “data/diabetes.arff”. Click the “Add new…” button in the “Algorithms” pane and add 3 copes of the “IBk” algorithm. Click each IBk algorithm in the list and click the “Edit selected…” button and change “KNN” to 1, 3, 5 for each of the 3 different algorithms. Click the “Run” tab and click the “Start” button. Click the “Analyse” tab and click the “Experiment” button and then the “Perform test” button.

You just designed, executed and analyzed the results of a controlled experiment to compare algorithm parameters.

We can see that the results for large K values is better than the default of 1 and the difference is significant.

Explore changing other configuration properties of KNN and build confidence in developing experiments to tune machine learning algorithms.

Lesson 14: Save Your Model

Once you have found a top performing model on your problem you need to finalize it for later use.

In this final lesson you will discover how to train a final model and save it to a file for later use.

Open the Weka GUI Chooser and then the Weka Explorer. Load the data/diabetes.arff dataset. Click the “Classify” tab. Change the “Test options” to “Use training set” and click the “Start” button. Right click on the results in the “Result list” and click “Save model” and enter a filename like “diabetes-final”.

You have just trained a final model on the entire training dataset and saved the resulting model to a file.

You can load this model back into Weka and use it to make predictions on new data.

Right-click on the “Result list” click “Load model” and select your model file (“diabetes-final.model”). Change the “Test options” to “Supplied test set” and choose data/diabetes.arff (this could be a new file for which you do not have predictions) Click “More options” in the “Test options” and change “Output predictions” to “Plain Text” Right click on the loaded model and choose “Re-evaluate model on current test set”.

The new predictions will now be listed in the “Classifier output” pane.

Experiment saving different models and making predictions for entirely new datasets.

Machine Learning With Weka Mini-Course Review

Congratulations, you made it. Well done!

Take a moment and look back at how far you have come:

You discovered how to start and use the Weka Explorer and Weka Experiment Environment, perhaps for the first time.

You loaded data, analyzed it and used data filters and feature selection to prepare data for modeling.

You discovered a suite of machine learning algorithms and how to design controlled experiments to evaluate their performance.

Don’t make light of this, you have come a long way in a short amount of time. This is just the beginning of your journey in applied machine learning with Weka. Keep practicing and developing your skills.

Did you enjoy this mini-course? Do you have any questions or sticking points?

Leave a comment and let me know.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
252;machinelearningmastery.com;https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/;2019-01-29;How to Choose Loss Functions When Training Deep Learning Neural Networks;"# mlp for the blobs multi-class classification problem with kl divergence loss

from sklearn . datasets import make_blobs

from keras . layers import Dense

from keras . models import Sequential

from keras . optimizers import SGD

from keras . utils import to_categorical

from matplotlib import pyplot

# generate 2d classification dataset

X , y = make_blobs ( n_samples = 1000 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 )

# one hot encode output variable

y = to_categorical ( y )

# split into train and test

n_train = 500

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# define model

model = Sequential ( )

model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 3 , activation = 'softmax' ) )

# compile model

opt = SGD ( lr = 0.01 , momentum = 0.9 )

model . compile ( loss = 'kullback_leibler_divergence' , optimizer = opt , metrics = [ 'accuracy' ] )

# fit model

history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 100 , verbose = 0 )

# evaluate the model

_ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 )

_ , test_acc = model . evaluate ( testX , testy , verbose = 0 )

print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) )

# plot loss during training

pyplot . subplot ( 211 )

pyplot . title ( 'Loss' )

pyplot . plot ( history . history [ 'loss' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' )

pyplot . legend ( )

# plot accuracy during training

pyplot . subplot ( 212 )

pyplot . title ( 'Accuracy' )

pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' )

pyplot . legend ( )"
253;machinelearningmastery.com;https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/;2019-07-14;How to Implement Wasserstein Loss for Generative Adversarial Networks;"Tweet Share Share

The Wasserstein Generative Adversarial Network, or Wasserstein GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a loss function that correlates with the quality of generated images.

It is an important extension to the GAN model and requires a conceptual shift away from a discriminator that predicts the probability of a generated image being “real” and toward the idea of a critic model that scores the “realness” of a given image.

This conceptual shift is motivated mathematically using the earth mover distance, or Wasserstein distance, to train the GAN that measures the distance between the data distribution observed in the training dataset and the distribution observed in the generated examples.

In this post, you will discover how to implement Wasserstein loss for Generative Adversarial Networks.

After reading this post, you will know:

The conceptual shift in the WGAN from discriminator predicting a probability to a critic predicting a score.

The implementation details for the WGAN as minor changes to the standard deep convolutional GAN.

The intuition behind the Wasserstein loss function and how implement it from scratch.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into five parts; they are:

GAN Stability and the Discriminator What Is a Wasserstein GAN? Implementation Details of the Wasserstein GAN How to Implement Wasserstein Loss Common Point of Confusion With Expected Labels

GAN Stability and the Discriminator

Generative Adversarial Networks, or GANs, are challenging to train.

The discriminator model must classify a given input image as real (from the dataset) or fake (generated), and the generator model must generate new and plausible images.

The reason GANs are difficult to train is that the architecture involves the simultaneous training of a generator and a discriminator model in a zero-sum game. Stable training requires finding and maintaining an equilibrium between the capabilities of the two models.

The discriminator model is a neural network that learns a binary classification problem, using a sigmoid activation function in the output layer, and is fit using a binary cross entropy loss function. As such, the model predicts a probability that a given input is real (or fake as 1 minus the predicted) as a value between 0 and 1.

The loss function has the effect of penalizing the model proportionally to how far the predicted probability distribution differs from the expected probability distribution for a given image. This provides the basis for the error that is back propagated through the discriminator and the generator in order to perform better on the next batch.

The WGAN relaxes the role of the discriminator when training a GAN and proposes the alternative of a critic.

What Is a Wasserstein GAN?

The Wasserstein GAN, or WGAN for short, was introduced by Martin Arjovsky, et al. in their 2017 paper titled “Wasserstein GAN.”

It is an extension of the GAN that seeks an alternate way of training the generator model to better approximate the distribution of data observed in a given training dataset.

Instead of using a discriminator to classify or predict the probability of generated images as being real or fake, the WGAN changes or replaces the discriminator model with a critic that scores the realness or fakeness of a given image.

This change is motivated by a mathematical argument that training the generator should seek a minimization of the distance between the distribution of the data observed in the training dataset and the distribution observed in generated examples. The argument contrasts different distribution distance measures, such as Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and the Earth-Mover (EM) distance, referred to as Wasserstein distance.

The most fundamental difference between such distances is their impact on the convergence of sequences of probability distributions.

— Wasserstein GAN, 2017.

They demonstrate that a critic neural network can be trained to approximate the Wasserstein distance, and, in turn, used to effectively train a generator model.

… we define a form of GAN called Wasserstein-GAN that minimizes a reasonable and efficient approximation of the EM distance, and we theoretically show that the corresponding optimization problem is sound.

— Wasserstein GAN, 2017.

Importantly, the Wasserstein distance has the properties that it is continuous and differentiable and continues to provide a linear gradient, even after the critic is well trained.

The fact that the EM distance is continuous and differentiable a.e. means that we can (and should) train the critic till optimality. […] the more we train the critic, the more reliable gradient of the Wasserstein we get, which is actually useful by the fact that Wasserstein is differentiable almost everywhere.

— Wasserstein GAN, 2017.

This is unlike the discriminator model that, once trained, may fail to provide useful gradient information for updating the generator model.

The discriminator learns very quickly to distinguish between fake and real, and as expected provides no reliable gradient information. The critic, however, can’t saturate, and converges to a linear function that gives remarkably clean gradients everywhere.

— Wasserstein GAN, 2017.

The benefit of the WGAN is that the training process is more stable and less sensitive to model architecture and choice of hyperparameter configurations.

… training WGANs does not require maintaining a careful balance in training of the discriminator and the generator, and does not require a careful design of the network architecture either. The mode dropping phenomenon that is typical in GANs is also drastically reduced.

— Wasserstein GAN, 2017.

Perhaps most importantly, the loss of the discriminator appears to relate to the quality of images created by the generator.

Specifically, the lower the loss of the critic when evaluating generated images, the higher the expected quality of the generated images. This is important as unlike other GANs that seek stability in terms of finding an equilibrium between two models, the WGAN seeks convergence, lowering generator loss.

To our knowledge, this is the first time in GAN literature that such a property is shown, where the loss of the GAN shows properties of convergence. This property is extremely useful when doing research in adversarial networks as one does not need to stare at the generated samples to figure out failure modes and to gain information on which models are doing better over others.

— Wasserstein GAN, 2017.

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Implementation Details of the Wasserstein GAN

Although the theoretical grounding for the WGAN is dense, the implementation of a WGAN requires a few minor changes to the standard deep convolutional GAN, or DCGAN.

Those changes are as follows:

Use a linear activation function in the output layer of the critic model (instead of sigmoid).

Use Wasserstein loss to train the critic and generator models that promote larger difference between scores for real and generated images.

Constrain critic model weights to a limited range after each mini batch update (e.g. [-0.01,0.01]).

In order to have parameters w lie in a compact space, something simple we can do is clamp the weights to a fixed box (say W = [−0.01, 0.01]l ) after each gradient update.

— Wasserstein GAN, 2017.

Update the critic model more times than the generator each iteration (e.g. 5).

Use the RMSProp version of gradient descent with small learning rate and no momentum (e.g. 0.00005).

… we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam […] We therefore switched to RMSProp …

— Wasserstein GAN, 2017.

The image below provides a summary of the main training loop for training a WGAN, taken from the paper. Note the listing of recommended hyperparameters used in the model.

How to Implement Wasserstein Loss

The Wasserstein loss function seeks to increase the gap between the scores for real and generated images.

We can summarize the function as it is described in the paper as follows:

Critic Loss = [average critic score on real images] – [average critic score on fake images]

Generator Loss = -[average critic score on fake images]

Where the average scores are calculated across a mini-batch of samples.

This is precisely how the loss is implemented for graph-based deep learning frameworks such as PyTorch and TensorFlow.

The calculations are straightforward to interpret once we recall that stochastic gradient descent seeks to minimize loss.

In the case of the generator, a larger score from the critic will result in a smaller loss for the generator, encouraging the critic to output larger scores for fake images. For example, an average score of 10 becomes -10, an average score of 50 becomes -50, which is smaller, and so on.

In the case of the critic, a larger score for real images results in a larger resulting loss for the critic, penalizing the model. This encourages the critic to output smaller scores for real images. For example, an average score of 20 for real images and 50 for fake images results in a loss of -30; an average score of 10 for real images and 50 for fake images results in a loss of -40, which is better, and so on.

The sign of the loss does not matter in this case, as long as loss for real images is a small number and the loss for fake images is a large number. The Wasserstein loss encourages the critic to separate these numbers.

We can also reverse the situation and encourage the critic to output a large score for real images and a small score for fake images and achieve the same result. Some implementations make this change.

In the Keras deep learning library (and some others), we cannot implement the Wasserstein loss function directly as described in the paper and as implemented in PyTorch and TensorFlow. Instead, we can achieve the same effect without having the calculation of the loss for the critic dependent upon the loss calculated for real and fake images.

A good way to think about this is a negative score for real images and a positive score for fake images, although this negative/positive split of scores learned during training is not required; just larger and smaller is sufficient.

Small Critic Score (e.g.< 0): Real – Large Critic Score (e.g. >0): Fake

We can multiply the average predicted score by -1 in the case of fake images so that larger averages become smaller averages and the gradient is in the correct direction, i.e. minimizing loss. For example, average scores on fake images of [0.5, 0.8, and 1.0] across three batches of fake images would become [-0.5, -0.8, and -1.0] when calculating weight updates.

Loss For Fake Images = -1 * Average Critic Score

No change is needed for the case of real scores, as we want to encourage smaller average scores for real images.

Loss For Real Images = Average Critic Score

This can be implemented consistently by assigning an expected outcome target of -1 for fake images and 1 for real images and implementing the loss function as the expected label multiplied by the average score. The -1 label will be multiplied by the average score for fake images and encourage a larger predicted average, and the +1 label will be multiplied by the average score for real images and have no effect, encouraging a smaller predicted average.

Wasserstein Loss = Label * Average Critic Score

Or

Wasserstein Loss(Real Images) = 1 * Average Predicted Score

Wasserstein Loss(Fake Images) = -1 * Average Predicted Score

We can implement this in Keras by assigning the expected labels of -1 and 1 for fake and real images respectively. The inverse labels could be used to the same effect, e.g. -1 for real and +1 for fake to encourage small scores for fake images and large scores for real images. Some developers do implement the WGAN in this alternate way, which is just as correct.

The loss function can be implemented by multiplying the expected label for each sample by the predicted score (element wise), then calculating the mean.

def wasserstein_loss(y_true, y_pred): return mean(y_true * y_pred) 1 2 def wasserstein_loss ( y_true , y_pred ) : return mean ( y_true * y_pred )

The above function is the elegant way to implement the loss function; an alternative, less-elegant implementation that might be more intuitive is as follows:

def wasserstein_loss(y_true, y_pred): return mean(y_true) * mean(y_pred) 1 2 def wasserstein_loss ( y_true , y_pred ) : return mean ( y_true ) * mean ( y_pred )

In Keras, the mean function can be implemented using the Keras backend API to ensure the mean is calculated across samples in the provided tensors; for example:

from keras import backend # implementation of wasserstein loss def wasserstein_loss(y_true, y_pred): return backend.mean(y_true * y_pred) 1 2 3 4 5 from keras import backend # implementation of wasserstein loss def wasserstein_loss ( y_true , y_pred ) : return backend . mean ( y_true * y_pred )

Now that we know how to implement the Wasserstein loss function in Keras, let’s clarify one common point of misunderstanding.

Common Point of Confusion With Expected Labels

Recall we are using the expected labels of -1 for fake images and +1 for real images.

A common point of confusion is that a perfect critic model will output -1 for every fake image and +1 for every real image.

This is incorrect.

Again, recall we are using stochastic gradient descent to find the set of weights in the critic (and generator) models that minimize the loss function.

We have established that we want the critic model to output larger scores on average for fake images and smaller scores on average for real images. We then designed a loss function to encourage this outcome.

This is the key point about loss functions used to train neural network models. They encourage a desired model behavior, and they do not have to achieve this by providing the expected outcomes. In this case, we defined our Wasserstein loss function to interpret the average score predicted by the critic model and used labels for the real and fake cases to help with this interpretation.

So what is a good loss for real and fake images under Wasserstein loss?

Wasserstein is not an absolute and comparable loss for comparing across GAN models. Instead, it is relative and depends on your model configuration and dataset. What is important is that it is consistent for a given critic model and convergence of the generator (better loss) does correlate with better generated image quality.

It could be negative scores for real images and positive scores for fake images, but this is not required. All scores could be positive or all scores could be negative.

The loss function only encourages a separation between scores for fake and real images as larger and smaller, not necessarily positive and negative.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

Articles

Summary

In this post, you discovered how to implement Wasserstein loss for Generative Adversarial Networks.

Specifically, you learned:

The conceptual shift in the WGAN from discriminator predicting a probability to a critic predicting a score.

The implementation details for the WGAN as minor changes to the standard deep convolutional GAN.

The intuition behind the Wasserstein loss function and how implement it from scratch.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Generative Adversarial Networks Today! Develop Your GAN Models in Minutes ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Generative Adversarial Networks with Python It provides self-study tutorials and end-to-end projects on:

DCGAN, conditional GANs, image translation, Pix2Pix, CycleGAN

and much more... Finally Bring GAN Models to your Vision Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
254;towardsdatascience.com;https://towardsdatascience.com/can-we-use-machine-learning-to-forecast-oil-prices-during-the-2020-collapse-4873f03336e9?source=collection_home---4------3-----------------------;2020-04-19;Can We Use Machine Learning To Forecast Oil Prices During The 2020 Collapse?;"A Sensitive Commodity

Photo by Science in HD on Unsplash

Fundamentals of Oil Pricing

Oil is a commodity notorious for being able to go in the complete opposite direction after a single market event.

This is because the fundamentals of oil prices are rarely based on real-time data, instead, it is driven by externalities, making our attempt to forecast it all the more challenging.

2020: A Year Of Ups & Downs

COVID-19

In 2020, COVID-19’s repercussions acted as a reminder of how unpredictable and sensitive oil prices are relative to external shocks.

Early 2020

At the beginning of the year, oil prices were soaring because of the OPEC-led supply cuts, U.S. transactions on multiple major oil exporters, and escalating tensions in Libya.

Mid-2020

However, all of that took a major turn when the health of the global economy was put into speculation after COVID-19, and to make matters worse, industry experts believe it is now “virtually impossible” to confidently forecast the price of oil.

What is more confusing is that presidents have been preaching the virtues of cheap oil for decades, including President Trump himself just a month ago.

Donald Trump Tweet Praising Low Oil Prices — Mar 9, 2020

Donald Trump Praising High Oil Prices — Apr 2, 2020

However, Trump is now doing whatever it takes to push the prices back up, including posting a tweet that caused oil prices to temporarily soar up by 25% — the biggest one-day gain in recorded history.

OPEC Deal — April 9th, 2020

The historic OPEC deal to cut production by 10% has only worked to stem the damage that is still being done to the market. Oil and gas producers are still cutting their dividends and capital spending in efforts to protect their balance sheets in the face of escalating financial losses.

Why Should We Care About Oil Prices?

The reason we are creating this model is because of how linked the health of the economy is to oil prices, whenever there is a slight deviation from the norm in oil prices, the economy is impacted drastically as evident by the parallel movements on Google Trends.

(Closely Intertwined Economy & Oil Prices — Google Trends, 2020)

Time Series Analysis

Time series analysis is an insightful way to look at how a certain commodity changes over time, however, we need to go a step further and create a forecasting model using machine learning’s ARIMA.

What is ARIMA?

An autoregressive integrated moving average model is a form of regression analysis that predicts future moves by examining the difference between the values in the series as opposed to actual values.

It is the perfect time to implement this algorithm as we don’t expect any more majorly historic deals anytime soon given the recency of the OPEC deal.

Forecast Period

Timeframe 1: 20th April 2020–1st October 2020

(COVID-19 Statistics — Google News Apr 18, 2020)

The first timeframe we are forecasting is from the 20th April 2020–1st October 2020, making up almost half of the year.

The rationale behind this is because we have not reached the global peak of COVID-19 yet, giving us a reasonable level of assurance that a fully free COVID-19 market can’t exist within merely 5 and a half months as based on previous pandemic’s timelines, making this our more accurate forecast.

Timeframe 2: 20th April 2020–1st January 2025

This timeframe will act as our prediction for the estimated recovery time until oil prices can go back to their high $50s before 2020's crash.

Photo by Erik Mclean on Unsplash

Dataset

Our dataset is sourced from the U.S. Energy Information Administration and contains 37 years’ worth of daily historic Brent Oil prices from the 17th of May 1987 — 17th of April 2020, meaning it includes a week of oil price movements after the recent OPEC deal.

Training

After preprocessing the data, we found out that training the data from 2000 onwards demonstrates a higher level of accuracy.

Training Dataset — Oil Prices [2000–2020]

Statistical Fine-Tuning of Model

Differencing in ARIMA

The entire point of differencing is to make the time series stationary, and the difference is measured between today and yesterday until we reach a point where the statistical properties are constant over time.

Testing If We Have a Stationary Time Series

We run an Augmented Dickey-Fuller, and if the p-value > 0.005, which was true in our case at 0.297299, we go ahead with differencing.

Visual Representation of Non-Stationary Time Series — Oil Prices [2019/05–2020/05]

After running three tests, we are reassured that the decision of selecting an order of differencing value of 1 is the most appropriate.

Thereafter, we identify if the model requires AR terms by inspecting the Partial Autocorrelation (PACF) plot, which shows the correlation between the series and its lag.

PACF: 1st and 2nd Diffferencing Autocorrelation

Then, we find the order of the Moving Average term q, which is the error of the lagged forecast, by looking at the ACF to see how many MA terms are required to remove any autocorrelation in the stationiarized series.

Autocorrelation 1st and 2nd differencing

After testing, we decide to set q to 1, making the three parameters (p,d,q) as (1,0,1).

Model’s Forecasts:

20th April 2020–1st October 2020 Forecast

Timeframe: Expected Peak of COVID-19 [± Next 5.5 months]

Forecasted Average Price of Brent Oil: $27.79*

However, the US election is a month away from the end of our forecast, we have therefore decided to provide two more scenarios based on the volatility an election can bring to the economy and oil prices.

The Wildcard: Texas

Photo by Matthew T Rader on Unsplash

Texas produces more oil than every OPEC nation apart from Saudi Arabia, but right now it is getting crushed by cheap oil.

This means two things:

Trump needs high oil prices to gain Texas’s vote to gain re-election There is the possibility of Texas limiting its output for the first time in more than 40 years to further raise oil prices

In essence, both of these reasons give us enough reasons to speculate that there will be more spikes in oil prices in the foreseeable future, which means our forecast may see more ups and downs than we expected, however, with the way things are going now, it may lean towards the optimistic estimate, even if only temporary.

Conservative Estimate — Average Price of Brent Oil: $23.62 Optimistic Estimate — Average Price of Brent Oil: $34.74*

2020–2024 Forecast

Regardless of the spikes, our model does a good job of predicting the general movement, giving us a decent indicator of the expected time-frame of the recovery of oil prices.

ARIMA Forecast — Oil Prices [2020–2024]

ARIMA Forecast — Oil Prices Per Period [1: Q1/Q2 vs. 2: Q3/Q4]

Breakdown of Results

Oil prices used to have somewhat predictable seasonal swings, with a spike in the spring, and then a drop in the fall and winter.

However, there are 4 main factors that may cause future oil prices to further deviate from our forecasts:

1) Slowing Global Demand

The global oil demand is around 17m lower in April 2020 than the 2019 annual average, which is the largest drop in history.

2) Rising U.S. Oil Production

In 2018, the US became the world’s largest oil producer, and a year after, it exported more oil than it imported for the first time since 1948.

3) Diminished OPEC Clout

OPEC has not cut output enough to put a defined floor under prices.

4) Rising Dollar Value

Foreign Exchange traders have been inflating the value of the dollar since 2014, and oil transactions are paid in U.S. dollars, which means a 25% rise in the dollar offsets a 25% drop in oil prices, and global economic uncertainty keeps the U.S. dollar strong, which is where we are headed.

Recovery Time

In conclusion, we believe that it will take around 4 years, give or take a year based on externalities, for oil prices to recover to what they were before the 2020 crash.

LinkedIn

GitHub Code"
255;news.mit.edu;http://news.mit.edu/2020/mit-press-candlewick-press-collaborate-new-book-imprints-children-teens-0417;;The MIT Press and Candlewick Press to collaborate on new imprints for children, teens;"The MIT Press and Candlewick Press have announced an innovative publishing project that will pair the expertise, reach, and creativity of both organizations in a wholly new endeavor — the first joint project of its kind by a university press and a children’s publisher.

Two new imprints, MIT Kids Press and MITeen Press, to be led creatively and brought to market by Candlewick, will publish engaging and ambitious books for children and young adult readers under new MIT branding. Covering topics ranging from planetary science to the internet and the environment, the list will be reviewed and approved by an MIT-based advisory board comprising members of the MIT Press and eminent faculty, who will also propose acquisitions, identify writers, and help check all titles for scientific validity and factual accuracy. All manufacturing and commercial details will be handled by Candlewick, with some marketing and publicity support from the MIT Press. The books will be published in the UK and Australia by Candlewick’s sister companies, Walker Books UK and Walker Books Australia, and will be sold by the Walker/Candlewick International Sales team in all languages, formats, and territories throughout the world.

The imprints’ first lists, currently slated for autumn 2021, will include books for readers across a span of ages. These will include:

• “Ada and the Galaxies,” a picture book by Alan Lightman, a professor of the practice of the humanities at MIT who is perhaps best known for his internationally best-selling novel “Einstein’s Dreams,” and coauthor Olga Pastuchiv, illustrated by Susanna Chapman;

• “Hanmoji,” a guide to learning Chinese through fun emoji mashups, by Jennifer 8. Lee, Jason Li, and An Xiao Mina; and

• “MIT App Inventor,” a practical guide to app coding for middle-grade readers, by Hal Abelson, Class of 1922 Professor of Computer Science and Engineering in the Department of Electrical Engineering and Computer Science at MIT and a founding director of Creative Commons and of the Free Software Foundation.

“We at the MIT Press have long made it our mission to reach a broad audience of thinkers and doers with the most innovative work in a range of fields,” says Amy Brand, director of the MIT Press. “Consistent with our mission, we have also had some success in recent years reaching younger audiences and we’re truly thrilled to be partnering with Candlewick to take this opportunity to the next level by launching dedicated imprints for young kids and teens under the MIT name.”

Karen Lotz, president and publisher of Candlewick Press and group managing director of the Walker Books Group, noted that the unprecedented collaboration marked a new frontier in publishing and offered a unique opportunity to open young readers’ minds.

“This is a dream come true,” Lotz said. “What started out as a conversation among friends grew and grew. When we started brainstorming in earnest with Amy Brand and Bill Smith of the MIT Press and their great team, as well as extraordinary individuals from the broader MIT community who are committed to innovative K-12 learning, such as Angela Belcher, MIT’s James Mason Crafts Professor of Biological Engineering and Materials Science and Engineering, we realized there was so much in common between us.”

“First and foremost, we shared an ambition to fill the truly international need for new kinds of nonfiction and fiction on important STEAM [science, technology, engineering, arts, and mathematics] topics. We also have a passion for finding new and better ways to connect readers with our books. Through these conversations, it became apparent to us at Candlewick how lucky we would be to secure a broad partnership together with the MIT Press. It has taken a little time to work through the details, as uncharted journeys often do, but we are thrilled with the model and how our publishing plans have coalesced. We have been so gratified to collaborate with MIT Press on creating the new lists, with titles we hope will expand the imaginations and aspirations of the next generations of budding thinkers, designers, scientists, leaders, and inventors,” Lotz says.

“The collaboration of MIT and Candlewick is a no-brainer. In this joint venture, which I am proud to be a part of, we bring together a leading institution of science and a leading children’s book publisher to help inspire awe and understanding of the natural world in our young people,” says Alan Lightman, coauthor of “Ada and the Galaxies.”

“In these uncertain times, the internet and language are more vital than ever to build bridges across cultures and borders. English and Chinese may be lingua francas for billions of people, but emoji is the universal language of this moment,” “Hanmoji” coauthor Jason Li wrote. “To that end, we’re thrilled to be partnering with MITeen Press to bring together the worlds of emoji enthusiasts, Chinese language learners, and anyone interested in the fascinating evolution of the written word.”"
256;machinelearningmastery.com;https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/;2017-10-19;How to Develop a Deep Learning Bag-of-Words Model for Sentiment Analysis (Text Classification);"from numpy import array

from string import punctuation

from os import listdir

from collections import Counter

from nltk . corpus import stopwords

from keras . preprocessing . text import Tokenizer

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Dropout

from pandas import DataFrame

from matplotlib import pyplot

# load doc into memory

def load_doc ( filename ) :

# open the file as read only

file = open ( filename , 'r' )

# read all text

text = file . read ( )

# close the file

file . close ( )

return text

# turn a doc into clean tokens

def clean_doc ( doc ) :

# split into tokens by white space

tokens = doc . split ( )

# remove punctuation from each token

table = str . maketrans ( '' , '' , punctuation )

tokens = [ w . translate ( table ) for w in tokens ]

# remove remaining tokens that are not alphabetic

tokens = [ word for word in tokens if word . isalpha ( ) ]

# filter out stop words

stop_words = set ( stopwords . words ( 'english' ) )

tokens = [ w for w in tokens if not w in stop_words ]

# filter out short tokens

tokens = [ word for word in tokens if len ( word ) > 1 ]

return tokens

# load doc, clean and return line of tokens

def doc_to_line ( filename , vocab ) :

# load the doc

doc = load_doc ( filename )

# clean doc

tokens = clean_doc ( doc )

# filter by vocab

tokens = [ w for w in tokens if w in vocab ]

return ' ' . join ( tokens )

# load all docs in a directory

def process_docs ( directory , vocab , is_trian ) :

lines = list ( )

# walk through all files in the folder

for filename in listdir ( directory ) :

# skip any reviews in the test set

if is_trian and filename . startswith ( 'cv9' ) :

continue

if not is_trian and not filename . startswith ( 'cv9' ) :

continue

# create the full path of the file to open

path = directory + '/' + filename

# load and clean the doc

line = doc_to_line ( path , vocab )

# add to list

lines . append ( line )

return lines

# evaluate a neural network model

def evaluate_mode ( Xtrain , ytrain , Xtest , ytest ) :

scores = list ( )

n_repeats = 30

n_words = Xtest . shape [ 1 ]

for i in range ( n_repeats ) :

# define network

model = Sequential ( )

model . add ( Dense ( 50 , input_shape = ( n_words , ) , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile network

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( Xtrain , ytrain , epochs = 50 , verbose = 2 )

# evaluate

loss , acc = model . evaluate ( Xtest , ytest , verbose = 0 )

scores . append ( acc )

print ( '%d accuracy: %s' % ( ( i + 1 ) , acc ) )

return scores

# prepare bag of words encoding of docs

def prepare_data ( train_docs , test_docs , mode ) :

# create the tokenizer

tokenizer = Tokenizer ( )

# fit the tokenizer on the documents

tokenizer . fit_on_texts ( train_docs )

# encode training data set

Xtrain = tokenizer . texts_to_matrix ( train_docs , mode = mode )

# encode training data set

Xtest = tokenizer . texts_to_matrix ( test_docs , mode = mode )

return Xtrain , Xtest

# load the vocabulary

vocab_filename = 'vocab.txt'

vocab = load_doc ( vocab_filename )

vocab = vocab . split ( )

vocab = set ( vocab )

# load all training reviews

positive_lines = process_docs ( 'txt_sentoken/pos' , vocab , True )

negative_lines = process_docs ( 'txt_sentoken/neg' , vocab , True )

train_docs = negative_lines + positive_lines

# load all test reviews

positive_lines = process_docs ( 'txt_sentoken/pos' , vocab , False )

negative_lines = process_docs ( 'txt_sentoken/neg' , vocab , False )

test_docs = negative_lines + positive_lines

# prepare labels

ytrain = array ( [ 0 for _ in range ( 900 ) ] + [ 1 for _ in range ( 900 ) ] )

ytest = array ( [ 0 for _ in range ( 100 ) ] + [ 1 for _ in range ( 100 ) ] )

modes = [ 'binary' , 'count' , 'tfidf' , 'freq' ]

results = DataFrame ( )

for mode in modes :

# prepare data for mode

Xtrain , Xtest = prepare_data ( train_docs , test_docs , mode )

# evaluate model on data for mode

results [ mode ] = evaluate_mode ( Xtrain , ytrain , Xtest , ytest )

# summarize results

print ( results . describe ( ) )

# plot results

results . boxplot ( )"
257;news.mit.edu;http://news.mit.edu/2019/computing-and-ai-humanistic-perspectives-0924;;Computing and artificial intelligence: Humanistic perspectives from MIT;"The MIT Stephen A. Schwarzman College of Computing (SCC) will reorient the Institute to bring the power of computing and artificial intelligence to all fields at MIT, and to allow the future of computing and AI to be shaped by all MIT disciplines.

To support ongoing planning for the new college, Dean Melissa Nobles invited faculty from all 14 of MIT’s humanistic disciplines in the School of Humanities, Arts, and Social Sciences to respond to two questions:

1) What domain knowledge, perspectives, and methods from your field should be integrated into the new MIT Schwarzman College of Computing, and why?

2) What are some of the meaningful opportunities that advanced computing makes possible in your field?

As Nobles says in her foreword to the series, “Together, the following responses to these two questions offer something of a guidebook to the myriad, productive ways that technical, humanistic, and scientific fields can join forces at MIT, and elsewhere, to further human and planetary well-being.”

The following excerpts highlight faculty responses, with links to full commentaries. The excerpts are sequenced by fields in the following order: the humanities, arts, and social sciences.

Foreword by Melissa Nobles, professor of political science and the Kenan Sahin Dean of the MIT School of Humanities, Arts, and Social Sciences

“The advent of artificial intelligence presents our species with an historic opportunity — disguised as an existential challenge: Can we stay human in the age of AI? In fact, can we grow in humanity, can we shape a more humane, more just, and sustainable world? With a sense of promise and urgency, we are embarked at MIT on an accelerated effort to more fully integrate the technical and humanistic forms of discovery in our curriculum and research, and in our habits of mind and action.” Read more >>

Comparative Media Studies: William Uricchio, professor of comparative media studies

“Given our research and practice focus, the CMS perspective can be key for understanding the implications of computation for knowledge and representation, as well as computation’s relationship to the critical process of how knowledge works in culture — the way it is formed, shared, and validated.”

Recommended action: “Bring media and computer scholars together to explore issues that require both areas of expertise: text-generating algorithms (that force us to ask what it means to be human); the nature of computational gatekeepers (that compels us to reflect on implicit cultural priorities); and personalized filters and texts (that require us to consider the shape of our own biases).” Read more >>

Global Languages: Emma J. Teng, the T.T. and Wei Fong Chao Professor of Asian Civilizations

“Language and culture learning are gateways to international experiences and an important means to develop cross-cultural understanding and sensitivity. Such understanding is essential to addressing the social and ethical implications of the expanding array of technology affecting everyday life across the globe.”

Recommended action: “We aim to create a 21st-century language center to provide a convening space for cross-cultural communication, collaboration, action research, and global classrooms. We also plan to keep the intimate size and human experience of MIT’s language classes, which only increase in value as technology saturates the world.” Read more >>

History: Jeffrey Ravel, professor of history and head of MIT History

“Emerging innovations in computational methods will continue to improve our access to the past and the tools through which we interpret evidence. But the field of history will continue to be served by older methods of scholarship as well; critical thinking by human beings is fundamental to our endeavors in the humanities.”

Recommended action: “Call on the nuanced debates in which historians engage about causality to provide a useful frame of reference for considering the issues that will inevitably emerge from new computing technologies. This methodology of the history field is a powerful way to help imagine our way out of today’s existential threats.” Read more >>

Linguistics: Faculty of MIT Linguistics

“Perhaps the most obvious opportunities for computational and linguistics research concern the interrelation between specific hypotheses about the formal properties of language and their computational implementation in the form of systems that learn, parse, and produce human language.”

Recommended action: “Critically, transformative new tools have come from researchers at institutions where linguists work side-by-side with computational researchers who are able to translate back and forth between computational properties of linguistic grammars and of other systems.” Read more >>

Literature: Shankar Raman, with Mary C. Fuller, professors of literature

“In the age of AI, we could invent new tools for reading. Making the expert reading skills we teach MIT students even partially available to readers outside the academy would widen access to our materials in profound ways.”

Recommended action: At least three priorities of current literary engagement with the digital should be integrated into the SCC’s research and curriculum: democratization of knowledge; new modes of and possibilities for knowledge production; and critical analysis of the social conditions governing what can be known and who can know it.” Read more >>

Philosophy: Alex Byrne, professor of philosophy and head of MIT Philosophy; and Tamar Schapiro, associate professor of philosophy

“Computing and AI pose many ethical problems related to: privacy (e.g., data systems design), discrimination (e.g., bias in machine learning), policing (e.g., surveillance), democracy (e.g., the Facebook-Cambridge Analytica data scandal), remote warfare, intellectual property, political regulation, and corporate responsibility.”

Recommended action: “The SCC presents an opportunity for MIT to be an intellectual leader in the ethics of technology. The ethics lab we propose could turn this opportunity into reality.” Read more >>

Science, Technology, and Society: Eden Medina and Dwaipayan Banerjee, associate professors of science, technology, and society

“A more global view of computing would demonstrate a broader range of possibilities than one centered on the American experience, while also illuminating how computer systems can reflect and respond to different needs and systems. Such experiences can prove generative for thinking about the future of computing writ large.”

Recommended action: “Adopt a global approach to the research and teaching in the SCC, an approach that views the U.S. experience as one among many.” Read more >>

Women's and Gender Studies: Ruth Perry, the Ann Friedlaender Professor of Literature; with Sally Haslanger, the Ford Professor of Philosophy, and Elizabeth Wood, professor of history

“The SCC presents MIT with a unique opportunity to take a leadership role in addressing some of most pressing challenges that have emerged from the role computing technologies play in our society — including how these technologies are reinforcing social inequalities.”

Recommended action: “Ensure that women’s voices are heard and that coursework and research is designed with a keen awareness of the difference that gender makes. This is the single-most powerful way that MIT can address the inequities in the computing fields.” Read more >>

Writing: Tom Levenson, professor of science writing

“Computation and its applications in fields that directly affect society cannot be an unexamined good. Professional science and technology writers are a crucial resource for the mission of new college of computing, and they need to be embedded within its research apparatus.”

Recommended action: “Intertwine writing and the ideas in coursework to provide conceptual depth that purely technical mastery cannot offer.” Read more >>

Music: Eran Egozy, professor of the practice in music technology

“Creating tomorrow’s music systems responsibly will require a truly multidisciplinary education, one that covers everything from scientific models and engineering challenges to artistic practice and societal implications. The new music technology will be accompanied by difficult questions. Who owns the output of generative music algorithms that are trained on human compositions? How do we ensure that music, an art form intrinsic to all humans, does not become controlled by only a few?”

Recommended action: Through the SCC, our responsibility will be not only to develop the new technologies of music creation, distribution, and interaction, but also to study their cultural implications and define the parameters of a harmonious outcome for all.” Read more >>

Theater Arts: Sara Brown, assistant professor of theater arts and MIT Theater Arts director of design

“As a subject, AI problematizes what is means to be human. There are an unending series of questions posed by the presence of an intelligent machine. The theater, as a synthetic art form that values and exploits liveness, is an ideal place to explore the complex and layered problems posed by AI and advanced computing.”

Recommended action: “There are myriad opportunities for advanced computing to be integrated into theater, both as a tool and as a subject of exploration. As a tool, advanced computing can be used to develop performance systems that respond directly to a live performer in real time, or to integrate virtual reality as a previsualization tool for designers.” Read more >>

Anthropology: Heather Paxson, the William R. Kenan, Jr. Professor of Anthropology

“The methods used in anthropology — a field that systematically studies human cultural beliefs and practices — are uniquely suited to studying the effects of automation and digital technologies in social life. For anthropologists, ‘Can artificial intelligence be ethical?’ is an empirical, not a hypothetical, question. Ethical for what? To whom? Under what circumstances?”

Recommended action: “Incorporate anthropological thinking into the new college to prepare students to live and work effectively and responsibly in a world of technological, demographic, and cultural exchanges. We envision an ethnography lab that will provide digital and computing tools tailored to anthropological research and projects.” Read more >>

Economics: Nancy L. Rose, the Charles P. Kindleberger Professor of Applied Economics and head of the Department of Economics; and David Autor, the Ford Professor of Economics and co-director of the MIT Task Force on the Work of the Future

“The intellectual affinity between economics and computer science traces back almost a century, to the founding of game theory in 1928. Today, the practical synergies between economics and computer science are flourishing. We outline some of the many opportunities for the two disciplines to engage more deeply through the new SCC.”

Recommended action: “Research that engages the tools and expertise of economics on matters of fairness, expertise, and cognitive biases in machine-supported and machine-delegated decision-making; and on market design, industrial organization, and the future of work. Scholarship at the intersection of data science, econometrics, and causal inference. Cultivate depth in network science, algorithmic game theory and mechanism design, and online learning. Develop tools for rapid, cost-effective, and ongoing education and retraining for workers.” Read more >>

Political Science: Faculty of the Department of Political Science

“The advance of computation gives rise to a number of conceptual and normative questions that are political, rather than ethical in character. Political science and theory have a significant role in addressing such questions as: How do major players in the technology sector seek to legitimate their authority to make decisions that affect us all? And where should that authority actually reside in a democratic polity?”

Recommended action: “Incorporate the research and perspectives of political science in SCC research and education to help ensure that computational research is socially aware, especially with issues involving governing institutions, the relations between nations, and human rights.” Read more >>

Series prepared by SHASS Communications

Series Editor and Designer: Emily Hiestand

Series Co-Editor: Kathryn O’Neill"
258;machinelearningmastery.com;https://machinelearningmastery.com/taxonomy-of-time-series-forecasting-problems/;2018-08-07;Taxonomy of Time Series Forecasting Problems;"Tweet Share Share

Last Updated on August 5, 2019

When you are presented with a new time series forecasting problem, there are many things to consider.

The choice that you make directly impacts each step of the project from the design of a test harness to evaluate forecast models to the fundamental difficulty of the forecast problem that you are working on.

It is possible to very quickly narrow down the options by working through a series of questions about your time series forecasting problem. By considering a few themes and questions within each theme, you narrow down the type of problem, test harness, and even choice of algorithms for your project.

In this post, you will discover a framework that you can use to quickly understand and frame your time series forecasting problem.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Framework Overview

Time series forecasting involves developing and using a predictive model on data where there is an ordered relationship between observations.

You can learn more about what time series forecasting is in this post:

Before you get started on your project, you can answer a few questions and greatly improve your understanding of the structure of your forecast problem, the structure of the model requires, and how to evaluate it.

The framework presented in this post is divided into seven parts; they are:

Inputs vs. Outputs Endogenous vs. Exogenous Unstructured vs. Structured Regression vs. Classification Univariate vs. Multivariate Single-step vs. Multi-step Static vs. Dynamic Contiguous vs. Discontiguous

I recommend working through this framework before starting any time series forecasting project.

Your answers may not be crisp on the first time through and the questions may require to you study the data, the domain, and talk to experts and stakeholders.

Update your answers as you learn more as it will help to keep you on track, avoid distractions, and develop the actual model that you need for your project.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Inputs vs. Outputs

Generally, a prediction problem involves using past observations to predict or forecast one or more possible future observations.

The goal is to guess about what might happen in the future.

When you are required to make a forecast, it is critical to think about the data that you will have available to make the forecast and what you will be guessing about the future.

We can summarize this as what are the inputs and outputs of the model when making a single forecast.

Inputs : Historical data provided to the model in order to make a single forecast.

: Historical data provided to the model in order to make a single forecast. Outputs: Prediction or forecast for a future time step beyond the data provided as input.

The input data is not the data used to train the model. We are not at that point yet. It is the data used to make one forecast, for example the last seven days of sales data to forecast the next one day of sales data.

Defining the inputs and outputs of the model forces you to think about what exactly is or may be required to make a forecast.

You may not be able to be specific when it comes to input data. For example, you may not know whether one or multiple prior time steps are required to make a forecast. But you will be able to identify the variables that could be used to make a forecast.

What are the inputs and outputs for a forecast?

2. Endogenous vs. Exogenous

The input data can be further subdivided in order to better understand its relationship to the output variable.

An input variable is endogenous if it is affected by other variables in the system and the output variable depends on it.

In a time series, the observations for an input variable depend upon one another. For example, the observation at time t is dependent upon the observation at t-1; t-1 may depend on t-2, and so on.

An input variable is an exogenous variable if it is independent of other variables in the system and the output variable depends upon it.

Put simply, endogenous variables are influenced by other variables in the system (including themselves) whereas as exogenous variables are not and are considered as outside the system.

Endogenous : Input variables that are influenced by other variables in the system and on which the output variable depends.

: Input variables that are influenced by other variables in the system and on which the output variable depends. Exogenous: Input variables that are not influenced by other variables in the system and on which the output variable depends.

Typically, a time series forecasting problem has endogenous variables (e.g. the output is a function of some number of prior time steps) and may or may not have exogenous variables.

Often, exogenous variables are ignored given the strong focus on the time series. Explicitly thinking about both variable types may help to identify easily overlooked exogenous data or even engineered features that may improve the model.

What are the endogenous and exogenous variables?

3. Regression vs. Classification

Regression predictive modeling problems are those where a quantity is predicted.

A quantity is a numerical value; for example a price, a count, a volume, and so on. A time series forecasting problem in which you want to predict one or more future numerical values is a regression type predictive modeling problem.

Classification predictive modeling problems are those where a category is predicted.

A category is a label from a small well-defined set of labels; for example {“hot”, “cold”}, {“up”, “down”}, and {“buy”, “sell”} are categories.

A time series forecasting problem in which you want to classify input time series data is a classification type predictive modeling problem.

Regression : Forecast a numerical quantity.

: Forecast a numerical quantity. Classification: Classify as one of two or more labels.

Are you working on a regression or classification predictive modeling problem?

There is some flexibility between these types.

For example, a regression problem can be reframed as classification and a classification problem can be reframed as regression. Some problems, like predicting an ordinal value, can be framed as either classification and regression. It is possible that a reframing of your time series forecasting problem may simplify it.

What are some alternate ways to frame your time series forecasting problem?

4. Unstructured vs. Structured

It is useful to plot each variable in a time series and inspect the plot looking for possible patterns.

A time series for a single variable may not have any obvious pattern.

We can think of a series with no pattern as unstructured, as in there is no discernible time-dependent structure.

Alternately, a time series may have obvious patterns, such as a trend or seasonal cycles as structured.

We can often simplify the modeling process by identifying and removing the obvious structures from the data, such as an increasing trend or repeating cycle. Some classical methods even allow you to specify parameters to handle these systematic structures directly.

Unstructured : No obvious systematic time-dependent pattern in a time series variable.

: No obvious systematic time-dependent pattern in a time series variable. Structured: Systematic time-dependent patterns in a time series variable (e.g. trend and/or seasonality).

Are the time series variables unstructured or structured?

5. Univariate vs. Multivariate

A single variable measured over time is referred to as a univariate time series. Univariate means one variate or one variable.

Multiple variables measured over time is referred to as a multivariate time series: multiple variates or multiple variables.

Univariate : One variable measured over time.

: One variable measured over time. Multivariate: Multiple variables measured over time.

Are you working on a univariate or multivariate time series problem?

Considering this question with regard to inputs and outputs may add a further distinction. The number of variables may differ between the inputs and outputs, e.g. the data may not be symmetrical.

For example, you may have multiple variables as input to the model and only be interested in predicting one of the variables as output. In this case, there is an assumption in the model that the multiple input variables aid and are required in predicting the single output variable.

Univariate and Multivariate Inputs : One or multiple input variables measured over time.

: One or multiple input variables measured over time. Univariate and Multivariate Outputs: One or multiple output variables to be predicted.

6. Single-step vs. Multi-step

A forecast problem that requires a prediction of the next time step is called a one-step forecast model.

Whereas a forecast problem that requires a prediction of more than one time step is called a multi-step forecast model.

The more time steps to be projected into the future, the more challenging the problem given the compounding nature of the uncertainty on each forecasted time step.

One-Step : Forecast the next time step.

: Forecast the next time step. Multi-Step: Forecast more than one future time steps.

Do you require a single-step or a multi-step forecast?

7. Static vs. Dynamic

It is possible to develop a model once and use it repeatedly to make predictions.

Given that the model is not updated or changed between forecasts, we can think of this model as being static.

Conversely, we may receive new observations prior to making a subsequent forecast that could be used to create a new model or update the existing model. We can think of developing a new or updated model prior to each forecasts as a dynamic problem.

For example, it if the problem requires a forecast at the beginning of the week for the week ahead, we may receive the true observation at the end of the week that we can use to update the model prior to making next weeks forecast. This would be a dynamic model. If we do not get a true observation at the end of the week or we do and choose to not re-fit the model, this would be a static model.

We may prefer a dynamic model, but the constraints of the domain or limitations of a chosen algorithm may impose constraints that make this intractable.

Static . A forecast model is fit once and used to make predictions.

. A forecast model is fit once and used to make predictions. Dynamic. A forecast model is fit on newly available data prior to each prediction.

Do you require a static or a dynamically updated model?

8. Contiguous vs. Discontiguous

A time series where the observations are uniform over time may be described as contiguous.

Many time series problems have contiguous observations, such as one observation each hour, day, month or year.

A time series where the observations are not uniform over time may be described as discontiguous.

The lack of uniformity of the observations may be caused by missing or corrupt values. It may also be a feature of the problem where observations are only made available sporadically or at increasingly or decreasingly spaced time intervals.

In the case of non-uniform observations, specific data formatting may be required when fitting some models to make the observations uniform over time.

Contiguous . Observations are made uniform over time.

. Observations are made uniform over time. Discontiguous. Observations are not uniform over time.

Are your observations contiguous or discontiguous?

Further Reading

This section lists some resources for further reading.

Summary

To review, the themes and questions you can ask about your problem are as follows:

Inputs vs. Outputs What are the inputs and outputs for a forecast? Endogenous vs. Exogenous What are the endogenous and exogenous variables? Unstructured vs. Structured Are the time series variables unstructured or structured? Regression vs. Classification Are you working on a regression or classification predictive modeling problem? What are some alternate ways to frame your time series forecasting problem? Univariate vs. Multivariate Are you working on a univariate or multivariate time series problem? Single-step vs. Multi-step Do you require a single-step or a multi-step forecast? Static vs. Dynamic Do you require a static or a dynamically updated model? Contiguous vs. Discontiguous Are your observations contiguous or discontiguous?

Did you find this framework useful for your time series forecasting problem?

Let me know in the comments below.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
259;machinelearningmastery.com;https://machinelearningmastery.com/best-practices-document-classification-deep-learning/;2017-10-22;Best Practices for Text Classification with Deep Learning;"Tweet Share Share

Last Updated on August 7, 2019

Text classification describes a general class of problems such as predicting the sentiment of tweets and movie reviews, as well as classifying email as spam or not.

Deep learning methods are proving very good at text classification, achieving state-of-the-art results on a suite of standard academic benchmark problems.

In this post, you will discover some best practices to consider when developing deep learning models for text classification.

After reading this post, you will know:

The general combination of deep learning methods to consider when starting your text classification problems.

The first architecture to try with specific advice on how to configure hyperparameters.

That deeper networks may be the future of the field in terms of flexibility and capability.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into 5 parts; they are:

Word Embeddings + CNN = Text Classification Use a Single Layer CNN Architecture Dial in CNN Hyperparameters Consider Character-Level CNNs Consider Deeper CNNs for Classification

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Word Embeddings + CNN = Text Classification

The modus operandi for text classification involves the use of a word embedding for representing words and a Convolutional Neural Network (CNN) for learning how to discriminate documents on classification problems.

Yoav Goldberg, in his primer on deep learning for natural language processing, comments that neural networks in general offer better performance than classical linear classifiers, especially when used with pre-trained word embeddings.

The non-linearity of the network, as well as the ability to easily integrate pre-trained word embeddings, often lead to superior classification accuracy.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

He also comments that convolutional neural networks are effective at document classification, namely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in a way that is invariant to their position within the input sequences.

Networks with convolutional and pooling layers are useful for classification tasks in which we expect to find strong local clues regarding class membership, but these clues can appear in different places in the input. […] We would like to learn that certain sequences of words are good indicators of the topic, and do not necessarily care where they appear in the document. Convolutional and pooling layers allow the model to learn to find such local indicators, regardless of their position.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

The architecture is therefore comprised of three key pieces:

Word Embedding: A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation. Convolutional Model: A feature extraction model that learns to extract salient features from documents represented using a word embedding. Fully Connected Model: The interpretation of extracted features in terms of a predictive output.

Yoav Goldberg highlights the CNNs role as a feature extractor model in his book:

… the CNN is in essence a feature-extracting architecture. It does not constitute a standalone, useful network on its own, but rather is meant to be integrated into a larger network, and to be trained to work in tandem with it in order to produce an end result. The CNNs layer’s responsibility is to extract meaningful sub-structures that are useful for the overall prediction task at hand.

— Page 152, Neural Network Methods for Natural Language Processing, 2017.

The tying together of these three elements is demonstrated in perhaps one of the most widely cited examples of the combination, described in the next section.

2. Use a Single Layer CNN Architecture

You can get good results for document classification with a single layer CNN, perhaps with differently sized kernels across the filters to allow grouping of word representations at different scales.

Yoon Kim in his study of the use of pre-trained word vectors for classification tasks with Convolutional Neural Networks found that using pre-trained static word vectors does very well. He suggests that pre-trained word embeddings that were trained on very large text corpora, such as the freely available word2vec vectors trained on 100 billion tokens from Google news may offer good universal features for use in natural language processing.

Despite little tuning of hyperparameters, a simple CNN with one layer of convolution performs remarkably well. Our results add to the well-established evidence that unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP

— Convolutional Neural Networks for Sentence Classification, 2014.

He also discovered that further task-specific tuning of the word vectors offer a small additional improvement in performance.

Kim describes the general approach of using CNN for natural language processing. Sentences are mapped to embedding vectors and are available as a matrix input to the model. Convolutions are performed across the input word-wise using differently sized kernels, such as 2 or 3 words at a time. The resulting feature maps are then processed using a max pooling layer to condense or summarize the extracted features.

The architecture is based on the approach used by Ronan Collobert, et al. in their paper “Natural Language Processing (almost) from Scratch“, 2011. In it, they develop a single end-to-end neural network model with convolutional and pooling layers for use across a range of fundamental natural language processing problems.

Kim provides a diagram that helps to see the sampling of the filters using differently sized kernels as different colors (red and yellow).

Usefully, he reports his chosen model configuration, discovered via grid search and used across a suite of 7 text classification tasks, summarized as follows:

Transfer function: rectified linear.

Kernel sizes: 2, 4, 5.

Number of filters: 100

Dropout rate: 0.5

Weight regularization (L2): 3

Batch Size: 50

Update Rule: Adadelta

These configurations could be used to inspire a starting point for your own experiments.

3. Dial in CNN Hyperparameters

Some hyperparameters matter more than others when tuning a convolutional neural network on your document classification problem.

Ye Zhang and Byron Wallace performed a sensitivity analysis into the hyperparameters needed to configure a single layer convolutional neural network for document classification. The study is motivated by their claim that the models are sensitive to their configuration.

Unfortunately, a downside to CNN-based models – even simple ones – is that they require practitioners to specify the exact model architecture to be used and to set the accompanying hyperparameters. To the uninitiated, making such decisions can seem like something of a black art because there are many free parameters in the model.

— A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification, 2015.

Their aim was to provide general configurations that can be used for configuring CNNs on new text classification tasks.

They provide a nice depiction of the model architecture and the decision points for configuring the model, reproduced below.

The study makes a number of useful findings that could be used as a starting point for configuring shallow CNN models for text classification.

The general findings were as follows:

The choice of pre-trained word2vec and GloVe embeddings differ from problem to problem, and both performed better than using one-hot encoded word vectors.

The size of the kernel is important and should be tuned for each problem.

The number of feature maps is also important and should be tuned.

The 1-max pooling generally outperformed other types of pooling.

Dropout has little effect on the model performance.

They go on to provide more specific heuristics, as follows:

Use word2vec or GloVe word embeddings as a starting point and tune them while fitting the model.

Grid search across different kernel sizes to find the optimal configuration for your problem, in the range 1-10.

Search the number of filters from 100-600 and explore a dropout of 0.0-0.5 as part of the same search.

Explore using tanh, relu, and linear activation functions.

The key caveat is that the findings are based on empirical results on binary text classification problems using single sentences as input.

I recommend reading the full paper to get more details:

4. Consider Character-Level CNNs

Text documents can be modeled at the character level using convolutional neural networks that are capable of learning the relevant hierarchical structure of words, sentences, paragraphs, and more.

Xiang Zhang, et al. use a character-based representation of text as input for a convolutional neural network. The promise of the approach is that all of the labor-intensive effort required to clean and prepare text could be overcome if a CNN can learn to abstract the salient details.

… deep ConvNets do not require the knowledge of words, in addition to the conclusion from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language. This simplification of engineering could be crucial for a single system that can work for different languages, since characters always constitute a necessary construct regardless of whether segmentation into words is possible. Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons may be naturally learnt.

— Character-level Convolutional Networks for Text Classification, 2015.

The model reads in one-hot encoded characters in a fixed-sized alphabet. Encoded characters are read in blocks or sequences of 1,024 characters. A stack of 6 convolutional layers with pooling follows, with 3 fully connected layers at the output end of the network in order to make a prediction.

The model achieves some success, performing better on problems that offer a larger corpus of text.

… analysis shows that character-level ConvNet is an effective method. […] how well our model performs in comparisons depends on many factors, such as dataset size, whether the texts are curated and choice of alphabet.

— Character-level Convolutional Networks for Text Classification, 2015.

Results using an extended version of this approach were pushed to the state-of-the-art in a follow-up paper covered in the next section.

5. Consider Deeper CNNs for Classification

Better performance can be achieved with very deep convolutional neural networks, although standard and reusable architectures have not been adopted for classification tasks, yet.

Alexis Conneau, et al. comment on the relatively shallow networks used for natural language processing and the success of much deeper networks used for computer vision applications. For example, Kim (above) restricted the model to a single convolutional layer.

Other architectures used for natural language reviewed in the paper are limited to 5 and 6 layers. These are contrasted with successful architectures used in computer vision with 19 or even up to 152 layers.

They suggest and demonstrate that there are benefits for hierarchical feature learning with very deep convolutional neural network model, called VDCNN.

… we propose to use deep architectures of many convolutional layers to approach this goal, using up to 29 layers. The design of our architecture is inspired by recent progress in computer vision […] The proposed deep convolutional network shows significantly better results than previous ConvNets approach.

Key to their approach is an embedding of individual characters, rather than a word embedding.

We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations.

— Very Deep Convolutional Networks for Text Classification, 2016.

Results on a suite of 8 large text classification tasks show better performance than more shallow networks. Specifically, state-of-the-art results on all but two of the datasets tested, at the time of writing.

Generally, they make some key findings from exploring the deeper architectural approach:

The very deep architecture worked well on small and large datasets.

Deeper networks decrease classification error.

Max-pooling achieves better results than other, more sophisticated types of pooling.

Generally going deeper degrades accuracy; the shortcut connections used in the architecture are important.

… this is the first time that the “benefit of depths” was shown for convolutional neural networks in NLP.

— Very Deep Convolutional Networks for Text Classification, 2016.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Have you come across some good resources on deep learning for document classification?

Let me know in the comments below.

Summary

In this post, you discovered some best practices for developing deep learning models for document classification.

Specifically, you learned:

That a key approach is to use word embeddings and convolutional neural networks for text classification.

That a single layer model can do well on moderate-sized problems, and ideas on how to configure it.

That deeper models that operate directly on text may be the future of natural language processing.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside"
260;news.mit.edu;http://news.mit.edu/2020/3-questions-charles-stewart-pandemic-impact-2020-elections-0415;2020-03-19;3 Questions: Charles Stewart on the Covid-19 pandemic’s impact on the 2020 elections;"American voters are facing the unprecedented prospect of electing a new president amid a global pandemic. Already, the Covid-19 crisis has led some states to cancel in-person voting in favor of voting by mail, while other states have delayed primaries or held them with physical distancing guidelines that have forced voters to wait in long lines beyond the physical confines of their polling places. The Democratic National Convention, at which the Democratic Party will formally select its challenger and running mate to face incumbent President Donald Trump, has been delayed a month and is now scheduled for late August — though it may end up taking place online.



Charles Stewart III is the Kenan Sahin Distinguished Professor of Political Science at MIT and the founder of the MIT Election Data and Science Lab. SHASS Communications spoke with him recently about the broader impacts of the pandemic on the U.S. elections, in particular the decisions that need to be made, quickly, to increase the extent of voting-by-mail and other safe voting methods for the general election in November.

Q: Given the need for social distancing and the uncertain timeline of the pandemic, what are the greatest risks you see to the forthcoming presidential election? What steps can be taken now to ensure that a fair and representative election takes place in November?

A: The most obvious risk is that fear of infection, indeed, fear of death, will reduce turnout in the November election. The consequences of reduced turnout would be catastrophic for American democracy. Not only would it deny a voice to millions of Americans, it would throw the legitimacy of the outcome into question.

There are other risks to be navigated that are related to the turnout issue, but are also distinct conceptually. The first of these is logistical chaos. Because of the Covid-19 crisis, it is clear that more voting needs to be done by mail than has ever been done in the U.S. This is a view that I entirely support.

Yet this is easier said than done. In 2016, only 20 percent of voters cast their ballot by mail. The recent growth in vote-by-mail has been concentrated among a small number of western states, so that in the east, the percentage is much lower — around 10 percent. If we are to get the percentage of voters overall voting by mail to the 50-60 percent level, this will require states, like Massachusetts, that have previously only had 5 percent mail-vote rates to ramp it up to 50 percent.

That’s a big lift. As I point out in my recent LawFare blog post, voting by mail at a large scale requires serious attention to a number of processes and logistical challenges. The states that currently cast all their votes by mail — Colorado, Oregon, and Washington — have taken decades to get where they are. Can the rest of the states get even halfway to where these three other states are in the confines of six months? If they can’t — if the mail ballots end up going to the wrong places, or they end up being rejected in large numbers because of signature-matching problems — then the post-election period will put Florida 2000 to shame. That’s the worst-case scenario. I’m more optimistic than to believe this will be where we end up. But that’s the risk we face.

The second risk is lack of legitimacy because of how the election ends up being run. This points us in two directions. On the one hand, states could inadequately implement expanded vote-by-mail programs. This will lead to contentious lawsuits and voters believing that their votes didn’t count, and that the winner was chosen through an arbitrary process. On the other hand, the further states push voting by mail, the further residents who distrust mail voting — mainly because of the fraud opportunities — the more people will doubt the legitimacy of the outcome because they believe the election was stolen because so many mail ballots were just flowing around, willy-nilly.

For reasons I don’t entirely understand, voting by mail has been associated with the Democratic Party. Thus, if the Democrat wins in 2020, a lot of Republicans will believe that happened because Democrats were able to steal the election through the expansion of mail balloting.

So, what can be done? First, despite the risks, mail balloting should be expanded. If decisions are made now to take that road, almost all states can spend the next six months getting the logistical ducks in a row to make this happen successfully. And, speaking practically, the seven states that are the most closely divided politically — the battleground states — seem well-positioned to prepare for an onslaught of mail ballots. Yet, as mentioned in the LawFare blog post, expanding mail balloting will be a big lift for most states. As I said, you can’t just flip a switch.

Second, states should spend time planning how to make in-person voting safe. For a variety of reasons, I’m predicting that millions of voters will still vote in person. Obviously, with personal distancing and concerns about transmitting the virus, it’s going to be harder to vote in the confined spaces that are most polling places. Still, there are many reasons why people will prefer — or need — to vote in person. Thus, state and local election officials will need to collaborate with public health professionals to create protocol that will make in-person voting as safe as possible — at least as safe as grocery shopping.

Finally, we will need to be patient, especially when it comes to finding out who won the November election. With the increase in mail voting and the likely shortage of personnel to count the ballots, voting counting will be delayed. Election officials need to plan for this, and create ways for the delayed vote-count still to be transparent. Still, it’s going to be delayed, and we need to be prepared for that.



Q: The current pandemic is putting extraordinary stress on our political system, leading in some cases to an expansion of executive power. What developments concern you, and what can Americans do to safeguard our democracy — our democratic traditions, norms, institutions, voting rights, and electoral infrastructure — in this time of crisis?

A: Be attentive, and figure out what is important. I’ve been encountering this question in a very specific formulation: Will the November 2020 election be delayed? The answer is, “no.” There is no statutory or constitutional authority to do that. Even if the asteroids are raining on our heads and the zombies are roaming the streets on November 3, we will be voting. To make this happen will require public officials and the public to prioritize democracy.

So, the first thing we can do is all agree that voting on November 3 is a non-negotiable, and then to think backwards about how we ensure that. The second thing we can do is to support the efforts to make that happen. I am worried that people — including some governors — will wake up on November 3 and say, “it’s too unsafe to vote today.” To prevent that question from even arising, we all need to be committed to creating a voting environment that makes this scenario highly unlikely to happen.

Q: What political protections exist that will help the American system of democracy weather the Covid-19 outbreak? Are there political activities currently underway that give you hope?

A: The thing that gives me hope is that I’m totally unable to answer my email, read all the Slack channel messages I receive, and attend all the Zoom meetings I’m invited to. Election officials at the grassroots level are working at a high level to respond to the crisis and ensure that voting will be safe and secure in November.

Sometimes, their political superiors are not as supportive of necessary measures as the election officials themselves, so Americans should be communicating with their state elected officials — governors and state legislators — to advocate for the measures necessary to meet this crisis. This doesn’t mean that people need to advocate for permanent change to voting practices.

This is an emergency. Policies that in regular circumstances would be controversial shouldn’t be in this moment. The receptiveness of election officials to try new things in this environment is heartening. They need the political support that comes from the public rallying around them.

Interview prepared by MIT SHASS Communications

Editorial team: Emily Hiestand and Kathryn O'Neill

"
261;machinelearningmastery.com;https://machinelearningmastery.com/create-lists-of-machine-learning-algorithms/;2014-11-02;Take Control By Creating Targeted Lists of Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

Any book on machine learning will list and describe dozens of machine learning algorithms.

Once you start using tools and libraries you will discover dozens more. This can really wear you down, if you think you need to know about every possible algorithm out there.

A simple trick to tackle this feeling and take some control back is to make lists of machine learning algorithms.

This ridiculously simple tactic can give you a lot of power. You can use it to give you a list of methods to try when tackling a whole new class of problem. It can also give you a list of ideas when you get stuck on a dataset or your favorite method does not give you good results.

In this post you will discover the benefits of creating lists of machine learning algorithms, how to do it, how to do it well and why you should start creating your first list of algorithms today.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Dealing with So Many Algorithms

There are hundreds of machine learning algorithms.

I see this leading to two problems:

1. Overwhelm

The fact that there are so many algorithms to choose from and try on a given machine learning problem causes some people to freeze up and do nothing.

The fact is, you don’t need to get the best result, you only need a result – a beachhead on the problem – and you can get there by spot checking a few algorithms.

2. Favorites

Because there are so many algorithms, some people select one or two favorite algorithms and only use them. This limits the results they can achieve and the problems that they can address.

Favorites are dangerous. Some algorithms are more powerful than others, but that power comes at a cost of complexity and parsimony. They are tools, leave your emotional attachment at the door.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Take Control of the Algorithms

You need focus, a starting point to address the problem of dealing with so many machine learning algorithms.

This involves finding the edges and pushing back the fog on what is out there and what you can use when. This will give you a sense of control over the algorithms and help you to wield them rather than make you feel overwhelmed.

The great thing is, you don’t need to become an expert in each algorithm to make progress. You don’t need to know very much about each algorithm at all.

Collecting simple information such as algorithm names and the general problems to which they are applicable can help you quickly and confidently build familiarization and confidence with the extent of machine learning algorithms available.

How to Build and Maintain a List of Algorithms

The answer is to build your own personal list of machine learning algorithms.

I’m a list maker, and this method really lights up my brain.

Open a text file, word document or spreadsheet and start listing down the names of algorithms. It’s that simple. You can also list the general class to which the algorithm belongs and the general types of problems that it can address.

Define your own categories. This list is a tool to help you understand and navigate the machine learning tools at your disposal. Customize the list to include the algorithm details that you care about.

Examples of Algorithm Lists To Create

Below are 10 examples of machine learning algorithm lists that you could create.

Regression algorithms

SVM algorithms

Data projection algorithms

Deep learning algorithms

Time series forecasting algorithms

Rating system algorithms

Recommender system algorithms

Feature selection algorithms

Class imbalance algorithms

Decision tree algorithms

Tips for Great Algorithm Lists

Creating a list of algorithms is relatively easy. The hard part is knowing why you want the list. The “why” will help you define the type of list you want to create and the algorithm properties you want to describe in your list.

Start with the current project you are working on or your current interests. For example, if you are working on a time series or image classification problem, list all the algorithms that you could apply to that problem. If you are deeply interested in Support Vector Machines, list all the variations of SVM that you can find.

Don’t try to create the perfect list in one sitting. Create it and keep adding to it over days and weeks. It is a useful resource that you can turn back to again and again and add to as your knowledge and experience grows.

In summary, 5 tips for creating great algorithm lists are:

Start with why you want the list and use that to define the type of list to create.

Only capture the algorithm properties you actually need, keep it as simple as possible.

Start with a current project or interest and create a list of related algorithms.

Don’t aim for abstract perfection, the list is for you and your needs alone.

Add to your list over time and expand it as your skills and experience grow.

When To Use An Algorithm List

Algorithm lists are more valuable than you think.

For example, you can use it as a technique when tackling a problem type that you have never worked on before, such as recommender systems, face detection or rating systems. A simple algorithm list gives you a list of things to try.

When working on a familiar problem, your prior biases often limit the results that you can achieve. A list of algorithms relevant to a problem domain can get you unstuck and even push you to achieve new and better results. That does not mean you should try all algorithms you can find, you still require reasoned and systematic application. Nevertheless, a list can provide a useful starting point.

Algorithm lists are a tool, but you can take them further. To make effective use of machine learning algorithms, you need to study them, research them, and even describe them. This is a natural extension for the algorithm list method and your lists can provide the basis for your self-study curriculum.

You could start by collecting additional properties about each algorithm and expand your list into a mini-encyclopaedia of algorithms, with one page per algorithm. I use an algorithm description template and focus on template elements that I will find useful as I refer back to the descriptions in the future, such as pseudo code and usage heuristics.

In summary, 3 examples of when you can use an algorithm list are:

When you start working on a new class of problem.

When you are stuck or looking for algorithms to try on an existing problem.

When you are looking for algorithms to describe in more detail or research.

Anyone Can Create Machine Learning Algorithm Lists

You don’t need to deep dive into machine learning textbooks or open source libraries. A simple google search or browse of wikipedia will uncover many algorithm names to start-off your list.

If you are stuck on what to create for your first list, pick one of the examples above or browse a site like DataTau and pick a class of algorithm to list mentioned in an article or article title.

Again, you do not have to list every algorithm that you could list, narrow your scope to those algorithms in the libraries and tools you prefer. You don’t need to list every permutation of every algorithm, for example, you could focus on one aspect of an algorithm, such as the kernel functions for an SVM or the transfer functions for a neural network.

Don’t list all possible features of each algorithm. Stick to just the name and maybe the general class of algorithm and general types of problems for which it can be used. If you want to go deeper into an algorithm, consider the algorithm description method and template described previously.

You don’t need to understand the algorithms yet and you don’t need to be an academic. This is a beachhead that you are taking to expand your idea of what is out there, to overcome overwhelm and to finally to provide a point of departure on your journey deeper into applied machine learning.

Action Steps

In this post you learned about the simple tactic of creating lists of machine learning algorithms.

You discovered that this simple tactic can help you to overcome algorithm overwhelm and to help get you unstuck from the dangers of having favorite algorithms.

Your action step for this post is to create your first algorithm list. Pick something small, like a subclass of an algorithm. Pick something fun, like an algorithm that is hot right now.

Share your list if you like (or what your list is about), it would help to motivate others.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
262;news.mit.edu;http://news.mit.edu/2020/mit-companies-covid-19-0326;;MIT-affiliated companies take on Covid-19;"As the world grapples with the public health crises and myriad disruptions brought on by the Covid-19 pandemic, many efforts to address its impact are underway.

Several of those initiatives are being led by companies that were founded by MIT alumni, professors, students, and researchers.

These companies’ efforts are as wide ranging and complex as the challenges brought on by Covid-19. They leverage expertise in biological engineering, mobile technology, data analytics, community engagement, and other fields MIT has long focused on.

The companies, a few of whom are featured here, are also at very different stages of deployment, but they are all driven by a desire to use science, engineering, and entrepreneurship to solve the world’s most pressing problems.

Moderna Therapeutics

On Jan. 11, Chinese authorities shared the genetic sequence of Covid-19. Just two days later, members of a research team from Moderna Therapeutics, in collaboration with the National Institutes of Health, finalized the design of a vaccine they hope will prevent infection from the disease.

Moderna was founded by Institute Professor Robert Langer, who is also a faculty member at the Institute for Medical Engineering and Sciences (IMES), investor Noubar Afeyan PhD ’87, and researchers from Harvard Medical School in 2010. The company develops treatments that leverage specialized transporter molecules in cells known as messenger RNAs. Messenger RNAs bring instructions from genes to the cellular machinery that makes proteins. By creating specially modified mRNA, Moderna believes it can develop therapies to treat and prevent a number of diseases in humans.

Following its design of a potential Covid-19 vaccine, the company quickly moved to manufacture the mRNA vaccine for clinical trials. On March 16, just 65 days after Covid-19 was sequenced, Moderna began human trials, according to the company.

The first stage of the trials is expected to last six weeks and will focus on the safety of the vaccine as well as the immune response it provokes in participants. The company has said that while a commercially available vaccine is not likely to be available for at least 12-18 months, it is possible that under emergency use, a vaccine could be available to some people sooner.

Alnylam Pharmaceuticals

On March 5, Alnylam Pharmaceuticals announced that its partnership with Vir Biotechnology, which focuses on treating infectious diseases, would extend to developing therapeutics for coronavirus infections, including Covid-19.

Alnylam was founded in 2002 by Institute Professor Phil Sharp, who is also a faculty member at the Institute for Medical Engineering and Sciences (IMES), Professor David Bartel, former MIT professor Paul Schimmel, MIT postdocs Tom Tuschl and Phil Zamore, and investors.

The company is already approved to treat patients with certain rare genetic diseases using its patented RNA interference technology. RNA interference, or RNAi, is a method of stopping the expression of specific genes through the manipulation of existing regulatory processes in the human body.

“[RNAi] technology is now strongly validated in a variety of ways and the promise of it is really remarkable,” says Sharp, who currently sits on Alnylam’s scientific advisory board with Bartel and Schimmel. “It’s the creation of a whole new therapeutic modality that I think we’ll be using 100 years from now.”

Under the terms of the extended collaboration, the companies will use Alnylam’s recent advances in delivering its RNAi technology to the lungs, in addition to Vir’s infectious disease capabilities, to identify and advance drug candidates.

Sharp says that even if the collaboration doesn’t lead to a treatment for the current Covid-19 outbreak, it holds tremendous potential for helping victims of infectious diseases down the line.

Dimagi

Dimagi, which provides a platform for creating mobile apps that can be used offline by cell phones of all types, recently began freely offering its mobile tool to organizations responding to the Covid-19 outbreak around the world.

The company’s platform is currently being used by hundreds of thousands of front-line health care workers globally. By enabling people with no coding experience to create mobile apps that work in environments with no cellular service, the company has transformed health care treatment for millions of people in low- and middle-income countries.

The company has already seen governments adopt its platform for Covid-19 response, including the Ogun state government of Nigeria, and it is also exploring use cases with officials from the U.S. Centers for Disease Control and Prevention in California.

The company was formed in 2002 when Jonathan Jackson’03 SM ’05 met co-founder Vikram Kumar, who was then a graduate research assistant in MIT’s Media Lab and on his way to earning his MD in the MIT-Harvard Division of Health Sciences and Technology.

Since then, Dimagi’s solutions have been used for a variety of large health care initiatives, including the Ebola crisis in West Africa, where the company worked directly with health organizations to give them mobile applications that helped provide critical care during their Ebola response.

Jackson believes Dimagi can help health care workers with tracking person-to-person contact, data collection, decision support, and spreading useful information. The company is also compiling a library of free, open-source templated Covid-19 mobile applications for quick deploymnent.

“Think of it as a free app store where health organizations working on the front lines can go, download their Covid-19 applications and quickly equip their health workforces with Covid-19 apps,” Jackson says.

Biobot Analytics

Biobot Analytics, a startup that analyzes wastewater to gain insights into public health, has begun requesting sewage samples from wastewater treatment facilities across the U.S. to test for SARS-CoV-2, the virus causing Covid-19.

The company’s technology, developed by CEO Mariana Matus PhD ’18 during her time at MIT in partnership with Newsha Ghaeli, then a research fellow in the Department of Urban Studies and Planning, has been geared toward estimating drug consumption in communities since its founding in 2017.

Biobot uses a proprietary device to gather representative samples of sewage, then ships those samples to its scientists for near-real time testing. Samples can be used to track opioid use, nutrition, environmental contaminants, antibiotic resistance, and the spread of infectious diseases. The resulting insights can be used to understand the health and well-being of small communities or large cities.

In the company’s Covid-19 testing program, which it launched pro bono in collaboration with researchers at MIT, Harvard, and Brigham and Women’s Hospital, the teams will process sewage samples from treatment facilities across the U.S., then use a laboratory technique known as a reverse transcription polymerase chain reaction to determine the presence of SARS-CoV-2.

The collaborators believe the program could complement existing testing methods in addition to helping guide community reponses, measure the effectiveness of interventions, and provide an early warning for re-emergence of the outbreak.

“There is an incredible opportunity to use this technology to get ahead of and monitor the Covid-19 epidemic,” the company wrote in a recent Medium post announcing the program. “A wastewater epidemiology system that aggregates samples from wastewater treatment plants across the U.S. would provide a dynamic map of Covid-19 as it spreads to new places. [This will be a tracker for the outbreak complementary to individual testing]. Government officials, school administrators, and employers would no longer need to rely on confirmed cases or hospital reporting to make tough decisions like enforcing work from home policies.”

Soofa

Soofa, a startup that creates solar-powered digital signs in public spaces, has begun offering its city partners templates to quickly post emergency announcements regarding Covid-19. In Massachusetts, the templates have been used in Brookline to post updates about school and playground closures, in Somerville to redirect people to the town’s coronavirus webpage, and in Everett, which has posted their updates in both English and Spanish to reach more people.

Soofa was founded in 2014 by Jutta Friedrichs and Sandra Richter, a former researcher in MIT’s Media Lab. The founders refer to their signs as “neighborhood news feeds” because they offer an easy, inclusive way for community members to view and post messages.

The company’s digital signage has also proven useful for its partners outside of government. Boston Architectural College, for example, now gives viewers instructions to attend their spring virtual open house.

Pathr

Pathr is a startup that uses data analytics and machine learning to understand how people move through environments. The company, which has primarily used its technology to help retailers, casino operators, and owners of public spaces gain insights into customer behavior, recently launched a new product called SocialDistance.ai.

SocialDistance.ai will use Pathr’s “spatial intelligence” platform to give operators of large spaces information on how infectious diseases might spread in different scenarios.

Pathr's platform can be used to simulate the spread of infectious diseases in different scenarios. In this video playlist, simulations incorporating measures such as social distancing (second video) and mask distribution (third video) are shown.

SocialDistance.ai was formed when Pathr’s team got locked down in the San Francisco Bay Area, where the company is based, and began thinking about how their technology could help address disruptions related to the Covid-19 outbreak.

“There’s a spatial component to disease outbreak in general, and we’ve been hearing a lot about that with this coronavirus, so that was the spark, just thinking about what we could do to help,” says Pathr founder and CEO George Shaw SM ’11.

Shaw says his team has been in touch with officials who run malls, casinos, retail stores, and various public spaces to help them make more informed decisions about allowing people to use their spaces in the time periods surrounding an outbreak.

“Nobody who operates a big space wants to limit the number of people [in that space], so this would be a way to strike that balance, to get the right social distance, the right density of crowds; it could also help owners reconfigure a space so the flow of people is more conducive to social distancing,” Shaw says.

Shaw developed the spatial intelligence platform as a graduate student in the lab of Professor Deb Roy while working on a project in the Media Lab."
263;news.mit.edu;http://news.mit.edu/2020/to-self-drive-in-snow-look-under-road-0226;;To self-drive in the snow, look under the road;"Car companies have been feverishly working to improve the technologies behind self-driving cars. But so far even the most high-tech vehicles still fail when it comes to safely navigating in rain and snow.

This is because these weather conditions wreak havoc on the most common approaches for sensing, which usually involve either lidar sensors or cameras. In the snow, for example, cameras can no longer recognize lane markings and traffic signs, while the lasers of lidar sensors malfunction when there’s, say, stuff flying down from the sky.

MIT researchers have recently been wondering whether an entirely different approach might work. Specifically, what if we instead looked under the road?

A team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) has developed a new system that uses an existing technology called ground-penetrating radar (GPR) to send electromagnetic pulses underground that measure the area’s specific combination of soil, rocks, and roots. Specifically, the CSAIL team used a particular form of GPR instrumentation developed at MIT Lincoln Laboratory called localizing ground-penetrating radar, or LGPR. The mapping process creates a unique fingerprint of sorts that the car can later use to localize itself when it returns to that particular plot of land.

“If you or I grabbed a shovel and dug it into the ground, all we’re going to see is a bunch of dirt,” says CSAIL PhD student Teddy Ort, lead author on a new paper about the project that will be published in the IEEE Robotics and Automation Letters journal later this month. “But LGPR can quantify the specific elements there and compare that to the map it’s already created, so that it knows exactly where it is, without needing cameras or lasers.”

In tests, the team found that in snowy conditions the navigation system’s average margin of error was on the order of only about an inch compared to clear weather. The researchers were surprised to find that it had a bit more trouble with rainy conditions, but was still only off by an average of 5.5 inches. (This is because rain leads to more water soaking into the ground, leading to a larger disparity between the original mapped LGPR reading and the current condition of the soil.)

The researchers said the system’s robustness was further validated by the fact that, over a period of six months of testing, they never had to unexpectedly step in to take the wheel.

“Our work demonstrates that this approach is actually a practical way to help self-driving cars navigate poor weather without actually having to be able to ‘see’ in the traditional sense using laser scanners or cameras,” says MIT Professor Daniela Rus, director of CSAIL and senior author on the new paper, which will also be presented in May at the International Conference on Robotics and Automation in Paris.

While the team has only tested the system at low speeds on a closed country road, Ort said that existing work from Lincoln Laboratory suggests that the system could easily be extended to highways and other high-speed areas.

This is the first time that developers of self-driving systems have employed ground-penetrating radar, which has previously been used in fields like construction planning, landmine detection, and even lunar exploration. The approach wouldn’t be able to work completely on its own, since it can’t detect things above ground. But its ability to localize in bad weather means that it would couple nicely with lidar and vision approaches.

“Before releasing autonomous vehicles on public streets, localization and navigation have to be totally reliable at all times,” says Roland Siegwart, a professor of autonomous systems at ETH Zurich who was not involved in the project. “The CSAIL team’s innovative and novel concept has the potential to push autonomous vehicles much closer to real-world deployment.”

One major benefit of mapping out an area with LGPR is that underground maps tend to hold up better over time than maps created using vision or lidar, since features of an above-ground map are much more likely to change. LGPR maps also take up only about 80 percent of the space used by traditional 2D sensor maps that many companies use for their cars.

While the system represents an important advance, Ort notes that it’s far from road-ready. Future work will need to focus on designing mapping techniques that allow LGPR datasets to be stitched together to be able to deal with multi-lane roads and intersections. In addition, the current hardware is bulky and 6 feet wide, so major design advances need to be made before it’s small and light enough to fit into commercial vehicles.

Ort and Rus co-wrote the paper with CSAIL postdoc Igor Gilitschenski. The project was supported, in part, by MIT Lincoln Laboratory."
264;news.mit.edu;http://news.mit.edu/2020/sugar-factories-colonial-indonesia-olken-dell-0206;;The complex effects of colonial rule in Indonesia;"The areas of Indonesia where Dutch colonial rulers built a huge sugar-producing industry in the 1800s remain more economically productive today than other parts of the country, according to a study co-authored by an MIT economist.

The research, focused on the Indonesian island of Java, introduces new data into the study of the economic effects of colonialism. The finding shows that around villages where the Dutch built sugar-processing factories from the 1830 through the 1870s, there is today greater economic activity, more extensive manufacturing, and even more schools, along with higher local education levels.

“The places where the Dutch established [sugar factories] persisted as manufacturing centers,” says Benjamin Olken, a professor of economics at MIT and co-author of a paper detailing the results, which appears in the January issue of the Review of Economic Studies.

The historical link between this “Dutch Cultivation System” and economic activity today has likely been transmitted “through a couple of forces,” Olken suggests. One of them, he says, is the building of “complementary infrastructure” such as railroads and roads, which remain in place in contemporary Indonesia.

The other mechanism, Olken says, is that “industries grew up around the sugar [industry], and those industries persisted. And once you have this manufacturing environment, that can lead to other changes: More infrastructure and more schools have persisted in these areas as well.”

To be sure, Olken says, the empirical conclusions of the study do not represent validation of Dutch colonial rule, which lasted from the early 1600s until 1949 and significantly restricted the rights and self-constructed political institutions of Indonesians. Dutch rule had long-lasting effects in many areas of civic life, and the Dutch Cultivation System used forced labor, for one thing.

“This paper is not trying to argue that the [Dutch] colonial enterprise was a net good for the people of the time,” Olken emphasizes. “I want to be very clear on that. That’s not what we’re saying.”

Instead, the study was designed to evaluate the empirical effects of the Dutch Cultivation System, and the outcome of the research was not necessarily what Olken would have anticipated.

“The results are striking,” Olken says. “They just jump out at you.”

The paper, “The Development Effects of the Extractive Colonial Economy: The Dutch Cultivation System in Java,” is co-authored by Olken and Melissa Dell PhD ’12, a professor of economics at Harvard University.

On the ground

Historically in Java, the most populous of Indonesia’s many islands, the main crop had been rice. Starting in the 1830s, the Dutch instituted a sugar-growing system in some areas, building 94 sugar-processing factories, as well as roads and railroads to transport materials and products.

Generally the Dutch would export high-quality sugar from Indonesia while keeping lower-quality sugar in the country. Overall, the system became massive; at one point in the mid-19th century, sugar production in Java accounted for one-third of the Dutch government’s revenues and 4 percent of Dutch GDP. By one estimate, a quarter of the population was involved in the industry.

In developing their research, Olken and Dell used 19th century data from government archives in the Netherlands, as well as modern data from Indonesia. The Dutch built the processing plants next to rivers in places with enough flat land to sustain extensive sugar crops; to conduct the study, the researchers looked at economic activity near sugar-processing factories and compared it with economic activity in similar areas that lacked factories.

“In the 1850s, the Dutch spent four years on the ground collecting detailed information for the over 10,000 villages that contributed land and labor to the Cultivation System,” Dell notes. The researchers digitized those records and, as she states, “painstakingly merged them” with economic and demograhic records from the same locations today

As the results show, places close to factories are 25-30 percentage points less agricultural in economic composition than those away from factories, and they have more manufacturing, by 6-7 percentage points. They also have 9 percent more employment in retail.

Areas within 1 kilometer of a sugar factory have a railroad density twice that of similar places 5 to 20 kilometers from factories; by 1980, they were also 45 percent more likely to have electricity and 4 percent more likely to have a high school. They also have local populations with a full year more of education, on average, than areas not situated near old sugar factories.

The study shows there is also about 10 to 15 percent more public-land use in villages that were part of the Dutch Cultivation System, a data point that holds steady in both 1980 and 2003.

“The key thing that underlies this paper, in multiple respects, is the linking of the historical data and the modern data,” Olken says. The researchers also observed that the disparity between industrialized places and their more rural counterparts has not arisen since 1980, further suggesting how much Java’s deep economic roots matter.

Net Effects?

The paper blends the expertise of Olken, who has spent years conducting antipoverty studies in Indonesia, and Dell, whose work at times examines the effects of political history on current-day economic outcomes.

“I had never really done a historical project before,” Olken says. “But the opportunity to collaborate with Melissa on this was really exciting.”

One of Dell’s best-known papers, published in 2010 while she was still a PhD student at MIT, shows that in areas of Peru where colonial Spanish rulers instituted a system of forced mining labor from the 1500s to the 1800s, there are significant and negative economic effects that persist today.

However, somewhat to their surprise, the researchers did not observe similarly promounced effects from the Dutch Cultivation System.

“One might have thought that could have had negative consequences on local social capital and local development in other respects,” says Olken, adding that he “wasn’t sure what to expect” before looking at the data.

“The differences between the long-run effects of forced labor in Peru and Java suggest that for understanding persistent impacts on economic activity, we need to know more than just whether there was forced labor in a location,” Dell says. “We need to understand how the historical institutions influenced economic incentives and activities initially, and how these initial effects may or may not have persisted moving forward.”

Olken adds that the study “can’t measure every possible thing,” and that “it’s possible there are other effects we didn’t see.”

Moreover, Olken notes, the paper cannot determine the net effect of the Dutch Cultivation System on Indonesian economic growth. That is, in the absence of Dutch rule, Indonesia’s economy would have certainly grown on it own — but it is impossible to say whether it would have expanded at a rate faster, slower, or equivalent to the trajectory it had under the Dutch.

“We can’t say what would have happened if the Dutch had never showed up in Indonesia,” Olken says. “And of course the Dutch [colonizing] Indonesia had all kinds of effects well beyond the scope of this paper, many of them negative for the contemporaneous population.”"
265;news.mit.edu;http://news.mit.edu/2018/using-data-science-improve-public-policy-hackathon-0423;;Using data science to improve public policy;"100 researchers and students from MIT and six other universities gathered on campus this April for the first weekend-long MIT Policy Hackathon. This interdisciplinary event teamed data science, engineering, and policy students to explore solutions to real societal challenges submitted by sponsor organizations.

The hackathon, subtitled “Data to Decisions,” was organized and run by students from MIT’s Institute for Data, Systems, and Society (IDSS). Participants used datasets provided by nonprofit, education, and government institutions to pitch solutions to complex challenges in cybersecurity, health, energy and climate, transportation, and the future of work. A panel of judges evaluated the pitches and read final policy proposals.

“It’s a different type of hackathon in that it is focused on public policy outcomes,” says Amy Umaretiya, a student organizer with IDSS’s Master’s program in Technology and Policy (TPP). “We have these concrete challenges put forth by organizations that have data analytics needs for social good that aren’t being met.”

“We wanted to create a hackathon where interdisciplinary teams tackle complex societal problems,” explains Marco Miotti, a doctoral student at IDSS. “The challenges were structured so you can’t solve them without both data and policy expertise.”

Data and diaper need

The winning team, called NappyTime, worked on the health challenge with sponsors from Yale’s Mental health Outreach for MotherS (MOMS) Partnership and the National Diaper Bank Network (NDBN). The problem: lack of a sufficient supply of diapers to keep babies and toddlers clean, dry, and healthy. Diaper need affects one in three low- and middle-income families in the U.S., and can have a significant impact on the physical, mental, and economic well-being of both children and parents.

“The crux of this proposal was the need for more data on diaper need, and quantifying the benefits of addressing it,” says Lawrence Baker, a TPP student on the winning team. “We wanted to propose policies that would be inexpensive for Connecticut, easy to implement, and would allow the collection of more data.”

Lori Wallace, a postdoctoral associate with MOMS, mentored the challenge. She was joined by Lynn Comer of NDBN, who was also a judge. Both remarked on the applicability and ingenuity of the winning proposal, which is under a nondisclosure agreement. “By focusing on cost-effective means to embed the provision of diapers into our childcare system, the hackathon participants presented actionable solutions to this real life issue,” they said.

Hackathon judge Frank Field, a senior research engineer with IDSS who is associate director of TPP, was also impressed with the team’s proposal and presentation, both during their pitch and the follow-up Q&A session. Says Field: “Their proposal went right at the problem. It was pitched in the form of a staged implementation, which cleverly skirts some of the classic impediments to policy innovations by targeting some populations immediately while centering on issues whose resolution could inform and refine a wider deployment.""

Real data, real problems

Two local challenges were sponsored by organizations within the Massachusetts government. The first, in transportation, looked for solutions to congestion on local highways and train lines. Transportation proposals considered the impact of autonomous vehicles and suggested ideas ranging from shielded bike paths to establishing a car-free area within Kendall Square.

Meanwhile, the energy and climate challenge asked teams to help Boston meet its goal of carbon neutrality by 2050. Climate hackers worked with green building data that had previously been locked up in PDF format. Hackathon organizers partnered with WattzOn, a software company working in utility data, to extract the information into a usable dataset.

“It was great that the problem was drawn from the real world, that we had real data, and that our mentors had working knowledge of relevant policies. It really showed us how to work through a policy problem with various administrative constraints, interfering regulations, and incomplete data,” says energy and climate hacker Zhen Dai, a PhD student researching the science and policy of solar geoengineering at Harvard University.

The cybersecurity challenge, sponsored by Boston University Law School’s Technology and Cyberlaw Clinic, called upon students to examine the multitude of legislative and policy approaches that U.S. states use to respond to cybersecurity breaches such as those experienced by Equifax and Yahoo. Teams were asked to consolidate and compare these approaches across states, and then propose ways to identify and implement some of the more effective policies.

“It’s important to have people who can think at the intersection of computer science and policy,” says Nathaniel Fruchter, a hackathon organizer and TPP student. “The breach dataset provided to the hackers has been largely unexplored by academia or industry, so we were really excited to see what new perspective these teams could bring.”

The future of work challenge came from the MIT Initiative on the Digital Economy and Wonolo, an online labor market focused on local blue collar jobs. Hackers used Wonolo’s job data to explore the design of policies for ensuring a social safety net for workers in the gig economy.

“It is very satisfying to use the skills we normally only apply to narrowly focused, high-level scientific problems in the lab instead on a broad range of social issues that impact the daily life of many community members,” says hacker Ryan Badman, a Cornell University PhD student in physics.

To student organizer Christoph Tries, the hackathon demonstrated the applicability of data science tools to addressing many complex societal challenges — making IDSS an ideal host for the event. “We are lucky to have IDSS at MIT. Other students are more siloed at their universities and don’t have that interdisciplinary focus. There are a lot of folks studying data science who are looking for ways to apply their knowledge to public policy and connect with people who actively need their help.”"
266;machinelearningmastery.com;http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/;2016-06-16;Display Deep Learning Model Training History in Keras;"# Visualize training history

from keras . models import Sequential

from keras . layers import Dense

import matplotlib . pyplot as plt

import numpy

# load pima indians dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# create model

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# Compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# Fit the model

history = model . fit ( X , Y , validation_split = 0.33 , epochs = 150 , batch_size = 10 , verbose = 0 )

# list all data in history

print ( history . history . keys ( ) )

# summarize history for accuracy

plt . plot ( history . history [ 'accuracy' ] )

plt . plot ( history . history [ 'val_accuracy' ] )

plt . title ( 'model accuracy' )

plt . ylabel ( 'accuracy' )

plt . xlabel ( 'epoch' )

plt . legend ( [ 'train' , 'test' ] , loc = 'upper left' )

plt . show ( )

# summarize history for loss

plt . plot ( history . history [ 'loss' ] )

plt . plot ( history . history [ 'val_loss' ] )

plt . title ( 'model loss' )

plt . ylabel ( 'loss' )

plt . xlabel ( 'epoch' )

plt . legend ( [ 'train' , 'test' ] , loc = 'upper left' )"
267;towardsdatascience.com;https://towardsdatascience.com/redacting-sensitive-information-from-doctors-patient-notes-51773a9c494b?source=collection_home---4------2-----------------------;2020-04-19;Redacting sensitive information from doctors’ patient notes;"Redacting sensitive information from doctors’ patient notes

COVID-19 public dataset on GCP from cases in Italy (part 2)

This article is the second one of a series about the release of a public dataset made of doctors medical notes about patients affected by COVID19. You will find learnings about Google Cloud DLP API that was used for the redaction of sensitive information from medical notes. If you haven’t seen it already, you can find the first article here.

To reiterate my commitment to the community, I will keep the public database up-to-date with the latest cases published by the Italian Society of Medical and Interventional Radiology (ISMIR). And if, as a side effect, you can learn a thing or two about GCP, then this series will exceed my expectations 💪 🙏 . The code used in this pipeline is available in my Github repo.

By the way, I’m very proud to see that the community is thinking of ways to leverage this data. An avenue raised by Jérôme MASSOT is to leverage this dataset and conduct a:"
268;machinelearningmastery.com;https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/;2018-12-30;How to Develop a Stacking Ensemble for Deep Learning Neural Networks in Python With Keras;"# stacked generalization with neural net meta model on blobs dataset

from sklearn . datasets import make_blobs

from sklearn . metrics import accuracy_score

from keras . models import load_model

from keras . utils import to_categorical

from keras . utils import plot_model

from keras . models import Model

from keras . layers import Input

from keras . layers import Dense

from keras . layers . merge import concatenate

from numpy import argmax

# load models from file

def load_all_models ( n_models ) :

all_models = list ( )

for i in range ( n_models ) :

# define filename for this ensemble

filename = 'models/model_' + str ( i + 1 ) + '.h5'

# load model from file

model = load_model ( filename )

# add to list of members

all_models . append ( model )

print ( '>loaded %s' % filename )

return all_models

# define stacked model from multiple member input models

def define_stacked_model ( members ) :

# update all layers in all models to not be trainable

for i in range ( len ( members ) ) :

model = members [ i ]

for layer in model . layers :

# make not trainable

layer . trainable = False

# rename to avoid 'unique layer name' issue

layer . name = 'ensemble_' + str ( i + 1 ) + '_' + layer . name

# define multi-headed input

ensemble_visible = [ model . input for model in members ]

# concatenate merge output from each model

ensemble_outputs = [ model . output for model in members ]

merge = concatenate ( ensemble_outputs )

hidden = Dense ( 10 , activation = 'relu' ) ( merge )

output = Dense ( 3 , activation = 'softmax' ) ( hidden )

model = Model ( inputs = ensemble_visible , outputs = output )

# plot graph of ensemble

plot_model ( model , show_shapes = True , to_file = 'model_graph.png' )

# compile

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

return model

# fit a stacked model

def fit_stacked_model ( model , inputX , inputy ) :

# prepare input data

X = [ inputX for _ in range ( len ( model . input ) ) ]

# encode output data

inputy_enc = to_categorical ( inputy )

# fit model

model . fit ( X , inputy_enc , epochs = 300 , verbose = 0 )

# make a prediction with a stacked model

def predict_stacked_model ( model , inputX ) :

# prepare input data

X = [ inputX for _ in range ( len ( model . input ) ) ]

# make prediction

return model . predict ( X , verbose = 0 )

# generate 2d classification dataset

X , y = make_blobs ( n_samples = 1100 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 )

# split into train and test

n_train = 100

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

print ( trainX . shape , testX . shape )

# load all models

n_members = 5

members = load_all_models ( n_members )

print ( 'Loaded %d models' % len ( members ) )

# define ensemble model

stacked_model = define_stacked_model ( members )

# fit stacked model on test dataset

fit_stacked_model ( stacked_model , testX , testy )

# make predictions and evaluate

yhat = predict_stacked_model ( stacked_model , testX )

yhat = argmax ( yhat , axis = 1 )

acc = accuracy_score ( testy , yhat )"
269;news.mit.edu;http://news.mit.edu/2020/how-crystals-form-surfaces-paes-0402;;Technique reveals how crystals form on surfaces;"The process of crystallization, in which atoms or molecules line up in orderly arrays like soldiers in formation, is the basis for many of the materials that define modern life, including the silicon in microchips and solar cells. But while many useful applications for crystals involve their growth on solid surfaces (rather than in solution), there has been a dearth of good tools for studying this type of growth.

Now, a team of researchers at MIT and Draper has found a way to reproduce the growth of crystals on surfaces, but at a larger scale that makes the process much easier to study and analyze. The new approach is described in a paper in the journal Nature Materials, by Robert Macfarlane and Leonardo Zomberg at MIT, and Diana Lewis PhD ’19 and David Carter at Draper.

Rather than assembling these crystals from actual atoms, the key to making the process easy to observe and quantify was the use of “programmable atom equivalents,” or PAEs, Macfarlane explains. This works because the ways atoms line up into crystal lattices is entirely a matter of geometry and doesn’t rely on the specific chemical or electronic properties of its constituents.

The team used spherical nanoparticles of gold, coated with specially selected single strands of genetically engineered DNA, giving the particles roughly the appearance of Koosh balls. Single DNA strands have the inherent property of attaching themselves tightly to the corresponding reciprocal strands, to form the classic double helix, so this configuration provides a surefire way of getting the particles to align themselves in precisely the desired way.

“If I put a very dense brush of DNA on the particle, it’s going to make as many bonds with as many nearest neighbors as it can,” Macfarlane says. “And if you design everything appropriately and process it correctly, they will form ordered crystal structures.” While that process has been known for some years, this work is the first to apply that principle to study the growth of crystals on surfaces.

“Understanding how crystals grow upward from a surface is incredibly important for a lot of different fields,” he says. The semiconductor industry, for example, is based on the growth of large single-crystal or multi-crystalline materials that must be controlled with great precision, yet the details of the process are difficult to study. That’s why the use of oversized analogs such as the PAEs can be of such benefit.

The PAEs, he says, “crystallize in exactly the same pathways that molecules and atoms do. And so they are a very nice proxy system for understanding how crystallization occurs.” With this system, the properties of the DNA dictate how the particles assemble and the 3D configuration they end up in.

They designed the system such that the crystals nucleate and grow starting from a surface and “by tailoring the interactions both between particles, and between the particles and the DNA-coated surface, we can dictate the size, the shape, the orientation and the degree of anisotropy (directionality) in the crystal,” Macfarlane says.

“By understanding the process this is going through to actually form these crystals, we can potentially use that to understand crystallization processes in general,” he adds.

He explains that not only are the resulting crystal structures about 100 times larger than the actual atomic ones, but their formation processes are also much slower. The combination makes the process much easier to analyze in detail. Earlier methods of characterizing such crystalline structures only showed their final states, thus missing complexities in the formation process.

“I could change the DNA sequence. I can change the number of DNA strands in the particle. I can change the size of the particle and I can tweak each of these individual handles independently,” Macfarlane says. “So if I wanted to be able to say, OK, I hypothesize that this particular structure might be favored under these conditions if I tuned the energetics in such a way, that’s a much easier system to study with the PAEs than it would be with atoms themselves.”

The system is very effective, he says, but DNA strands modified in a manner that allows for attachment to nanoparticles can be quite expensive. As a next step, the Macfarlane lab has also developed polymer-based building blocks that show promise in replicating these same crystallization processes and materials, but can be made inexpensively at a multigram scale.

The work was funded by the Air Force Office of Scientific Research and supported by a Draper fellowship and the National Science Foundation and used facilities of the Materials Technology Laboratory at MIT."
270;machinelearningmastery.com;http://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/;2016-02-04;Tune Machine Learning Algorithms in R (random forest case study);"customRF < - list ( type = ""Classification"" , library = ""randomForest"" , loop = NULL )

customRF $ parameters < - data . frame ( parameter = c ( ""mtry"" , ""ntree"" ) , class = rep ( ""numeric"" , 2 ) , label = c ( ""mtry"" , ""ntree"" ) )

customRF $ grid < - function ( x , y , len = NULL , search = ""grid"" ) { }

customRF $ fit < - function ( x , y , wts , param , lev , last , weights , classProbs , . . . ) {

randomForest ( x , y , mtry = param $ mtry , ntree = param $ ntree , . . . )

}

customRF $ predict < - function ( modelFit , newdata , preProc = NULL , submodels = NULL )

predict ( modelFit , newdata )

customRF $ prob < - function ( modelFit , newdata , preProc = NULL , submodels = NULL )

predict ( modelFit , newdata , type = ""prob"" )

customRF $ sort < - function ( x ) x [ order ( x [ , 1 ] ) , ]"
271;machinelearningmastery.com;https://machinelearningmastery.com/cost-sensitive-decision-trees-for-imbalanced-classification/;2020-01-28;Cost-Sensitive Decision Trees for Imbalanced Classification;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

# grid search class weights with decision tree for imbalance classification from numpy import mean from sklearn . datasets import make_classification from sklearn . model_selection import GridSearchCV from sklearn . model_selection import RepeatedStratifiedKFold from sklearn . tree import DecisionTreeClassifier # generate dataset X , y = make_classification ( n_samples = 10000 , n_features = 2 , n_redundant = 0 , n_clusters_per_class = 1 , weights = [ 0.99 ] , flip_y = 0 , random_state = 3 ) # define model model = DecisionTreeClassifier ( ) # define grid balance = [ { 0 : 100 , 1 : 1 } , { 0 : 10 , 1 : 1 } , { 0 : 1 , 1 : 1 } , { 0 : 1 , 1 : 10 } , { 0 : 1 , 1 : 100 } ] param_grid = dict ( class_weight = balance ) # define evaluation procedure cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 ) # define grid search grid = GridSearchCV ( estimator = model , param_grid = param_grid , n_jobs = - 1 , cv = cv , scoring = 'roc_auc' ) # execute the grid search grid_result = grid . fit ( X , y ) # report the best configuration print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) ) # report all configurations means = grid_result . cv_results_ [ 'mean_test_score' ] stds = grid_result . cv_results_ [ 'std_test_score' ] params = grid_result . cv_results_ [ 'params' ] for mean , stdev , param in zip ( means , stds , params ) : print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )"
272;machinelearningmastery.com;https://machinelearningmastery.com/statistical-methods-in-an-applied-machine-learning-project/;2018-06-24;10 Examples of How to Use Statistical Methods in a Machine Learning Project;"Tweet Share Share

Last Updated on August 8, 2019

Statistics and machine learning are two very closely related fields.

In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project.

It would be fair to say that statistical methods are required to effectively work through a machine learning predictive modeling project.

In this post, you will discover specific examples of statistical methods that are useful and required at key steps in a predictive modeling problem.

After completing this post, you will know:

Exploratory data analysis, data summarization, and data visualizations can be used to help frame your predictive modeling problem and better understand the data.

That statistical methods can be used to clean and prepare data ready for modeling.

That statistical hypothesis tests and estimation statistics can aid in model selection and in presenting the skill and predictions from final models.

Discover statistical hypothesis testing, resampling methods, estimation statistics and nonparametric methods in my new book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we are going to look at 10 examples of where statistical methods are used in an applied machine learning project.

This will demonstrate that a working knowledge of statistics is essential for successfully working through a predictive modeling problem.

Problem Framing Data Understanding Data Cleaning Data Selection Data Preparation Model Evaluation Model Configuration Model Selection Model Presentation Model Predictions

Need help with Statistics for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Problem Framing

Perhaps the point of biggest leverage in a predictive modeling problem is the framing of the problem.

This is the selection of the type of problem, e.g. regression or classification, and perhaps the structure and types of the inputs and outputs for the problem.

The framing of the problem is not always obvious. For newcomers to a domain, it may require significant exploration of the observations in the domain.

For domain experts that may be stuck seeing the issues from a conventional perspective, they too may benefit from considering the data from multiple perspectives.

Statistical methods that can aid in the exploration of the data during the framing of a problem include:

Exploratory Data Analysis . Summarization and visualization in order to explore ad hoc views of the data.

. Summarization and visualization in order to explore ad hoc views of the data. Data Mining. Automatic discovery of structured relationships and patterns in the data.

2. Data Understanding

Data understanding means having an intimate grasp of both the distributions of variables and the relationships between variables.

Some of this knowledge may come from domain expertise, or require domain expertise in order to interpret. Nevertheless, both experts and novices to a field of study will benefit from actually handeling real observations form the domain.

Two large branches of statistical methods are used to aid in understanding data; they are:

Summary Statistics . Methods used to summarize the distribution and relationships between variables using statistical quantities.

. Methods used to summarize the distribution and relationships between variables using statistical quantities. Data Visualization. Methods used to summarize the distribution and relationships between variables using visualizations such as charts, plots, and graphs.

3. Data Cleaning

Observations from a domain are often not pristine.

Although the data is digital, it may be subjected to processes that can damage the fidelity of the data, and in turn any downstream processes or models that make use of the data.

Some examples include:

Data corruption.

Data errors.

Data loss.

The process of identifying and repairing issues with the data is called data cleaning

Statistical methods are used for data cleaning; for example:

Outlier detection . Methods for identifying observations that are far from the expected value in a distribution.

. Methods for identifying observations that are far from the expected value in a distribution. Imputation. Methods for repairing or filling in corrupt or missing values in observations.

4. Data Selection

Not all observations or all variables may be relevant when modeling.

The process of reducing the scope of data to those elements that are most useful for making predictions is called data selection.

Two types of statistical methods that are used for data selection include:

Data Sample . Methods to systematically create smaller representative samples from larger datasets.

. Methods to systematically create smaller representative samples from larger datasets. Feature Selection. Methods to automatically identify those variables that are most relevant to the outcome variable.

5. Data Preparation

Data can often not be used directly for modeling.

Some transformation is often required in order to change the shape or structure of the data to make it more suitable for the chosen framing of the problem or learning algorithms.

Data preparation is performed using statistical methods. Some common examples include:

Scaling . Methods such as standardization and normalization.

. Methods such as standardization and normalization. Encoding . Methods such as integer encoding and one hot encoding.

. Methods such as integer encoding and one hot encoding. Transforms. Methods such as power transforms like the Box-Cox method.

6. Model Evaluation

A crucial part of a predictive modeling problem is evaluating a learning method.

This often requires the estimation of the skill of the model when making predictions on data not seen during the training of the model.

Generally, the planning of this process of training and evaluating a predictive model is called experimental design. This is a whole subfield of statistical methods.

Experimental Design. Methods to design systematic experiments to compare the effect of independent variables on an outcome, such as the choice of a machine learning algorithm on prediction accuracy.

As part of implementing an experimental design, methods are used to resample a dataset in order to make economic use of available data in order to estimate the skill of the model. These two represent a subfield of statistical methods.

Resampling Methods. Methods for systematically splitting a dataset into subsets for the purposes of training and evaluating a predictive model.

7. Model Configuration

A given machine learning algorithm often has a suite of hyperparameters that allow the learning method to be tailored to a specific problem.

The configuration of the hyperparameters is often empirical in nature, rather than analytical, requiring large suites of experiments in order to evaluate the effect of different hyperparameter values on the skill of the model.

The interpretation and comparison of the results between different hyperparameter configurations is made using one of two subfields of statistics, namely:

Statistical Hypothesis Tests . Methods that quantify the likelihood of observing the result given an assumption or expectation about the result (presented using critical values and p-values).

. Methods that quantify the likelihood of observing the result given an assumption or expectation about the result (presented using critical values and p-values). Estimation Statistics. Methods that quantify the uncertainty of a result using confidence intervals.

8. Model Selection

One among many machine learning algorithms may be appropriate for a given predictive modeling problem.

The process of selecting one method as the solution is called model selection.

This may involve a suite of criteria both from stakeholders in the project and the careful interpretation of the estimated skill of the methods evaluated for the problem.

As with model configuration, two classes of statistical methods can be used to interpret the estimated skill of different models for the purposes of model selection. They are:

Statistical Hypothesis Tests. Methods that quantify the likelihood of observing the result given an assumption or expectation about the result (presented using critical values and p-values).

Estimation Statistics. Methods that quantify the uncertainty of a result using confidence intervals.

9. Model Presentation

Once a final model has been trained, it can be presented to stakeholders prior to being used or deployed to make actual predictions on real data.

A part of presenting a final model involves presenting the estimated skill of the model.

Methods from the field of estimation statistics can be used to quantify the uncertainty in the estimated skill of the machine learning model through the use of tolerance intervals and confidence intervals.

Estimation Statistics. Methods that quantify the uncertainty in the skill of a model via confidence intervals.

10. Model Predictions

Finally, it will come time to start using a final model to make predictions for new data where we do not know the real outcome.

As part of making predictions, it is important to quantify the confidence of the prediction.

Just like with the process of model presentation, we can use methods from the field of estimation statistics to quantify this uncertainty, such as confidence intervals and prediction intervals.

Estimation Statistics. Methods that quantify the uncertainty for a prediction via prediction intervals.

Summary

In this tutorial, you discovered the importance of statistical methods throughout the process of working through a predictive modeling project.

Specifically, you learned:

Exploratory data analysis, data summarization, and data visualizations can be used to help frame your predictive modeling problem and better understand the data.

That statistical methods can be used to clean and prepare data ready for modeling.

That statistical hypothesis tests and estimation statistics can aid in model selection and in presenting the skill and predictions from final models.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Statistics for Machine Learning! Develop a working understanding of statistics ...by writing lines of code in python Discover how in my new Ebook:

Statistical Methods for Machine Learning It provides self-study tutorials on topics like:

Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more... Discover how to Transform Data into Knowledge Skip the Academics. Just Results. See What's Inside"
273;machinelearningmastery.com;https://machinelearningmastery.com/long-short-term-memory-recurrent-neural-networks-mini-course/;2017-08-15;Mini-Course on Long Short-Term Memory Recurrent Neural Networks with Keras;"Tweet Share Share

Last Updated on August 14, 2019

Long Short-Term Memory (LSTM) recurrent neural networks are one of the most interesting types of deep learning at the moment.

They have been used to demonstrate world-class results in complex problem domains such as language translation, automatic image captioning, and text generation.

LSTMs are different to multilayer Perceptrons and convolutional neural networks in that they are designed specifically for sequence prediction problems.

In this mini-course, you will discover how you can quickly bring LSTM models to your own sequence forecasting problems.

After completing this mini-course, you will know:

What LSTMs are, how they are trained, and how to prepare data for training LSTM models.

How to develop a suite of LSTM models including stacked, bidirectional, and encoder-decoder models.

How you can get the most out of your models with hyperparameter optimization, updating, and finalizing models.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Note: This is a big guide; you may want to bookmark it.

Who Is This Mini-Course For?

Before we get started, let’s make sure you are in the right place.

This course is for developers that know some applied machine learning and need to get good at LSTMs fast.

Maybe you want or need to start using LSTMs on your project. This guide was written to help you do that quickly and efficiently.

You know your way around Python.

You know your way around SciPy.

You know how to install software on your workstation.

You know how to wrangle your own data.

You know how to work through a predictive modeling problem with machine learning.

You may know a little bit of deep learning.

You may know a little bit of Keras.

You know how to set up your workstation to use Keras and scikit-learn; if not, you can learn how to here:

This guide was written in the top-down and results-first machine learning style that you’re used to. It will teach you how to get results, but it is not a panacea.

You will develop useful skills by working through this guide.

After completing this course, you will:

Know how LSTMs work.

Know how to prepare data for LSTMs.

Know how to apply a suite of types of LSTMs.

Know how to tune LSTMs to a problem.

Know how to save an LSTM model and use it to make predictions.

Next, let’s review the lessons.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Mini-Course Overview

This mini-course is broken down into 14 lessons.

You could complete one lesson per day (recommended) or complete all of the lessons in one day (hardcore!).

It really depends on the time you have available and your level of enthusiasm.

Below are 14 lessons that will get you started and productive with LSTMs in Python. The lessons are divided into three main themes: foundations, models, and advanced.

Foundations

The focus of these lessons are the things that you need to know before using LSTMs.

Lesson 01 : What are LSTMs?

: What are LSTMs? Lesson 02 : How LSTMs are trained

: How LSTMs are trained Lesson 03 : How to prepare data for LSTMs

: How to prepare data for LSTMs Lesson 04: How to develop LSTMs in Keras

Models

Lesson 05 : How to develop Vanilla LSTMs

: How to develop Vanilla LSTMs Lesson 06 : How to develop Stacked LSTMs

: How to develop Stacked LSTMs Lesson 07 : How to develop CNN LSTMs

: How to develop CNN LSTMs Lesson 08 : How to develop Encoder-Decoder LSTMs

: How to develop Encoder-Decoder LSTMs Lesson 09 : How to develop Bi-directional LSTMs

: How to develop Bi-directional LSTMs Lesson 10 : How to develop LSTMs with Attention

: How to develop LSTMs with Attention Lesson 11: How to develop Generative LSTMs

Advanced

Lesson 12 : How to tune LSTM hyperparameters

: How to tune LSTM hyperparameters Lesson 13 : How to update LSTM models

: How to update LSTM models Lesson 14: How to make predictions with LSTMs

Each lesson could take you 60 seconds or up to 60 minutes. Take your time and complete the lessons at your own pace. Ask questions, and even post results in the comments below.

The lessons expect you to go off and find out how to do things. I will give you hints, but part of the point of each lesson is to force you to learn where to go to look for help (hint, I have all of the answers on this blog; use the search).

I do provide more help in the early lessons because I want you to build up some confidence and inertia.

Hang in there; don’t give up!

Foundations

The lessons in this section are designed to give you an understanding of how LSTMs work and how to implement LSTM models using the Keras library.

Lesson 1: What are LSTMs?

Goal

The goal of this lesson is to understand LSTMs from a high-level sufficiently so that you can explain what they are and how they work to a colleague or manager.

Questions

What is sequence prediction and what are some general examples?

What are the limitations of traditional neural networks for sequence prediction?

What is the promise of RNNs for sequence prediction?

What is the LSTM and what are its constituent parts?

What are some prominent applications of LSTMs?

Further Reading

Lesson 2: How LSTMs are trained

Goal

The goal of this lesson is to understand how LSTM models are trained on example sequences.

Questions

What common problems afflict the training of traditional RNNs?

How does the LSTM overcome these problems?

What algorithm is used to train LSTMs?

How does Backpropagation Through Time work?

What is truncated BPTT and what benefit does it offer?

How is BPTT implemented and configured in Keras?

Further Reading

Lesson 3: How to prepare data for LSTMs

Goal

The goal of this lesson is to understand how to prepare sequence prediction data for use with LSTM models.

Questions

How do you prepare numeric data for use with LSTMs?

How do you prepare categorical data for use with LSTMs?

How do you handle missing values in sequences when using LSTMs?

How do you frame a sequence as a supervised learning problem?

How do you handle long sequences when working with LSTMs?

How do you handle input sequences with different lengths?

How do you reshape input data for LSTMs in Keras?

Experiment

Demonstrate how to transform a numerical input sequence into a form suitable for training an LSTM.

Further Reading

Lesson 4: How to develop LSTMs in Keras

Goal

The goal of this lesson is to understand how to define, fit, and evaluate LSTM models using the Keras deep learning library in Python.

Questions

How do you define an LSTM Model?

How do you compile an LSTM Model?

How do you fit an LSTM Model?

How do you evaluate an LSTM Model?

How do you make predictions with an LSTM Model?

How can LSTMs be applied to different types of sequence prediction problems?

Experiment

Prepare an example that demonstrates the life-cycle of an LSTM model on a sequence prediction problem.

Further Reading

Models

The lessons in this section are designed to teach you how to get results with LSTM models on sequence prediction problems.

Lesson 5: How to develop Vanilla LSTMs

Goal

The goal of this lesson is to learn how to develop and evaluate vanilla LSTM models.

What is the vanilla LSTM architecture?

What are some examples where the vanilla LSTM has been applied?

Experiment

Design and execute an experiment that demonstrates a vanilla LSTM on a sequence prediction problem.

Further Reading

Lesson 6: How to develop Stacked LSTMs

Goal

The goal of this lesson is to learn how to develop and evaluate stacked LSTM models.

Questions

What are the difficulties in using a vanilla LSTM on a sequence problem with hierarchical structure?

What are stacked LSTMs?

What are some examples of where the stacked LSTM has been applied?

What benefits do stacked LSTMs provide?

How can a stacked LSTM be implemented in Keras?

Experiment

Design and execute an experiment that demonstrates a stacked LSTM on a sequence prediction problem with hierarchical input structure.

Further Reading

Lesson 7: How to develop CNN LSTMs

Goal

The goal of this lesson is to learn how to develop LSTM models that use a Convolutional Neural Network on the front end.

Questions

What are the difficulties of using a vanilla LSTM with spatial input data?

What is the CNN LSTM architecture?

What are some examples of the CNN LSTM?

What benefits does the CNN LSTM provide?

How can the CNN LSTM architecture be implemented in Keras?

Experiment

Design and execute an experiment that demonstrates a CNN LSTM on a sequence prediction problem with spatial input.

Further Reading

Lesson 8: How to develop Encoder-Decoder LSTMs

Goal

The goal of this lesson is to learn how to develop encoder-decoder LSTM models.

Questions

What are sequence-to-sequence (seq2seq) prediction problems?

What are the difficulties of using a vanilla LSTM on seq2seq problems?

What is the encoder-decoder LSTM architecture?

What are some examples of encoder-decoder LSTMs?

What are the benefits of encoder-decoder LSTMs?

How can encoder-decoder LSTMs be implemented in Keras?

Experiment

Design and execute an experiment that demonstrates an encoder-decoder LSTM on a sequence-to-sequence prediction problem.

Further Reading

Lesson 9: How to develop Bi-directional LSTMs

Goal

The goal of this lesson is to learn how to developer Bidirectional LSTM models.

Questions

What is a bidirectional LSTM?

What are some examples where bidirectional LSTMs have been used?

What benefit does a bidirectional LSTM offer over a vanilla LSTM?

What concerns regarding time steps does a bidirectional architecture raise?

How can bidirectional LSTMs be implemented in Keras?

Experiment

Design and execute an experiment that compares forward, backward, and bidirectional LSTM models on a sequence prediction problem.

Further Reading

Lesson 10: How to develop LSTMs with Attention

Goal

The goal of this lesson is to learn how to develop LSTM models with attention.

Questions

What impact do long sequences with neutral information have on LSTMs?

What is attention in LSTM models?

What are some examples where attention has been used in LSTMs?

What benefit does attention provide to sequence prediction?

How can an attention architecture be implemented in Keras?

Experiment

Design and execute an experiment that applies attention to a sequence prediction problem with long sequences of neutral information.

Further Reading

Lesson 11: How to develop Generative LSTMs

Goal

The goal of this lesson is to learn how to develop LSTMs for use in generative models.

What are generative models?

How can LSTMs be used as generative models?

What are some examples of LSTMs as generative models?

What benefits do LSTMs have as generative models?

Experiment

Design and execute an experiment to learn a corpus of text and generate new samples of text with the same syntax, grammar, and style.

Further Reading

Advanced

The lessons in this section are designed to teach you how to get the most from your LSTM models on your own sequence prediction problems.

Lesson 12: How to tune LSTM hyperparameters

Goal

The goal of this lesson is to learn how to tune LSTM hyperparameters.

Questions

How can we diagnose over-learning or under-learning of an LSTM model?

What are two schemes for tuning model hyperparameters?

How can model skill be reliably estimated given LSTMs are stochastic algorithms?

List LSTM hyperparameters that can be tuned, with examples of values that could be evaluated for: Model initialization and behavior. Model architecture and structure. Learning behavior.



Experiment

Design and execute an experiment to tune one hyperparameter of an LSTM and select the best configuration.

Further Reading

Lesson 13: How to update LSTM models

Goal

The goal of this lesson is to learn how to update LSTM models after new data becomes available.

Questions

What are the benefits of updating LSTM models in response to new data?

What are some schemes for updating an LSTM model with new data?

Experiment

Design and execute an experiment to fit an LSTM model to a sequence prediction problem that contrasts the effect on the model skill of different model update schemes.

Further Reading

Lesson 14: How to make predictions with LSTMs

Goal

The goal of this lesson is to learn how to finalize an LSTM model and use it to make predictions on new data.

Questions

How do you save model structure and weights in Keras?

How do you fit a final LSTM model?

How do you make a prediction with a finalized model?

Experiment

Design and execute an experiment to fit a final LSTM model, save it to file, then later load it and make a prediction on a held back validation dataset.

Further Reading

The End!

(Look How Far You Have Come)

You made it. Well done!

Take a moment and look back at how far you have come. Here is what you have learned:

What LSTMs are and why they are the go-to deep learning technique for sequence prediction. That LSTMs are trained using the BPTT algorithm which also imposes a way of thinking about your sequence prediction problem. That data preparation for sequence prediction may involve Masking missing values and splitting, padding and truncating input sequences. That Keras provides a 5-step life-cycle for LSTM models, including define, compile, fit, evaluate, and predict. That the vanilla LSTM is comprised of an input layer, a hidden LSTM layer, and a dense output layer. That hidden LSTM layers can be stacked but must expose the output of the entire sequence from layer to layer. That CNNs can be used as input layers for LSTMs when working with image and video data. That the encoder-decoder architecture can be used when predicting variable length output sequences. That providing input sequences forward and backward in bidirectional LSTMs can lift the skill on some problems. That attention can provide an optimization for long input sequences that contain neutral information. That LSTMs can learn the structured relationship of input data which in turn can be used to generate new examples. That the LSTMs hyperparameters of LSTMs can be tuned much like any other stochastic model. That fit LSTM models can be updated when new data is made available. That a final LSTM model can be saved to file and later loaded in order to make predictions on new data.

Don’t make light of this; you have come a long way in a short amount of time.

This is just the beginning of your LSTM journey with Keras. Keep practicing and developing your skills.

Summary

How Did You Do With The Mini-Course?

Did you enjoy this mini-course?

Do you have any questions? Were there any sticking points?

Let me know. Leave a comment below.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside"
274;machinelearningmastery.com;http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/;2016-08-16;A Gentle Introduction to XGBoost for Applied Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.

XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.

In this post you will discover XGBoost and get a gentle introduction to what is, where it came from and how you can learn more.

After reading this post you will know:

What XGBoost is and the goals of the project.

Why XGBoost must be apart of your machine learning toolkit.

Where you can learn more to start using XGBoost on your next machine learning project.

Discover how to configure, fit, tune and evaluation gradient boosting models with XGBoost in my new book, with 15 step-by-step tutorial lessons, and full python code.

Let’s get started.

Need help with XGBoost in Python? Take my free 7-day email course and discover configuration, tuning and more (with sample code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

What is XGBoost?

XGBoost stands for eXtreme Gradient Boosting.

The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.

— Tianqi Chen, in answer to the question “What is the difference between the R gbm (gradient boosting machine) and xgboost (extreme gradient boosting)?” on Quora

It is an implementation of gradient boosting machines created by Tianqi Chen, now with contributions from many developers. It belongs to a broader collection of tools under the umbrella of the Distributed Machine Learning Community or DMLC who are also the creators of the popular mxnet deep learning library.

Tianqi Chen provides a brief and interesting back story on the creation of XGBoost in the post Story and Lessons Behind the Evolution of XGBoost.

XGBoost is a software library that you can download and install on your machine, then access from a variety of interfaces. Specifically, XGBoost supports the following main interfaces:

Command Line Interface (CLI).

C++ (the language in which the library is written).

Python interface as well as a model in scikit-learn.

R interface as well as a model in the caret package.

Julia.

Java and JVM languages like Scala and platforms like Hadoop.

XGBoost Features

The library is laser focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features.

Model Features

The implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:

Gradient Boosting algorithm also called gradient boosting machine including the learning rate.

algorithm also called gradient boosting machine including the learning rate. Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.

with sub-sampling at the row, column and column per split levels. Regularized Gradient Boosting with both L1 and L2 regularization.

System Features

The library provides a system for use in a range of computing environments, not least:

Parallelization of tree construction using all of your CPU cores during training.

of tree construction using all of your CPU cores during training. Distributed Computing for training very large models using a cluster of machines.

for training very large models using a cluster of machines. Out-of-Core Computing for very large datasets that don’t fit into memory.

for very large datasets that don’t fit into memory. Cache Optimization of data structures and algorithm to make best use of hardware.

Algorithm Features

The implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:

Sparse Aware implementation with automatic handling of missing data values.

implementation with automatic handling of missing data values. Block Structure to support the parallelization of tree construction.

to support the parallelization of tree construction. Continued Training so that you can further boost an already fitted model on new data.

XGBoost is free open source software available for use under the permissive Apache-2 license.

Why Use XGBoost?

The two reasons to use XGBoost are also the two goals of the project:

Execution Speed. Model Performance.

1. XGBoost Execution Speed

Generally, XGBoost is fast. Really fast when compared to other implementations of gradient boosting.

Szilard Pafka performed some objective benchmarks comparing the performance of XGBoost to other implementations of gradient boosting and bagged decision trees. He wrote up his results in May 2015 in the blog post titled “Benchmarking Random Forest Implementations“.

He also provides all the code on GitHub and a more extensive report of results with hard numbers.

His results showed that XGBoost was almost always faster than the other benchmarked implementations from R, Python Spark and H2O.

From his experiment, he commented:

I also tried xgboost, a popular library for boosting which is capable to build random forests as well. It is fast, memory efficient and of high accuracy

— Szilard Pafka, Benchmarking Random Forest Implementations.

2. XGBoost Model Performance

XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems.

The evidence is that it is the go-to algorithm for competition winners on the Kaggle competitive data science platform.

For example, there is an incomplete list of first, second and third place competition winners that used titled: XGBoost: Machine Learning Challenge Winning Solutions.

To make this point more tangible, below are some insightful quotes from Kaggle competition winners:

As the winner of an increasing amount of Kaggle competitions, XGBoost showed us again to be a great all-round algorithm worth having in your toolbox.

— Dato Winners’ Interview: 1st place, Mad Professors

When in doubt, use xgboost.

— Avito Winner’s Interview: 1st place, Owen Zhang

I love single models that do well, and my best single model was an XGBoost that could get the 10th place by itself.

— Caterpillar Winners’ Interview: 1st place

I only used XGBoost.

— Liberty Mutual Property Inspection, Winner’s Interview: 1st place, Qingchen Wang

The only supervised learning method I used was gradient boosting, as implemented in the excellent xgboost package.

— Recruit Coupon Purchase Winner’s Interview: 2nd place, Halla Yang

What Algorithm Does XGBoost Use?

The XGBoost library implements the gradient boosting decision tree algorithm.

This algorithm goes by lots of different names such as gradient boosting, multiple additive regression trees, stochastic gradient boosting or gradient boosting machines.

Boosting is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made. A popular example is the AdaBoost algorithm that weights data points that are hard to predict.

Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.

This approach supports both regression and classification predictive modeling problems.

For more on boosting and gradient boosting, see Trevor Hastie’s talk on Gradient Boosting Machine Learning.

Official XGBoost Resources

The best source of information on XGBoost is the official GitHub repository for the project.

From there you can get access to the Issue Tracker and the User Group that can be used for asking questions and reporting bugs.

A great source of links with example code and help is the Awesome XGBoost page.

There is also an official documentation page that includes a getting started guide for a range of different languages, tutorials, how-to guides and more.

There are some more formal papers on XGBoost that are worth a read for more background on the library:

Talks on XGBoost

When getting started with a new tool like XGBoost, it can be helpful to review a few talks on the topic before diving into the code.

XGBoost: A Scalable Tree Boosting System

Tianqi Chen, the creator of the library gave a talk to the LA Data Science group in June 2016 titled “XGBoost: A Scalable Tree Boosting System“.

You can review the slides from his talk here:

There is more information on the DataScience LA blog.

XGBoost: eXtreme Gradient Boosting

Tong He, a contributor to XGBoost for the R interface gave a talk at the NYC Data Science Academy in December 2015 titled “XGBoost: eXtreme Gradient Boosting“.

You can review the slides from his talk here:

There is more information about this talk on the NYC Data Science Academy blog.

Installing XGBoost

There is a comprehensive installation guide on the XGBoost documentation website.

It covers installation for Linux, Mac OS X and Windows.

It also covers installation on platforms such as R and Python.

XGBoost in R

If you are an R user, the best place to get started is the CRAN page for the xgboost package.

From this page you can access the R vignette Package ‘xgboost’ [pdf].

There are also some excellent R tutorials linked from this page to get you started:

There is also the official XGBoost R Tutorial and Understand your dataset with XGBoost tutorial.

XGBoost in Python

Installation instructions are available on the Python section of the XGBoost installation guide.

The official Python Package Introduction is the best place to start when working with XGBoost in Python.

To get started quickly, you can type:

sudo pip install xgboost 1 sudo pip install xgboost

There is also an excellent list of sample source code in Python on the XGBoost Python Feature Walkthrough.

Summary

In this post you discovered the XGBoost algorithm for applied machine learning.

You learned:

That XGBoost is a library for developing fast and high performance gradient boosting tree models.

That XGBoost is achieving the best performance on a range of difficult machine learning tasks.

That you can use this library from the command line, Python and R and how to get started.

Have you used XGBoost? Share your experiences in the comments below.

Do you have any questions about XGBoost or about this post? Ask your question in the comments below and I will do my best to answer them.

Discover The Algorithm Winning Competitions! Develop Your Own XGBoost Models in Minutes ...with just a few lines of Python Discover how in my new Ebook:

XGBoost With Python It covers self-study tutorials like:

Algorithm Fundamentals, Scaling, Hyperparameters, and much more... Bring The Power of XGBoost To Your Own Projects Skip the Academics. Just Results. See What's Inside"
275;machinelearningmastery.com;https://machinelearningmastery.com/how-to-caption-photos-with-deep-learning/;2017-11-12;How to Automatically Generate Textual Descriptions for Photographs with Deep Learning;"Tweet Share Share

Last Updated on August 7, 2019

Captioning an image involves generating a human readable textual description given an image, such as a photograph.

It is an easy problem for a human, but very challenging for a machine as it involves both understanding the content of an image and how to translate this understanding into natural language.

Recently, deep learning methods have displaced classical methods and are achieving state-of-the-art results for the problem of automatically generating descriptions, called “captions,” for images.

In this post, you will discover how deep neural network models can be used to automatically generate descriptions for images, such as photographs.

After completing this post, you will know:

About the challenge of generating textual descriptions for images and the need to combine breakthroughs from computer vision and natural language processing.

About the elements that comprise a neural feature captioning model, namely the feature extractor and language model.

How the elements of the model can be arranged into an Encoder-Decoder, possibly with the use of an attention mechanism.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This post is divided into 3 parts; they are:

Describing an Image with Text Neural Captioning Model Encoder-Decoder Architecture

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

Describing an Image with Text

Describing an image is the problem of generating a human-readable textual description of an image, such as a photograph of an object or scene.

The problem is sometimes called “automatic image annotation” or “image tagging.”

It is an easy problem for a human, but very challenging for a machine.

A quick glance at an image is sufficient for a human to point out and describe an immense amount of details about the visual scene. However, this remarkable ability has proven to be an elusive task for our visual recognition models

— Deep Visual-Semantic Alignments for Generating Image Descriptions, 2015.

A solution requires both that the content of the image be understood and translated to meaning in the terms of words, and that the words must string together to be comprehensible. It combines both computer vision and natural language processing and marks a true challenging problem in broader artificial intelligence.

Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.

— Show and Tell: A Neural Image Caption Generator, 2015.

Further, the problems can range in difficulty; let’s look at three different variations on the problem with examples.

1. Classify Image

Assign an image a class label from one of hundreds or thousands of known classes.

2. Describe Image

Generate a textual description of the contents image.

3. Annotate Image

Generate textual descriptions for specific regions on the image.

The general problem can also be extended to describe images over time in video.

In this post, we will focus our attention on describing images, which we will describe as ‘image captioning.’

Neural Captioning Model

Neural network models have come to dominate the field of automatic caption generation; this is primarily because the methods are demonstrating state-of-the-art results.

The two dominant methods prior to end-to-end neural network models for generating image captions were template-based methods and nearest-neighbor-based methods and modifying existing captions.

Prior to the use of neural networks for generating captions, two main approaches were dominant. The first involved generating caption templates which were filled in based on the results of object detections and attribute discovery. The second approach was based on first retrieving similar captioned images from a large database then modifying these retrieved captions to fit the query. […] Both of these approaches have since fallen out of favour to the now dominant neural network methods.

— Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2015.

Neural network models for captioning involve two main elements:

Feature Extraction. Language Model.

Feature Extraction Model

The feature extraction model is a neural network that given an image is able to extract the salient features, often in the form of a fixed-length vector.

The extracted features are an internal representation of the image, not something directly intelligible.

A deep convolutional neural network, or CNN, is used as the feature extraction submodel. This network can be trained directly on the images in the image captioning dataset.

Alternately, a pre-trained model, such as a state-of-the-art model used for image classification, can be used, or some hybrid where a pre-trained model is used and fine tuned on the problem.

It is popular to use top performing models in the ImageNet dataset developed for the ILSVRC challenge, such as the Oxford Vision Geometry Group model, called VGG for short.

[…] we explored several techniques to deal with overfitting. The most obvious way to not overfit is to initialize the weights of the CNN component of our system to a pretrained model (e.g., on ImageNet)

— Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2015.

Language Model

Generally, a language model predicts the probability of the next word in the sequence given the words already present in the sequence.

For image captioning, the language model is a neural network that given the extracted features from the network is capable of predicting the sequence of words in the description and build up the description conditional on the words that have already been generated.

It is popular to use a recurrent neural network, such as a Long Short-Term Memory network, or LSTM, as the language model. Each output time step generates a new word in the sequence.

Each word that is generated is then encoded using a word embedding (such as word2vec) and passed as input to the decoder for generating the subsequent word.

An improvement to the model involves gathering the probability distribution of words across the vocabulary for the output sequence and searching it to generate multiple possible descriptions. These descriptions can be scored and ranked by likelihood. It is common to use a Beam Search for this search.

The language model can be trained standalone using pre-computed features extracted from the image dataset; it can be trained jointly with the feature extraction network, or some combination.

Encoder-Decoder Architecture

A popular way to structure the sub-models is to use an Encoder-Decoder architecture where both models are trained jointly.

[the model] is based on a convolution neural network that encodes an image into a compact representation, followed by a recurrent neural network that generates a corresponding sentence. The model is trained to maximize the likelihood of the sentence given the image.

— Show and Tell: A Neural Image Caption Generator, 2015.

This is an architecture developed for machine translation where an input sequence, say in French, is encoded as a fixed-length vector by an encoder network. A separate decoder network then reads the encoding and generates an output sequence in the new language, say English.

A benefit of this approach in addition to the impressive skill of the approach is that a single end-to-end model can be trained on the problem.

When adapted for image captioning, the encoder network is a deep convolutional neural network, and the decoder network is a stack of LSTM layers.

[in machine translation] An “encoder” RNN reads the source sentence and transforms it into a rich fixed-length vector representation, which in turn in used as the initial hidden state of a “decoder” RNN that generates the target sentence. Here, we propose to follow this elegant recipe, replacing the encoder RNN by a deep convolution neural network (CNN).

— Show and Tell: A Neural Image Caption Generator, 2015.

Captioning Model with Attention

A limitation of the Encoder-Decoder architecture is that a single fixed-length representation is used to hold the extracted features.

This was addressed in machine translation through the development of attention across a richer encoding, allowing the decoder to learn where to place attention as each word in the translation is generated.

The approach of attention has also been used to improve the performance of the Encoder-Decoder architecture for image captioning by allowing the decoder to learn where to put attention in the image when generating each word in the description.

Encouraged by recent advances in caption generation and inspired by recent success in employing attention in machine translation and object recognition we investigate models that can attend to salient part of an image while generating its caption.

— Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2015.

A benefit of this approach is that it is possible to visualize exactly where attention is placed while generating each word in a description.

We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence.

— Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2015.

This is easiest to understand with an example; see below.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Papers

Articles

Projects

Summary

In this post, you discovered how deep neural network models can be used to automatically generate descriptions for images, such as photographs.

Specifically, you learned:

About the challenge of generating textual descriptions for images and the need to combine breakthroughs from computer vision and natural language processing.

About the elements that comprise a neural feature captioning model, namely the feature extractor and language model.

How the elements of the model can be arranged into an Encoder-Decoder, possibly with the use of an attention mechanism.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside"
276;machinelearningmastery.com;https://machinelearningmastery.com/how-to-load-visualize-and-explore-a-complex-multivariate-multistep-time-series-forecasting-dataset/;2018-10-11;How to Load, Visualize, and Explore a Multivariate Multistep Time Series Dataset;"Tweet Share Share

Last Updated on August 5, 2019

Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites.

The EMC Data Science Global Hackathon dataset, or the ‘Air Quality Prediction‘ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days.

In this tutorial, you will discover and explore the Air Quality Prediction dataset that represents a challenging multivariate, multi-site, and multi-step time series forecasting problem.

After completing this tutorial, you will know:

How to load and explore the chunk-structure of the dataset.

How to explore and visualize the input and target variables for the dataset.

How to use the new understanding to outline a suite of methods for framing the problem, preparing the data, and modeling the dataset.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Update Apr/2019: Fixed bug in the calculation of the total missing values (thanks zhangzhe).

Tutorial Overview

This tutorial is divided into seven parts; they are:

Problem Description Load Dataset Chunk Data Structure Input Variables Target Variables A Wrinkle With Target Variables Thoughts on Modeling

Problem Description

The EMC Data Science Global Hackathon dataset, or the ‘Air Quality Prediction‘ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days.

Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next three days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period; they are:

+1, +2, +3, +4, +5, +10, +17, +24, +48, +72 1 +1, +2, +3, +4, +5, +10, +17, +24, +48, +72

Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast.

Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed.

The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012.

Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain).

A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post:

In this tutorial, we will explore this dataset in order to better understand the nature of the forecast problem and suggest approaches for how it may be modeled.

Load Dataset

The first step is to download the dataset and load it into memory.

The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset.

Download the entire dataset, e.g. “Download All” to your workstation and unzip the archive in your current working directory with the folder named ‘AirQualityPrediction‘

You should have five files in the AirQualityPrediction/ folder; they are:

SiteLocations.csv

SiteLocations_with_more_sites.csv

SubmissionZerosExceptNAs.csv

TrainingData.csv

sample_code.r

Our focus will be the ‘TrainingData.csv‘ that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables.

The test dataset (remaining three days of each chunk) is not available for this dataset at the time of writing.

Open the ‘TrainingData.csv‘ file and review the contents. The unzipped data file is relatively small (21 megabytes) and will easily fit into RAM.

Reviewing the contents of the file, we can see that the data file contains a header row.

We can also see that missing data is marked with the ‘NA‘ value, which Pandas will automatically convert to NumPy.NaN.

We can see that the ‘weekday‘ column contains the day as a string, whereas all other data is numeric.

Below are the first few lines of the data file for reference.

""rowID"",""chunkID"",""position_within_chunk"",""month_most_common"",""weekday"",""hour"",""Solar.radiation_64"",""WindDirection..Resultant_1"",""WindDirection..Resultant_1018"",""WindSpeed..Resultant_1"",""WindSpeed..Resultant_1018"",""Ambient.Max.Temperature_14"",""Ambient.Max.Temperature_22"",""Ambient.Max.Temperature_50"",""Ambient.Max.Temperature_52"",""Ambient.Max.Temperature_57"",""Ambient.Max.Temperature_76"",""Ambient.Max.Temperature_2001"",""Ambient.Max.Temperature_3301"",""Ambient.Max.Temperature_6005"",""Ambient.Min.Temperature_14"",""Ambient.Min.Temperature_22"",""Ambient.Min.Temperature_50"",""Ambient.Min.Temperature_52"",""Ambient.Min.Temperature_57"",""Ambient.Min.Temperature_76"",""Ambient.Min.Temperature_2001"",""Ambient.Min.Temperature_3301"",""Ambient.Min.Temperature_6005"",""Sample.Baro.Pressure_14"",""Sample.Baro.Pressure_22"",""Sample.Baro.Pressure_50"",""Sample.Baro.Pressure_52"",""Sample.Baro.Pressure_57"",""Sample.Baro.Pressure_76"",""Sample.Baro.Pressure_2001"",""Sample.Baro.Pressure_3301"",""Sample.Baro.Pressure_6005"",""Sample.Max.Baro.Pressure_14"",""Sample.Max.Baro.Pressure_22"",""Sample.Max.Baro.Pressure_50"",""Sample.Max.Baro.Pressure_52"",""Sample.Max.Baro.Pressure_57"",""Sample.Max.Baro.Pressure_76"",""Sample.Max.Baro.Pressure_2001"",""Sample.Max.Baro.Pressure_3301"",""Sample.Max.Baro.Pressure_6005"",""Sample.Min.Baro.Pressure_14"",""Sample.Min.Baro.Pressure_22"",""Sample.Min.Baro.Pressure_50"",""Sample.Min.Baro.Pressure_52"",""Sample.Min.Baro.Pressure_57"",""Sample.Min.Baro.Pressure_76"",""Sample.Min.Baro.Pressure_2001"",""Sample.Min.Baro.Pressure_3301"",""Sample.Min.Baro.Pressure_6005"",""target_1_57"",""target_10_4002"",""target_10_8003"",""target_11_1"",""target_11_32"",""target_11_50"",""target_11_64"",""target_11_1003"",""target_11_1601"",""target_11_4002"",""target_11_8003"",""target_14_4002"",""target_14_8003"",""target_15_57"",""target_2_57"",""target_3_1"",""target_3_50"",""target_3_57"",""target_3_1601"",""target_3_4002"",""target_3_6006"",""target_4_1"",""target_4_50"",""target_4_57"",""target_4_1018"",""target_4_1601"",""target_4_2001"",""target_4_4002"",""target_4_4101"",""target_4_6006"",""target_4_8003"",""target_5_6006"",""target_7_57"",""target_8_57"",""target_8_4002"",""target_8_6004"",""target_8_8003"",""target_9_4002"",""target_9_8003"" 1,1,1,10,""Saturday"",21,0.01,117,187,0.3,0.3,NA,NA,NA,14.9,NA,NA,NA,NA,NA,NA,NA,NA,5.8,NA,NA,NA,NA,NA,NA,NA,NA,747,NA,NA,NA,NA,NA,NA,NA,NA,750,NA,NA,NA,NA,NA,NA,NA,NA,743,NA,NA,NA,NA,NA,2.67923294292042,6.1816228132982,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,2.38965627997991,NA,5.56815355612325,0.690015329704154,NA,NA,NA,NA,NA,NA,2.84349016287551,0.0920223353681394,1.69321097077376,0.368089341472558,0.184044670736279,0.368089341472558,0.276067006104418,0.892616653070952,1.74842437199465,NA,NA,5.1306307034019,1.34160578423204,2.13879182993514,3.01375212399952,NA,5.67928016629218,NA 2,1,2,10,""Saturday"",22,0.01,231,202,0.5,0.6,NA,NA,NA,14.9,NA,NA,NA,NA,NA,NA,NA,NA,5.8,NA,NA,NA,NA,NA,NA,NA,NA,747,NA,NA,NA,NA,NA,NA,NA,NA,750,NA,NA,NA,NA,NA,NA,NA,NA,743,NA,NA,NA,NA,NA,2.67923294292042,8.47583334194495,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,1.99138023331659,NA,5.56815355612325,0.923259948195698,NA,NA,NA,NA,NA,NA,3.1011527019063,0.0920223353681394,1.94167127626774,0.368089341472558,0.184044670736279,0.368089341472558,0.368089341472558,1.73922213845783,2.14412041407765,NA,NA,5.1306307034019,1.19577906855465,2.72209869264472,3.88871241806389,NA,7.42675098668978,NA 3,1,3,10,""Saturday"",23,0.01,247,227,0.5,1.5,NA,NA,NA,14.9,NA,NA,NA,NA,NA,NA,NA,NA,5.8,NA,NA,NA,NA,NA,NA,NA,NA,747,NA,NA,NA,NA,NA,NA,NA,NA,750,NA,NA,NA,NA,NA,NA,NA,NA,743,NA,NA,NA,NA,NA,2.67923294292042,8.92192983362627,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,1.7524146053186,NA,5.56815355612325,0.680296803933673,NA,NA,NA,NA,NA,NA,3.06434376775904,0.0920223353681394,2.52141198908702,0.460111676840697,0.184044670736279,0.368089341472558,0.368089341472558,1.7852333061419,1.93246904273093,NA,NA,5.13639545700122,1.40965825154816,3.11096993445111,3.88871241806389,NA,7.68373198968942,NA 4,1,4,10,""Sunday"",0,0.01,219,218,0.2,1.2,NA,NA,NA,14,NA,NA,NA,NA,NA,NA,NA,NA,4.8,NA,NA,NA,NA,NA,NA,NA,NA,751,NA,NA,NA,NA,NA,NA,NA,NA,754,NA,NA,NA,NA,NA,NA,NA,NA,748,NA,NA,NA,NA,NA,2.67923294292042,5.09824561921501,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,2.38965627997991,NA,5.6776192223642,0.612267123540305,NA,NA,NA,NA,NA,NA,3.21157950434806,0.184044670736279,2.374176252498,0.460111676840697,0.184044670736279,0.368089341472558,0.276067006104418,1.86805340797323,2.08890701285676,NA,NA,5.21710200739181,1.47771071886428,2.04157401948354,3.20818774490271,NA,4.83124285639335,NA ... 1 2 3 4 5 6 ""rowID"",""chunkID"",""position_within_chunk"",""month_most_common"",""weekday"",""hour"",""Solar.radiation_64"",""WindDirection..Resultant_1"",""WindDirection..Resultant_1018"",""WindSpeed..Resultant_1"",""WindSpeed..Resultant_1018"",""Ambient.Max.Temperature_14"",""Ambient.Max.Temperature_22"",""Ambient.Max.Temperature_50"",""Ambient.Max.Temperature_52"",""Ambient.Max.Temperature_57"",""Ambient.Max.Temperature_76"",""Ambient.Max.Temperature_2001"",""Ambient.Max.Temperature_3301"",""Ambient.Max.Temperature_6005"",""Ambient.Min.Temperature_14"",""Ambient.Min.Temperature_22"",""Ambient.Min.Temperature_50"",""Ambient.Min.Temperature_52"",""Ambient.Min.Temperature_57"",""Ambient.Min.Temperature_76"",""Ambient.Min.Temperature_2001"",""Ambient.Min.Temperature_3301"",""Ambient.Min.Temperature_6005"",""Sample.Baro.Pressure_14"",""Sample.Baro.Pressure_22"",""Sample.Baro.Pressure_50"",""Sample.Baro.Pressure_52"",""Sample.Baro.Pressure_57"",""Sample.Baro.Pressure_76"",""Sample.Baro.Pressure_2001"",""Sample.Baro.Pressure_3301"",""Sample.Baro.Pressure_6005"",""Sample.Max.Baro.Pressure_14"",""Sample.Max.Baro.Pressure_22"",""Sample.Max.Baro.Pressure_50"",""Sample.Max.Baro.Pressure_52"",""Sample.Max.Baro.Pressure_57"",""Sample.Max.Baro.Pressure_76"",""Sample.Max.Baro.Pressure_2001"",""Sample.Max.Baro.Pressure_3301"",""Sample.Max.Baro.Pressure_6005"",""Sample.Min.Baro.Pressure_14"",""Sample.Min.Baro.Pressure_22"",""Sample.Min.Baro.Pressure_50"",""Sample.Min.Baro.Pressure_52"",""Sample.Min.Baro.Pressure_57"",""Sample.Min.Baro.Pressure_76"",""Sample.Min.Baro.Pressure_2001"",""Sample.Min.Baro.Pressure_3301"",""Sample.Min.Baro.Pressure_6005"",""target_1_57"",""target_10_4002"",""target_10_8003"",""target_11_1"",""target_11_32"",""target_11_50"",""target_11_64"",""target_11_1003"",""target_11_1601"",""target_11_4002"",""target_11_8003"",""target_14_4002"",""target_14_8003"",""target_15_57"",""target_2_57"",""target_3_1"",""target_3_50"",""target_3_57"",""target_3_1601"",""target_3_4002"",""target_3_6006"",""target_4_1"",""target_4_50"",""target_4_57"",""target_4_1018"",""target_4_1601"",""target_4_2001"",""target_4_4002"",""target_4_4101"",""target_4_6006"",""target_4_8003"",""target_5_6006"",""target_7_57"",""target_8_57"",""target_8_4002"",""target_8_6004"",""target_8_8003"",""target_9_4002"",""target_9_8003"" 1,1,1,10,""Saturday"",21,0.01,117,187,0.3,0.3,NA,NA,NA,14.9,NA,NA,NA,NA,NA,NA,NA,NA,5.8,NA,NA,NA,NA,NA,NA,NA,NA,747,NA,NA,NA,NA,NA,NA,NA,NA,750,NA,NA,NA,NA,NA,NA,NA,NA,743,NA,NA,NA,NA,NA,2.67923294292042,6.1816228132982,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,2.38965627997991,NA,5.56815355612325,0.690015329704154,NA,NA,NA,NA,NA,NA,2.84349016287551,0.0920223353681394,1.69321097077376,0.368089341472558,0.184044670736279,0.368089341472558,0.276067006104418,0.892616653070952,1.74842437199465,NA,NA,5.1306307034019,1.34160578423204,2.13879182993514,3.01375212399952,NA,5.67928016629218,NA 2,1,2,10,""Saturday"",22,0.01,231,202,0.5,0.6,NA,NA,NA,14.9,NA,NA,NA,NA,NA,NA,NA,NA,5.8,NA,NA,NA,NA,NA,NA,NA,NA,747,NA,NA,NA,NA,NA,NA,NA,NA,750,NA,NA,NA,NA,NA,NA,NA,NA,743,NA,NA,NA,NA,NA,2.67923294292042,8.47583334194495,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,1.99138023331659,NA,5.56815355612325,0.923259948195698,NA,NA,NA,NA,NA,NA,3.1011527019063,0.0920223353681394,1.94167127626774,0.368089341472558,0.184044670736279,0.368089341472558,0.368089341472558,1.73922213845783,2.14412041407765,NA,NA,5.1306307034019,1.19577906855465,2.72209869264472,3.88871241806389,NA,7.42675098668978,NA 3,1,3,10,""Saturday"",23,0.01,247,227,0.5,1.5,NA,NA,NA,14.9,NA,NA,NA,NA,NA,NA,NA,NA,5.8,NA,NA,NA,NA,NA,NA,NA,NA,747,NA,NA,NA,NA,NA,NA,NA,NA,750,NA,NA,NA,NA,NA,NA,NA,NA,743,NA,NA,NA,NA,NA,2.67923294292042,8.92192983362627,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,1.7524146053186,NA,5.56815355612325,0.680296803933673,NA,NA,NA,NA,NA,NA,3.06434376775904,0.0920223353681394,2.52141198908702,0.460111676840697,0.184044670736279,0.368089341472558,0.368089341472558,1.7852333061419,1.93246904273093,NA,NA,5.13639545700122,1.40965825154816,3.11096993445111,3.88871241806389,NA,7.68373198968942,NA 4,1,4,10,""Sunday"",0,0.01,219,218,0.2,1.2,NA,NA,NA,14,NA,NA,NA,NA,NA,NA,NA,NA,4.8,NA,NA,NA,NA,NA,NA,NA,NA,751,NA,NA,NA,NA,NA,NA,NA,NA,754,NA,NA,NA,NA,NA,NA,NA,NA,748,NA,NA,NA,NA,NA,2.67923294292042,5.09824561921501,NA,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,0.114975168664303,NA,2.38965627997991,NA,5.6776192223642,0.612267123540305,NA,NA,NA,NA,NA,NA,3.21157950434806,0.184044670736279,2.374176252498,0.460111676840697,0.184044670736279,0.368089341472558,0.276067006104418,1.86805340797323,2.08890701285676,NA,NA,5.21710200739181,1.47771071886428,2.04157401948354,3.20818774490271,NA,4.83124285639335,NA ...

We can load the data file into memory using the Pandas read_csv() function and specify the header row on line 0.

# load dataset dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) 1 2 # load dataset dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 )

We can also get a quick idea of how much missing data there is in the dataset. We can do that by first trimming the first few columns to remove the string weekday data and convert the remaining columns to floating point values.

# trim and transform to floats values = dataset.values data = values[:, 6:].astype('float32') 1 2 3 # trim and transform to floats values = dataset . values data = values [ : , 6 : ] . astype ( 'float32' )

We can then calculate the total number of missing observations and the percentage of values that are missing.

# summarize amount of missing data total_missing = count_nonzero(isnan(data)) percent_missing = total_missing / data.size * 100 print('Total Missing: %d/%d (%.1f%%)' % (total_missing, data.size, percent_missing)) 1 2 3 4 # summarize amount of missing data total_missing = count_nonzero ( isnan ( data ) ) percent_missing = total_missing / data . size * 100 print ( 'Total Missing: %d/%d (%.1f%%)' % ( total_missing , data . size , percent_missing ) )

The complete example is listed below.

# load dataset from numpy import isnan from numpy import count_nonzero from pandas import read_csv # load dataset dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # summarize print(dataset.shape) # trim and transform to floats values = dataset.values data = values[:, 6:].astype('float32') # summarize amount of missing data total_missing = count_nonzero(isnan(data)) percent_missing = total_missing / data.size * 100 print('Total Missing: %d/%d (%.1f%%)' % (total_missing, data.size, percent_missing)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # load dataset from numpy import isnan from numpy import count_nonzero from pandas import read_csv # load dataset dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # summarize print ( dataset . shape ) # trim and transform to floats values = dataset . values data = values [ : , 6 : ] . astype ( 'float32' ) # summarize amount of missing data total_missing = count_nonzero ( isnan ( data ) ) percent_missing = total_missing / data . size * 100 print ( 'Total Missing: %d/%d (%.1f%%)' % ( total_missing , data . size , percent_missing ) )

Running the example first prints the shape of the loaded dataset.

We can see that we have about 37,000 rows and 95 columns. We know these numbers are misleading given that the data is in fact divided into chunks and the columns are divided into the same observations at different sites.

We can also see that a little over 40% of the data is missing. This is a lot. The data is very patchy and we are going to have to understand this well before modeling the problem.

(37821, 95) Total Missing: 1922092/3366069 (57.1%) 1 2 (37821, 95) Total Missing: 1922092/3366069 (57.1%)

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Chunk Data Structure

A good starting point is to look at the data in terms of the chunks.

Chunk Durations

We can group data by the ‘chunkID’ variable (column index 1).

If each chunk is eight days and the observations are hourly, then we would expect (8 * 24) or 192 rows of data per chunk.

If there are 37,821 rows of data, then there must be chunks with more or less than 192 hours as 37,821/192 is about 196.9 chunks.

Let’s first split the data into chunks. We can first get a list of the unique chunk identifiers.

chunk_ids = unique(values[:, 1]) 1 chunk_ids = unique ( values [ : , 1 ] )

We can then collect all rows for each chunk identifier and store them in a dictionary for easy access.

chunks = dict() # sort rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] 1 2 3 4 5 chunks = dict ( ) # sort rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ]

Below defines a function named to_chunks() that takes a NumPy array of the loaded data and returns a dictionary of chunk_id to rows for the chunk.

# split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks 1 2 3 4 5 6 7 8 9 10 # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks

The ‘position_within_chunk‘ in the data file indicates the order of a row within a chunk. At this stage, we assume that rows are already ordered and do not need to be sorted. A skim of the raw data file seems to confirm this assumption.

Once the data is sorted into chunks, we can calculate the number of rows in each chunk and have a look at the distribution, such as with a box and whisker plot.

# plot distribution of chunk durations def plot_chunk_durations(chunks): # chunk durations in hours chunk_durations = [len(v) for k,v in chunks.items()] # boxplot pyplot.subplot(2, 1, 1) pyplot.boxplot(chunk_durations) # histogram pyplot.subplot(2, 1, 2) pyplot.hist(chunk_durations) # histogram pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 # plot distribution of chunk durations def plot_chunk_durations ( chunks ) : # chunk durations in hours chunk_durations = [ len ( v ) for k , v in chunks . items ( ) ] # boxplot pyplot . subplot ( 2 , 1 , 1 ) pyplot . boxplot ( chunk_durations ) # histogram pyplot . subplot ( 2 , 1 , 2 ) pyplot . hist ( chunk_durations ) # histogram pyplot . show ( )

The complete example that ties all of this together is listed below

# split data into chunks from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # plot distribution of chunk durations def plot_chunk_durations(chunks): # chunk durations in hours chunk_durations = [len(v) for k,v in chunks.items()] # boxplot pyplot.subplot(2, 1, 1) pyplot.boxplot(chunk_durations) # histogram pyplot.subplot(2, 1, 2) pyplot.hist(chunk_durations) # histogram pyplot.show() # load dataset dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) print('Total Chunks: %d' % len(chunks)) # plot chunk durations plot_chunk_durations(chunks) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # split data into chunks from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # plot distribution of chunk durations def plot_chunk_durations ( chunks ) : # chunk durations in hours chunk_durations = [ len ( v ) for k , v in chunks . items ( ) ] # boxplot pyplot . subplot ( 2 , 1 , 1 ) pyplot . boxplot ( chunk_durations ) # histogram pyplot . subplot ( 2 , 1 , 2 ) pyplot . hist ( chunk_durations ) # histogram pyplot . show ( ) # load dataset dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) print ( 'Total Chunks: %d' % len ( chunks ) ) # plot chunk durations plot_chunk_durations ( chunks )

Running the example first prints the number of chunks in the dataset.

We can see that there are 208, which suggests that indeed the number of hourly observations must vary across the chunks.

Total Chunks: 208 1 Total Chunks: 208

A box and whisker plot and a histogram plot of chunk durations is created. We can see that indeed the median is 192, meaning that most chunks have eight days of observations or close to it.

We can also see a long tail of durations down to about 25 rows. Although there are not many of these cases, we would expect that will be challenging to forecast given the lack of data.

The distribution also raises questions about how contiguous the observations within each chunk may be.

Chunk Contiguousness

It may be helpful to get an idea of how contiguous (or not) the observations are within those chunks that do not have the full eight days of data.

One approach to considering this is to create a line plot for each discontiguous chunk and show the gaps in the observations.

We can do this on a single plot. Each chunk has a unique identifier, from 1 to 208, and we can use this as the value for the series and mark missing observations within the eight day interval via NaN values that will not appear on the plot.

Inverting this, we can assume that we have NaN values for all time steps within a chunk, then use the ‘position_within_chunk‘ column (index 2) to determine the time steps that do have values and mark them with the chunk id.

The plot_discontinuous_chunks() below implements this behavior, creating one series or line for each chunk with missing rows all on the same plot. The expectation is that breaks in the line will help us see how contiguous or discontiguous these incomplete chunks happen to be.

# plot chunks that do not have all data def plot_discontiguous_chunks(chunks, row_in_chunk_ix=2): n_steps = 8 * 24 for c_id,rows in chunks.items(): # skip chunks with all data if rows.shape[0] == n_steps: continue # create empty series series = [nan for _ in range(n_steps)] # mark all rows with data for row in rows: # convert to zero offset r_id = row[row_in_chunk_ix] - 1 # mark value series[r_id] = c_id # plot pyplot.plot(series) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # plot chunks that do not have all data def plot_discontiguous_chunks ( chunks , row_in_chunk_ix = 2 ) : n_steps = 8 * 24 for c_id , rows in chunks . items ( ) : # skip chunks with all data if rows . shape [ 0 ] == n_steps : continue # create empty series series = [ nan for _ in range ( n_steps ) ] # mark all rows with data for row in rows : # convert to zero offset r_id = row [ row_in_chunk_ix ] - 1 # mark value series [ r_id ] = c_id # plot pyplot . plot ( series ) pyplot . show ( )

The complete example is listed below.

# plot discontiguous chunks from numpy import nan from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # plot chunks that do not have all data def plot_discontiguous_chunks(chunks, row_in_chunk_ix=2): n_steps = 8 * 24 for c_id,rows in chunks.items(): # skip chunks with all data if rows.shape[0] == n_steps: continue # create empty series series = [nan for _ in range(n_steps)] # mark all rows with data for row in rows: # convert to zero offset r_id = row[row_in_chunk_ix] - 1 # mark value series[r_id] = c_id # plot pyplot.plot(series) pyplot.show() # load dataset dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) # plot discontiguous chunks plot_discontiguous_chunks(chunks) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # plot discontiguous chunks from numpy import nan from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # plot chunks that do not have all data def plot_discontiguous_chunks ( chunks , row_in_chunk_ix = 2 ) : n_steps = 8 * 24 for c_id , rows in chunks . items ( ) : # skip chunks with all data if rows . shape [ 0 ] == n_steps : continue # create empty series series = [ nan for _ in range ( n_steps ) ] # mark all rows with data for row in rows : # convert to zero offset r_id = row [ row_in_chunk_ix ] - 1 # mark value series [ r_id ] = c_id # plot pyplot . plot ( series ) pyplot . show ( ) # load dataset dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) # plot discontiguous chunks plot_discontiguous_chunks ( chunks )

Running the example creates a single figure with one line for each of the chunks with missing data.

The number and lengths of the breaks in the line for each chunk give an idea of how discontiguous the observations within each chunk happen to be.

Many of the chunks do have long stretches of contiguous data, which is a good sign for modeling.

There are cases where chunks have very few observations and those observations that are present are in small contiguous patches. These may be challenging to model.

Further, not all of these chunks have observations at the end of chunk: the period right before a forecast is required. These specifically will be a challenge for those models that seek to persist recent observations.

The discontinuous nature of the series data within the chunks will also make it challenging to evaluate models. For example, one cannot simply split chunk data in half, train on the first half and test on the second when the observations are patchy. At least, when the incomplete chunk data is considered.

Daily Coverage Within Chunks

The discontiguous nature of the chunks also suggests that it may be important to look at the hours covered by each chunk.

The time of day is important in environmental data, and models that assume that each chunk covers the same daily or weekly cycle may stumble if the start and end time of day vary across chunks.

We can quickly check this by plotting the distribution of the first hour (in a 24 hour day) of each chunk.

The number of bins in the histogram is set to 24 so we can clearly see the distribution for each hour of the day in 24-hour time.

Further, when collecting the first hour of the chunk, we are careful to only collect it from those chunks that have all eight days of data, in case a chunk with missing data does not have observations at the beginning of the chunk, which we know happens.

# plot distribution of chunk start hour def plot_chunk_start_hour(chunks, hour_in_chunk_ix=5): # chunk start hour chunk_start_hours = [v[0, hour_in_chunk_ix] for k,v in chunks.items() if len(v)==192] # boxplot pyplot.subplot(2, 1, 1) pyplot.boxplot(chunk_start_hours) # histogram pyplot.subplot(2, 1, 2) pyplot.hist(chunk_start_hours, bins=24) # histogram pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 # plot distribution of chunk start hour def plot_chunk_start_hour ( chunks , hour_in_chunk_ix = 5 ) : # chunk start hour chunk_start_hours = [ v [ 0 , hour_in_chunk_ix ] for k , v in chunks . items ( ) if len ( v ) == 192 ] # boxplot pyplot . subplot ( 2 , 1 , 1 ) pyplot . boxplot ( chunk_start_hours ) # histogram pyplot . subplot ( 2 , 1 , 2 ) pyplot . hist ( chunk_start_hours , bins = 24 ) # histogram pyplot . show ( )

The complete example is listed below.

# plot distribution of chunk start hour from numpy import nan from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # plot distribution of chunk start hour def plot_chunk_start_hour(chunks, hour_in_chunk_ix=5): # chunk start hour chunk_start_hours = [v[0, hour_in_chunk_ix] for k,v in chunks.items() if len(v)==192] # boxplot pyplot.subplot(2, 1, 1) pyplot.boxplot(chunk_start_hours) # histogram pyplot.subplot(2, 1, 2) pyplot.hist(chunk_start_hours, bins=24) # histogram pyplot.show() # load dataset dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) # plot distribution of chunk start hour plot_chunk_start_hour(chunks) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # plot distribution of chunk start hour from numpy import nan from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # plot distribution of chunk start hour def plot_chunk_start_hour ( chunks , hour_in_chunk_ix = 5 ) : # chunk start hour chunk_start_hours = [ v [ 0 , hour_in_chunk_ix ] for k , v in chunks . items ( ) if len ( v ) == 192 ] # boxplot pyplot . subplot ( 2 , 1 , 1 ) pyplot . boxplot ( chunk_start_hours ) # histogram pyplot . subplot ( 2 , 1 , 2 ) pyplot . hist ( chunk_start_hours , bins = 24 ) # histogram pyplot . show ( ) # load dataset dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) # plot distribution of chunk start hour plot_chunk_start_hour ( chunks )

Running the example creates a box and whisker plot and a histogram of the first hour within each chunk.

We can see a reasonably uniform distribution of the start time across the 24 hours in the day.

Further, this means that the interval to be forecast for each chunk will also vary across the 24 hour period. This adds a wrinkle for models that might expect a standard three day forecast period (midnight to midnight).

Now that we have some idea of the chunk-structure of the data, let’s take a closer look at the input variables that describe the meteorological observations.

Input Variables

There are 56 input variables.

The first six (indexes 0 to 5) are metadata information for the chunk and time of the observations. They are:

rowID chunkID position_within_chunk month_most_common weekday hour 1 2 3 4 5 6 rowID chunkID position_within_chunk month_most_common weekday hour

The remaining 50 describe meteorological information for specific sites; they are:

Solar.radiation_64 WindDirection..Resultant_1 WindDirection..Resultant_1018 WindSpeed..Resultant_1 WindSpeed..Resultant_1018 Ambient.Max.Temperature_14 Ambient.Max.Temperature_22 Ambient.Max.Temperature_50 Ambient.Max.Temperature_52 Ambient.Max.Temperature_57 Ambient.Max.Temperature_76 Ambient.Max.Temperature_2001 Ambient.Max.Temperature_3301 Ambient.Max.Temperature_6005 Ambient.Min.Temperature_14 Ambient.Min.Temperature_22 Ambient.Min.Temperature_50 Ambient.Min.Temperature_52 Ambient.Min.Temperature_57 Ambient.Min.Temperature_76 Ambient.Min.Temperature_2001 Ambient.Min.Temperature_3301 Ambient.Min.Temperature_6005 Sample.Baro.Pressure_14 Sample.Baro.Pressure_22 Sample.Baro.Pressure_50 Sample.Baro.Pressure_52 Sample.Baro.Pressure_57 Sample.Baro.Pressure_76 Sample.Baro.Pressure_2001 Sample.Baro.Pressure_3301 Sample.Baro.Pressure_6005 Sample.Max.Baro.Pressure_14 Sample.Max.Baro.Pressure_22 Sample.Max.Baro.Pressure_50 Sample.Max.Baro.Pressure_52 Sample.Max.Baro.Pressure_57 Sample.Max.Baro.Pressure_76 Sample.Max.Baro.Pressure_2001 Sample.Max.Baro.Pressure_3301 Sample.Max.Baro.Pressure_6005 Sample.Min.Baro.Pressure_14 Sample.Min.Baro.Pressure_22 Sample.Min.Baro.Pressure_50 Sample.Min.Baro.Pressure_52 Sample.Min.Baro.Pressure_57 Sample.Min.Baro.Pressure_76 Sample.Min.Baro.Pressure_2001 Sample.Min.Baro.Pressure_3301 Sample.Min.Baro.Pressure_6005 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Solar.radiation_64 WindDirection..Resultant_1 WindDirection..Resultant_1018 WindSpeed..Resultant_1 WindSpeed..Resultant_1018 Ambient.Max.Temperature_14 Ambient.Max.Temperature_22 Ambient.Max.Temperature_50 Ambient.Max.Temperature_52 Ambient.Max.Temperature_57 Ambient.Max.Temperature_76 Ambient.Max.Temperature_2001 Ambient.Max.Temperature_3301 Ambient.Max.Temperature_6005 Ambient.Min.Temperature_14 Ambient.Min.Temperature_22 Ambient.Min.Temperature_50 Ambient.Min.Temperature_52 Ambient.Min.Temperature_57 Ambient.Min.Temperature_76 Ambient.Min.Temperature_2001 Ambient.Min.Temperature_3301 Ambient.Min.Temperature_6005 Sample.Baro.Pressure_14 Sample.Baro.Pressure_22 Sample.Baro.Pressure_50 Sample.Baro.Pressure_52 Sample.Baro.Pressure_57 Sample.Baro.Pressure_76 Sample.Baro.Pressure_2001 Sample.Baro.Pressure_3301 Sample.Baro.Pressure_6005 Sample.Max.Baro.Pressure_14 Sample.Max.Baro.Pressure_22 Sample.Max.Baro.Pressure_50 Sample.Max.Baro.Pressure_52 Sample.Max.Baro.Pressure_57 Sample.Max.Baro.Pressure_76 Sample.Max.Baro.Pressure_2001 Sample.Max.Baro.Pressure_3301 Sample.Max.Baro.Pressure_6005 Sample.Min.Baro.Pressure_14 Sample.Min.Baro.Pressure_22 Sample.Min.Baro.Pressure_50 Sample.Min.Baro.Pressure_52 Sample.Min.Baro.Pressure_57 Sample.Min.Baro.Pressure_76 Sample.Min.Baro.Pressure_2001 Sample.Min.Baro.Pressure_3301 Sample.Min.Baro.Pressure_6005

Really, there are only eight meteorological input variables:

Solar.radiation WindDirection..Resultant WindSpeed..Resultant Ambient.Max.Temperature Ambient.Min.Temperature Sample.Baro.Pressure Sample.Max.Baro.Pressure Sample.Min.Baro.Pressure 1 2 3 4 5 6 7 8 Solar.radiation WindDirection..Resultant WindSpeed..Resultant Ambient.Max.Temperature Ambient.Min.Temperature Sample.Baro.Pressure Sample.Max.Baro.Pressure Sample.Min.Baro.Pressure

These variables are recorded across 23 unique sites; they are:

1, 14, 22, 50, 52, 57, 64, 76, 1018, 2001, 3301, 6005 1 1, 14, 22, 50, 52, 57, 64, 76, 1018, 2001, 3301, 6005

The data is beautifully complex.

Not all variables are recorded at all sites.

There is some overlap in the site identifiers used in the target variables, such as 1, 50, 64, etc.

There are site identifiers used in the target variables that are not used in the input variables, such as 4002. There are also site identifiers that are used in the input that are not used in the target identifiers, such as 15.

This suggests, at the very least, that not all variables are recorded at all locations. That recording stations are heterogeneous across sites. Further, there might be something special about sites that only collect measures of a given type or collect all measurements.

Let’s take a closer look at the data for the input variables.

Temporal Structure of Inputs for a Chunk

We can start off by looking at the structure and distribution of inputs per chunk.

The first few chunks that have all eight days of observations have the chunkId of 1, 3, and 5.

We can enumerate all of the input columns and create one line plot for each. This will create a time series line plot for each input variable to give a rough idea of how each varies across time.

We can repeat this for a few chunks to get an idea how the temporal structure may differ across chunks.

The function below named plot_chunk_inputs() takes the data in chunk format and a list of chunk ids to plot. It will create a figure with 50 line plots, one for each input variable, and n lines per plot, one for each chunk.

# plot all inputs for one or more chunk ids def plot_chunk_inputs(chunks, c_ids): pyplot.figure() inputs = range(6, 56) for i in range(len(inputs)): ax = pyplot.subplot(len(inputs), 1, i+1) ax.set_xticklabels([]) ax.set_yticklabels([]) column = inputs[i] for chunk_id in c_ids: rows = chunks[chunk_id] pyplot.plot(rows[:,column]) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 # plot all inputs for one or more chunk ids def plot_chunk_inputs ( chunks , c_ids ) : pyplot . figure ( ) inputs = range ( 6 , 56 ) for i in range ( len ( inputs ) ) : ax = pyplot . subplot ( len ( inputs ) , 1 , i + 1 ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) column = inputs [ i ] for chunk_id in c_ids : rows = chunks [ chunk_id ] pyplot . plot ( rows [ : , column ] ) pyplot . show ( )

The complete example is listed below.

# plot inputs for a chunk from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # plot all inputs for one or more chunk ids def plot_chunk_inputs(chunks, c_ids): pyplot.figure() inputs = range(6, 56) for i in range(len(inputs)): ax = pyplot.subplot(len(inputs), 1, i+1) ax.set_xticklabels([]) ax.set_yticklabels([]) column = inputs[i] for chunk_id in c_ids: rows = chunks[chunk_id] pyplot.plot(rows[:,column]) pyplot.show() # load data dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) # plot inputs for some chunks plot_chunk_inputs(chunks, [1]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # plot inputs for a chunk from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # plot all inputs for one or more chunk ids def plot_chunk_inputs ( chunks , c_ids ) : pyplot . figure ( ) inputs = range ( 6 , 56 ) for i in range ( len ( inputs ) ) : ax = pyplot . subplot ( len ( inputs ) , 1 , i + 1 ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) column = inputs [ i ] for chunk_id in c_ids : rows = chunks [ chunk_id ] pyplot . plot ( rows [ : , column ] ) pyplot . show ( ) # load data dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) # plot inputs for some chunks plot_chunk_inputs ( chunks , [ 1 ] )

Running the example creates a single figure with 50 line plots, one for each of the meteorological input variables.

The plots are hard to see, so you may want to increase the size of the created figure.

We can see that the observations for the first five variables look pretty complete; these are solar radiation, wind speed, and wind direction. The rest of the variables appear pretty patchy, at least for this chunk.

We can update the example and plot the input variables for the first three chunks with the full eight days of observations.

plot_chunk_inputs(chunks, [1, 3 ,5]) 1 plot_chunk_inputs ( chunks , [ 1 , 3 , 5 ] )

Running the example creates the same 50 line plots, each with three series or lines per plot, one for each chunk.

Again, the figure makes the individual plots hard to see, so you may need to increase the size of the figure to better review the patterns.

We can see that these three figures do show similar structures within each line plot. This is helpful finding as it suggests that it may be useful to model the same variables across multiple chunks.

It does raise the question as to whether the distribution of the variables differs greatly across sites.

Input Data Distribution

We can look at the distribution of input variables crudely using box and whisker plots.

The plot_chunk_input_boxplots() below will create one box and whisker per input feature for the data for one chunk.

# boxplot for input variables for a chuck def plot_chunk_input_boxplots(chunks, c_id): rows = chunks[c_id] pyplot.boxplot(rows[:,6:56]) pyplot.show() 1 2 3 4 5 # boxplot for input variables for a chuck def plot_chunk_input_boxplots ( chunks , c_id ) : rows = chunks [ c_id ] pyplot . boxplot ( rows [ : , 6 : 56 ] ) pyplot . show ( )

The complete example is listed below.

# boxplots of inputs for a chunk from numpy import unique from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # boxplot for input variables for a chuck def plot_chunk_input_boxplots(chunks, c_id): rows = chunks[c_id] pyplot.boxplot(rows[:,6:56]) pyplot.show() # load data dataset = read_csv('TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) # boxplot for input variables plot_chunk_input_boxplots(chunks, 1) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # boxplots of inputs for a chunk from numpy import unique from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # boxplot for input variables for a chuck def plot_chunk_input_boxplots ( chunks , c_id ) : rows = chunks [ c_id ] pyplot . boxplot ( rows [ : , 6 : 56 ] ) pyplot . show ( ) # load data dataset = read_csv ( 'TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) # boxplot for input variables plot_chunk_input_boxplots ( chunks , 1 )

Running the example creates 50 boxplots, one for each input variable for the observations in the first chunk in the training dataset.

We can see that variables of the same type may have the same spread of observations, and each group of variables appears to have differing units. Perhaps degrees for wind direction, hectopascales for pressure, degrees Celsius for temperature, and so on.

It may be interesting to further investigate the distribution and spread of observations for each of the eight variable types. This is left as a further exercise.

We have some rough ideas about the input variables, and perhaps they may be useful in predicting the target variables. We cannot be sure.

We can now turn our attention to the target variables.

Target Variables

The goal of the forecast problem is to predict multiple variables across multiple sites for three days.

There are 39 time series variables to predict.

From the column header, they are:

""target_1_57"",""target_10_4002"",""target_10_8003"",""target_11_1"",""target_11_32"",""target_11_50"",""target_11_64"",""target_11_1003"",""target_11_1601"",""target_11_4002"",""target_11_8003"",""target_14_4002"",""target_14_8003"",""target_15_57"",""target_2_57"",""target_3_1"",""target_3_50"",""target_3_57"",""target_3_1601"",""target_3_4002"",""target_3_6006"",""target_4_1"",""target_4_50"",""target_4_57"",""target_4_1018"",""target_4_1601"",""target_4_2001"",""target_4_4002"",""target_4_4101"",""target_4_6006"",""target_4_8003"",""target_5_6006"",""target_7_57"",""target_8_57"",""target_8_4002"",""target_8_6004"",""target_8_8003"",""target_9_4002"",""target_9_8003"" 1 ""target_1_57"",""target_10_4002"",""target_10_8003"",""target_11_1"",""target_11_32"",""target_11_50"",""target_11_64"",""target_11_1003"",""target_11_1601"",""target_11_4002"",""target_11_8003"",""target_14_4002"",""target_14_8003"",""target_15_57"",""target_2_57"",""target_3_1"",""target_3_50"",""target_3_57"",""target_3_1601"",""target_3_4002"",""target_3_6006"",""target_4_1"",""target_4_50"",""target_4_57"",""target_4_1018"",""target_4_1601"",""target_4_2001"",""target_4_4002"",""target_4_4101"",""target_4_6006"",""target_4_8003"",""target_5_6006"",""target_7_57"",""target_8_57"",""target_8_4002"",""target_8_6004"",""target_8_8003"",""target_9_4002"",""target_9_8003""

The naming convention for these column headers is:

target_[variable identifier]_[site identifier]] 1 target_[variable identifier]_[site identifier]]

We can convert these column headers into a small dataset of variable ids and site ids with a little regex.

The results are as follows:

var, site 1,57 10,4002 10,8003 11,1 11,32 11,50 11,64 11,1003 11,1601 11,4002 11,8003 14,4002 14,8003 15,57 2,57 3,1 3,50 3,57 3,1601 3,4002 3,6006 4,1 4,50 4,57 4,1018 4,1601 4,2001 4,4002 4,4101 4,6006 4,8003 5,6006 7,57 8,57 8,4002 8,6004 8,8003 9,4002 9,8003 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 var, site 1,57 10,4002 10,8003 11,1 11,32 11,50 11,64 11,1003 11,1601 11,4002 11,8003 14,4002 14,8003 15,57 2,57 3,1 3,50 3,57 3,1601 3,4002 3,6006 4,1 4,50 4,57 4,1018 4,1601 4,2001 4,4002 4,4101 4,6006 4,8003 5,6006 7,57 8,57 8,4002 8,6004 8,8003 9,4002 9,8003

Helpfully, the targets are grouped by variable id.

We can see that one variable may have to be predicted across multiple sites; for example, variable 11 predicted at sites 1, 32, 50, and so on:

var, site 11,1 11,32 11,50 11,64 11,1003 11,1601 11,4002 11,8003 1 2 3 4 5 6 7 8 9 var, site 11,1 11,32 11,50 11,64 11,1003 11,1601 11,4002 11,8003

We can see that different variables may need to be predicted for a given site. For example, site 50 requires variables 11, 3, and 4:

var, site 11,50 3,50 4,50 1 2 3 4 var, site 11,50 3,50 4,50

We can save the small dataset of targets to a file called ‘targets.txt‘ and load it up for some quick analysis.

# summarize targets from numpy import unique from pandas import read_csv # load dataset dataset = read_csv('targets.txt', header=0) values = dataset.values # summarize unique print('Unique Variables: %d' % len(unique(values[:, 0]))) print('Unique Sites: %d' % len(unique(values[:, 1]))) 1 2 3 4 5 6 7 8 9 # summarize targets from numpy import unique from pandas import read_csv # load dataset dataset = read_csv ( 'targets.txt' , header = 0 ) values = dataset . values # summarize unique print ( 'Unique Variables: %d' % len ( unique ( values [ : , 0 ] ) ) ) print ( 'Unique Sites: %d' % len ( unique ( values [ : , 1 ] ) ) )

Running the example prints the number of unique variables and sites.

We can see that 39 target variables is far less than (12*14) 168 if we were predicting all variables for all sites.

Unique Variables: 12 Unique Sites: 14 1 2 Unique Variables: 12 Unique Sites: 14

Let’s take a closer look at the data for the target variables.

Temporal Structure of Targets for a Chunk

We can start off by looking at the structure and distribution of targets per chunk.

The first few chunks that have all eight days of observations have the chunkId of 1, 3, and 5.

We can enumerate all of the target columns and create one line plot for each. This will create a time series line plot for each target variable to give a rough idea of how it varies across time.

We can repeat this for a few chunks to get a rough idea of how the temporal structure may vary across chunks.

The function below, named plot_chunk_targets(), takes the data in chunk format and a list of chunk ids to plot. It will create a figure with 39 line plots, one for each target variable, and n lines per plot, one for each chunk.

# plot all targets for one or more chunk ids def plot_chunk_targets(chunks, c_ids): pyplot.figure() targets = range(56, 95) for i in range(len(targets)): ax = pyplot.subplot(len(targets), 1, i+1) ax.set_xticklabels([]) ax.set_yticklabels([]) column = targets[i] for chunk_id in c_ids: rows = chunks[chunk_id] pyplot.plot(rows[:,column]) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 # plot all targets for one or more chunk ids def plot_chunk_targets ( chunks , c_ids ) : pyplot . figure ( ) targets = range ( 56 , 95 ) for i in range ( len ( targets ) ) : ax = pyplot . subplot ( len ( targets ) , 1 , i + 1 ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) column = targets [ i ] for chunk_id in c_ids : rows = chunks [ chunk_id ] pyplot . plot ( rows [ : , column ] ) pyplot . show ( )

The complete example is listed below.

# plot targets for a chunk from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # plot all targets for one or more chunk ids def plot_chunk_targets(chunks, c_ids): pyplot.figure() targets = range(56, 95) for i in range(len(targets)): ax = pyplot.subplot(len(targets), 1, i+1) ax.set_xticklabels([]) ax.set_yticklabels([]) column = targets[i] for chunk_id in c_ids: rows = chunks[chunk_id] pyplot.plot(rows[:,column]) pyplot.show() # load data dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) # plot targets for some chunks plot_chunk_targets(chunks, [1]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # plot targets for a chunk from numpy import unique from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # plot all targets for one or more chunk ids def plot_chunk_targets ( chunks , c_ids ) : pyplot . figure ( ) targets = range ( 56 , 95 ) for i in range ( len ( targets ) ) : ax = pyplot . subplot ( len ( targets ) , 1 , i + 1 ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) column = targets [ i ] for chunk_id in c_ids : rows = chunks [ chunk_id ] pyplot . plot ( rows [ : , column ] ) pyplot . show ( ) # load data dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) # plot targets for some chunks plot_chunk_targets ( chunks , [ 1 ] )

Running the example creates a single figure with 39 line plots for chunk identifier “1”.

The plots are small, but give a rough idea of the temporal structure for the variables.

We can see that there are more than a few variables that have no data for this chunk. These cannot be forecasted directly, and probably not indirectly.

This suggests that in addition to not having all variables for all sites, that even those specified in the column header may not be present for some chunks.

We can also see breaks in some of the series for missing values. This suggests that even though we may have observations for each time step within the chunk, that we may not have a contiguous series for all variables in the chunk.

There is a cyclic structure to many of the plots. Most have eight peaks, very likely corresponding to the eight days of observations within the chunk. This seasonal structure could be modeled directly, and perhaps removed from the data when modeling and added back to the forecasted interval.

There does not appear to be any trend to the series.

We can re-run the example and plot the target variables for the first three chunks with complete data.

# plot targets for some chunks plot_chunk_targets(chunks, [1, 3 ,5]) 1 2 # plot targets for some chunks plot_chunk_targets ( chunks , [ 1 , 3 , 5 ] )

Running the example creates a figure with 39 plots and three time series per plot, one for the targets for each chunk.

The plot is busy, and you may want to increase the size of the plot window to better see the comparison across the chunks for the target variables.

For many of the variables that have a cyclic daily structure, we can see the structure repeated across the chunks.

This is encouraging as it suggests that modeling a variable for a site may be helpful across chunks.

Further, plots 3-to-10 correspond to variable 11 across seven different sites. The string similarity in temporal structure across these plots suggest that modeling the data per variable which is used across sites may be beneficial.

Boxplot Distribution of Target Variables

It is also useful to take a look at the distribution of the target variables.

We can start by taking a look at the distribution of each target variable for one chuck by creating box and whisker plots for each target variable.

A separate boxplot can be created for each target side-by-side, allowing the shape and range of values to be directly compared on the same scale.

# boxplot for target variables for a chuck def plot_chunk_targets_boxplots(chunks, c_id): rows = chunks[c_id] pyplot.boxplot(rows[:,56:]) pyplot.show() 1 2 3 4 5 # boxplot for target variables for a chuck def plot_chunk_targets_boxplots ( chunks , c_id ) : rows = chunks [ c_id ] pyplot . boxplot ( rows [ : , 56 : ] ) pyplot . show ( )

The complete example is listed below.

# boxplots of targets for a chunk from numpy import unique from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # boxplot for target variables for a chuck def plot_chunk_targets_boxplots(chunks, c_id): rows = chunks[c_id] pyplot.boxplot(rows[:,56:]) pyplot.show() # load data dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) # boxplot for target variables plot_chunk_targets_boxplots(chunks, 1) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # boxplots of targets for a chunk from numpy import unique from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # boxplot for target variables for a chuck def plot_chunk_targets_boxplots ( chunks , c_id ) : rows = chunks [ c_id ] pyplot . boxplot ( rows [ : , 56 : ] ) pyplot . show ( ) # load data dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) # boxplot for target variables plot_chunk_targets_boxplots ( chunks , 1 )

Running the example creates a figure containing 39 boxplots, one for each of the 39 target variables for the first chunk.

We can see that many of the variables have a median close to zero or one; we can also see a large asymmetrical spread for most variables, suggesting the variables likely have a skew with outliers.

It is encouraging that the boxplots from 4-10 for variable 11 across seven sites show a similar distribution. This is further supporting evidence that data may be grouped by variable and used to fit a model that could be used across sites.

We can re-create this plot using data across all chunks to see dataset-wide patterns.

The complete example is listed below.

# boxplots of targets for all chunks from pandas import read_csv from matplotlib import pyplot # boxplot for all target variables def plot_target_boxplots(values): pyplot.boxplot(values[:,56:]) pyplot.show() # load data dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # boxplot for target variables values = dataset.values plot_target_boxplots(values) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # boxplots of targets for all chunks from pandas import read_csv from matplotlib import pyplot # boxplot for all target variables def plot_target_boxplots ( values ) : pyplot . boxplot ( values [ : , 56 : ] ) pyplot . show ( ) # load data dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # boxplot for target variables values = dataset . values plot_target_boxplots ( values )

Running the example creates a new figure showing 39 box and whisker plots for the entire training dataset regardless of chunk.

It is a little bit of a mess, where the circle outliers obscure the main data distributions.

We can see that outlier values do extend into the range 5-to-10 units. This suggests there might be some use in standardizing and/or rescaling the targets when modeling.

Perhaps the most useful finding is that there are some targets that do not have any (or very much) data regardless of chunk. These columns probably should be excluded from the dataset.

Apparently Empty Target Columns

We can investigate the apparent missing data further by creating a bar chart of the ratio of missing data per column, excluding the metadata columns at the beginning (e.g. the first five columns).

The plot_col_percentage_missing() function below implements this.

# bar chart of the ratio of missing data per column def plot_col_percentage_missing(values, ix_start=5): ratios = list() # skip early columns, with meta data or strings for col in range(ix_start, values.shape[1]): col_data = values[:, col].astype('float32') ratio = count_nonzero(isnan(col_data)) / len(col_data) * 100 ratios.append(ratio) if ratio > 90.0: print(col, ratio) col_id = [x for x in range(ix_start, values.shape[1])] pyplot.bar(col_id, ratios) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 # bar chart of the ratio of missing data per column def plot_col_percentage_missing ( values , ix_start = 5 ) : ratios = list ( ) # skip early columns, with meta data or strings for col in range ( ix_start , values . shape [ 1 ] ) : col_data = values [ : , col ] . astype ( 'float32' ) ratio = count_nonzero ( isnan ( col_data ) ) / len ( col_data ) * 100 ratios . append ( ratio ) if ratio > 90.0 : print ( col , ratio ) col_id = [ x for x in range ( ix_start , values . shape [ 1 ] ) ] pyplot . bar ( col_id , ratios ) pyplot . show ( )

The complete example is listed below.

# summarize missing data per column from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # bar chart of the ratio of missing data per column def plot_col_percentage_missing(values, ix_start=5): ratios = list() # skip early columns, with meta data or strings for col in range(ix_start, values.shape[1]): col_data = values[:, col].astype('float32') ratio = count_nonzero(isnan(col_data)) / len(col_data) * 100 ratios.append(ratio) if ratio > 90.0: print(ratio) col_id = [x for x in range(ix_start, values.shape[1])] pyplot.bar(col_id, ratios) pyplot.show() # load data dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # plot ratio of missing data per column values = dataset.values plot_col_percentage_missing(values) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # summarize missing data per column from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # bar chart of the ratio of missing data per column def plot_col_percentage_missing ( values , ix_start = 5 ) : ratios = list ( ) # skip early columns, with meta data or strings for col in range ( ix_start , values . shape [ 1 ] ) : col_data = values [ : , col ] . astype ( 'float32' ) ratio = count_nonzero ( isnan ( col_data ) ) / len ( col_data ) * 100 ratios . append ( ratio ) if ratio > 90.0 : print ( ratio ) col_id = [ x for x in range ( ix_start , values . shape [ 1 ] ) ] pyplot . bar ( col_id , ratios ) pyplot . show ( ) # load data dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # plot ratio of missing data per column values = dataset . values plot_col_percentage_missing ( values )

Running the example first prints the column id (zero offset) and the ratio of missing data, if the ratio is above 90%.

We can see that there are in fact no columns with zero non-NaN data, but perhaps two dozen (12) that have above 90% missing data.

Interestingly, seven of these are target variables (index 56 or higher).

11 91.48885539779488 20 91.48885539779488 29 91.48885539779488 38 91.48885539779488 47 91.48885539779488 58 95.38880516115385 66 96.9805134713519 68 95.38880516115385 72 97.31630575606145 86 95.38880516115385 92 95.38880516115385 94 95.38880516115385 1 2 3 4 5 6 7 8 9 10 11 12 11 91.48885539779488 20 91.48885539779488 29 91.48885539779488 38 91.48885539779488 47 91.48885539779488 58 95.38880516115385 66 96.9805134713519 68 95.38880516115385 72 97.31630575606145 86 95.38880516115385 92 95.38880516115385 94 95.38880516115385

A bar chart of column index number to ratio of missing data is created.

We can see that there might be some stratification to the ratio of missing data, with a cluster below 10%, a cluster around 70%, and a cluster above 90%.

We can also see a separation between input variable and target variables where the former are quite regular as they show the same variable type measured across different sites.

Such small amounts of data for some target variables suggest the need to leverage other factors besides past observations in order to make predictions.

Histogram Distribution of Target Variables

The distribution of the target variables are not neat and may be non-Gaussian at the least, or highly multimodal at worst.

We can check this by looking at histograms of the target variables, for the data for a single chunk.

A problem with the hist() function in matplotlib is that it is not robust to NaN values. We can overcome this by checking that each column has non-NaN values prior to plotting and excluding the rows with NaN values.

The function below does this and creates one histogram for each target variable for one or more chunks.

# plot distribution of targets for one or more chunk ids def plot_chunk_targets_hist(chunks, c_ids): pyplot.figure() targets = range(56, 95) for i in range(len(targets)): ax = pyplot.subplot(len(targets), 1, i+1) ax.set_xticklabels([]) ax.set_yticklabels([]) column = targets[i] for chunk_id in c_ids: rows = chunks[chunk_id] # extract column of interest col = rows[:,column].astype('float32') # check for some data to plot if count_nonzero(isnan(col)) < len(rows): # only plot non-nan values pyplot.hist(col[~isnan(col)], bins=100) pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # plot distribution of targets for one or more chunk ids def plot_chunk_targets_hist ( chunks , c_ids ) : pyplot . figure ( ) targets = range ( 56 , 95 ) for i in range ( len ( targets ) ) : ax = pyplot . subplot ( len ( targets ) , 1 , i + 1 ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) column = targets [ i ] for chunk_id in c_ids : rows = chunks [ chunk_id ] # extract column of interest col = rows [ : , column ] . astype ( 'float32' ) # check for some data to plot if count_nonzero ( isnan ( col ) ) < len ( rows ) : # only plot non-nan values pyplot . hist ( col [ ~ isnan ( col ) ] , bins = 100 ) pyplot . show ( )

The complete example is listed below.

# plot distribution of targets for a chunk from numpy import unique from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks(values, chunk_ix=1): chunks = dict() # get the unique chunk ids chunk_ids = unique(values[:, chunk_ix]) # group rows by chunk id for chunk_id in chunk_ids: selection = values[:, chunk_ix] == chunk_id chunks[chunk_id] = values[selection, :] return chunks # plot distribution of targets for one or more chunk ids def plot_chunk_targets_hist(chunks, c_ids): pyplot.figure() targets = range(56, 95) for i in range(len(targets)): ax = pyplot.subplot(len(targets), 1, i+1) ax.set_xticklabels([]) ax.set_yticklabels([]) column = targets[i] for chunk_id in c_ids: rows = chunks[chunk_id] # extract column of interest col = rows[:,column].astype('float32') # check for some data to plot if count_nonzero(isnan(col)) < len(rows): # only plot non-nan values pyplot.hist(col[~isnan(col)], bins=100) pyplot.show() # load data dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # group data by chunks values = dataset.values chunks = to_chunks(values) # plot targets for some chunks plot_chunk_targets_hist(chunks, [1]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # plot distribution of targets for a chunk from numpy import unique from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # split the dataset by 'chunkID', return a dict of id to rows def to_chunks ( values , chunk_ix = 1 ) : chunks = dict ( ) # get the unique chunk ids chunk_ids = unique ( values [ : , chunk_ix ] ) # group rows by chunk id for chunk_id in chunk_ids : selection = values [ : , chunk_ix ] == chunk_id chunks [ chunk_id ] = values [ selection , : ] return chunks # plot distribution of targets for one or more chunk ids def plot_chunk_targets_hist ( chunks , c_ids ) : pyplot . figure ( ) targets = range ( 56 , 95 ) for i in range ( len ( targets ) ) : ax = pyplot . subplot ( len ( targets ) , 1 , i + 1 ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) column = targets [ i ] for chunk_id in c_ids : rows = chunks [ chunk_id ] # extract column of interest col = rows [ : , column ] . astype ( 'float32' ) # check for some data to plot if count_nonzero ( isnan ( col ) ) < len ( rows ) : # only plot non-nan values pyplot . hist ( col [ ~ isnan ( col ) ] , bins = 100 ) pyplot . show ( ) # load data dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # group data by chunks values = dataset . values chunks = to_chunks ( values ) # plot targets for some chunks plot_chunk_targets_hist ( chunks , [ 1 ] )

Running the example creates a figure with 39 histograms, one for each target variable for the first chunk.

The plot is hard to read, but the large number of bins goes to show the distribution of the variables.

It might be fair to say that perhaps none of the target variables have an obvious Gaussian distribution. Many may have a skewed distribution with a long right tail.

Other variables have what appears to be quite a discrete distribution that might be an artifact of the chosen measurement device or measurement scale.

We can re-create the same plot with target variables for all chunks.

The complete example is listed below.

# plot distribution of all targets from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # plot histogram for each target variable def plot_target_hist(values): pyplot.figure() targets = range(56, 95) for i in range(len(targets)): ax = pyplot.subplot(len(targets), 1, i+1) ax.set_xticklabels([]) ax.set_yticklabels([]) column = targets[i] # extract column of interest col = values[:,column].astype('float32') # check for some data to plot if count_nonzero(isnan(col)) < len(values): # only plot non-nan values pyplot.hist(col[~isnan(col)], bins=100) pyplot.show() # load data dataset = read_csv('AirQualityPrediction/TrainingData.csv', header=0) # plot targets for all chunks values = dataset.values plot_target_hist(values) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # plot distribution of all targets from numpy import isnan from numpy import count_nonzero from pandas import read_csv from matplotlib import pyplot # plot histogram for each target variable def plot_target_hist ( values ) : pyplot . figure ( ) targets = range ( 56 , 95 ) for i in range ( len ( targets ) ) : ax = pyplot . subplot ( len ( targets ) , 1 , i + 1 ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) column = targets [ i ] # extract column of interest col = values [ : , column ] . astype ( 'float32' ) # check for some data to plot if count_nonzero ( isnan ( col ) ) < len ( values ) : # only plot non-nan values pyplot . hist ( col [ ~ isnan ( col ) ] , bins = 100 ) pyplot . show ( ) # load data dataset = read_csv ( 'AirQualityPrediction/TrainingData.csv' , header = 0 ) # plot targets for all chunks values = dataset . values plot_target_hist ( values )

Running the example creates a figure with 39 histograms, one for each of the target variables in the training dataset.

We can see fuller distributions, which are more insightful.

The first handful of plots perhaps show a highly skewed distribution, the core of which may or may not be Gaussian.

We can see many Gaussian-like distributions with gaps, suggesting discrete measurements imposed on a Gaussian-distributed continuous variable.

We can also see some variables that show an exponential distribution.

Together, this suggests either the use of power transforms to explore reshaping the data to be more Gaussian, and/or the use of nonparametric modeling methods that are not dependent upon a Gaussian distribution of the variables. For example, classical linear methods may be expected to have a hard time.

A Wrinkle With Target Variables

After the end of the competition, the person who provided the data, David Chudzicki, summarized the true meaning of the 12 output variables.

This was provided in a form post titled “what the target variables really were“, reproduced partially below:

Description Target Variable Carbon monoxide, 8 Sulfur dioxide, 4 SO2 max 5-min avg, 3 Nitric oxide (NO), 10 Nitrogen dioxide (NO2), 14 Oxides of nitrogen (NOx), 9 Ozone, 11 PM10 Total 0-10um STP, 5 OC CSN Unadjusted PM2.5 LC TOT, 15 Total Nitrate PM2.5 LC, 2 EC CSN PM2.5 LC TOT, 1 Total Carbon PM2.5 LC TOT, 7 Sulfate PM2.5 LC, 8 PM2.5 Raw Data, 4 PM2.5 AQI & Speciation Mass, 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Description Target Variable Carbon monoxide, 8 Sulfur dioxide, 4 SO2 max 5-min avg, 3 Nitric oxide (NO), 10 Nitrogen dioxide (NO2), 14 Oxides of nitrogen (NOx), 9 Ozone, 11 PM10 Total 0-10um STP, 5 OC CSN Unadjusted PM2.5 LC TOT, 15 Total Nitrate PM2.5 LC, 2 EC CSN PM2.5 LC TOT, 1 Total Carbon PM2.5 LC TOT, 7 Sulfate PM2.5 LC, 8 PM2.5 Raw Data, 4 PM2.5 AQI & Speciation Mass, 3

This is interesting as we can see that the target variables are meteorological in nature and related to air quality as the name of the competition suggests.

A problem is that there are 15 variables and only 12 different types of target variables in the dataset.

The cause of this problem is that the same target variable in the dataset may be used to represent different target variables. Specifically:

Target 8 could be data for ‘Carbon monoxide‘ or ‘Sulfate PM2.5 LC‘.

Target 4 could be data for ‘Sulfur dioxide‘ or ‘PM2.5 Raw Data‘.

Target 3 could be data for ‘SO2 max 5-min avg‘ or ‘PM2.5 AQI & Speciation Mass‘.

From the names of the variables, the doubling-up of data into the same target variable was done so with variables with differing chemical characters and perhaps even measures, e.g. it appears to be accidental rather than strategic.

It is not clear, but it is likely that a target represents one variable within a chunk but may represent different variables across chunks. Alternately, it may be possible that the variables differ across sites within each chunk. In the former case, it means that models that expect consistency in these target variables across chunks, which is a very reasonable assumption, may have difficulty. In the latter, models can treat the variable-site combinations as distinct variables.

It may be possible to tease out the differences by comparing the distribution and scales of these variables across chunks.

This is disappointing, and depending on how consequential it is to model skill, it may require the removal of these variables from the dataset, which are a lot of the target variables (20 of 39).

Thoughts on Modeling

In this section, we will harness what we have discovered about the problem and suggest some approaches to modeling this problem.

I like this dataset; it is messy, realistic, and resists naive approaches.

This section is divided into four sections; they are:

Framing.

Data Preparation.

Modeling.

Evaluation.

Framing

The problem is generally framed as a multivariate multi-step time series forecasting problem.

Further, the multiple variables are required to be forecasted across multiple sites, which is a common structural breakdown for time series forecasting problems, e.g. predict the variable thing at different physical locations such as stores or stations.

Let’s walk through some possible framings of the data.

Univariate by Variable and Site

A first-cut approach might be to treat each variable at each site as a univariate time series forecasting problem.

A model is given eight days of hourly observations for a variable and is asked to forecast three days, from which a specific subset of forecast lead times are taken and used or evaluated.

It may be possible in a few select cases, and this could be confirmed with some further data analysis.

Nevertheless, the data generally resists this framing because not all chunks have eight days of observations for each target variable. Further, the time series for the target variable can be dramatically discontiguous, if not mostly (90%-to-95%) incomplete.

We could relax the expectation of the structure and amount of prior data required by the model, designing the model to make use of whatever is available.

This approach would require 39 models per chunk and a total of (208 * 39) or 8,112 separate models. It sounds possible, but perhaps less scalable than we may prefer from an engineering perspective.

The variable-site combinations could be modeled across chunks, requiring only 39 models.

Univariate by Variable

The target variables can be aggregated across sites.

We can also relax what lag lead times are used to make a forecast and present what is available either with zero-padding or imputing for missing values, or even lag observations that disregard lead time.

We can then frame the problem as given some prior observations for a given variable, forecast the following three days.

The models may have more to work with, but will disregard any variable differences based on site. This may or may not be reasonless and could be checked by comparing variable distributions across sites.

There are 12 unique variables.

We could model each variable per chunk, giving (208 * 12) or 2,496 models. It might make more sense to model the 12 variables across chunks, requiring only 12 models.

Multivariate Models

Perhaps one or more target variables are dependent on one or more of the meteorological variables, or even on the other target variables.

This could be explored by investigating the correlation between each target variable and each input variable, as well as with the other target variables.

If such dependencies exist, or could be assumed, it may be possible to not only forecast the variables with more complete data, but also those target variables with above 90% missing data.

Such models could use some subset of prior meteorological observations and/or target variable observations as input. The discontiguous nature of the data may require the relaxing of the traditional lag temporal structure for the input variables, allowing the model to use whatever was available for a specific forecast.

Data Preparation

Depending on the choice of model, the input and target variables may benefit from some data preparation, such as:

Standardization.

Normalization.

Power Transform, where Gaussian.

Seasonal Differencing, where seasonal structures are present.

To address the missing data, in some cases imputing may be required with simple persistence or averaging.

In other cases, and depending on the choice of model, it may be possible to learn directly from the NaN values as observations (e.g. XGBoost can do this) or to fill with 0 values and mask the inputs (e.g. LSTMs can do this).

It may be interesting to investigate downscaling input to 2, 4, or 12, hourly data or similar in an attempt to fill the gaps in discontiguous data, e.g. forecast hourly from 12 hourly data.

Modeling

Modeling may require some prototyping to discover what works well in terms of methods and chosen input observations.

Classical Methods

There may be rare examples of chunks with complete data where classical methods like ETS or SARIMA could be used for univariate forecasting.

Generally, the problem resists the classical methods.

Machine Learning

A good choice would be the use of nonlinear machine learning methods that are agnostic about the temporal structure of the input data, making use of whatever is available.

Such models could be used in a recursive or direct strategy to forecast the lead times. A direct strategy may make more sense, with one model per required lead time.

There are 10 lead times, and 39 target variables, in which case a direct strategy would require (39 * 10) or 390 models.

A downside of the direct approach to modeling the problem is the inability of the model to leverage any dependencies between target variables in the forecast interval, specifically across sites, across variables, or across lead times. If these dependencies exist (and some surely do), it may be possible to add a flavor of them in using a second-tier of of ensemble models.

Feature selection could be used to discover the variables and/or the lag lead times that may provide the most value in forecasting each target variable and lead time.

This approach would provide a lot of flexibility, and as was shown in the competition, ensembles of decision trees perform well with little tuning.

Deep Learning

Like machine learning methods, deep learning methods may be able to use whatever multivariate data is available in order to make a prediction.

Two classes of neural networks may be worth exploring for this problem:

Convolutional Neural Networks or CNNs.

Recurrent Neural Networks, specifically Long Short-Term Memory networks or LSTMs.

CNNs are capable of distilling long sequences of multivariate input time series data into small feature maps, and in essence learn the features from the sequences that are most relevant for forecasting. Their ability to handle noise and feature invariance across the input sequences may be useful. Like other neural networks, CNNs can output a vector in order to predict the forecast lead times.

LSTMs are designed to work with sequence data and can directly support missing data via masking. They too are capable of automatic feature learning from long input sequences and alone or combined with CNNs may perform well on this problem. Together with an encoder-decoder architecture, the LSTM network can be used to natively forecast multiple lead times.

Evaluation

A naive approach that mirrors that used in the competition might be best for evaluating models.

That is, splitting each chunk into train and test sets, in this case using the first five days of data for training and the remaining three for test.

It may be possible and interesting to finalize a model by training it on the entire dataset and submitting a forecast to the Kaggle website for evaluation on the held out test dataset.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this tutorial, you discovered and explored the Air Quality Prediction dataset that represents a challenging multivariate, multi-site, and multi-step time series forecasting problem.

Specifically, you learned:

How to load and explore the chunk-structure of the dataset.

How to explore and visualize the input and target variables for the dataset.

How to use the new understand to outline a suite of methods for framing the problem, preparing the data, and modeling the dataset.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
277;news.mit.edu;http://news.mit.edu/2020/inactive-pill-ingredients-could-raise-dose-your-medication-0317;;“Inactive” pill ingredients could raise the dose of your medication;"The average medication contains a mix of eight “inactive” ingredients added to pills to make them taste better, last longer, and stabilize the active ingredients within. Some of those additives are now getting a closer look for their ability to cause allergic reactions in some patients. But now, in a new twist, MIT researchers have discovered that two other inactive ingredients may actually boost medication strength to the benefit of some patients.

In a study published March 17 in Cell Reports, researchers report that vitamin A palmitate, a common supplement, and gum resin, a popular glazing agent for pills and chewing gum — could make hundreds of drugs more effective, from blood-clotting agents and anti-cancer drugs to over-the-counter pain relievers. They also outline a method for using machine learning to find other inactive ingredients with untapped therapeutic value.

“Anything you ingest has a potential effect, but tracing that effect to the molecular level can be a Herculean effort,” says the study’s senior author Giovanni Traverso, an assistant professor in the Department of Mechanical Engineering and a gastroenterologist at Brigham and Women’s Hospital. “Machine learning gives you a way to narrow down the search space.”

The researchers chose to focus their search on two proteins in the body known for their outsized role in drug delivery: the transporter protein P-glycoprotein (P-gp) and the metabolic protein UDP-Glucuronosyltranferase-2B7 (UGT2B7). One or both are involved in modulating the effects of 20 percent of the nearly 1,900 drugs approved by the U.S. Food and Drug Administration (FDA).

The researchers wanted to know if any of the FDA’s 800 approved food and drug additives would skew the functioning of either protein. Screening all 800 compounds by hand would be tedious and expensive. So, instead, they built a computer platform to do the work for them, adapting a method used by pharmaceutical companies to rule out drug-on-drug interactions.

They fed the system the chemical structures of the FDA’s 800 inactive ingredients, as well as millions of drugs and other compounds known to interfere with enzyme functioning. They then asked the platform to predict which food and drug additives would be most likely to disrupt P-gp and UGT2B7 and alter a drug’s potency by letting more into the body, in the case of P-gp, or slowing its exit, as in UGT2B7.

Machine learning allowed the researchers to quickly make comparisons between millions of drugs and inactive ingredients to identify the additives most likely to have an effect. Two top candidates emerged: vitamin A palmitate, as a predicted inhibitor of P-gp, and abietic acid, an ingredient in gum resin (basically, tree sap), as a predicted inhibitor of UGT2B7.

The researchers next moved to physically test the computer’s predictions in the lab. In one experiment, they gave mice vitamin A-fortified water followed by a normal dose of the blood-clotter, warfarin. With a simple blood test, they confirmed the mice had absorbed 30 percent more medication, a strong indication that vitamin A had improved the uptake of warfarin.

In a second experiment, they treated a small slice of pig liver with a substance that loses its ability to fluoresce as UGT2B7 digests it. When abietic acid was added, the substance continued to fluoresce. Drug developers use the test to confirm that a drug acts as an enzyme inhibitor, and here, researchers confirmed that abietic acid had, in fact, targeted UGT2B7 as predicted. Though no actual drug was tested, the results suggest that if gum resin were taken with a common pain reliever like ibuprofen, it could increase its strength, Traverso says, much as vitamin A had for warfarin in mice.

Machine learning methods are increasingly helping to identify and design new drugs. In a recent discovery, MIT researchers used a deep learning algorithm to find an entirely new antibiotic in the Drug Repurposing Hub, a database of compounds approved, or under review, for human use. Hiding in plain sight as a proposed diabetes treatment, the compound was identified because the algorithm had no preconceived ideas of what a bacteria-killing agent should look like.

Much like the Drug Repurposing Hub, the FDA’s inactive ingredient list is a big draw for drug developers. The ingredients are already on the market, even if they have yet to be approved for a new use, says the study’s lead author, Daniel Reker, a Swiss National Science Foundation postdoc at MIT’s Koch Institute for Integrative Cancer Research. If a promising biological association is uncovered, the discovery can be moved quickly to clinical trials. It can take years, by contrast, to test the safety of new molecules synthesized or discovered in the lab.

“While further tests are necessary to understand how strong these effects are in humans, our algorithms drew new conclusions that could have immediate impact,” says Reker. “Drug discovery is such a long and costly process, we’re excited that machine learning can help to improve the odds.”

The team became interested in the hidden effects of inactive ingredients after a patient of Traverso’s with celiac disease grew sicker after taking a drug later found to contain gluten. Traverso and his colleagues wondered what other supposedly inert ingredients might be affecting patients.

Sifting through the ingredient lists of some 42,000 medications sold in the United States, they found that more than half contained at least one type of sugar that people with irritable bowel syndrome are told to avoid; 45 percent contained lactose; and a third contained a food dye linked to allergic reactions. Though one pill might not contain enough of an ingredient to cause trouble, it could add up in patients on multiple medications, the researchers warned. They reported their findings last year in Science Translational Medicine.

In the current study, the researchers chose to explore the therapeutic value of those same ingredients. Comparing the chemical structures of the 800 “inactive” ingredients with nearly 1,900 approved drug compounds, they found a surprising amount of overlap in their chemical structures and properties. That motivated them to try and predict the biological effects of all 800 ingredients. In the end, they found thousands of previously undocumented effects, an indication that other beneficial compounds beyond vitamin A palmitate and gum resin may be awaiting discovery.

If confirmed in clinical trials, vitamin A and gum resin could be added to hundreds of drugs modulated by the P-gp or UGT2B7 proteins, including cancer-fighting drugs, to improve uptake of the medicine or reduce the amount needed in formulations, the researchers say. Vitamin A, gum rosin, and compounds like them, might also be a starting point for entirely new drugs.

“As machine learning brings us closer to a more personalized form of medicine, doctors will be able to treat patients more effectively by taking into account their diet, the inactive ingredients in their medications, and other factors,” says Reker.

The study was funded, in part, by the MIT-IBM Watson AI Lab and the National Institutes of Health."
278;machinelearningmastery.com;https://machinelearningmastery.com/implement-machine-learning-algorithm-performance-metrics-scratch-python/;2016-10-18;How To Implement Machine Learning Metrics From Scratch in Python;"# Example of Calculating and Displaying a Pretty Confusion Matrix

# calculate a confusion matrix

def confusion_matrix ( actual , predicted ) :

unique = set ( actual )

matrix = [ list ( ) for x in range ( len ( unique ) ) ]

for i in range ( len ( unique ) ) :

matrix [ i ] = [ 0 for x in range ( len ( unique ) ) ]

lookup = dict ( )

for i , value in enumerate ( unique ) :

lookup [ value ] = i

for i in range ( len ( actual ) ) :

x = lookup [ actual [ i ] ]

y = lookup [ predicted [ i ] ]

matrix [ y ] [ x ] += 1

return unique , matrix

# pretty print a confusion matrix

def print_confusion_matrix ( unique , matrix ) :

print ( '(A)' + ' ' . join ( str ( x ) for x in unique ) )

print ( '(P)---' )

for i , x in enumerate ( unique ) :

print ( ""%s| %s"" % ( x , ' ' . join ( str ( x ) for x in matrix [ i ] ) ) )

# Test confusion matrix with integers

actual = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ]

predicted = [ 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 ]

unique , matrix = confusion_matrix ( actual , predicted )"
279;news.mit.edu;http://news.mit.edu/2017/maryann-gong-named-2017-ncaa-woman-year-top-30-honoree-0907;;Maryann Gong named 2017 NCAA Woman of the Year Top 30 honoree;"Former MIT All-American cross country/track standout Maryann Gong, from Livermore, California, has been named as a Top 30 honoree for the 2017 NCAA Woman of the Year Award. Gong is just the third student-athlete in MIT history to be honored as a Top 30 selection, joining Margaret Guo '16 (swimming and diving) and Lisa K. Arel '92 (gymnastics). Guo captured the 2016 Woman of the Year award and became the first MIT student-athlete to earn the honor.

This year, the NCAA received a program-record 543 school nominees, which were then trimmed to 145 female student-athletes that were nominated by conferences and an independent selection committee. Gong was one of 53 Division III student-athletes to advance to that stage, and she is now among the final 10 from Division III.

“It’s kind of hard to believe because there are so many people, so to be one of the top 30 is really an honor and I’m really grateful about it,” Gong says. “It’s definitely a great way to end my undergraduate career at MIT.”

Named as the CoSIDA Division III National Academic All-America of the Year for a second straight season, Gong is a 15-time All-American and one of the most decorated female student-athletes in MIT history. In 2016-17, she was a three-time indoor track and field All-American as she anchored the distance medley relay team that finished as the national runner-up. Posting a perfect 5.0 GPA as an undergraduate at MIT, she is currently pursuing her master’s degree at MIT in engineering with a concentration in artificial intelligence. A former NCAA champion in the 3,000 meters, Gong was the recipient of the 2017 NCAA Outdoor Track and Field Elite 90 award.

“The Top 30 honorees are remarkable representatives of the thousands of women competing in college sports each year,” says Sarah Hebberd, chair of the Woman of the Year selection committee and director of compliance at Georgia. “They have seized every opportunity available to them on the field of play, in the classroom and in the community, and we are proud to recognize them for their outstanding achievements.”

The selection committee will name nine finalists, with three from each division, in late September. From those nine finalists, the NCAA Committee on Women’s Athletics will select the 2017 Woman of the Year. That ceremony will take place on Oct. 23 at a ceremony in Indianapolis.

For the latest on MIT Athletics, follow the Engineers via social media on Twitter, Facebook, Instagram and YouTube."
280;machinelearningmastery.com;https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/;2019-03-24;How to Manually Scale Image Pixel Data for Deep Learning;"# example of per-channel centering (subtract mean)

from numpy import asarray

from PIL import Image

# load image

image = Image . open ( 'sydney_bridge.jpg' )

pixels = asarray ( image )

# convert from integers to floats

pixels = pixels . astype ( 'float32' )

# calculate per-channel means and standard deviations

means = pixels . mean ( axis = ( 0 , 1 ) , dtype = 'float64' )

print ( 'Means: %s' % means )

print ( 'Mins: %s, Maxs: %s' % ( pixels . min ( axis = ( 0 , 1 ) ) , pixels . max ( axis = ( 0 , 1 ) ) ) )

# per-channel centering of pixels

pixels -= means

# confirm it had the desired effect

means = pixels . mean ( axis = ( 0 , 1 ) , dtype = 'float64' )

print ( 'Means: %s' % means )"
281;machinelearningmastery.com;http://machinelearningmastery.com/a-data-driven-approach-to-machine-learning/;2014-09-28;A Data-Driven Approach to Choosing Machine Learning Algorithms;"Tweet Share Share

Last Updated on April 4, 2018

If You Knew Which Algorithm or Algorithm Configuration To Use,

You Would Not Need To Use Machine Learning

There is no best machine learning algorithm or algorithm parameters.

I want to cure you of this type of silver bullet mindset.

I see these questions a lot, even daily:

Which is the best machine learning algorithm?

What is the mapping between machine learning algorithms and problems?

What are the best parameters for a machine learning algorithm?

There is a pattern to these questions.

You generally do not and cannot know the answers to these questions beforehand. You must discover it through empirical study.

There are some broad brush heuristics to answer these questions, but even these can trip you up if you are looking to get the most from an algorithm or a problem.

In this post, I want to encourage you to break free of this mindset and take hold of a data-driven approach that is going to change they way you approach machine learning.

Best Machine Learning Algorithm

Some algorithms have more “power” than others. They are non-parametric or highly flexible and adaptive, or highly self-tuning or all of the above.

Typically this power comes at a cost of difficulty to implement, the need for very large datasets, limited scalability, or a large number of coefficients that may result in over-fitting.

With bigger datasets, there has been a renewed interest in simpler methods that scale and perform well.

Which is the best algorithm, the algorithm that you should always try and spend the most time learning?

I could throw out some names, but the smartest answer is “none” and “all“.

No Best Machine Learning Algorithm

You cannot know a priori which algorithm will be best suited for your problem.

Read the above line again. Meditate on it.

Meditate on it.

You can apply your favorite algorithm.

You can apply the algorithm recommended in a book or paper.

You can apply the algorithm that is winning the most Kaggle competitions right now.

You can apply the algorithm that works best with your test rig, infrastructure, database, or whatever.

These are biases.

They are short-cuts in thinking that save time. Some may, in fact, be very useful short-cuts, but which ones?

By definition, biases will limit the solutions that you can achieve, the accuracy you can achieve, and ultimately the impact that you can have.

Mapping of Algorithms to Problems

There are general classes of problems, say supervised problems like classification and regression and unsupervised problems like manifold learning and clustering.

There are more specific instances of these problems in sub-fields of machine learning like Computer Vision, Natural Language Processing and Speech Processing. We can also go the other way, more abstract and consider all of these problems as instances of function approximation and function optimization.

You can map algorithms to classes of problems, for example, there are algorithms that can handle supervised regression problems and supervised classification problems, and both types of problems.

You can also construct catalogs of algorithms, and that may be useful to inspire you as to which algorithms to try.

You can race algorithms on a problem and report the results. Sometimes this is called a bake-off and is popular in some conference proceedings for presenting new algorithms.

Limited Transferability of Algorithm Results

Generally, racing algorithms is anti-intellectual. It is rarely scientifically rigorous (apples to apples).

A key problem with racing algorithms is that you cannot easily transfer the findings from one problem to another. If you believe this statement is true, then reading about algorithm races in papers and blogs does not inform you about which algorithm to try on your problem.

If algorithm A kills algorithm B on Problem X, what does that tell you about algorithm A and B on problem Y? You have to work to relate problems X and Y. Do they have the same or similar properties (attributes, attribute distributions, functional form) that are exploited by the algorithms under study? That’s some hard work.

We do not have fine-grained understandings of when one machine learning algorithm works better than another.

Best Algorithm Parameters

Machine learning algorithms are parametrized so that you can tailor their behavior and results to your problem.

The problem is that “how” to do the tailoring is rarely (if ever) explained. Often, it is poorly understood, even by the algorithm developers themselves.

Generally, machine learning algorithms with stochastic elements are complex systems and must be studied as such. The first order – what effect does the parameter have on the complex system may be described. If so, you may have available some heuristics on how to configure the algorithm as a system.

It is the second order, what effect will it have on your results which is not known. Sometimes you can talk in generalities about the parameters’ effects on the algorithm as a system and how this translates to classes of problem, often not.

No Best Algorithm Parameters

New sets of algorithm configurations are essentially new instances of algorithms for you to challenge your problem (albeit, relatively constrained or similar in the results they can achieve).

You cannot know the best algorithm parameters for your problem a priori.

You can use the parameters used in the seminal paper.

You can use the parameters in a book.

You can use the parameters listed in a “how I did it” kaggle post.

Good rules of thumb. Right? Maybe, maybe not.

Data-Driven Approach

We do not need to fall into a heap of despair. We become scientists.

You have biases that can short-cut decisions for algorithm selection and algorithm parameter selection. They can serve you well in many cases, we think.

I want you to challenge this, to consider abandoning heuristics and best practices and take on a data-driven approach to algorithm selection.

Rather than picking your favorite algorithm, try 10 or 20 algorithms.

Double down on those that show signs of being better in performance, robustness, speed or whatever concerns interest you most.

Rather than picking the common parameters, grid search tens, hundreds or thousands of combinations of parameters.

Become the objective scientist, leave behind anecdotes and study the intersection of complex learning systems and data observations from your problem domain.

Data-Driven Approach in Action

This is a powerful approach that requires less up-front knowledge, but a lot more back-end computation and experimentation.

As such, you will very likely be required to work with a smaller sample of your dataset so that you can get results quickly. You will want a test harness that you can have complete faith in.

Side note: how can you have complete trust in your test harness?

You develop trust by selecting the test options in a data-driven manner that gives you objective confidence that your chosen configuration is reliable. The type of estimation method (split, boosting, k-fold cross validation, etc.) and it’s configuration (size of k, etc.).

Fast Robust Results

You get good results, fast.

If random forest is your favorite algorithm, you could spend days or weeks trying in vain to get the most from the algorithm on your problem, which may not be suited to the method in the first place. With a data-driven methodology, you can discount (relative) poor performers early. You can fail fast.

It takes discipline to not fall back on biases and favorite algorithms and configurations. It’s hard work to get good and robust results.

The result is that you no longer care about algorithm hype, it’s just another method to include in your spot checking suite. You no longer fret over whether you’re missing out by not using algorithm X or Y or configuration A or B (fear of loss), you throw them in the mix.

Leverage Automation

The data-driven approach is a problem of search. You can leverage automation.

You can write re-usable scripts to search the for the most reliable test harness for your problem before you begin. No more ad hoc guessing.

You can write a reusable script to try automatically 10, 20, 100 algorithms across a variety of libraries and implementations. No more favorite algorithms or libraries.

The line between different algorithms is gone. A new parameter configuration is a new algorithm. You can write re-usable scripts to grid or random search each algorithm to truly sample its capability.

Add feature engineering on the front so that each “view” on the data is a new problem for algorithms to be challenged against. Bolt-on ensembles at the end to combine some or all results (meta-algorithms).

I have been down this rabbit hole. It’s a powerful mindset and it gets robust results.

Summary

In this post, we have looked at the common heuristic and best-practice approach to algorithm and algorithm parameter selection.

We have considered that this approach leads to limitations in our thinking. We yearn for silver bullet general purpose best algorithms and best algorithm configurations, when no such things exist.

There is no best general purpose machine learning algorithm.

There are no best general purpose machine learning algorithm parameters.

The transferability of capability for an algorithm from one problem to another is questionable.

The solution is to become the scientist and to study algorithms on our problems.

We must take a data-driven problem, to spot check algorithms, to grid search algorithm parameters and to quickly find methods that yield good results, reliably and fast."
282;machinelearningmastery.com;https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/;2018-06-19;Statistical Significance Tests for Comparing Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 8, 2019

Comparing machine learning methods and selecting a final model is a common operation in applied machine learning.

Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean skill scores is real or the result of a statistical fluke.

Statistical significance tests are designed to address this problem and quantify the likelihood of the samples of skill scores being observed given the assumption that they were drawn from the same distribution. If this assumption, or null hypothesis, is rejected, it suggests that the difference in skill scores is statistically significant.

Although not foolproof, statistical hypothesis testing can improve both your confidence in the interpretation and the presentation of results during model selection.

In this tutorial, you will discover the importance and the challenge of selecting a statistical hypothesis test for comparing machine learning models.

After completing this tutorial, you will know:

Statistical hypothesis tests can aid in comparing machine learning models and choosing a final model.

The naive application of statistical hypothesis tests can lead to misleading results.

Correct use of statistical tests is challenging, and there is some consensus for using the McNemar’s test or 5×2 cross-validation with a modified paired Student t-test.

Discover statistical hypothesis testing, resampling methods, estimation statistics and nonparametric methods in my new book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Update Oct/2018: Added link to an example of using McNemar’s test.

Tutorial Overview

This tutorial is divided into 5 parts; they are:

The Problem of Model Selection Statistical Hypothesis Tests Problem of Choosing a Hypothesis Test Summary of Some Findings Recommendations

Need help with Statistics for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

The Problem of Model Selection

A big part of applied machine learning is model selection.

We can describe this in its simplest form:

Given the evaluation of two machine learning methods on a dataset, which model do you choose?

You choose the model with the best skill.

That is, the model whose estimated skill when making predictions on unseen data is best. This might be maximum accuracy or minimum error in the case of classification and regression problems respectively.

The challenge with selecting the model with the best skill is determining how much can you trust the estimated skill of each model. More generally:

Is the difference in skill between two machine learning models real, or due to a statistical chance?

We can use statistical hypothesis testing to address this question.

Statistical Hypothesis Tests

Generally, a statistical hypothesis test for comparing samples quantifies how likely it is to observe two data samples given the assumption that the samples have the same distribution.

The assumption of a statistical test is called the null hypothesis and we can calculate statistical measures and interpret them in order to decide whether or not to accept or reject the null hypothesis.

In the case of selecting models based on their estimated skill, we are interested to know whether there is a real or statistically significant difference between the two models.

If the result of the test suggests that there is insufficient evidence to reject the null hypothesis, then any observed difference in model skill is likely due to statistical chance.

If the result of the test suggests that there is sufficient evidence to reject the null hypothesis, then any observed difference in model skill is likely due to a difference in the models.

The results of the test are probabilistic, meaning, it is possible to correctly interpret the result and for the result to be wrong with a type I or type II error. Briefly, a false positive or false negative finding.

Comparing machine learning models via statistical significance tests imposes some expectations that in turn will impact the types of statistical tests that can be used; for example:

Skill Estimate . A specific measure of model skill must be chosen. This could be classification accuracy (a proportion) or mean absolute error (summary statistic) which will limit the type of tests that can be used.

. A specific measure of model skill must be chosen. This could be classification accuracy (a proportion) or mean absolute error (summary statistic) which will limit the type of tests that can be used. Repeated Estimates . A sample of skill scores is required in order to calculate statistics. The repeated training and testing of a given model on the same or different data will impact the type of test that can be used.

. A sample of skill scores is required in order to calculate statistics. The repeated training and testing of a given model on the same or different data will impact the type of test that can be used. Distribution of Estimates . The sample of skill score estimates will have a distribution, perhaps Gaussian or perhaps not. This will determine whether parametric or nonparametric tests can be used.

. The sample of skill score estimates will have a distribution, perhaps Gaussian or perhaps not. This will determine whether parametric or nonparametric tests can be used. Central Tendency. Model skill will often be described and compared using a summary statistic such as a mean or median, depending on the distribution of skill scores. The test may or may not take this directly into account.

The results of a statistical test are often a test statistic and a p-value, both of which can be interpreted and used in the presentation of the results in order to quantify the level of confidence or significance in the difference between models. This allows stronger claims to be made as part of model selection than not using statistical hypothesis tests.

Given that using statistical hypothesis tests seems desirable as part of model selection, how do you choose a test that is suitable for your specific use case?

Problem of Choosing a Hypothesis Test

Let’s look at a common example for evaluating and comparing classifiers for a balanced binary classification problem.

It is common practice to evaluate classification methods using classification accuracy, to evaluate each model using 10-fold cross-validation, to assume a Gaussian distribution for the sample of 10 model skill estimates, and to use the mean of the sample as a summary of the model’s skill.

We could require that each classifier evaluated using this procedure be evaluated on exactly the same splits of the dataset via 10-fold cross-validation. This would give samples of matched paired measures between two classifiers, matched because each classifier was evaluated on the same 10 test sets.

We could then select and use the paired Student’s t-test to check if the difference in the mean accuracy between the two models is statistically significant, e.g. reject the null hypothesis that assumes that the two samples have the same distribution.

In fact, this is a common way to compare classifiers with perhaps hundreds of published papers using this methodology.

The problem is, a key assumption of the paired Student’s t-test has been violated.

Namely, the observations in each sample are not independent. As part of the k-fold cross-validation procedure, a given observation will be used in the training dataset (k-1) times. This means that the estimated skill scores are dependent, not independent, and in turn that the calculation of the t-statistic in the test will be misleadingly wrong along with any interpretations of the statistic and p-value.

This observation requires a careful understanding of both the resampling method used, in this case k-fold cross-validation, and the expectations of the chosen hypothesis test, in this case the paired Student’s t-test. Without this background, the test appears appropriate, a result will be calculated and interpreted, and everything will look fine.

Unfortunately, selecting an appropriate statistical hypothesis test for model selection in applied machine learning is more challenging than it first appears. Fortunately, there is a growing body of research helping to point out the flaws of the naive approaches, and suggesting corrections and alternate methods.

Summary of Some Findings

In this section, let’s take a look at some of the research into the selection of appropriate statistical significance tests for model selection in machine learning.

Use McNemar’s test or 5×2 Cross-Validation

Perhaps the seminal work on this topic is the 1998 paper titled “Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms” by Thomas Dietterich.

It’s an excellent paper on the topic and a recommended read. It covers first a great framework for thinking about the points during a machine learning project where a statistical hypothesis test may be required, discusses the expectation on common violations of statistical tests relevant to comparing classifier machine learning methods, and finishes with an empirical evaluation of methods to confirm the findings.

This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task.

The focus of the selection and empirical evaluation of statistical hypothesis tests in the paper is that calibration of Type I error or false positives. That is, selecting a test that minimizes the case of suggesting a significant difference when no such difference exists.

There are a number of important findings in this paper.

The first finding is that using paired Student’s t-test on the results of skill estimated via random resamples of a training dataset should never be done.

… we can confidently conclude that the resampled t test should never be employed.

The assumptions of the paired t-test are violated in the case of random resampling and in the case of k-fold cross-validation (as noted above). Nevertheless, in the case of k-fold cross-validation, the t-test will be optimistic, resulting in a higher Type I error, but only a modest Type II error. This means that this combination could be used in cases where avoiding Type II errors is more important than succumbing to a Type I error.

The 10-fold cross-validated t test has high type I error. However, it also has high power, and hence, it can be recommended in those cases where type II error (the failure to detect a real difference between algorithms) is more important.

Dietterich recommends the McNemar’s statistical hypothesis test in cases where there is a limited amount of data and each algorithm can only be evaluated once.

McNemar’s test is like the Chi-Squared test, and in this case is used to determine whether the difference in observed proportions in the algorithm’s contingency table are significantly different from the expected proportions. This is a useful finding in the case of large deep learning neural networks that can take days or weeks to train.

Our experiments lead us to recommend […] McNemar’s test, for situations where the learning algorithms can be run only once.

Dietterich also recommends a resampling method of his own devising called 5×2 cross-validation that involves 5 repeats of 2-fold cross-validation.

Two folds are chosen to ensure that each observation appears only in the train or test dataset for a single estimate of model skill. A paired Student’s t-test is used on the results, updated to better reflect the limited degrees of freedom given the dependence between the estimated skill scores.

Our experiments lead us to recommend […] 5 x 2cv t test, for situations in which the learning algorithms are efficient enough to run ten times

Refinements on 5×2 Cross-Validation

The use of either McNemar’s test or 5×2 cross-validation has become a staple recommendation for much of the 20 years since the paper was published.

Nevertheless, further improvements have been made to better correct the paired Student’s t-test for the violation of the independence assumption from repeated k-fold cross-validation.

Two important papers among many include:

Claude Nadeau and Yoshua Bengio propose a further correction in their 2003 paper titled “Inference for the Generalization Error“. It’s a dense paper and not recommended for the faint of heart.

This analysis allowed us to construct two variance estimates that take into account both the variability due to the choice of the training sets and the choice of the test examples. One of the proposed estimators looks similar to the cv method (Dietterich, 1998) and is specifically designed to overestimate the variance to yield conservative inference.

Remco Bouckaert and Eibe Frank in their 2004 paper titled “Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms” take a different perspective and considers the ability to replicate results as more important than Type I or Type II errors.

In this paper we argue that the replicability of a test is also of importance. We say that a test has low replicability if its outcome strongly depends on the particular random partitioning of the data that is used to perform it

Surprisingly, they recommend using either 100 runs of random resampling or 10×10-fold cross-validation with the Nadeau and Bengio correction to the paired Student-t test in order to achieve good replicability.

The latter approach is recommended in Ian Witten and Eibe Frank’s book and in their open-source data mining platform Weka, referring to the Nadeau and Bengio correction as the “corrected resampled t-test“.

Various modifications of the standard t-test have been proposed to circumvent this problem, all of them heuristic and lacking sound theoretical justification. One that appears to work well in practice is the corrected resampled t-test. […] The same modified statistic can be used with repeated cross-validation, which is just a special case of repeated holdout in which the individual test sets for one cross- validation do not overlap.

— Page 159, Chapter 5, Credibility: Evaluating What’s Been Learned, Data Mining: Practical Machine Learning Tools and Techniques, Third Edition, 2011.

Recommendations

There are no silver bullets when it comes to selecting a statistical significance test for model selection in applied machine learning.

Let’s look at five approaches that you may use on your machine learning project to compare classifiers.

1. Independent Data Samples

If you have near unlimited data, gather k separate train and test datasets to calculate 10 truly independent skill scores for each method.

You may then correctly apply the paired Student’s t-test. This is most unlikely as we are often working with small data samples.

… the assumption that there is essentially unlimited data so that several independent datasets of the right size can be used. In practice there is usually only a single dataset of limited size. What can be done?

— Page 158, Chapter 5, Credibility: Evaluating What’s Been Learned, Data Mining: Practical Machine Learning Tools and Techniques, Third Edition, 2011.

2. Accept the Problems of 10-fold CV

The naive 10-fold cross-validation can be used with an unmodified paired Student t-test can be used.

It has good repeatability relative to other methods and a modest type II error, but is known to have a high type I error.

The experiments also suggest caution in interpreting the results of the 10-fold cross-validated t test. This test has an elevated probability of type I error (as much as twice the target level), although it is not nearly as severe as the problem with the resampled t test.

— Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms, 1998.

It’s an option, but it’s very weakly recommended.

3. Use McNemar’s Test or 5×2 CV

The two-decade long recommendations of McNemar’s test for single-run classification accuracy results and 5×2-fold cross-validation with a modified paired Student’s t-test in general stand.

Further, the Nadeau and Bengio further correction to the test statistic may be used with the 5×2-fold cross validation or 10×10-fold cross-validation as recommended by the developers of Weka.

A challenge in using the modified t-statistic is that there is no off-the-shelf implementation (e.g. in SciPy), requiring the use of third-party code and the risks that this entails. You may have to implement it yourself.

The availability and complexity of a chosen statistical method is an important consideration, said well by Gitte Vanwinckelen and Hendrik Blockeel in their 2012 paper titled “On Estimating Model Accuracy with Repeated Cross-Validation“:

While these methods are carefully designed, and are shown to improve upon previous methods in a number of ways, they suffer from the same risk as previous methods, namely that the more complex a method is, the higher the risk that researchers will use it incorrectly, or interpret the result incorrectly.

I have an example of using McNemar’s test here:

4. Use a Nonparametric Paired Test

We can use a nonparametric test that makes fewer assumptions, such as not assuming that the distribution of the skill scores is Gaussian.

One example is the Wilcoxon signed-rank test, which is the nonparametric version of the paired Student’s t-test. This test has less statistical power than the paired t-test, although more power when the expectations of the t-test are violated, such as independence.

This statistical hypothesis test is recommended for comparing algorithms different datasets by Janez Demsar in his 2006 paper “Statistical Comparisons of Classifiers over Multiple Data Sets“.

We therefore recommend using the Wilcoxon test, unless the t-test assumptions are met, either because we have many data sets or because we have reasons to believe that the measure of performance across data sets is distributed normally.

Although the test is nonparametric, it still assumes that the observations within each sample are independent (e.g. iid), and using k-fold cross-validation would create dependent samples and violate this assumption.

5. Use Estimation Statistics Instead

Instead of statistical hypothesis tests, estimation statistics can be calculated, such as confidence intervals. These would suffer from similar problems where the assumption of independence is violated given the resampling methods by which the models are evaluated.

Tom Mitchell makes a similar recommendation in his 1997 book, suggesting to take the results of statistical hypothesis tests as heuristic estimates and seek confidence intervals around estimates of model skill:

To summarize, no single procedure for comparing learning methods based on limited data satisfies all the constraints we would like. It is wise to keep in mind that statistical models rarely fit perfectly the practical constraints in testing learning algorithms when available data is limited. Nevertheless, they do provide approximate confidence intervals that can be of great help in interpreting experimental comparisons of learning methods.

— Page 150, Chapter 5, Evaluating Hypotheses, Machine Learning, 1997.

Statistical methods such as the bootstrap can be used to calculate defensible nonparametric confidence intervals that can be used to both present results and compare classifiers. This is a simple and effective approach that you can always fall back upon and that I recommend in general.

In fact confidence intervals have received the most theoretical study of any topic in the bootstrap area.

— Page 321, An Introduction to the Bootstrap, 1994.

Extensions

This section lists some ideas for extending the tutorial that you may wish to explore.

Find and list three research papers that incorrectly use the unmodified paired Student’s t-test to compare and choose a machine learning model.

Summarize the framework for using statistical hypothesis tests in a machine learning project presented in Thomas Dietterich’s 1998 paper.

Find and list three research papers that correctly use either the McNemar’s test or 5×2 Cross-Validation for comparison and choose a machine learning model.

If you explore any of these extensions, I’d love to know.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

Books

Articles

Discussions

Summary

In this tutorial, you discovered the importance and the challenge of selecting a statistical hypothesis test for comparing machine learning models.

Specifically, you learned:

Statistical hypothesis tests can aid in comparing machine learning models and choosing a final model.

The naive application of statistical hypothesis tests can lead to misleading results.

Correct use of statistical tests is challenging, and there is some consensus for using the McNemar’s test or 5×2 cross-validation with a modified paired Student t-test.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Statistics for Machine Learning! Develop a working understanding of statistics ...by writing lines of code in python Discover how in my new Ebook:

Statistical Methods for Machine Learning It provides self-study tutorials on topics like:

Hypothesis Tests, Correlation, Nonparametric Stats, Resampling, and much more... Discover how to Transform Data into Knowledge Skip the Academics. Just Results. See What's Inside"
283;machinelearningmastery.com;https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/;2020-01-23;Tour of Data Sampling Methods for Imbalanced Classification;"Tweet Share Share

Machine learning techniques often fail or give misleadingly optimistic performance on classification datasets with an imbalanced class distribution.

The reason is that many machine learning algorithms are designed to operate on classification data with an equal number of observations for each class. When this is not the case, algorithms can learn that very few examples are not important and can be ignored in order to achieve good performance.

Data sampling provides a collection of techniques that transform a training dataset in order to balance or better balance the class distribution. Once balanced, standard machine learning algorithms can be trained directly on the transformed dataset without any modification. This allows the challenge of imbalanced classification, even with severely imbalanced class distributions, to be addressed with a data preparation method.

There are many different types of data sampling methods that can be used, and there is no single best method to use on all classification problems and with all classification models. Like choosing a predictive model, careful experimentation is required to discover what works best for your project.

In this tutorial, you will discover a suite of data sampling techniques that can be used to balance an imbalanced classification dataset.

After completing this tutorial, you will know:

The challenge of machine learning with imbalanced classification datasets.

The balancing of skewed class distributions using data sampling techniques.

Tour of data sampling methods for oversampling, undersampling, and combinations of methods.

Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more in my new book, with 30 step-by-step tutorials and full Python source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Problem of an Imbalanced Class Distribution Balance the Class Distribution With Data Sampling Tour of Popular Data Sampling Methods Oversampling Techniques Undersampling Techniques Combinations of Techniques

Problem of an Imbalanced Class Distribution

Imbalanced classification involves a dataset where the class distribution is not equal.

This means that the number of examples that belong to each class in the training dataset varies, often widely. It is not uncommon to have a severe skew in the class distribution, such as 1:10, 1:1000 or even 1:1000 ratio of examples in the minority class to those in the majority class.

… we define imbalanced learning as the learning process for data representation and information extraction with severe data distribution skews to develop effective decision boundaries to support the decision-making process.

— Page 1, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

Although often described in terms of two-class classification problems, class imbalance also affects those datasets with more than two classes that may have multiple minority classes or multiple majority classes.

A chief problem with imbalanced classification datasets is that standard machine learning algorithms do not perform well on them. Many machine learning algorithms rely upon the class distribution in the training dataset to gauge the likelihood of observing examples in each class when the model will be used to make predictions.

As such, many machine learning algorithms, like decision trees, k-nearest neighbors, and neural networks, will therefore learn that the minority class is not as important as the majority class and put more attention and perform better on the majority class.

The hitch with imbalanced datasets is that standard classification learning algorithms are often biased towards the majority classes (known as “negative”) and therefore there is a higher misclassification rate in the minority class instances (called the “positive” class).

— Page 79, Learning from Imbalanced Data Sets, 2018.

This is a problem because the minority class is exactly the class that we care most about in imbalanced classification problems.

The reason for this is because the majority class often reflects a normal case, whereas the minority class represents a positive case for a diagnostic, fault, fraud, or other types of exceptional circumstance.

Want to Get Started With Imbalance Classification? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Balance the Class Distribution With Data Sampling

The most popular solution to an imbalanced classification problem is to change the composition of the training dataset.

Techniques designed to change the class distribution in the training dataset are generally referred to as sampling methods or resampling methods as we are sampling an existing data sample.

Sampling methods seem to be the dominate type of approach in the community as they tackle imbalanced learning in a straightforward manner.

— Page 3, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

The reason that sampling methods are so common is because they are simple to understand and implement, and because once applied to transform the training dataset, a suite of standard machine learning algorithms can then be used directly.

This means that any from tens or hundreds of machine learning algorithms developed for balanced (or mostly balanced) classification can then be fit on the training dataset without any modification adapting them for the imbalance in observations.

Basically, instead of having the model deal with the imbalance, we can attempt to balance the class frequencies. Taking this approach eliminates the fundamental imbalance issue that plagues model training.

— Page 427, Applied Predictive Modeling, 2013.

Machine learning algorithms like the Naive Bayes Classifier learn the likelihood of observing examples from each class from the training dataset. By fitting these models on a sampled training dataset with an artificially more equal class distribution, it allows them to learn a less biased prior probability and instead focus on the specifics (or evidence) from each input variable to discriminate the classes.

Some models use prior probabilities, such as naive Bayes and discriminant analysis classifiers. Unless specified manually, these models typically derive the value of the priors from the training data. Using more balanced priors or a balanced training set may help deal with a class imbalance.

— Page 426, Applied Predictive Modeling, 2013.

Sampling is only performed on the training dataset, the dataset used by an algorithm to learn a model. It is not performed on the holdout test or validation dataset. The reason is that the intent is not to remove the class bias from the model fit but to continue to evaluate the resulting model on data that is both real and representative of the target problem domain.

As such, we can think of data sampling methods as addressing the problem of relative class imbalanced in the training dataset, and ignoring the underlying cause of the imbalance in the problem domain. The difference between so-called relative and absolute rarity of examples in a minority class.

Sampling methods are a very popular method for dealing with imbalanced data. These methods are primarily employed to address the problem with relative rarity but do not address the issue of absolute rarity.

— Page 29, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

Evaluating a model on a transformed dataset with examples deleted or synthesized would likely provide a misleading and perhaps optimistic estimation of performance.

There are two main types of data sampling used on the training dataset: oversampling and undersampling. In the next section, we will take a tour of popular methods from each type, as well as methods that combine multiple approaches.

Tour of Popular Data Sampling Methods

There are tens, if not hundreds, of data sampling methods to choose from in order to adjust the class distribution of the training dataset.

There is no best data sampling method, just like there is no best machine learning algorithm. The methods behave differently depending on the choice of learning algorithm and on the density and composition of the training dataset.

… in many cases, sampling can mitigate the issues caused by an imbalance, but there is no clear winner among the various approaches. Also, many modeling techniques react differently to sampling, further complicating the idea of a simple guideline for which procedure to use

— Page 429, Applied Predictive Modeling, 2013.

As such, it is important to carefully design experiments to test and evaluate a suite of different methods and different configurations for some methods in order to discover what works best for your specific project.

Although there are many techniques to choose from, there are perhaps a dozen that are more popular and perhaps more successful on average. In this section, we will take a tour of these methods organized into a rough taxonomy of oversampling, undersampling, and combined methods.

Representative work in this area includes random oversampling, random undersampling, synthetic sampling with data generation, cluster-based sampling methods, and integration of sampling and boosting.

— Page 3, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

The following sections review some of the more popular methods, described in the context of binary (two-class) classification problems, which is a common practice, although most can be used directly or adapted for imbalanced classification with more than two classes.

The list here is based mostly on the approaches available in the scikit-learn friendly library, called imbalanced-learn. For a longer list of data sampling methods, see Chapter 5 Data Level Preprocessing Methods in the 2018 book “Learning from Imbalanced Data Sets.”

What is your favorite data sampling technique?

Did I miss a good method?

Let me know in the comments below.

Oversampling Techniques

Oversampling methods duplicate examples in the minority class or synthesize new examples from the examples in the minority class.

Some of the more widely used and implemented oversampling methods include:

Random Oversampling

Synthetic Minority Oversampling Technique (SMOTE)

Borderline-SMOTE

Borderline Oversampling with SVM

Adaptive Synthetic Sampling (ADASYN)

Let’s take a closer look at these methods.

The simplest oversampling method involves randomly duplicating examples from the minority class in the training dataset, referred to as Random Oversampling.

The most popular and perhaps most successful oversampling method is SMOTE; that is an acronym for Synthetic Minority Oversampling Technique.

SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample as a point along that line.

There are many extensions to the SMOTE method that aim to be more selective for the types of examples in the majority class that are synthesized.

Borderline-SMOTE involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model, and only generating synthetic samples that are “difficult” to classify.

Borderline Oversampling is an extension to SMOTE that fits an SVM to the dataset and uses the decision boundary as defined by the support vectors as the basis for generating synthetic examples, again based on the idea that the decision boundary is the area where more minority examples are required.

Adaptive Synthetic Sampling (ADASYN) is another extension to SMOTE that generates synthetic samples inversely proportional to the density of the examples in the minority class. It is designed to create synthetic examples in regions of the feature space where the density of minority examples is low, and fewer or none where the density is high.

Undersampling Techniques

Undersampling methods delete or select a subset of examples from the majority class.

Some of the more widely used and implemented undersampling methods include:

Random Undersampling

Condensed Nearest Neighbor Rule (CNN)

Near Miss Undersampling

Tomek Links Undersampling

Edited Nearest Neighbors Rule (ENN)

One-Sided Selection (OSS)

Neighborhood Cleaning Rule (NCR)

Let’s take a closer look at these methods.

The simplest undersampling method involves randomly deleting examples from the majority class in the training dataset, referred to as random undersampling.

One group of techniques involves selecting a robust and representative subset of the examples in the majority class.

The Condensed Nearest Neighbors rule, or CNN for short, was designed for reducing the memory required for the k-nearest neighbors algorithm. It works by enumerating the examples in the dataset and adding them to the store only if they cannot be classified correctly by the current contents of the store, and can be applied to reduce the number of examples in the majority class after all examples in the minority class have been added to the store.

Near Miss refers to a family of methods that use KNN to select examples from the majority class. NearMiss-1 selects examples from the majority class that have the smallest average distance to the three closest examples from the minority class. NearMiss-2 selects examples from the majority class that have the smallest average distance to the three furthest examples from the minority class. NearMiss-3 involves selecting a given number of majority class examples for each example in the minority class that are closest.

Another group of techniques involves selecting examples from the majority class to delete. These approaches typically involve identifying those examples that are challenging to classify and therefore add ambiguity to the decision boundary.

Perhaps the most widely known deletion undersampling approach is referred to as Tomek Links, originally developed as part of an extension to the Condensed Nearest Neighbors rule. A Tomek Link refers to a pair of examples in the training dataset that are both nearest neighbors (have the minimum distance in feature space) and belong to different classes. Tomek Links are often misclassified examples found along the class boundary and the examples in the majority class are deleted.

The Edited Nearest Neighbors rule, or ENN for short, is another method for selecting examples for deletion. This rule involves using k=3 nearest neighbors to locate those examples in a dataset that are misclassified and deleting them.

The ENN procedure can be repeated multiple times on the same dataset, better refining the selection of examples in the majority class. This extension is referred to initially as “unlimited editing” although it is more commonly referred to as Repeatedly Edited Nearest Neighbors.

Staying with the “select to keep” vs. “select to delete” families of undersampling methods, there are also undersampling methods that combine both approaches.

One-Sided Selection, or OSS for short, is an undersampling technique combines Tomek Links and the Condensed Nearest Neighbor (CNN) Rule. The Tomek Links method is used to remove noisy examples on the class boundary, whereas CNN is used to remove redundant examples from the interior of the density of the majority class.

The Neighborhood Cleaning Rule, or NCR for short, is another combination undersampling technique that combines both the Condensed Nearest Neighbor (CNN) Rule to remove redundant examples and the Edited Nearest Neighbors (ENN) Rule to remove noisy or ambiguous examples.

Combinations of Techniques

Although an oversampling or undersampling method when used alone on a training dataset can be effective, experiments have shown that applying both types of techniques together can often result in better overall performance of a model fit on the resulting transformed dataset.

Some of the more widely used and implemented combinations of data sampling methods include:

SMOTE and Random Undersampling

SMOTE and Tomek Links

SMOTE and Edited Nearest Neighbors Rule

Let’s take a closer look at these methods.

SMOTE is perhaps the most popular and widely used oversampling technique. As such, it is typical paired with one from a range of different undersampling methods.

The simplest pairing involves combining SMOTE with random undersampling, which was suggested to perform better than using SMOTE alone in the paper that proposed the method.

It is common to pair SMOTE with an undersampling method that selects examples from the dataset to delete, and the procedure is applied to the dataset after SMOTE, allowing the editing step to be applied to both the minority and majority class. The intent is to remove noisy points along the class boundary from both classes, which seems to have the effect of the better performance of classifiers fit on the transformed dataset.

Two popular examples involve using SMOTE followed by the deletion of Tomek Links, and SMOTE followed by the deletion of those examples misclassified via a KNN model, the so-called Edited Nearest Neighbors rule.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Papers

Books

Articles

Summary

In this tutorial, you discovered a suite of data sampling techniques that can be used to balance an imbalanced classification dataset.

Specifically, you learned:

The challenge of machine learning with imbalanced classification datasets.

The balancing of skewed class distributions using data sampling techniques.

Tour of popular data sampling methods for oversampling, undersampling, and combinations of methods.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Imbalanced Classification! Develop Imbalanced Learning Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Imbalanced Classification with Python It provides self-study tutorials and end-to-end projects on:

Performance Metrics, Undersampling Methods, SMOTE, Threshold Moving, Probability Calibration, Cost-Sensitive Algorithms

and much more... Bring Imbalanced Classification Methods to Your Machine Learning Projects See What's Inside"
284;machinelearningmastery.com;https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/;2018-11-20;How to Use Weight Decay to Reduce Overfitting of Neural Network in Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34

# grid search regularization values for moons dataset from sklearn . datasets import make_moons from keras . layers import Dense from keras . models import Sequential from keras . regularizers import l2 from matplotlib import pyplot # generate 2d classification dataset X , y = make_moons ( n_samples = 100 , noise = 0.2 , random_state = 1 ) # split into train and test n_train = 30 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] # grid search values values = [ 1e - 1 , 1e - 2 , 1e - 3 , 1e - 4 , 1e - 5 , 1e - 6 ] all_train , all_test = list ( ) , list ( ) for param in values : # define model model = Sequential ( ) model . add ( Dense ( 500 , input_dim = 2 , activation = 'relu' , kernel_regularizer = l2 ( param ) ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( trainX , trainy , epochs = 4000 , verbose = 0 ) # evaluate the model _ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 ) _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) print ( 'Param: %f, Train: %.3f, Test: %.3f' % ( param , train_acc , test_acc ) ) all_train . append ( train_acc ) all_test . append ( test_acc ) # plot train and test means pyplot . semilogx ( values , all_train , label = 'train' , marker = 'o' ) pyplot . semilogx ( values , all_test , label = 'test' , marker = 'o' ) pyplot . legend ( ) pyplot . show ( )"
285;news.mit.edu;http://news.mit.edu/2020/researchers-discover-new-way-control-infrared-light-0130;;Researchers discover a new way to control infrared light;"In the 1950s, the field of electronics began to change when the transistor replaced vacuum tubes in computers. The change, which entailed replacing large and slow components with small and fast ones, was a catalyst for the enduring trend of miniaturization in computer design. No such revolution has yet hit the field of infrared optics, which remains reliant on bulky moving parts that preclude building small systems.

However, a team of researchers at MIT Lincoln Laboratory, together with Professor Juejun Hu and graduate students from MIT's Department of Materials Science and Engineering, is devising a way to control infrared light by using phase-change materials instead of moving parts. These materials have the ability to change their optical properties when energy is added to them.

“There are multiple possible ways where this material can enable new photonic devices that impact people’s lives,” says Hu. “For example, it can be useful for energy-efficient optical switches, which can improve network speed and reduce power consumption of internet data centers. It can enable reconfigurable meta-optical devices, such as compact, flat infrared zoom lenses without mechanical moving parts. It can also lead to new computing systems, which can make machine learning faster and more power-efficient compared to current solutions.”

A fundamental property of phase-change materials is that they can change how fast light travels through them (the refractive index). “There are already ways to modulate light using a refractive index change, but phase-change materials can change almost 1,000 times better,” says Jeffrey Chou, a team member formerly in the laboratory's Advanced Materials and Microsystems Group.

The team successfully controlled infrared light in multiple systems by using a new class of phase-change material containing the elements germanium, antimony, selenium, and tellurium, collectively known as GSST. This work is discussed in a paper published in Nature Communications.

A phase-change material's magic occurs in the chemical bonds that tie its atoms together. In one phase state, the material is crystalline, with its atoms arranged in an organized pattern. This state can be changed by applying a short, high-temperature spike of thermal energy to the material, causing the bonds in the crystal to break down and then reform in a more random, or amorphous, pattern. To change the material back to the crystalline state, a long- and medium-temperature pulse of thermal energy is applied.

“This changing of the chemical bonds allows for different optical properties to emerge, similar to the differences between coal (amorphous) and diamond (crystalline),” says Christopher Roberts, another Lincoln Laboratory member of the research team. “While both materials are mostly carbon, they have vastly different optical properties.”

Currently, phase-change materials are used for industry applications, such as Blu-ray technology and rewritable DVDs, because their properties are useful for storing and erasing a large amount of information. But so far, no one has used them in infrared optics because they tend to be transparent in one state and opaque in the other. (Think of the diamond, which light can pass through, and coal, which light cannot penetrate.) If light cannot pass through one of the states, then that light cannot be adequately controlled for a range of uses; instead, a system would only be able to work like an on/off switch, allowing light to either pass through the material or not pass through at all.

However, the research team found that that by adding the element selenium to the original material (called GST), the material's absorption of infrared light in the crystalline phase decreased dramatically — in essence, changing it from an opaque coal-like material to a more transparent diamond-like one. What's more, the large difference in the refractive index of the two states affects the propagation of light through them.

“This change in refractive index, without introducing optical loss, allows for the design of devices that control infrared light without the need for mechanical parts,” Roberts says.

As an example, imagine a laser beam that is pointing in one direction and needs to be changed to another. In current systems, a large mechanical gimbal would physically move a lens to steer the beam to another position. A thin-film lens made of GSST would be able change positions by electrically reprogramming the phase-change materials, enabling beam steering with no moving parts.

The team has already tested the material successfully in a moving lens. They have also demonstrated its use in infrared hyperspectral imaging, which is used to analyze images for hidden objects or information, and in a fast optical shutter that was able to close in nanoseconds.

The potential uses for GSST are vast, and an ultimate goal for the team is to design reconfigurable optical chips, lenses, and filters, which currently must be rebuilt from scratch each time a change is required. Once the team is ready to move the material beyond the research phase, it should be fairly easy to transition it into the commercial space. Because it's already compatible with standard microelectronic fabrication processes, GSST components could be made at a low cost and in large numbers.

Recently, the laboratory obtained a combinatorial sputtering chamber — a state-of-the-art machine that allows researchers to create custom materials out of individual elements. The team will use this chamber to further optimize the materials for improved reliability and switching speeds, as well as for low-power applications. They also plan to experiment with other materials that may prove useful in controlling visible light.

The next steps for the team are to look closely into real-world applications of GSST and understand what those systems need in terms of power, size, switching speed, and optical contrast.

“The impact [of this research] is twofold,” Hu says. ""Phase-change materials offer a dramatically enhanced refractive index change compared to other physical effects — induced by electric field or temperature change, for instance — thereby enabling extremely compact reprogrammable optical devices and circuits. Our demonstration of bistate optical transparency in these materials is also significant in that we can now create high-performance infrared components with minimal optical loss.” The new material, Hu continues, is expected to open up an entirely new design space in the field of infrared optics.

This research was supported with funding from the laboratory's Technology Office and the U.S. Defense Advanced Research Projects Agency."
286;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/;2017-11-09;How to Develop a Word-Level Neural Language Model and Use it to Generate Text;"Tweet Share Share

Last Updated on August 7, 2019

A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence.

Neural network models are a preferred method for developing statistical language models because they can use a distributed representation where different words with similar meanings have similar representation and because they can use a large context of recently observed words when making predictions.

In this tutorial, you will discover how to develop a statistical language model using deep learning in Python.

After completing this tutorial, you will know:

How to prepare text for developing a word-based language model.

How to design and fit a neural language model with a learned embedding and an LSTM hidden layer.

How to use the learned language model to generate new text with similar statistical properties as the source text.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Update Apr/2018: Fixed mismatch between 100 input words in description of the model and 50 in the actual model.

Tutorial Overview

This tutorial is divided into 4 parts; they are:

The Republic by Plato Data Preparation Train Language Model Use Language Model

The Republic by Plato

The Republic is the classical Greek philosopher Plato’s most famous work.

It is structured as a dialog (e.g. conversation) on the topic of order and justice within a city state

The entire text is available for free in the public domain. It is available on the Project Gutenberg website in a number of formats.

You can download the ASCII text version of the entire book (or books) here:

Download the book text and place it in your current working directly with the filename ‘republic.txt‘

Open the file in a text editor and delete the front and back matter. This includes details about the book at the beginning, a long analysis, and license information at the end.

The text should begin with:

BOOK I. I went down yesterday to the Piraeus with Glaucon the son of Ariston,

…

And end with

…

And it shall be well with us both in this life and in the pilgrimage of a thousand years which we have been describing.

Save the cleaned version as ‘republic_clean.txt’ in your current working directory. The file should be about 15,802 lines of text.

Now we can develop a language model from this text.

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

Data Preparation

We will start by preparing the data for modeling.

The first step is to look at the data.

Review the Text

Open the text in an editor and just look at the text data.

For example, here is the first piece of dialog:

BOOK I. I went down yesterday to the Piraeus with Glaucon the son of Ariston,

that I might offer up my prayers to the goddess (Bendis, the Thracian

Artemis.); and also because I wanted to see in what manner they would

celebrate the festival, which was a new thing. I was delighted with the

procession of the inhabitants; but that of the Thracians was equally,

if not more, beautiful. When we had finished our prayers and viewed the

spectacle, we turned in the direction of the city; and at that instant

Polemarchus the son of Cephalus chanced to catch sight of us from a

distance as we were starting on our way home, and told his servant to

run and bid us wait for him. The servant took hold of me by the cloak

behind, and said: Polemarchus desires you to wait. I turned round, and asked him where his master was. There he is, said the youth, coming after you, if you will only wait. Certainly we will, said Glaucon; and in a few minutes Polemarchus

appeared, and with him Adeimantus, Glaucon’s brother, Niceratus the son

of Nicias, and several others who had been at the procession. Polemarchus said to me: I perceive, Socrates, that you and your

companion are already on your way to the city. You are not far wrong, I said. …

What do you see that we will need to handle in preparing the data?

Here’s what I see from a quick look:

Book/Chapter headings (e.g. “BOOK I.”).

British English spelling (e.g. “honoured”)

Lots of punctuation (e.g. “–“, “;–“, “?–“, and more)

Strange names (e.g. “Polemarchus”).

Some long monologues that go on for hundreds of lines.

Some quoted dialog (e.g. ‘…’)

These observations, and more, suggest at ways that we may wish to prepare the text data.

The specific way we prepare the data really depends on how we intend to model it, which in turn depends on how we intend to use it.

Language Model Design

In this tutorial, we will develop a model of the text that we can then use to generate new sequences of text.

The language model will be statistical and will predict the probability of each word given an input sequence of text. The predicted word will be fed in as input to in turn generate the next word.

A key design decision is how long the input sequences should be. They need to be long enough to allow the model to learn the context for the words to predict. This input length will also define the length of seed text used to generate new sequences when we use the model.

There is no correct answer. With enough time and resources, we could explore the ability of the model to learn with differently sized input sequences.

Instead, we will pick a length of 50 words for the length of the input sequences, somewhat arbitrarily.

We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.

Instead, to keep the example brief, we will let all of the text flow together and train the model to predict the next word across sentences, paragraphs, and even books or chapters in the text.

Now that we have a model design, we can look at transforming the raw text into sequences of 50 input words to 1 output word, ready to fit a model.

Load Text

The first step is to load the text into memory.

We can develop a small function to load the entire text file into memory and return it. The function is called load_doc() and is listed below. Given a filename, it returns a sequence of loaded text.

# load doc into memory def load_doc(filename): # open the file as read only file = open(filename, 'r') # read all text text = file.read() # close the file file.close() return text 1 2 3 4 5 6 7 8 9 # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text

Using this function, we can load the cleaner version of the document in the file ‘republic_clean.txt‘ as follows:

# load document in_filename = 'republic_clean.txt' doc = load_doc(in_filename) print(doc[:200]) 1 2 3 4 # load document in_filename = 'republic_clean.txt' doc = load_doc ( in_filename ) print ( doc [ : 200 ] )

Running this snippet loads the document and prints the first 200 characters as a sanity check.

BOOK I. I went down yesterday to the Piraeus with Glaucon the son of Ariston,

that I might offer up my prayers to the goddess (Bendis, the Thracian

Artemis.); and also because I wanted to see in what

So far, so good. Next, let’s clean the text.

Clean Text

We need to transform the raw text into a sequence of tokens or words that we can use as a source to train the model.

Based on reviewing the raw text (above), below are some specific operations we will perform to clean the text. You may want to explore more cleaning operations yourself as an extension.

Replace ‘–‘ with a white space so we can split words better.

Split words based on white space.

Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).

Remove all words that are not alphabetic to remove standalone punctuation tokens.

Normalize all words to lowercase to reduce the vocabulary size.

Vocabulary size is a big deal with language modeling. A smaller vocabulary results in a smaller model that trains faster.

We can implement each of these cleaning operations in this order in a function. Below is the function clean_doc() that takes a loaded document as an argument and returns an array of clean tokens.

import string # turn a doc into clean tokens def clean_doc(doc): # replace '--' with a space ' ' doc = doc.replace('--', ' ') # split into tokens by white space tokens = doc.split() # remove punctuation from each token table = str.maketrans('', '', string.punctuation) tokens = [w.translate(table) for w in tokens] # remove remaining tokens that are not alphabetic tokens = [word for word in tokens if word.isalpha()] # make lower case tokens = [word.lower() for word in tokens] return tokens 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import string # turn a doc into clean tokens def clean_doc ( doc ) : # replace '--' with a space ' ' doc = doc . replace ( '--' , ' ' ) # split into tokens by white space tokens = doc . split ( ) # remove punctuation from each token table = str . maketrans ( '' , '' , string . punctuation ) tokens = [ w . translate ( table ) for w in tokens ] # remove remaining tokens that are not alphabetic tokens = [ word for word in tokens if word . isalpha ( ) ] # make lower case tokens = [ word . lower ( ) for word in tokens ] return tokens

We can run this cleaning operation on our loaded document and print out some of the tokens and statistics as a sanity check.

# clean document tokens = clean_doc(doc) print(tokens[:200]) print('Total Tokens: %d' % len(tokens)) print('Unique Tokens: %d' % len(set(tokens))) 1 2 3 4 5 # clean document tokens = clean_doc ( doc ) print ( tokens [ : 200 ] ) print ( 'Total Tokens: %d' % len ( tokens ) ) print ( 'Unique Tokens: %d' % len ( set ( tokens ) ) )

First, we can see a nice list of tokens that look cleaner than the raw text. We could remove the ‘Book I‘ chapter markers and more, but this is a good start.

['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said'] 1 ['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']

We also get some statistics about the clean document.

We can see that there are just under 120,000 words in the clean text and a vocabulary of just under 7,500 words. This is smallish and models fit on this data should be manageable on modest hardware.

Total Tokens: 118684 Unique Tokens: 7409 1 2 Total Tokens: 118684 Unique Tokens: 7409

Next, we can look at shaping the tokens into sequences and saving them to file.

Save Clean Text

We can organize the long list of tokens into sequences of 50 input words and 1 output word.

That is, sequences of 51 words.

We can do this by iterating over the list of tokens from token 51 onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens.

We will transform the tokens into space-separated strings for later storage in a file.

The code to split the list of clean tokens into sequences with a length of 51 tokens is listed below.

# organize into sequences of tokens length = 50 + 1 sequences = list() for i in range(length, len(tokens)): # select sequence of tokens seq = tokens[i-length:i] # convert into a line line = ' '.join(seq) # store sequences.append(line) print('Total Sequences: %d' % len(sequences)) 1 2 3 4 5 6 7 8 9 10 11 # organize into sequences of tokens length = 50 + 1 sequences = list ( ) for i in range ( length , len ( tokens ) ) : # select sequence of tokens seq = tokens [ i - length : i ] # convert into a line line = ' ' . join ( seq ) # store sequences . append ( line ) print ( 'Total Sequences: %d' % len ( sequences ) )

Running this piece creates a long list of lines.

Printing statistics on the list, we can see that we will have exactly 118,633 training patterns to fit our model.

Total Sequences: 118633 1 Total Sequences: 118633

Next, we can save the sequences to a new file for later loading.

We can define a new function for saving lines of text to a file. This new function is called save_doc() and is listed below. It takes as input a list of lines and a filename. The lines are written, one per line, in ASCII format.

# save tokens to file, one dialog per line def save_doc(lines, filename): data = '

'.join(lines) file = open(filename, 'w') file.write(data) file.close() 1 2 3 4 5 6 # save tokens to file, one dialog per line def save_doc ( lines , filename ) : data = '

' . join ( lines ) file = open ( filename , 'w' ) file . write ( data ) file . close ( )

We can call this function and save our training sequences to the file ‘republic_sequences.txt‘.

# save sequences to file out_filename = 'republic_sequences.txt' save_doc(sequences, out_filename) 1 2 3 # save sequences to file out_filename = 'republic_sequences.txt' save_doc ( sequences , out_filename )

Take a look at the file with your text editor.

You will see that each line is shifted along one word, with a new word at the end to be predicted; for example, here are the first 3 lines in truncated form:

book i i … catch sight of

i i went … sight of us

i went down … of us from

…

Complete Example

Tying all of this together, the complete code listing is provided below.

import string # load doc into memory def load_doc(filename): # open the file as read only file = open(filename, 'r') # read all text text = file.read() # close the file file.close() return text # turn a doc into clean tokens def clean_doc(doc): # replace '--' with a space ' ' doc = doc.replace('--', ' ') # split into tokens by white space tokens = doc.split() # remove punctuation from each token table = str.maketrans('', '', string.punctuation) tokens = [w.translate(table) for w in tokens] # remove remaining tokens that are not alphabetic tokens = [word for word in tokens if word.isalpha()] # make lower case tokens = [word.lower() for word in tokens] return tokens # save tokens to file, one dialog per line def save_doc(lines, filename): data = '

'.join(lines) file = open(filename, 'w') file.write(data) file.close() # load document in_filename = 'republic_clean.txt' doc = load_doc(in_filename) print(doc[:200]) # clean document tokens = clean_doc(doc) print(tokens[:200]) print('Total Tokens: %d' % len(tokens)) print('Unique Tokens: %d' % len(set(tokens))) # organize into sequences of tokens length = 50 + 1 sequences = list() for i in range(length, len(tokens)): # select sequence of tokens seq = tokens[i-length:i] # convert into a line line = ' '.join(seq) # store sequences.append(line) print('Total Sequences: %d' % len(sequences)) # save sequences to file out_filename = 'republic_sequences.txt' save_doc(sequences, out_filename) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 import string # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text # turn a doc into clean tokens def clean_doc ( doc ) : # replace '--' with a space ' ' doc = doc . replace ( '--' , ' ' ) # split into tokens by white space tokens = doc . split ( ) # remove punctuation from each token table = str . maketrans ( '' , '' , string . punctuation ) tokens = [ w . translate ( table ) for w in tokens ] # remove remaining tokens that are not alphabetic tokens = [ word for word in tokens if word . isalpha ( ) ] # make lower case tokens = [ word . lower ( ) for word in tokens ] return tokens # save tokens to file, one dialog per line def save_doc ( lines , filename ) : data = '

' . join ( lines ) file = open ( filename , 'w' ) file . write ( data ) file . close ( ) # load document in_filename = 'republic_clean.txt' doc = load_doc ( in_filename ) print ( doc [ : 200 ] ) # clean document tokens = clean_doc ( doc ) print ( tokens [ : 200 ] ) print ( 'Total Tokens: %d' % len ( tokens ) ) print ( 'Unique Tokens: %d' % len ( set ( tokens ) ) ) # organize into sequences of tokens length = 50 + 1 sequences = list ( ) for i in range ( length , len ( tokens ) ) : # select sequence of tokens seq = tokens [ i - length : i ] # convert into a line line = ' ' . join ( seq ) # store sequences . append ( line ) print ( 'Total Sequences: %d' % len ( sequences ) ) # save sequences to file out_filename = 'republic_sequences.txt' save_doc ( sequences , out_filename )

You should now have training data stored in the file ‘republic_sequences.txt‘ in your current working directory.

Next, let’s look at how to fit a language model to this data.

Train Language Model

We can now train a statistical language model from the prepared data.

The model we will train is a neural language model. It has a few unique characteristics:

It uses a distributed representation for words so that different words with similar meanings will have a similar representation.

It learns the representation at the same time as learning the model.

It learns to predict the probability for the next word using the context of the last 100 words.

Specifically, we will use an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context.

Let’s start by loading our training data.

Load Sequences

We can load our training data using the load_doc() function we developed in the previous section.

Once loaded, we can split the data into separate training sequences by splitting based on new lines.

The snippet below will load the ‘republic_sequences.txt‘ data file from the current working directory.

# load doc into memory def load_doc(filename): # open the file as read only file = open(filename, 'r') # read all text text = file.read() # close the file file.close() return text # load in_filename = 'republic_sequences.txt' doc = load_doc(in_filename) lines = doc.split('

') 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text # load in_filename = 'republic_sequences.txt' doc = load_doc ( in_filename ) lines = doc . split ( '

' )

Next, we can encode the training data.

Encode Sequences

The word embedding layer expects input sequences to be comprised of integers.

We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping.

To do this encoding, we will use the Tokenizer class in the Keras API.

First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.

We can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers.

# integer encode sequences of words tokenizer = Tokenizer() tokenizer.fit_on_texts(lines) sequences = tokenizer.texts_to_sequences(lines) 1 2 3 4 # integer encode sequences of words tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( lines ) sequences = tokenizer . texts_to_sequences ( lines )

We can access the mapping of words to integers as a dictionary attribute called word_index on the Tokenizer object.

We need to know the size of the vocabulary for defining the embedding layer later. We can determine the vocabulary by calculating the size of the mapping dictionary.

Words are assigned values from 1 to the total number of words (e.g. 7,409). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length.

Therefore, when specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than the actual vocabulary.

# vocabulary size vocab_size = len(tokenizer.word_index) + 1 1 2 # vocabulary size vocab_size = len ( tokenizer . word_index ) + 1

Sequence Inputs and Output

Now that we have encoded the input sequences, we need to separate them into input (X) and output (y) elements.

We can do this with array slicing.

After separating, we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.

This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.

Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.

Finally, we need to specify to the Embedding layer how long input sequences are. We know that there are 50 words because we designed the model, but a good generic way to specify that is to use the second dimension (number of columns) of the input data’s shape. That way, if you change the length of sequences when preparing data, you do not need to change this data loading code; it is generic.

# separate into input and output sequences = array(sequences) X, y = sequences[:,:-1], sequences[:,-1] y = to_categorical(y, num_classes=vocab_size) seq_length = X.shape[1] 1 2 3 4 5 # separate into input and output sequences = array ( sequences ) X , y = sequences [ : , : - 1 ] , sequences [ : , - 1 ] y = to_categorical ( y , num_classes = vocab_size ) seq_length = X . shape [ 1 ]

Fit Model

We can now define and fit our language model on the training data.

The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.

Common values are 50, 100, and 300. We will use 50 here, but consider testing smaller or larger values.

We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.

A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities.

# define model model = Sequential() model.add(Embedding(vocab_size, 50, input_length=seq_length)) model.add(LSTM(100, return_sequences=True)) model.add(LSTM(100)) model.add(Dense(100, activation='relu')) model.add(Dense(vocab_size, activation='softmax')) print(model.summary()) 1 2 3 4 5 6 7 8 # define model model = Sequential ( ) model . add ( Embedding ( vocab_size , 50 , input_length = seq_length ) ) model . add ( LSTM ( 100 , return_sequences = True ) ) model . add ( LSTM ( 100 ) ) model . add ( Dense ( 100 , activation = 'relu' ) ) model . add ( Dense ( vocab_size , activation = 'softmax' ) ) print ( model . summary ( ) )

A summary of the defined network is printed as a sanity check to ensure we have constructed what we intended.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 50, 50) 370500 _________________________________________________________________ lstm_1 (LSTM) (None, 50, 100) 60400 _________________________________________________________________ lstm_2 (LSTM) (None, 100) 80400 _________________________________________________________________ dense_1 (Dense) (None, 100) 10100 _________________________________________________________________ dense_2 (Dense) (None, 7410) 748410 ================================================================= Total params: 1,269,810 Trainable params: 1,269,810 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, 50, 50) 370500 _________________________________________________________________ lstm_1 (LSTM) (None, 50, 100) 60400 _________________________________________________________________ lstm_2 (LSTM) (None, 100) 80400 _________________________________________________________________ dense_1 (Dense) (None, 100) 10100 _________________________________________________________________ dense_2 (Dense) (None, 7410) 748410 ================================================================= Total params: 1,269,810 Trainable params: 1,269,810 Non-trainable params: 0 _________________________________________________________________

Next, the model is compiled specifying the categorical cross entropy loss needed to fit the model. Technically, the model is learning a multi-class classification and this is the suitable loss function for this type of problem. The efficient Adam implementation to mini-batch gradient descent is used and accuracy is evaluated of the model.

Finally, the model is fit on the data for 100 training epochs with a modest batch size of 128 to speed things up.

Training may take a few hours on modern hardware without GPUs. You can speed it up with a larger batch size and/or fewer training epochs.

# compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model model.fit(X, y, batch_size=128, epochs=100) 1 2 3 4 # compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( X , y , batch_size = 128 , epochs = 100 )

During training, you will see a summary of performance, including the loss and accuracy evaluated from the training data at the end of each batch update.

You will get different results, but perhaps an accuracy of just over 50% of predicting the next word in the sequence, which is not bad. We are not aiming for 100% accuracy (e.g. a model that memorized the text), but rather a model that captures the essence of the text.

... Epoch 96/100 118633/118633 [==============================] - 265s - loss: 2.0324 - acc: 0.5187 Epoch 97/100 118633/118633 [==============================] - 265s - loss: 2.0136 - acc: 0.5247 Epoch 98/100 118633/118633 [==============================] - 267s - loss: 1.9956 - acc: 0.5262 Epoch 99/100 118633/118633 [==============================] - 266s - loss: 1.9812 - acc: 0.5291 Epoch 100/100 118633/118633 [==============================] - 270s - loss: 1.9709 - acc: 0.5315 1 2 3 4 5 6 7 8 9 10 11 ... Epoch 96/100 118633/118633 [==============================] - 265s - loss: 2.0324 - acc: 0.5187 Epoch 97/100 118633/118633 [==============================] - 265s - loss: 2.0136 - acc: 0.5247 Epoch 98/100 118633/118633 [==============================] - 267s - loss: 1.9956 - acc: 0.5262 Epoch 99/100 118633/118633 [==============================] - 266s - loss: 1.9812 - acc: 0.5291 Epoch 100/100 118633/118633 [==============================] - 270s - loss: 1.9709 - acc: 0.5315

Save Model

At the end of the run, the trained model is saved to file.

Here, we use the Keras model API to save the model to the file ‘model.h5‘ in the current working directory.

Later, when we load the model to make predictions, we will also need the mapping of words to integers. This is in the Tokenizer object, and we can save that too using Pickle.

# save the model to file model.save('model.h5') # save the tokenizer dump(tokenizer, open('tokenizer.pkl', 'wb')) 1 2 3 4 # save the model to file model . save ( 'model.h5' ) # save the tokenizer dump ( tokenizer , open ( 'tokenizer.pkl' , 'wb' ) )

Complete Example

We can put all of this together; the complete example for fitting the language model is listed below.

from numpy import array from pickle import dump from keras.preprocessing.text import Tokenizer from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import Embedding # load doc into memory def load_doc(filename): # open the file as read only file = open(filename, 'r') # read all text text = file.read() # close the file file.close() return text # load in_filename = 'republic_sequences.txt' doc = load_doc(in_filename) lines = doc.split('

') # integer encode sequences of words tokenizer = Tokenizer() tokenizer.fit_on_texts(lines) sequences = tokenizer.texts_to_sequences(lines) # vocabulary size vocab_size = len(tokenizer.word_index) + 1 # separate into input and output sequences = array(sequences) X, y = sequences[:,:-1], sequences[:,-1] y = to_categorical(y, num_classes=vocab_size) seq_length = X.shape[1] # define model model = Sequential() model.add(Embedding(vocab_size, 50, input_length=seq_length)) model.add(LSTM(100, return_sequences=True)) model.add(LSTM(100)) model.add(Dense(100, activation='relu')) model.add(Dense(vocab_size, activation='softmax')) print(model.summary()) # compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit model model.fit(X, y, batch_size=128, epochs=100) # save the model to file model.save('model.h5') # save the tokenizer dump(tokenizer, open('tokenizer.pkl', 'wb')) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 from numpy import array from pickle import dump from keras . preprocessing . text import Tokenizer from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Dense from keras . layers import LSTM from keras . layers import Embedding # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text # load in_filename = 'republic_sequences.txt' doc = load_doc ( in_filename ) lines = doc . split ( '

' ) # integer encode sequences of words tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( lines ) sequences = tokenizer . texts_to_sequences ( lines ) # vocabulary size vocab_size = len ( tokenizer . word_index ) + 1 # separate into input and output sequences = array ( sequences ) X , y = sequences [ : , : - 1 ] , sequences [ : , - 1 ] y = to_categorical ( y , num_classes = vocab_size ) seq_length = X . shape [ 1 ] # define model model = Sequential ( ) model . add ( Embedding ( vocab_size , 50 , input_length = seq_length ) ) model . add ( LSTM ( 100 , return_sequences = True ) ) model . add ( LSTM ( 100 ) ) model . add ( Dense ( 100 , activation = 'relu' ) ) model . add ( Dense ( vocab_size , activation = 'softmax' ) ) print ( model . summary ( ) ) # compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( X , y , batch_size = 128 , epochs = 100 ) # save the model to file model . save ( 'model.h5' ) # save the tokenizer dump ( tokenizer , open ( 'tokenizer.pkl' , 'wb' ) )

Use Language Model

Now that we have a trained language model, we can use it.

In this case, we can use it to generate new sequences of text that have the same statistical properties as the source text.

This is not practical, at least not for this example, but it gives a concrete example of what the language model has learned.

We will start by loading the training sequences again.

Load Data

We can use the same code from the previous section to load the training data sequences of text.

Specifically, the load_doc() function.

# load doc into memory def load_doc(filename): # open the file as read only file = open(filename, 'r') # read all text text = file.read() # close the file file.close() return text # load cleaned text sequences in_filename = 'republic_sequences.txt' doc = load_doc(in_filename) lines = doc.split('

') 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text # load cleaned text sequences in_filename = 'republic_sequences.txt' doc = load_doc ( in_filename ) lines = doc . split ( '

' )

We need the text so that we can choose a source sequence as input to the model for generating a new sequence of text.

The model will require 100 words as input.

Later, we will need to specify the expected length of input. We can determine this from the input sequences by calculating the length of one line of the loaded data and subtracting 1 for the expected output word that is also on the same line.

seq_length = len(lines[0].split()) - 1 1 seq_length = len ( lines [ 0 ] . split ( ) ) - 1

Load Model

We can now load the model from file.

Keras provides the load_model() function for loading the model, ready for use.

# load the model model = load_model('model.h5') 1 2 # load the model model = load_model ( 'model.h5' )

We can also load the tokenizer from file using the Pickle API.

# load the tokenizer tokenizer = load(open('tokenizer.pkl', 'rb')) 1 2 # load the tokenizer tokenizer = load ( open ( 'tokenizer.pkl' , 'rb' ) )

We are ready to use the loaded model.

Generate Text

The first step in generating text is preparing a seed input.

We will select a random line of text from the input text for this purpose. Once selected, we will print it so that we have some idea of what was used.

# select a seed text seed_text = lines[randint(0,len(lines))] print(seed_text + '

') 1 2 3 # select a seed text seed_text = lines [ randint ( 0 , len ( lines ) ) ] print ( seed_text + '

' )

Next, we can generate new words, one at a time.

First, the seed text must be encoded to integers using the same tokenizer that we used when training the model.

encoded = tokenizer.texts_to_sequences([seed_text])[0] 1 encoded = tokenizer . texts_to_sequences ( [ seed_text ] ) [ 0 ]

The model can predict the next word directly by calling model.predict_classes() that will return the index of the word with the highest probability.

# predict probabilities for each word yhat = model.predict_classes(encoded, verbose=0) 1 2 # predict probabilities for each word yhat = model . predict_classes ( encoded , verbose = 0 )

We can then look up the index in the Tokenizers mapping to get the associated word.

out_word = '' for word, index in tokenizer.word_index.items(): if index == yhat: out_word = word break 1 2 3 4 5 out_word = '' for word , index in tokenizer . word_index . items ( ) : if index == yhat : out_word = word break

We can then append this word to the seed text and repeat the process.

Importantly, the input sequence is going to get too long. We can truncate it to the desired length after the input sequence has been encoded to integers. Keras provides the pad_sequences() function that we can use to perform this truncation.

encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') 1 encoded = pad_sequences ( [ encoded ] , maxlen = seq_length , truncating = 'pre' )

We can wrap all of this into a function called generate_seq() that takes as input the model, the tokenizer, input sequence length, the seed text, and the number of words to generate. It then returns a sequence of words generated by the model.

# generate a sequence from a language model def generate_seq(model, tokenizer, seq_length, seed_text, n_words): result = list() in_text = seed_text # generate a fixed number of words for _ in range(n_words): # encode the text as integer encoded = tokenizer.texts_to_sequences([in_text])[0] # truncate sequences to a fixed length encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') # predict probabilities for each word yhat = model.predict_classes(encoded, verbose=0) # map predicted word index to word out_word = '' for word, index in tokenizer.word_index.items(): if index == yhat: out_word = word break # append to input in_text += ' ' + out_word result.append(out_word) return ' '.join(result) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # generate a sequence from a language model def generate_seq ( model , tokenizer , seq_length , seed_text , n_words ) : result = list ( ) in_text = seed_text # generate a fixed number of words for _ in range ( n_words ) : # encode the text as integer encoded = tokenizer . texts_to_sequences ( [ in_text ] ) [ 0 ] # truncate sequences to a fixed length encoded = pad_sequences ( [ encoded ] , maxlen = seq_length , truncating = 'pre' ) # predict probabilities for each word yhat = model . predict_classes ( encoded , verbose = 0 ) # map predicted word index to word out_word = '' for word , index in tokenizer . word_index . items ( ) : if index == yhat : out_word = word break # append to input in_text += ' ' + out_word result . append ( out_word ) return ' ' . join ( result )

We are now ready to generate a sequence of new words given some seed text.

# generate new text generated = generate_seq(model, tokenizer, seq_length, seed_text, 50) print(generated) 1 2 3 # generate new text generated = generate_seq ( model , tokenizer , seq_length , seed_text , 50 ) print ( generated )

Putting this all together, the complete code listing for generating text from the learned-language model is listed below.

from random import randint from pickle import load from keras.models import load_model from keras.preprocessing.sequence import pad_sequences # load doc into memory def load_doc(filename): # open the file as read only file = open(filename, 'r') # read all text text = file.read() # close the file file.close() return text # generate a sequence from a language model def generate_seq(model, tokenizer, seq_length, seed_text, n_words): result = list() in_text = seed_text # generate a fixed number of words for _ in range(n_words): # encode the text as integer encoded = tokenizer.texts_to_sequences([in_text])[0] # truncate sequences to a fixed length encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') # predict probabilities for each word yhat = model.predict_classes(encoded, verbose=0) # map predicted word index to word out_word = '' for word, index in tokenizer.word_index.items(): if index == yhat: out_word = word break # append to input in_text += ' ' + out_word result.append(out_word) return ' '.join(result) # load cleaned text sequences in_filename = 'republic_sequences.txt' doc = load_doc(in_filename) lines = doc.split('

') seq_length = len(lines[0].split()) - 1 # load the model model = load_model('model.h5') # load the tokenizer tokenizer = load(open('tokenizer.pkl', 'rb')) # select a seed text seed_text = lines[randint(0,len(lines))] print(seed_text + '

') # generate new text generated = generate_seq(model, tokenizer, seq_length, seed_text, 50) print(generated) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 from random import randint from pickle import load from keras . models import load_model from keras . preprocessing . sequence import pad_sequences # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text # generate a sequence from a language model def generate_seq ( model , tokenizer , seq_length , seed_text , n_words ) : result = list ( ) in_text = seed_text # generate a fixed number of words for _ in range ( n_words ) : # encode the text as integer encoded = tokenizer . texts_to_sequences ( [ in_text ] ) [ 0 ] # truncate sequences to a fixed length encoded = pad_sequences ( [ encoded ] , maxlen = seq_length , truncating = 'pre' ) # predict probabilities for each word yhat = model . predict_classes ( encoded , verbose = 0 ) # map predicted word index to word out_word = '' for word , index in tokenizer . word_index . items ( ) : if index == yhat : out_word = word break # append to input in_text += ' ' + out_word result . append ( out_word ) return ' ' . join ( result ) # load cleaned text sequences in_filename = 'republic_sequences.txt' doc = load_doc ( in_filename ) lines = doc . split ( '

' ) seq_length = len ( lines [ 0 ] . split ( ) ) - 1 # load the model model = load_model ( 'model.h5' ) # load the tokenizer tokenizer = load ( open ( 'tokenizer.pkl' , 'rb' ) ) # select a seed text seed_text = lines [ randint ( 0 , len ( lines ) ) ] print ( seed_text + '

' ) # generate new text generated = generate_seq ( model , tokenizer , seq_length , seed_text , 50 ) print ( generated )

Running the example first prints the seed text.

when he said that a man when he grows old may learn many things for he can no more learn much than he can run much youth is the time for any extraordinary toil of course and therefore calculation and geometry and all the other elements of instruction which are a

Then 50 words of generated text are printed.

preparation for dialectic should be presented to the name of idle spendthrifts of whom the other is the manifold and the unjust and is the best and the other which delighted to be the opening of the soul of the soul and the embroiderer will have to be said at

You will get different results. Try running the generation piece a few times.

You can see that the text seems reasonable. In fact, the addition of concatenation would help in interpreting the seed and the generated text. Nevertheless, the generated text gets the right kind of words in the right kind of order.

Try running the example a few times to see other examples of generated text. Let me know in the comments below if you see anything interesting.

Extensions

This section lists some ideas for extending the tutorial that you may wish to explore.

Sentence-Wise Model . Split the raw data based on sentences and pad each sentence to a fixed length (e.g. the longest sentence length).

. Split the raw data based on sentences and pad each sentence to a fixed length (e.g. the longest sentence length). Simplify Vocabulary . Explore a simpler vocabulary, perhaps with stemmed words or stop words removed.

. Explore a simpler vocabulary, perhaps with stemmed words or stop words removed. Tune Model . Tune the model, such as the size of the embedding or number of memory cells in the hidden layer, to see if you can develop a better model.

. Tune the model, such as the size of the embedding or number of memory cells in the hidden layer, to see if you can develop a better model. Deeper Model . Extend the model to have multiple LSTM hidden layers, perhaps with dropout to see if you can develop a better model.

. Extend the model to have multiple LSTM hidden layers, perhaps with dropout to see if you can develop a better model. Pre-Trained Word Embedding. Extend the model to use pre-trained word2vec or GloVe vectors to see if it results in a better model.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Summary

In this tutorial, you discovered how to develop a word-based language model using a word embedding and a recurrent neural network.

Specifically, you learned:

How to prepare text for developing a word-based language model.

How to design and fit a neural language model with a learned embedding and an LSTM hidden layer.

How to use the learned language model to generate new text with similar statistical properties as the source text.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside"
287;news.mit.edu;http://news.mit.edu/2019/four-newmac-championships-successful-fall-season-mit-athletics-0114;;MIT athletes earn four NEWMAC championships;"The fall season was another successful one for MIT Athletics as the Engineers achieved both athletically and academically.

MIT teams claimed four New England Women’s and Men’s Athletic Conference (NEWMAC) Championships, eight programs were nationally-ranked, and four teams represented MIT at NCAA Championship events programs. Six student-athletes were named All-Americans, 52 earned All-Conference honors, and 11 Engineers were recognized with major awards from the NEWMAC.

Academically, MIT amassed 96 NEWMAC Academic All-Conference selections and seven Google Cloud Academic All-America Team members as MIT is now one of just two schools nationally to have over 300 all-time Google Cloud Academic All-America honorees.

At the conclusion of the fall, MIT was ranked No. 12 nationally in the Learfield Directors’ Cup standings out of 449 NCAA Division III institutions. The Engineers generated 218.5 points, which was based on each team’s finish at NCAA Championship events.

Men’s Cross Country finished 16th at the NCAA Championship and captured the program’s 21st NEWMAC Championship, maintaining its status as the only team to win the title in conference history. Head coach Halston Taylor was named NEWMAC Coach of the Year for the fifth year in a row and 16th time during his career. Sophomore Billy Woltz earned NEWMAC Athlete of the Year accolades, first-year Andrew Mah collected NEWMAC Rookie of the Year plaudits, and senior Josh Rosenkranz was selected to NEWMAC All-Sportsmanship Team. Seven student-athletes earned spots on the NEWMAC All-Conference team while Mah, Rosenkranz, Woltz, and junior Josh Derrick qualified for the U.S. Track and Field and Cross Country Coaches Association (USTFCCCA) New England All-Region Team.

Women’s Cross Country captured third place at the NCAA Championship and won the program’s 12th straight NEWMAC Championship. Senior Leandra Zimmermann earned USTFCCCA All-America honors while head coach Halston Taylor was voted the USTFCCCA New England Region Coach of the Year for the fifth time. First-year Einat Gavish was named the NEWMAC Rookie of the Year and junior Marissa McPhillips was selected for the NEWMAC All-Sportsmanship Team. Eight student-athletes represented MIT on the NEWMAC All-Conference Team, including USTFCCCA New England All-Region honorees Gavish, Zimmermann, junior Katie Bacher, and sophomores Katie Collins and Jenna Melanson.

Field Hockey advanced to the NEWMAC Championship for the third year in a row but fell to Smith College, 2-1, in overtime. Junior Devon Goetz was tabbed for National Field Hockey Coaches Association (NFHCA) All-America Third Team accolades and was joined by sophomores Megan Flynn and Amanda Garofalo on the NFHCA New England East All-Region Team. The trio also picked up NEWMAC All-Conference awards while sophomore Jennah Haque represented the Engineers on the NEWMAC All-Sportsmanship Team.

MIT Football was crowned NEWMAC Champions, marking the program’s second conference title, and qualified for the NCAA Tournament for the second time in Institute history. In addition to receiving 13 NEWMAC All-Conference Team selections, first-year head coach Brian Bubna was named the NEWMAC Coach of the Year, senior Udgam Goyal was voted the NEWMAC Offensive Athlete of the Year, and senior Riley Quinn earned a spot on the NEWMAC All-Sportsmanship Team. Junior Ben Bennington was selected to the New England Football Writers Division II/III All-New England Team while Quinn was the recipient of the organization’s Jerry Nason Award for Senior Achievement, which recognizes a student-athlete succeeding in football against all odds. For the second year in a row, Goyal was named the Google Cloud Academic All-America Team Member of the Year for Division III Football. He was also joined by junior AJ Iversen on the Google Cloud Academic All-America First Team.

Men’s Soccer fell to Wheaton College, 3-1, in penalty kicks after a scoreless double-overtime quarterfinal game in the NEWMAC Championship Tournament. Junior Jeremy Cowham, and seniors Thad Daguilh and Wesley Woo earned NEWMAC All-Conference accolades while senior David Wu was selected to the NEWMAC All-Sportsmanship Team. Woo represented the Engineers on the United Soccer Coaches All-Region Second Team and was voted to the Google Cloud Academic All-America Third Team.

Women’s Soccer claimed its second straight and fifth overall NEWMAC Championship after defeating Springfield College, 2-1, in overtime. The Engineers’ season ended with a 1-0 loss at Amherst College in the second round of the NCAA Tournament. In addition to receiving seven NEWMAC All-Conference Team selections, head coach Martin Desmarais was named the NEWMAC Coach of the Year for the fifth time in six seasons, junior Sophia Struckman was voted the NEWMAC Offensive Athlete of the Year, senior Hailey Nichols repeated as the NEWMAC Defensive Athlete of the Year, and senior Allie Hrabchak earned a spot on the NEWMAC All-Sportsmanship Team. Struckman was tabbed for USC All-America Third Team accolades and was joined by Nichols on the USC All-Region First Team. Nichols, junior Emily Berzolla, and senior Lily Mueller represented MIT on the Google Cloud Academic All-America Team for the second year in a row.

Women’s Volleyball fell to Babson College in four sets during the semifinal round of the NEWMAC Championship Tournament. Senior Abby Bertics was named the NEWMAC Athlete of the Year for the second year in a row and was joined by senior Carly Silvernale on the NEWMAC All-Conference First Team while senior Carly Staub earned a spot on the NEWMAC All-Sportsmanship Team. Bertics was selected to the American Volleyball Coaches Association (AVCA) All-America Third Team for the second consecutive season and was named the New England Region Player of the Year while senior Christina Liao received AVCA New England All-Region Honorable Mention plaudits. Bertics became the first student-athlete in program history to be voted the Google Cloud Academic All-America Team Member of the Year for Division III Volleyball and repeated as a Google Cloud Academic All-America First Team selection.

Men’s Water Polo defeated Iona College, 16-11, in the fifth-place match of the Northeast Water Polo Conference (NWPC) Championship Tournament, marking the Engineers’ best finish and first tournament win in the three-year history of the event. MIT also collected the program’s ninth Collegiate Water Polo Association (CWPA) Division III Eastern Championship title following a 15-12 victory over Johns Hopkins University. First-year head coach Bret Lathrope was named the NWPC Coach of the Year while first-year Miller Geschke and junior Clyde Huibregtse were tabbed to the NWPC All-Conference Second Team. Geschke was chosen for the CWPA Division III All-America First Team as sophomore Evan Kim collected Honorable Mention plaudits."
288;news.mit.edu;http://news.mit.edu/2020/neuroscience-memory-cells-interpret-new-0406;;Neuroscientists find memory cells that help us interpret new situations;"Imagine you are meeting a friend for dinner at a new restaurant. You may try dishes you haven’t had before, and your surroundings will be completely new to you. However, your brain knows that you have had similar experiences — perusing a menu, ordering appetizers, and splurging on dessert are all things that you have probably done when dining out.

MIT neuroscientists have now identified populations of cells that encode each of these distinctive segments of an overall experience. These chunks of memory, stored in the hippocampus, are activated whenever a similar type of experience takes place, and are distinct from the neural code that stores detailed memories of a specific location.

The researchers believe that this kind of “event code,” which they discovered in a study of mice, may help the brain interpret novel situations and learn new information by using the same cells to represent similar experiences.

“When you encounter something new, there are some really new and notable stimuli, but you already know quite a bit about that particular experience, because it’s a similar kind of experience to what you have already had before,” says Susumu Tonegawa, a professor of biology and neuroscience at the RIKEN-MIT Laboratory of Neural Circuit Genetics at MIT’s Picower Institute for Learning and Memory.

Tonegawa is the senior author of the study, which appears today in Nature Neuroscience. Chen Sun, an MIT graduate student, is the lead author of the paper. New York University graduate student Wannan Yang and Picower Institute technical associate Jared Martin are also authors of the paper.

Encoding abstraction

It is well-established that certain cells in the brain’s hippocampus are specialized to store memories of specific locations. Research in mice has shown that within the hippocampus, neurons called place cells fire when the animals are in a specific location, or even if they are dreaming about that location.

In the new study, the MIT team wanted to investigate whether the hippocampus also stores representations of more abstract elements of a memory. That is, instead of firing whenever you enter a particular restaurant, such cells might encode “dessert,” no matter where you’re eating it.

To test this hypothesis, the researchers measured activity in neurons of the CA1 region of the mouse hippocampus as the mice repeatedly ran a four-lap maze. At the end of every fourth lap, the mice were given a reward. As expected, the researchers found place cells that lit up when the mice reached certain points along the track. However, the researchers also found sets of cells that were active during one of the four laps, but not the others. About 30 percent of the neurons in CA1 appeared to be involved in creating this “event code.”

“This gave us the initial inkling that besides a code for space, cells in the hippocampus also care about this discrete chunk of experience called lap 1, or this discrete chunk of experience called lap 2, or lap 3, or lap 4,” Sun says.

To further explore this idea, the researchers trained mice to run a square maze on day 1 and then a circular maze on day 2, in which they also received a reward after every fourth lap. They found that the place cells changed their activity, reflecting the new environment. However, the same sets of lap-specific cells were activated during each of the four laps, regardless of the shape of the track. The lap-encoding cells’ activity also remained consistent when laps were randomly shortened or lengthened.

“Even in the new spatial locations, cells still maintain their coding for the lap number, suggesting that cells that were coding for a square lap 1 have now been transferred to code for a circular lap 1,” Sun says.

The researchers also showed that if they used optogenetics to inhibit sensory input from a part of the brain called the medial entorhinal cortex (MEC), lap-encoding did not occur. They are now investigating what kind of input the MEC region provides to help the hippocampus create memories consisting of chunks of an experience.

Two distinct codes

These findings suggest that, indeed, every time you eat dinner, similar memory cells are activated, no matter where or what you’re eating. The researchers theorize that the hippocampus contains “two mutually and independently manipulatable codes,” Sun says. One encodes continuous changes in location, time, and sensory input, while the other organizes an overall experience into smaller chunks that fit into known categories such as appetizer and dessert.

“We believe that both types of hippocampal codes are useful, and both are important,” Tonegawa says. “If we want to remember all the details of what happened in a specific experience, moment-to-moment changes that occurred, then the continuous monitoring is effective. But on the other hand, when we have a longer experience, if you put it into chunks, and remember the abstract order of the abstract chunks, that’s more effective than monitoring this long process of continuous changes.”

The new MIT results “significantly advance our knowledge about the function of the hippocampus,” says Gyorgy Buzsaki, a professor of neuroscience at New York University School of Medicine, who was not part of the research team.

“These findings are significant because they are telling us that the hippocampus does a lot more than just ‘representing’ space or integrating paths into a continuous long journey,” Buzsaki says. “From these remarkable results Tonegawa and colleagues conclude that they discovered an ‘event code,’ dedicated to organizing experience by events, and that this code is independent of spatial and time representations, that is, jobs also attributed to the hippocampus.”

Tonegawa and Sun believe that networks of cells that encode chunks of experiences may also be useful for a type of learning called transfer learning, which allows you to apply knowledge you already have to help you interpret new experiences or learn new things. Tonegawa’s lab is now working on trying to find cell populations that might encode these specific pieces of knowledge.

The research was funded by the RIKEN Center for Brain Science, the Howard Hughes Medical Institute, and the JPB Foundation."
289;machinelearningmastery.com;https://machinelearningmastery.com/how-to-load-and-explore-household-electricity-usage-data/;2018-09-27;How to Load and Explore Household Electricity Usage Data;"# load and clean-up data

from numpy import nan

from pandas import read_csv

# load all data

dataset = read_csv ( 'household_power_consumption.txt' , sep = ';' , header = 0 , low_memory = False , infer_datetime_format = True , parse_dates = { 'datetime' : [ 0 , 1 ] } , index_col = [ 'datetime' ] )

# summarize

print ( dataset . shape )

print ( dataset . head ( ) )

# mark all missing values

dataset . replace ( '?' , nan , inplace = True )

# add a column for for the remainder of sub metering

values = dataset . values . astype ( 'float32' )

dataset [ 'sub_metering_4' ] = ( values [ : , 0 ] * 1000 / 60 ) - ( values [ : , 4 ] + values [ : , 5 ] + values [ : , 6 ] )

# save updated dataset

dataset . to_csv ( 'household_power_consumption.csv' )

# load the new dataset and summarize

dataset = read_csv ( 'household_power_consumption.csv' , header = 0 , infer_datetime_format = True , parse_dates = [ 'datetime' ] , index_col = [ 'datetime' ] )"
290;machinelearningmastery.com;https://machinelearningmastery.com/how-to-perform-object-detection-in-photographs-with-mask-r-cnn-in-keras/;2019-05-23;How to Use Mask R-CNN in Keras for Object Detection in Photographs;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44

# example of inference with a pre-trained coco model from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from mrcnn . visualize import display_instances from mrcnn . config import Config from mrcnn . model import MaskRCNN # define 81 classes that the coco model knowns about class_names = [ 'BG' , 'person' , 'bicycle' , 'car' , 'motorcycle' , 'airplane' , 'bus' , 'train' , 'truck' , 'boat' , 'traffic light' , 'fire hydrant' , 'stop sign' , 'parking meter' , 'bench' , 'bird' , 'cat' , 'dog' , 'horse' , 'sheep' , 'cow' , 'elephant' , 'bear' , 'zebra' , 'giraffe' , 'backpack' , 'umbrella' , 'handbag' , 'tie' , 'suitcase' , 'frisbee' , 'skis' , 'snowboard' , 'sports ball' , 'kite' , 'baseball bat' , 'baseball glove' , 'skateboard' , 'surfboard' , 'tennis racket' , 'bottle' , 'wine glass' , 'cup' , 'fork' , 'knife' , 'spoon' , 'bowl' , 'banana' , 'apple' , 'sandwich' , 'orange' , 'broccoli' , 'carrot' , 'hot dog' , 'pizza' , 'donut' , 'cake' , 'chair' , 'couch' , 'potted plant' , 'bed' , 'dining table' , 'toilet' , 'tv' , 'laptop' , 'mouse' , 'remote' , 'keyboard' , 'cell phone' , 'microwave' , 'oven' , 'toaster' , 'sink' , 'refrigerator' , 'book' , 'clock' , 'vase' , 'scissors' , 'teddy bear' , 'hair drier' , 'toothbrush' ] # define the test configuration class TestConfig ( Config ) : NAME = ""test"" GPU_COUNT = 1 IMAGES_PER_GPU = 1 NUM_CLASSES = 1 + 80 # define the model rcnn = MaskRCNN ( mode = 'inference' , model_dir = './' , config = TestConfig ( ) ) # load coco model weights rcnn . load_weights ( 'mask_rcnn_coco.h5' , by_name = True ) # load photograph img = load_img ( 'elephant.jpg' ) img = img_to_array ( img ) # make prediction results = rcnn . detect ( [ img ] , verbose = 0 ) # get dictionary for first prediction r = results [ 0 ] # show photo with bounding boxes, masks, class labels and scores display_instances ( img , r [ 'rois' ] , r [ 'masks' ] , r [ 'class_ids' ] , class_names , r [ 'scores' ] )"
291;machinelearningmastery.com;https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/;2018-04-26;How to Calculate Correlation Between Variables in Python;"# generate related variables

from numpy import mean

from numpy import std

from numpy . random import randn

from numpy . random import seed

from matplotlib import pyplot

# seed random number generator

seed ( 1 )

# prepare data

data1 = 20 * randn ( 1000 ) + 100

data2 = data1 + ( 10 * randn ( 1000 ) + 50 )

# summarize

print ( 'data1: mean=%.3f stdv=%.3f' % ( mean ( data1 ) , std ( data1 ) ) )

print ( 'data2: mean=%.3f stdv=%.3f' % ( mean ( data2 ) , std ( data2 ) ) )

# plot

pyplot . scatter ( data1 , data2 )"
292;news.mit.edu;http://news.mit.edu/2020/event-horizon-telescope-observations-black-hole-powered-jet-0414;;Event Horizon Telescope observes a black hole-powered jet;"One year ago, the Event Horizon Telescope (EHT) Collaboration published the first image of a black hole in the nearby radio galaxy M87. The collaboration has now extracted additional new information from the EHT data on the distant quasar 3C 279, allowing them to image in the finest detail ever a relativistic jet that is believed to originate from the vicinity of a supermassive black hole. New analyses, led by Jae-Young Kim from the Max Planck Institute for Radio Astronomy in Bonn, Germany, enabled the collaboration to trace the jet back to its launch point, close to where violently variable radiation from across the electromagnetic spectrum arises. The results are published in the April 2020 issue of Astronomy & Astrophysics.

The EHT collaboration is continuing to extract information from the groundbreaking data collected in its April 2017 global campaign. One target of the observations was a galaxy 5 billion light years away in the constellation Virgo that scientists classify as a quasar because an ultra-luminous source of energy at its center shines and flickers as gas falls into a giant black hole. This target, labeled 3C 279, contains a black hole about 1 billion times more massive than our sun. Twin fire hose-like jets of plasma erupt from the black hole and disk system at velocities close to the speed of light, a consequence of the enormous forces unleashed as matter descends into the black hole’s immense gravity. To capture the new image, the EHT uses a technique called very long baseline interferometry (VLBI), which synchronizes and links radio dishes around the world. By combining this network to form one huge virtual Earth-size telescope, the EHT is able to resolve objects as small as 20 micro-arcseconds on the sky — the equivalent of someone on Earth identifying an orange on the moon. Data recorded at all the EHT sites around the world are transported to special supercomputers at MIT Haystack Observatory and MPIfR in Bonn, Germany, where they are combined. The combined dataset is then carefully calibrated and analyzed by a team of experts, which then enables EHT scientists to produce images with the finest detail possible from the surface of the Earth. For 3C 279, the unprecedented resolution of the EHT reveals fine features of the jet that have never been seen before. In particular, the newly analyzed data show that the normally straight jet has an unexpected twisted shape at its base. Jae-Young Kim, of the Max Planck Institute for Radio Astronomy and lead author of the paper, is enthusiastic and at the same time puzzled: “We knew that every time you open a new window to the universe you can find something new. Here, where we expected to find the region where the jet forms by going to the sharpest image possible, we find a kind of perpendicular structure. This is like finding a very different shape by opening the smallest matryoshka doll.” Colin Lonsdale, director of MIT Haystack Observatory and vice chair of the EHT directing board, explains: “This array was developed specifically for the purpose of imaging the shadows of black holes, but as so often happens in science, improved capabilities lead to unexpected discoveries. This surprising result for 3C 279 is a good example, providing new information on the process of jet formation that challenges current understanding.” “The results are very surprising,” says Kazunori Akiyama, a Jansky Fellow of the National Radio Astronomy Observatory at MIT Haystack Observatory. Akiyama developed imaging techniques for the EHT to create the first images of the black hole in M87; these algorithms were also used to create the images of quasar 3C 279. “When we observed the quasar for four days within one week, we assumed that we would not see these dynamical changes because the source is so far away (100 times further from Earth than M87). But the EHT observations were so sharp that for the first time we could see tiny changes in motions of the jets within this time frame.” Opportunities to conduct EHT observing campaigns occur once a year in early springtime, but the March-April 2020 campaign had to be canceled in response to the Covid-19 global outbreak. In announcing the cancellation, Michael Hecht, MIT Haystack Observatory astronomer and EHT deputy project director, concluded that: “We will now devote our full concentration to completion of scientific publications from the 2017 data and dive into the analysis of data obtained with the enhanced EHT array in 2018. We are looking forward to observations with the EHT array expanded to 11 observatories in the spring of 2021.” The individual telescopes involved in the EHT collaboration are: the Atacama Large Millimetre Telescope, the Atacama Pathfinder EXplorer, the Greenland Telescope (since 2018), the IRAM 30-meter Telescope, the IRAM NOEMA Observatory (expected 2021), the Kitt Peak Telescope (expected 2021), the James Clerk Maxwell Telescope, the Large Millimeter Telescope, the Submillimeter Array, the Submillimeter Telescope, and the South Pole Telescope. The EHT consortium consists of 13 stakeholder institutes: the Academia Sinica Institute of Astronomy and Astrophysics, the University of Arizona, the University of Chicago, the East Asian Observatory, Goethe-Universität Frankfurt, Institut de Radioastronomie Millimétrique, Large Millimeter Telescope, Max-Planck-Institut für Radioastronomie, MIT Haystack Observatory, National Astronomical Observatory of Japan, Perimeter Institute for Theoretical Physics, Radboud University, and the Smithsonian Astrophysical Observatory."
293;machinelearningmastery.com;https://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates/;2019-01-01;Impact of Dataset Size on Deep Learning Model Skill And Performance Estimates;"# study of test set size for an mlp on the circles problem

from sklearn . datasets import make_circles

from keras . layers import Dense

from keras . models import Sequential

from numpy import mean

from matplotlib import pyplot

# create dataset

def create_dataset ( n_test , n_train = 1000 , noise = 0.1 ) :

# generate samples

n_samples = n_train + n _ test

X , y = make_circles ( n_samples = n_samples , noise = noise , random_state = 1 )

# split into train and test, first n for test

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# return samples

return trainX , trainy , testX , testy

# fit an mlp model

def fit_model ( trainX , trainy ) :

# define model

model = Sequential ( )

model . add ( Dense ( 25 , input_dim = 2 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( trainX , trainy , epochs = 500 , verbose = 0 )

return model

# evaluate a test set of a given size on the fit models

def evaluate_test_set_size ( models , n_test ) :

# create dataset

_ , _ , testX , testy = create_dataset ( n_test )

scores = list ( )

for model in models :

# evaluate the model

_ , test_acc = model . evaluate ( testX , testy , verbose = 0 )

scores . append ( test_acc )

return scores

# create fixed training dataset

trainX , trainy , _ , _ = create_dataset ( 10 )

# fit one model for each repeat

n_repeats = 10

models = [ fit_model ( trainX , trainy ) for _ in range ( n_repeats ) ]

print ( 'Fit %d models' % n_repeats )

# define test set sizes to evaluate

sizes = [ 100 , 1000 , 5000 , 10000 ]

score_sets , means = list ( ) , list ( )

for n_test in sizes :

# evaluate a test set of a given size on the models

scores = evaluate_test_set_size ( models , n_test )

score_sets . append ( scores )

# summarize score for size

mean_score = mean ( scores )

means . append ( mean_score )

print ( 'Test Size=%d, Test Accuracy %.3f' % ( n_test , mean_score* 100 ) )

# summarize relationship of test size to test accuracy

pyplot . plot ( sizes , means , marker = 'o' )

pyplot . show ( )

# plot distributions of test size to test accuracy

pyplot . boxplot ( score_sets , labels = sizes )"
294;news.mit.edu;http://news.mit.edu/2020/graduate-engineering-business-programs-rank-us-news-2021-0317;;MIT graduate engineering, business programs ranked highly by U.S. News for 2021;"MIT’s graduate program in engineering has again earned a No. 1 spot in U.S. News and World Report’s annual rankings, a place it has held since 1990, when the magazine first ranked such programs.

The MIT Sloan School of Management also placed highly, occupying the No. 5 spot for the best graduate business programs.

Among individual engineering disciplines, MIT placed first in six areas: aerospace/aeronautical/astronautical engineering (tied with Caltech), chemical engineering, computer engineering, electrical/electronic/communications engineering (tied with Stanford University and the University of California at Berkeley), materials engineering, and mechanical engineering. It placed second in nuclear engineering.

In the rankings of individual MBA specialties, MIT placed first in four areas: business analytics, information systems, production/operations, and project management. It placed second in supply chain/logistics.

U.S. News does not issue annual rankings for all doctoral programs but revisits many every few years. In 2018, MIT ranked in the top five for 24 of the 37 science disciplines evaluated.

The magazine bases its rankings of graduate schools of engineering and business on two types of data: reputational surveys of deans and other academic officials, and statistical indicators that measure the quality of a school’s faculty, research, and students. The magazine’s less-frequent rankings of programs in the sciences, social sciences, and humanities are based solely on reputational surveys."
295;machinelearningmastery.com;https://machinelearningmastery.com/promise-deep-learning-natural-language-processing/;2017-09-24;Promise of Deep Learning for Natural Language Processing;"Tweet Share Share

Last Updated on August 7, 2019

The promise of deep learning in the field of natural language processing is the better performance by models that may require more data but less linguistic expertise to train and operate.

There is a lot of hype and large claims around deep learning methods, but beyond the hype, deep learning methods are achieving state-of-the-art results on challenging problems. Notably in natural language processing.

In this post, you will discover the specific promises that deep learning methods have for tackling natural language processing problems.

After reading this post, you will know:

The promises of deep learning for natural language processing.

What practitioners and research scientists have to say about the promise of deep learning in NLP.

Key deep learning methods and applications for natural language processing.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Promise of Deep Learning

Deep learning methods are popular, primarily because they are delivering on their promise.

That is not to say that there is no hype around the technology, but that the hype is based on very real results that are being demonstrated across a suite of very challenging artificial intelligence problems from computer vision and natural language processing.

Some of the first large demonstrations of the power of deep learning were in natural language processing, specifically speech recognition. More recently in machine translation.

In this post, we will look at five specific promises of deep learning methods in the field of natural language processing. Promises highlighted recently by researchers and practitioners in the field, people who may be more tempered than the average reported in what the promises may be.

In summary, they are:

The Promise of Drop-in Replacement Models. That is, deep learning methods can be dropped into existing natural language systems as replacement models that can achieve commensurate or better performance. The Promise of New NLP Models. That is, deep learning methods offer the opportunity of new modeling approaches to challenging natural language problems like sequence-to-sequence prediction. The Promise of Feature Learning. That is, that deep learning methods can learn the features from natural language required by the model, rather than requiring that the features be specified and extracted by an expert. The Promise of Continued Improvement. That is, that the performance of deep learning in natural language processing is based on real results and that the improvements appear to be continuing and perhaps speeding up. The Promise of End-to-End Models. That is, that large end-to-end deep learning models can be fit on natural language problems offering a more general and better-performing approach.

We will now take a closer look at each.

There are other promises of deep learning for natural language processing; these were just the 5 that I chose to highlight.

What do you think the promise of deep learning is for natural language processing?

Let me know in the comments below.

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Promise of Drop-in Replacement Models

The first promise for deep learning in natural language processing is the ability to replace existing linear models with better performing models capable of learning and exploiting nonlinear relationships.

Yoav Goldberg, in his primer on neural networks for NLP researchers, highlights both that deep learning methods are achieving impressive results.

More recently, neural network models started to be applied also to textual natural language signals, again with very promising results.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

He goes on to highlight that the methods are easy to use and can sometimes be used to wholesale replace existing linear methods.

Recently, the field has seen some success in switching from such linear models over sparse inputs to non-linear neural-network models over dense inputs. While most of the neural network techniques are easy to apply, sometimes as almost drop-in replacements of the old linear classifiers, there is in many cases a strong barrier of entry.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

2. Promise of New NLP Models

Another promise is that deep learning methods facilitate developing entirely new models.

One strong example is the use of recurrent neural networks that are able learn and condition output over very long sequences. The approach is sufficiently different in that they allow the practitioner to break free of traditional modeling assumptions and in turn achieve state-of-the-art results.

In his book expanding on deep learning for NLP, Yoav Goldberg comments that sophisticated neural network models like recurrent neural networks allow for wholly new NLP modeling opportunities.

Around 2014, the field has started to see some success in switching from such linear models over sparse inputs to nonlinear neural network models over dense inputs. … Others are more advanced, require a change of mindset, and provide new modeling opportunities, In particular, a family of approaches based on recurrent neural networks (RNNs) alleviates the reliance on the Markov Assumption that was prevalent in sequence models, allowing to condition on arbitrary long sequences and produce effective feature extractors. These advance lead to breakthroughs in language modeling, automatic machine translations and other applications.

— Page xvii, Neural Network Methods in Natural Language Processing, 2017.

3. Promise of Feature Learning

Deep learning methods have the ability to learn feature representations rather than requiring experts to manually specify and extract features from natural language.

The NLP researcher Chris Manning, in the first lecture of his course on deep learning for natural language processing, highlights a different perspective.

He describes the limitations of manually defined input features, where prior applications of machine learning in statistical NLP were really a testament to the humans defining the features and that the computers did very little learning.

Chris suggests that the promise of deep learning methods is the automatic feature learning. He highlights that feature learning is automatic rather than manual, easy to adapt rather than brittle, and can continually and automatically improve.

In general our manually designed features tend to be overspecified, incomplete, take a long time to design and validated, and only get you to a certain level of performance at the end of the day. Where the learned features are easy to adapt, fast to train and they can keep on learning so that they get to a better level of performance they we’ve been able to achieve previously.

— Chris Manning, Lecture 1 | Natural Language Processing with Deep Learning, 2017 (slides, video).

4. Promise of Continued Improvement

Another promise of deep learning for NLP is continued and rapid improvement on challenging problems.

In the same initial lecture on deep learning for NLP, Chris Manning goes on to describe that deep learning methods are popular for natural language because they are working.

The real reason why deep learning is so exciting to most people is it has been working.

— Chris Manning, Lecture 1 | Natural Language Processing with Deep Learning, 2017 (slides, video).

He highlights that initial results were impressive and achieved results in speech better than any other methods in the last 30 years.

Chris goes on to mention that it is not just the state-of-the-art results being achieved, but also the rate of improvement.

… what has just been totally stunning is over the last 6 or 7 years, there’s just been this amazing ramp in which deep learning methods have been keeping on being improved and getting better at just an amazing speed. … I’d actually just say it unprecedented, in terms of seeming a field that has been progressing quite so quickly in its ability to be sort of rolling out better methods of doing things month on month.

— Chris Manning, Lecture 1 | Natural Language Processing with Deep Learning, 2017 (slides, video).

5. Promise of End-to-End Models

A final promise of deep learning is the ability to develop and train end-to-end models for natural language problems instead of developing pipelines of specialized models.

This is desirable both for the speed and simplicity of development in addition to the improved performance of these models.

Neural machine translation, or NMT for short, refers to large neural networks that attempt to learn to translate one language to another. This was a task traditionally handled by a pipeline of classical hand-tuned models, each of which required specialized expertise.

This is described by Chris Manning in lecture 10 of his Stanford course on deep learning for NLP.

Neural machine translation is used to mean what we want to do is build one big neural network which we can train entire end-to-end machine translation process in and optimize end-to-end. … This move away from hand customized piecewise models towards end-to-end sequence-to-sequence prediction models has been the trend in speech recognition. Systems that do that are referred to as an NMT [neural machine translation] system.

— Chris Manning, Lecture 10: Neural Machine Translation and Models with Attention, 2017. (slides, video)

This trend towards end-to-end models rather than pipelines of specialized systems is also a trend in speech recognition.

In his presentation of speech recognition in the Stanford NLP course, the NLP researcher Navdeep Jaitly, now at Nvidia, highlights that each component of a speech recognition can be replaced with a neural network.

The large blocks of an automatic speech recognition pipeline are speech processing, acoustic models, pronunciation models, and language models.

The problem is, the properties and importantly the errors of each sub-system are different. This motivates the need to develop one neural network to learn the whole problem end-to-end.

Over time people starting noticing that each of these components could be done better if we used a neural network. … However, there’s still a problem. There’s neural networks in every component, but errors in each one are different, so they may not play well together. So that is the basic motivation for trying to go to a process where you train entire model as one big model itself.

— Navdeep Jaitly, Lecture 12: End-to-End Models for Speech Processing, Natural Language Processing with Deep Learning, 2017 (slides, video).

Types of Deep Learning Networks for NLP

Deep Learning is a large field of study, and not all of it is relevant to natural language processing.

It is easy to get bogged down in specific optimization methods or extensions to model types intended to lift performance.

From a high-level, there are 5 methods from deep learning that deserve the most attention for application in natural language processing.

They are:

Embedding Layers. Multilayer Perceptrons (MLP). Convolutional Neural Networks (CNNs). Recurrent Neural Networks (RNNs). Recursive Neural Networks (ReNNs).

Types of Problems in NLP

Deep learning will not solve natural language processing or artificial intelligence.

To date, deep learning methods have been evaluated in a broader suite of problems from natural language processing and achieved success on a small set, where success suggests performance or capability at or above what was possible previously with other methods.

Importantly, those areas where deep learning methods are showing greatest success are some of the more end-user facing, challenging, and perhaps more interesting problems.

5 examples include:

Word Representation and Meaning.

Text Classification.

Language Modeling.

Machine Translation.

Speech Recognition.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Summary

In this post, you discovered the promise of deep learning neural networks for natural language processing.

Specifically, you learned:

The promises of deep learning for natural language processing.

What practitioners and research scientists have to say about the promise of deep learning in NLP.

Key deep learning methods and applications for natural language processing.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside"
296;news.mit.edu;http://news.mit.edu/2019/how-new-kilogram-measures-up-0514;;How the new kilogram measures up;"CAMBRIDGE, Mass. – It promises to be a weighty moment: On Monday, May 20 (World Metrology Day) the definition for the kilogram, the base unit of mass, will change. You might not notice it when weighing fruits and vegetables at the grocery store, but the new definition could have positive implications for areas where extremely precise measurements are required, such as with the use of nanodevices, when concocting the correct dose of compounds used in a medicine, or when calculating the weight of a newly discovered subatomic particle.

On Monday, May 20 at 4:00 p.m., Professor Wolfgang Ketterle, the 2001 Nobel laureate in physics, will present a special lecture exploring the new standards of measurement and why this is such a historic shift for scientists around the world. Ketterle will delve into how, for the first time ever, the definition for the kilogram — as well as the definitions for the base units of charge, temperature, and mole — will shift from being based on physical objects that can change over time, to fundamental constants that scientists around the world will be able to reproduce.

Reporters are invited to attend to learn more about how this seemingly insignificant alteration to the underpinnings of our measurement system could not only help reduce uncertainty when calculating values using the international standards of measurement, but also help democratize access to the system of weights and measures so that it is no longer tied to a specific country or place.

WHAT:

World Metrology Day Special Lecture: The New Kilogram

Presented by Professor Wolfgang Ketterle, the 2001 Nobel Laureate in Physics

WHERE:

MIT’s Huntington Hall

Building 10, Room 250 (map: http://whereis.mit.edu/?go=10)

WHEN:

Monday, May 20, 2019

4:00 P.M. – 5:00 P.M.

Media RSVP:

Reporters interested in attending should email Abby Abazorius at abbya@mit.edu or expertrequests@mit.edu to RSVP and for more information."
297;machinelearningmastery.com;http://machinelearningmastery.com/save-load-keras-deep-learning-models/;2019-05-12;How to Save and Load Your Keras Deep Learning Model;"# MLP for Pima Indians Dataset Serialize to JSON and HDF5

from keras . models import Sequential

from keras . layers import Dense

from keras . models import model_from_json

import numpy

import os

# fix random seed for reproducibility

numpy . random . seed ( 7 )

# load pima indians dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# create model

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# Compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# Fit the model

model . fit ( X , Y , epochs = 150 , batch_size = 10 , verbose = 0 )

# evaluate the model

scores = model . evaluate ( X , Y , verbose = 0 )

print ( ""%s: %.2f%%"" % ( model . metrics_names [ 1 ] , scores [ 1 ] * 100 ) )

# serialize model to JSON

model_json = model . to_json ( )

with open ( ""model.json"" , ""w"" ) as json_file :

json_file . write ( model_json )

# serialize weights to HDF5

model . save_weights ( ""model.h5"" )

print ( ""Saved model to disk"" )

# later...

# load json and create model

json_file = open ( 'model.json' , 'r' )

loaded_model_json = json_file . read ( )

json_file . close ( )

loaded_model = model_from_json ( loaded_model_json )

# load weights into new model

loaded_model . load_weights ( ""model.h5"" )

print ( ""Loaded model from disk"" )

# evaluate loaded model on test data

loaded_model . compile ( loss = 'binary_crossentropy' , optimizer = 'rmsprop' , metrics = [ 'accuracy' ] )

score = loaded_model . evaluate ( X , Y , verbose = 0 )"
298;machinelearningmastery.com;https://machinelearningmastery.com/applications-of-deep-learning-for-computer-vision/;2019-03-12;9 Applications of Deep Learning for Computer Vision;"Tweet Share Share

Last Updated on July 5, 2019

The field of computer vision is shifting from statistical methods to deep learning neural network methods.

There are still many challenging problems to solve in computer vision. Nevertheless, deep learning methods are achieving state-of-the-art results on some specific problems.

It is not just the performance of deep learning models on benchmark problems that is most interesting; it is the fact that a single model can learn meaning from images and perform vision tasks, obviating the need for a pipeline of specialized and hand-crafted methods.

In this post, you will discover nine interesting computer vision tasks where deep learning methods are achieving some headway.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we will look at the following computer vision problems where deep learning has been used:

Image Classification Image Classification With Localization Object Detection Object Segmentation Image Style Transfer Image Colorization Image Reconstruction Image Super-Resolution Image Synthesis Other Problems

Note, when it comes to the image classification (recognition) tasks, the naming convention from the ILSVRC has been adopted. Although the tasks focus on images, they can be generalized to the frames of video.

I have tried to focus on the types of end-user problems that you may be interested in, as opposed to more academic sub-problems where deep learning does well.

Each example provides a description of the problem, an example, and references to papers that demonstrate the methods and results.

Do you have a favorite computer vision application for deep learning that is not listed?

Let me know in the comments below.

Image Classification

Image classification involves assigning a label to an entire image or photograph.

This problem is also referred to as “object classification” and perhaps more generally as “image recognition,” although this latter task may apply to a much broader set of tasks related to classifying the content of images.

Some examples of image classification include:

Labeling an x-ray as cancer or not (binary classification).

Classifying a handwritten digit (multiclass classification).

Assigning a name to a photograph of a face (multiclass classification).

A popular example of image classification used as a benchmark problem is the MNIST dataset.

A popular real-world version of classifying photos of digits is The Street View House Numbers (SVHN) dataset.

For state-of-the-art results and relevant papers on these and other image classification tasks, see:

There are many image classification tasks that involve photographs of objects. Two popular examples include the CIFAR-10 and CIFAR-100 datasets that have photographs to be classified into 10 and 100 classes respectively.

The Large Scale Visual Recognition Challenge (ILSVRC) is an annual competition in which teams compete for the best performance on a range of computer vision tasks on data drawn from the ImageNet database. Many important advancements in image classification have come from papers published on or about tasks from this challenge, most notably early papers on the image classification task. For example:

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Image Classification With Localization

Image classification with localization involves assigning a class label to an image and showing the location of the object in the image by a bounding box (drawing a box around the object).

This is a more challenging version of image classification.

Some examples of image classification with localization include:

Labeling an x-ray as cancer or not and drawing a box around the cancerous region.

Classifying photographs of animals and drawing a box around the animal in each scene.

A classical dataset for image classification with localization is the PASCAL Visual Object Classes datasets, or PASCAL VOC for short (e.g. VOC 2012). These are datasets used in computer vision challenges over many years.

The task may involve adding bounding boxes around multiple examples of the same object in the image. As such, this task may sometimes be referred to as “object detection.”

The ILSVRC2016 Dataset for image classification with localization is a popular dataset comprised of 150,000 photographs with 1,000 categories of objects.

Some examples of papers on image classification with localization include:

Object Detection

Object detection is the task of image classification with localization, although an image may contain multiple objects that require localization and classification.

This is a more challenging task than simple image classification or image classification with localization, as often there are multiple objects in the image of different types.

Often, techniques developed for image classification with localization are used and demonstrated for object detection.

Some examples of object detection include:

Drawing a bounding box and labeling each object in a street scene.

Drawing a bounding box and labeling each object in an indoor photograph.

Drawing a bounding box and labeling each object in a landscape.

The PASCAL Visual Object Classes datasets, or PASCAL VOC for short (e.g. VOC 2012), is a common dataset for object detection.

Another dataset for multiple computer vision tasks is Microsoft’s Common Objects in Context Dataset, often referred to as MS COCO.

Some examples of papers on object detection include:

Object Segmentation

Object segmentation, or semantic segmentation, is the task of object detection where a line is drawn around each object detected in the image. Image segmentation is a more general problem of spitting an image into segments.

Object detection is also sometimes referred to as object segmentation.

Unlike object detection that involves using a bounding box to identify objects, object segmentation identifies the specific pixels in the image that belong to the object. It is like a fine-grained localization.

More generally, “image segmentation” might refer to segmenting all pixels in an image into different categories of object.

Again, the VOC 2012 and MS COCO datasets can be used for object segmentation.

The KITTI Vision Benchmark Suite is another object segmentation dataset that is popular, providing images of streets intended for training models for autonomous vehicles.

Some example papers on object segmentation include:

Style Transfer

Style transfer or neural style transfer is the task of learning style from one or more images and applying that style to a new image.

This task can be thought of as a type of photo filter or transform that may not have an objective evaluation.

Examples include applying the style of specific famous artworks (e.g. by Pablo Picasso or Vincent van Gogh) to new photographs.

Datasets often involve using famous artworks that are in the public domain and photographs from standard computer vision datasets.

Some papers include:

Image Colorization

Image colorization or neural colorization involves converting a grayscale image to a full color image.

This task can be thought of as a type of photo filter or transform that may not have an objective evaluation.

Examples include colorizing old black and white photographs and movies.

Datasets often involve using existing photo datasets and creating grayscale versions of photos that models must learn to colorize.

Some papers include:

Image Reconstruction

Image reconstruction and image inpainting is the task of filling in missing or corrupt parts of an image.

This task can be thought of as a type of photo filter or transform that may not have an objective evaluation.

Examples include reconstructing old, damaged black and white photographs and movies (e.g. photo restoration).

Datasets often involve using existing photo datasets and creating corrupted versions of photos that models must learn to repair.

Some papers include:

Image Super-Resolution

Image super-resolution is the task of generating a new version of an image with a higher resolution and detail than the original image.

Often models developed for image super-resolution can be used for image restoration and inpainting as they solve related problems.

Datasets often involve using existing photo datasets and creating down-scaled versions of photos for which models must learn to create super-resolution versions.

Some papers include:

Image Synthesis

Image synthesis is the task of generating targeted modifications of existing images or entirely new images.

This is a very broad area that is rapidly advancing.

It may include small modifications of image and video (e.g. image-to-image translations), such as:

Changing the style of an object in a scene.

Adding an object to a scene.

Adding a face to a scene.

It may also include generating entirely new images, such as:

Generating faces.

Generating bathrooms.

Generating clothes.

Some papers include:

Other Problems

There are other important and interesting problems that I did not cover because they are not purely computer vision tasks.

Notable examples image to text and text to image:

Presumably, one learns to map between other modalities and images, such as audio.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Survey Papers

Datasets

Articles

References

Summary

In this post, you discovered nine applications of deep learning to computer vision tasks.

Was your favorite example of deep learning for computer vision missed?

Let me know in the comments.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning Models for Vision Today! Develop Your Own Vision Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Computer Vision It provides self-study tutorials on topics like:

classification, object detection (yolo and rcnn), face recognition (vggface and facenet), data preparation and much more... Finally Bring Deep Learning to your Vision Projects Skip the Academics. Just Results. See What's Inside"
299;machinelearningmastery.com;https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/;2016-11-08;How To Implement The Decision Tree Algorithm From Scratch In Python;"# CART on the Bank Note dataset

from random import seed

from random import randrange

from csv import reader

# Load a CSV file

def load_csv ( filename ) :

file = open ( filename , ""rt"" )

lines = reader ( file )

dataset = list ( lines )

return dataset

# Convert string column to float

def str_column_to_float ( dataset , column ) :

for row in dataset :

row [ column ] = float ( row [ column ] . strip ( ) )

# Split a dataset into k folds

def cross_validation_split ( dataset , n_folds ) :

dataset_split = list ( )

dataset_copy = list ( dataset )

fold_size = int ( len ( dataset ) / n_folds )

for i in range ( n_folds ) :

fold = list ( )

while len ( fold ) < fold_size :

index = randrange ( len ( dataset_copy ) )

fold . append ( dataset_copy . pop ( index ) )

dataset_split . append ( fold )

return dataset_split

# Calculate accuracy percentage

def accuracy_metric ( actual , predicted ) :

correct = 0

for i in range ( len ( actual ) ) :

if actual [ i ] == predicted [ i ] :

correct += 1

return correct / float ( len ( actual ) ) * 100.0

# Evaluate an algorithm using a cross validation split

def evaluate_algorithm ( dataset , algorithm , n_folds , * args ) :

folds = cross_validation_split ( dataset , n_folds )

scores = list ( )

for fold in folds :

train_set = list ( folds )

train_set . remove ( fold )

train_set = sum ( train_set , [ ] )

test_set = list ( )

for row in fold :

row_copy = list ( row )

test_set . append ( row_copy )

row_copy [ - 1 ] = None

predicted = algorithm ( train_set , test_set , * args )

actual = [ row [ - 1 ] for row in fold ]

accuracy = accuracy_metric ( actual , predicted )

scores . append ( accuracy )

return scores

# Split a dataset based on an attribute and an attribute value

def test_split ( index , value , dataset ) :

left , right = list ( ) , list ( )

for row in dataset :

if row [ index ] < value :

left . append ( row )

else :

right . append ( row )

return left , right

# Calculate the Gini index for a split dataset

def gini_index ( groups , classes ) :

# count all samples at split point

n_instances = float ( sum ( [ len ( group ) for group in groups ] ) )

# sum weighted Gini index for each group

gini = 0.0

for group in groups :

size = float ( len ( group ) )

# avoid divide by zero

if size == 0 :

continue

score = 0.0

# score the group based on the score for each class

for class_val in classes :

p = [ row [ - 1 ] for row in group ] . count ( class_val ) / size

score += p * p

# weight the group score by its relative size

gini += ( 1.0 - score ) * ( size / n_instances )

return gini

# Select the best split point for a dataset

def get_split ( dataset ) :

class_values = list ( set ( row [ - 1 ] for row in dataset ) )

b_index , b_value , b_score , b_groups = 999 , 999 , 999 , None

for index in range ( len ( dataset [ 0 ] ) - 1 ) :

for row in dataset :

groups = test_split ( index , row [ index ] , dataset )

gini = gini_index ( groups , class_values )

if gini < b_score :

b_index , b_value , b_score , b_groups = index , row [ index ] , gini , groups

return { 'index' : b_index , 'value' : b_value , 'groups' : b_groups }

# Create a terminal node value

def to_terminal ( group ) :

outcomes = [ row [ - 1 ] for row in group ]

return max ( set ( outcomes ) , key = outcomes . count )

# Create child splits for a node or make terminal

def split ( node , max_depth , min_size , depth ) :

left , right = node [ 'groups' ]

del ( node [ 'groups' ] )

# check for a no split

if not left or not right :

node [ 'left' ] = node [ 'right' ] = to_terminal ( left + right )

return

# check for max depth

if depth >= max_depth :

node [ 'left' ] , node [ 'right' ] = to_terminal ( left ) , to_terminal ( right )

return

# process left child

if len ( left ) <= min_size :

node [ 'left' ] = to_terminal ( left )

else :

node [ 'left' ] = get_split ( left )

split ( node [ 'left' ] , max_depth , min_size , depth + 1 )

# process right child

if len ( right ) <= min_size :

node [ 'right' ] = to_terminal ( right )

else :

node [ 'right' ] = get_split ( right )

split ( node [ 'right' ] , max_depth , min_size , depth + 1 )

# Build a decision tree

def build_tree ( train , max_depth , min_size ) :

root = get_split ( train )

split ( root , max_depth , min_size , 1 )

return root

# Make a prediction with a decision tree

def predict ( node , row ) :

if row [ node [ 'index' ] ] < node [ 'value' ] :

if isinstance ( node [ 'left' ] , dict ) :

return predict ( node [ 'left' ] , row )

else :

return node [ 'left' ]

else :

if isinstance ( node [ 'right' ] , dict ) :

return predict ( node [ 'right' ] , row )

else :

return node [ 'right' ]

# Classification and Regression Tree Algorithm

def decision_tree ( train , test , max_depth , min_size ) :

tree = build_tree ( train , max_depth , min_size )

predictions = list ( )

for row in test :

prediction = predict ( tree , row )

predictions . append ( prediction )

return ( predictions )

# Test CART on Bank Note dataset

seed ( 1 )

# load and prepare data

filename = 'data_banknote_authentication.csv'

dataset = load_csv ( filename )

# convert string attributes to integers

for i in range ( len ( dataset [ 0 ] ) ) :

str_column_to_float ( dataset , i )

# evaluate algorithm

n_folds = 5

max_depth = 5

min_size = 10

scores = evaluate_algorithm ( dataset , decision_tree , n_folds , max_depth , min_size )

print ( 'Scores: %s' % scores )"
300;news.mit.edu;http://news.mit.edu/2020/3-questions-how-marine-life-can-recover-by-2050-0403;2020-03-19;3 Questions: Greg Britten on how marine life can recover by 2050;"As the largest ecosystem on the planet, the ocean provides incredible resources and benefits to humanity — including contributing 2.5 percent of global GDP and 1.5 percent of global employment, as well as regulating our climate, providing clean energy, and producing much of the oxygen we breathe. But exploitation and human pressures — like pollution, overfishing, and climate change — have stressed its life-support systems, depleting biodiversity, reducing habitats, and undermining ocean productivity.

Study and public awareness of the of these problems, as well as the beauty of these ecosystems, has led to conservation efforts beginning in the 1980s. By that time, however, significant damage had been done and some losses were permanent. Years of increased management and international policy since then have made measurable gains. At the same time, growing human populations are leaning harder on ocean resources. Understanding the critical need to rebuild these habitats and species populations has reached the level of the United Nations, which instated the Sustainable Development Goal 14 to “conserve and sustainably use the oceans, seas and marine resources for sustainable development.” The effort sets benchmarks and indicators of environmental successes in the area but threats, both local and international, persist and in some cases are worsening.

In a new Nature Review paper, Greg Britten, a postdoc in the MIT Department of Earth, Atmospheric and Planetary Sciences, and his colleagues examine different aspects of marine life and argue that aggressive interventions could lead to recovery of marine life by 2050. Here, he elucidates some of the findings from this work, which was supported, in part, by the Simons Collaboration on Computational Biogeochemical Modeling of Marine Ecosystems/CBIOMES.

Q: What is the current state of the world’s marine life and what recovery efforts have been attempted in the past?

A: While marine populations have been exploited throughout all of human history, the rate and magnitude of exploitation expanded exponentially between the 1950s and 1990s, largely due to the advent of industrial-scale fishing technology and large-scale habitat destruction via development of coastal areas. By the year 2000, it was estimated that the oceans’ “big fish” (tunas, large sharks, and billfish) were depleted by 90 percent relative to pre-exploitation levels. Further, approximately 60 percent of the world’s fisheries were considered “collapsed,” meaning that catches were at, or below, 10 percent of their historical maximum. At the same time, habitat destruction reached unprecedented levels — particularly in coastal areas.

These findings caused a tremendous response when revealed to the public that led to widespread calls for conservation intervention. Since then, marine exploitation has been significantly curtailed in much of the developed world, to a point where levels of exploitation are widely considered “sustainable”. Major global policy initiatives, like the Convention on the Trade of Endangered Species (CITES) and improvements to the Clean Water Act, also significantly reduced conservation threats like pollution, as well as the implementation of the International Convention for the Prevention of Pollution from Ships.

But this does not mean that populations immediately rebounded — indeed, they did not. It can take many years and decades for populations to fully rebuild to previous levels after the rate of exploitation has been reduced, and the impact of historical pollution and habitat destruction can linger for decades or longer. Furthermore, rates of exploitation and habitat destruction in the rest of the developing world have not been reduced as quickly, or remain unknown, while agreements to limit pollution and habitat destruction are generally also much weaker in developing countries.

Q: Tell us about your assessment of various interventions and potential future outcomes. What efforts have been successful so far, and where is there room for improvement?

A: We used a very large synthesis of available data to calculate historical and future trajectories of depleted marine populations under various levels of exploitation globally. We also documented the rates of recovery of habitats and ecosystems after pollution reductions and remediations were implemented.

We found that conservation and pollution reduction efforts, along with global environmental policy initiatives, have had a strong net positive influence on the recovery of marine populations, habitats, and ecosystems. We documented many cases of coral reef and mangrove recovery after local pollution remediation efforts. These occurred on a similar time scale as fish stocks, ranging from one to two decades for saltmarshes, to 30 years to a century for deep-sea corals and sponges that grow more slowly and are facing climate change, trawling, and oil spills. Globally, our research showed that the number of species listed as endangered by the International Union for the Conversation of Nature decreased from 18 percent in 2000 to 11.4 percent in the 2019, while the area of Marine Protected Areas (MPAs) increased from 0.13 million square kilometers to 27.4 million over the same period. These MPAs help protect multiple layers of the ecosystem, from coastal habitats to fish and megafauna species. The switch to unleaded gasoline in the 1980s reduced marine lead concentrations to those comparable to the time before leaded gasoline was introduced, due to the relatively low residence time of lead in marine surface waters.

Going forward, we found the vast majority of populations and habitats (with available data) could be rebuilt based on documented recovery rates by the year 2050, if exploitation is not increased beyond current levels. However, large-scale environmental agreements were most successful in developed countries, whereas enforcement and financial commitment was generally poorer in developing countries. Marine environmental “success stories” were generally of smaller scale in the developing world and often involved the intervention of international, non-governmental organizations.

Our analysis of recovery times showed that there are reasons for hope. Assuming that there’s a 2.95 percent annual recovery rate across ecosystems, and provided conditions aren’t depleted to less than 50 percent of their original level, we estimate that, on average, 90 percent of the original ecosystem could be regained in about 21 years — what we would consider a “substantial recovery.” However, since pressures like climate change and plastic pollution are increasing, and species and habitats are on the decline, more time is needed for recovery. Taking into account uncertainties associated with poor data coverage and varied national commitments, we believe it is possible to rebuild the vast majority of depleted marine populations and ecosystems by some 50 to 90 percent by 2050 — a goal we have labeled a “Grand Challenge for humanity.”

Q: What are barriers to recovery and why is it critical to act now to find a way around them for humanity and the planet?

A: Lack of consistency in national marine commitments, funding, and regulations around the globe is perhaps the largest barrier to marine population and habitat recovery. For example, many nations differ in their fishing policies, in and around MPAs, which means that migratory populations like bluefin tuna and large sharks may be protected across much of their habitat while also encountering areas where fishing policies are less stringent, which can significantly slow rebuilding efforts.

Since developing nations lack conservation capacity and financial resources, we argue that enhancing the regulatory power of international bodies such as CITES and the United National Environment Program has the potential to solve these issues. But, it will require concerted effort among all countries, along with significant financial commitments, to improve and enforce these agreements internationally. However, achieving the desired results may be problematic if groups are failing to meet commitments to existing problems, like the Paris Agreement with climate change — an issue that affects whole ecosystems, causing species displacement and mass mortalities, and dictates rebuilding efforts.

If international, regional, and local communities prioritize “blue infrastructure” and marine life, the societal benefits and economic return by 2050 would be numerous. For every dollar invested, yields would be 10 dollars and over a million jobs. Revitalized fish populations, supported by policies and incentives, would see a huge jump in profits while improving overall health and sustainability of life in the area. Worldwide, the seafood profits would increase $53 billion. Further, $52 billion would be saved by restoring wetlands, which control storm surge, flooding, subsistence, and assist with climate change. Multi-tiered, complementary strategies, accountability, and buy-in can make this an achievable goal."
301;machinelearningmastery.com;https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/;;Deep Learning for Time Series Forecasting;"Deep Learning for Time Series Forecasting

Predict the Future with MLPs , CNNs and LSTMs in Python

$37 USD Deep learning methods offer a lot of promise for time series forecasting, such as the automatic learning of temporal dependence and the automatic handling of temporal structures like trends and seasonality. In this new Ebook written in the friendly Machine Learning Mastery style that you’re used to, skip the math and jump straight to getting results. With clear explanations, standard Python libraries, and step-by-step tutorial lessons you’ll discover how to develop deep learning models for your own time series forecasting projects. About this Ebook: Read on all devices : PDF format Ebook, no DRM.

: PDF format Ebook, no DRM. Tons of tutorials : 5 parts, 25 step-by-step lessons, 575 pages.

: 5 parts, 25 step-by-step lessons, 575 pages. Real-world projects : 2 large end-to-end tutorial projects.

: 2 large end-to-end tutorial projects. Many datasets : Univariate, multivariate, multi-step, and more.

: Univariate, multivariate, multi-step, and more. Working code: 131 Python (.py) code files included. Clear, Complete End-to-End Examples. Convinced?

Click to jump straight to the packages.

The book is very clear, well written and easy to apply. Moreover, Jason is always reactive to questions, helpful, and ready to give advices and help target the part that treats the ML problem you are trying to solve or to give advice. I definitely recommend the it. Saad Mouti Postdoctoral Fellow

Excellent book covering / comparing both deep learning and classical methods for time series. The code is clear and easily transferable to my own work. Claude Pelletier Machine Learning Lead

…why deep learning?

The Promise of Deep Learning for Time Series Forecasting

Traditionally, time series forecasting has been dominated by linear methods because they are well understood and effective on many simpler forecasting problems.

Deep learning neural networks are able to automatically learn arbitrary complex mappings from inputs to outputs and support multiple inputs and outputs.

Multilayer Perceptrons (MLPs)

Generally, neural networks like Multilayer Perceptrons or MLPs provide capabilities that are offered by few algorithms, such as:

Robust to Noise . Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values.

. Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values. Nonlinear . Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships.

. Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships. Multivariate Inputs . An arbitrary number of input features can be specified, providing direct support for multivariate forecasting.

. An arbitrary number of input features can be specified, providing direct support for multivariate forecasting. Multi-step Forecasts. An arbitrary number of output values can be specified, providing

direct support for multi-step and even multivariate forecasting.

For these capabilities alone, feedforward neural networks may be useful for time series forecasting.

Convolutional Neural Networks (CNNs)

Convolutional Neural Networks or CNNs are a type of neural network that was designed to efficiently handle image data.

The ability of CNNs to learn and automatically extract features from raw input data can be applied to time series forecasting problems. A sequence of observations can be treated like a one-dimensional image that a CNN model can read and distill into the most salient elements.

Feature Learning. Automatic identification, extraction and distillation of salient features from raw input data that pertain directly to the prediction problem that is being modeled.

CNNs get the benefits of Multilayer Perceptrons for time series forecasting, namely support for multivariate input, multivariate output and learning arbitrary but complex functional relationships, but do not require that the model learn directly from lag observations. Instead, the model can learn a representation from a large input sequence that is most relevant for the prediction problem.

Long Short-Term Memory Networks (LSTMs)

Recurrent neural networks like the Long Short-Term Memory network or LSTM add the explicit handling of order between observations when learning a mapping function from inputs to outputs, not offered by MLPs or CNNs. They are a type of neural network that adds native support for input data comprised of sequences of observations.

Native Support for Sequences. Recurrent neural networks directly add support for input sequence data.

This capability of LSTMs has been used to great effect in complex natural language processing problems such as neural machine translation where the model must learn the complex interrelationships between words both within a given language and across languages in translating form one language to another.

Learned Temporal Dependence. The most relevant context of input observations to the expected output is learned and can change dynamically.

The model both learns a mapping from inputs to outputs and learns what context from the input sequence is useful for the mapping, and can dynamically change this context as needed.

…but how?

How do you Apply Deep Learning Methods to Time Series Data?

Deep learning methods are trained using supervised learning and expect data in the form of samples with inputs and outputs.

Time series are long sequences of numbers.

How do you transform a time series into a form suitable for supervised learning?

Methods like Convolutional Neural Networks and Long Short-Term Memory networks expect samples to have a three-dimensional structure.

How do you transform time series data to have this structure?

Once you have the data in the right format, you must design a deep learning model for the problem. This is where things get really interesting.

What architecture should you use?

What hyperparametres should you use?

How do you evaluate a model?

How do you make predictions?

How do you tune the model hyperparametres?

Perhaps one of the topics that I am asked the most about is how to use deep learning methods for time series forecasting.

I have carefully designed a suite of tutorials to address these specific questions.

Introducing:

“ Deep Learning for Time Series Forecasting “

This book was designed to show you exactly how to apply deep learning methods to time series forecasting problems.

In writing this book, I imagined that you were provided with a dataset and a desire to use deep learning methods to address it. I designed the chapters to walk you through the process of first establishing a baseline of performance with naive and classical methods. I then provide step-by-step tutorials to show exactly how to develop a suite of different types of neural network models for time series forecasting. After we cover these basics, I then hammer home how to use them on real-world datasets with example after example on larger projects.

This is not a book for beginners.

The focus on deep learning methods means that we don’t focus on many other important areas of time series forecasting, such as data visualization, how classical methods work, the development of machine learning solutions, or even depth and details on how the deep learning methods work. I assume that you are familiar with these introductory topics.

In addition to providing a playbook to show you how to develop deep learning models for your own time series forecasting problems, I designed this book to highlight the areas where deep learning methods may show the most promise. Deep learning may be the future of complex and challenging time series forecasting and I think this book will help you get started and make rapid progress on your own forecasting problems. I hope that you agree and are as excited as I am about the journey ahead.

…YOU will:

Develop Practical Skills for Time Series Forecasting

That You Can Apply Immediately

Forecasting Problems You will work through 5 different types of time series forecasting problems. Univariate. A single series of observations over time. Multivariate. Multiple inter-related observations over time. Multi-step. Forecast multiple time steps into the future. Multivariate Multi-step. Forecast multiple time steps into the future for multiple different series. Classification. Predict a discrete class given a sequence of observations over time. Deep Learning Algorithms You will discover 4 deep learning methods that you can use to develop defensible time series forecasting methods. MLPs. The classical neural network architecture including how to grid search model hyperparameters. CNNs. Simple CNN models as well as multi-channel models and advanced multi-headed and multi-output models. LSTMs. Simple LSTM models, Stacked LSTMs, Bidirectional LSTMs and Encoder-Decoder models for sequence-to-sequence learning. Hybrids. Hybrids of MLP, CNN and LSTM models such as CNN-LSTMs, ConvLSTMs and more.

Baseline Methods You will discover 3 methods that you can use to develop robust baselines for your time series forecasting problems. Simple forecast methods. Methods such as naive or persistence forecasting and averaging methods, as well as how to optimize their performance Autoregressive forecasting methods. Methods such as ARIMA and Seasonal ARIMA (SARIMA) and how to grid search their hyperparameters. Exponential smoothing forecasting methods. Methods such single, double and triple exponential smoothing also called ETS and how to grid search their hyperparameters. Real-World Projects You will work through 3 main types of real-world practical time series forecasting projects. Univariate Datasets. Forecast a range of datasets such as sales, births, temperature and more. Household Power Usage. Weekly forecasts of the total amount of electricity consumed by a single household. Human Activity Recognition. Predict the specific type of movement based on smartphone accelerometer data.

…so is this book right for YOU?

Who Is This Book For?

Let’s make sure you are in the right place.

This book is for developers that know some applied machine learning and some deep learning. This is not a beginners book.

Maybe you want or need to start using deep learning for time series on your research project or on a project at work. This book was written to help you do that quickly and efficiently by compressing years worth of knowledge and experience into a laser-focused course of hands-on tutorials.

This guide was written in the top-down and results-first style that you’re used to from Machine Learning Mastery.

The lessons in this book assume a few things about you.

You need to know: You need to know the basics of Python programming.

You need to know the basics of working with time series data.

You need to know the basics of deep learning methods.

You do NOT need to know: You do not need to be a math wiz!

You do not need to be a deep learning expert!

You do not need to be a master of time series forecasting!

…so what will YOU know after reading it?

About Your Learning Outcomes

This book will teach you how to get results.

After reading and working through this book,

you will know:

About the promise of neural networks and deep learning methods in general for time series forecasting.

How to transform time series data in order to train a supervised learning algorithm, such as deep learning methods.

How to develop baseline forecasts using naive and classical methods by which to determine whether forecasts from deep learning models have skill or not.

How to develop Multilayer Perceptron, Convolutional Neural Network, Long Short-Term Memory Networks, and hybrid neural network models for time series forecasting.

How to forecast univariate, multivariate, multi-step, and multivariate multi-step time series forecasting problems in general.

How to transform sequence data into a three-dimensional structure in order to train convolutional and LSTM neural network models.

How to grid search deep learning model hyperparameters to ensure that you are getting good performance from a given model.

How to prepare data and develop deep learning models for forecasting a range of univariate time series problems with different temporal structures.

How to prepare data and develop deep learning models for multi-step forecasting a real-world household electricity consumption dataset.

How to prepare data and develop deep learning models for a real-world human activity recognition project.

This book will NOT teach you how to be a research scientist and all the theory behind why specific methods work. It will teach you how to get results and deliver value on your time series forecasting projects.

This new understanding of applied deep learning methods will impact your practice of working through time series forecasting problems in the following ways:

Confidently use naive and classical methods like SARIMA and ETS to quickly develop robust baseline models for a range of different time series forecasting problems, the performance of which can be used to challenge whether more elaborate machine learning and deep learning models are adding value. Transform native time series forecasting data into a form for fitting supervised learning algorithms and confidently tune the amount of lag observations and framing of the prediction problem. Develop MLP, CNN, RNN, and hybrid deep learning models quickly for a range of different time series forecasting problems, and confidently evaluate and interpret their performance.

This book is not a substitute for an undergraduate course in deep learning or time series forecasting, nor is it a textbook for such courses, although it could be a useful complement. For a good list of top courses, textbooks, and other resources, see the Further Reading section at the end of each tutorial.

… so what is in the Ebook?

25 Step-by-Step Tutorials to Transform you into a

Deep Learning Time Series Practitioner

This book was designed around major deep learning techniques that are directly relevant to time series forecasting.

There are a lot of things you could learn about deep learning and time series forecasting, from theory to abstract concepts to APIs. My goal is to take you straight to developing an intuition for the elements you must understand with laser-focused tutorials

I designed the tutorials to focus on how to get results with deep learning methods. The tutorials give you the tools to both rapidly understand and apply each technique or operation

Each of the lessons are designed to take you about one hour to read through and complete, excluding the extensions and further reading.

You can choose to work through the lessons one per day, one per week, or at your own pace. I think momentum is critically important, and this book was intended to be read and used, not to sit idle. I would recommend picking a schedule and sticking to it.

The tutorials are divided into six parts:

Part 1: Foundations . Provides a gentle introduction to the promise of deep learning methods for time series forecasting, a taxonomy of the types of time series forecasting problems, how to prepare time series data for supervised learning, and a high-level procedure for getting the best performing model on time series forecasting problems in general.

. Provides a gentle introduction to the promise of deep learning methods for time series forecasting, a taxonomy of the types of time series forecasting problems, how to prepare time series data for supervised learning, and a high-level procedure for getting the best performing model on time series forecasting problems in general. Part 2: Deep Learning Modeling . Provides a step-by-step introduction to deep learning methods applied to different types of time series forecasting problems with additional tutorials to better understand the 3D-structure required for some models.

. Provides a step-by-step introduction to deep learning methods applied to different types of time series forecasting problems with additional tutorials to better understand the 3D-structure required for some models. Part 3: Univariate Forecasting . Provides a methodical approach to univariate time series forecasting with a focus on naive and classical methods that are generally known to out-perform deep learning methods and how to grid search deep learning model hyperparameters.

. Provides a methodical approach to univariate time series forecasting with a focus on naive and classical methods that are generally known to out-perform deep learning methods and how to grid search deep learning model hyperparameters. Part 4: Multi-step Forecasting . Provides a step-by-step series of tutorials for working through a challenging multi-step time series forecasting problem for predicting household electricity consumption using classical and deep learning methods.

. Provides a step-by-step series of tutorials for working through a challenging multi-step time series forecasting problem for predicting household electricity consumption using classical and deep learning methods. Part 5: Time Series Classification. Provides a step-by-step series of tutorials for working through a challenging time series classification problem for

Table of Contents

Below is an overview of the 25 step-by-step tutorial lessons you will complete:

Each lesson was designed to be completed in about 30-to-60 minutes by the average developer.

Part 1: Foundations

Lesson 01 : Promise of Deep Learning for Time Series Forecasting

: Promise of Deep Learning for Time Series Forecasting Lesson 02 : Taxonomy of Time Series Forecasting Problems

: Taxonomy of Time Series Forecasting Problems Lesson 03 : How to Develop a Skillful Forecasting Model

: How to Develop a Skillful Forecasting Model Lesson 04 : How to Transform Time Series to a Supervised Learning Problem

: How to Transform Time Series to a Supervised Learning Problem Lesson 05: How to Prepare Time Series Data for CNNs and LSTMs

Part 2: Deep Learning Modeling

Lesson 06 : How to Prepare Time Series Data for CNNs and LSTMs

: How to Prepare Time Series Data for CNNs and LSTMs Lesson 07 : How to Develop MLPs for Time Series Forecasting

: How to Develop MLPs for Time Series Forecasting Lesson 08 : How to Develop CNNs for Time Series Forecasting

: How to Develop CNNs for Time Series Forecasting Lesson 09: How to Develop LSTMs for Time Series Forecasting

Part 3: Univariate Forecasting

Lesson 10 : Review of Top Methods For Univariate Time Series Forecasting

: Review of Top Methods For Univariate Time Series Forecasting Lesson 11 : How to Develop Simple Methods for Univariate Forecasting

: How to Develop Simple Methods for Univariate Forecasting Lesson 12 : How to Develop ETS Models for Univariate Forecasting

: How to Develop ETS Models for Univariate Forecasting Lesson 13 : How to Develop SARIMA Models for Univariate Forecasting

: How to Develop SARIMA Models for Univariate Forecasting Lesson 14 : How to Develop MLPs, CNNs and LSTMs for Univariate Forecasting

: How to Develop MLPs, CNNs and LSTMs for Univariate Forecasting Lesson 15: How to Grid Search Deep Learning Models for Univariate Forecasting

Part 4: Multi-step Forecasting

Lesson 16 : How to Load and Explore Household Energy Usage Data

: How to Load and Explore Household Energy Usage Data Lesson 17 : How to Develop Naive Models for Multi-step Energy Usage Forecasting

: How to Develop Naive Models for Multi-step Energy Usage Forecasting Lesson 18 : How to Develop ARIMA Models for Multi-step Energy Usage Forecasting

: How to Develop ARIMA Models for Multi-step Energy Usage Forecasting Lesson 19 : How to Develop CNNs for Multi-step Energy Usage Forecasting

: How to Develop CNNs for Multi-step Energy Usage Forecasting Lesson 20: How to Develop LSTMs for Multi-step Energy Usage Forecasting

Part 5: Time Series Classification

Lesson 21 : Review of Deep Learning Models for Human Activity Recognition

: Review of Deep Learning Models for Human Activity Recognition Lesson 22 : How to Load and Explore Human Activity Data

: How to Load and Explore Human Activity Data Lesson 23 : How to Develop ML Models for Human Activity Recognition

: How to Develop ML Models for Human Activity Recognition Lesson 24 : How to Develop CNNs for Human Activity Recognition

: How to Develop CNNs for Human Activity Recognition Lesson 25: How to Develop LSTMs for Human Activity Recognition

You can see that each part targets a specific learning outcome, and so does each tutorial within each part. This acts as a filter to ensure you are only focused on the things you need to know to get to a specific result and do not get bogged down in the math or near-infinite number of configuration parameters.

The tutorials were not designed to teach you everything there is to know about each of the techniques or time series forecasting problems. They were designed to give you an understanding of how they work, how to use them on your projects the fastest way I know how: to learn by doing.

Take a Sneak Peek Inside The Ebook

Click image to Enlarge.

…you’ll also get 131 fully working Python scripts

BONUS: Deep Learning Time Series Forecasting Code Recipes

Sample Code Recipes Each recipe presented in the book is standalone, meaning that you can copy and paste it into your project and use it immediately. You get one Python script (.py) for each example provided in the book. This means that you can follow along and compare your answers to a known working implementation of each example in the provided Python files. This helps a lot to speed up your progress when working through the details of a specific task, such as: Preparing Data.

Transforming Data.

Defining Models.

Fitting Models.

Evaluating Models.

Making Predictions. The provided code was developed in a text editor and intended to be run on the command line. No special IDE or notebooks are required. All code examples were tested with Python 3 and Keras 2. All code examples will run on modest and modern computer hardware and were executed on a CPU. No GPUs are required to run the presented examples, although a GPU would make the code run faster. Python Technical Details This section provides some technical details about the code provided with the book. Python Version : You can use Python 3.

: You can use Python 3. SciPy : You will use NumPy, Pandas and scikit-learn.

: You will use NumPy, Pandas and scikit-learn. Keras : You will need Keras version 2 with either a Theano or TensorFlow backend.

: You will need Keras version 2 with either a Theano or TensorFlow backend. Operating System : You can use Windows, Linux or Mac OS X.

: You can use Windows, Linux or Mac OS X. Hardware : A standard modern workstation will do, no GPUs required.

: A standard modern workstation will do, no GPUs required. Editor: You can use a text editor and run the example from the command line. Don’t have a Python environment? No Problem! The appendix contains step-by-step tutorials showing you exactly how to setup a Python deep learning environment.

About The Author

Hi, I'm Jason Brownlee. I run this site and I wrote and published this book.

I live in Australia with my wife and sons. I love to read books, write tutorials, and develop systems.

I have a computer science and software engineering background as well as Masters and PhD degrees in Artificial Intelligence with a focus on stochastic optimization.

I've written books on algorithms, won and ranked well in competitions, consulted for startups, and spent years in industry. (Yes, I have spend a long time building and maintaining REAL operational systems!)

I get a lot of satisfaction helping developers get started and get really good at applied machine learning.

I teach an unconventional top-down and results-first approach to machine learning where we start by working through tutorials and problems, then later wade into theory as we need it.

I'm here to help if you ever have any questions. I want you to be awesome at machine learning.





Download Your Sample Chapter Do you want to take a closer look at the book? Download a free sample chapter PDF. Enter your email address and your sample chapter will be sent to your inbox. >> Click Here to Download Your Sample Chapter

Check Out What Customers Are Saying:

It is a great introductory about time series forecasting methods. The deep learning methods bring a beneficial to the topic. Also, the discussion about the forecasting accuracy of the classical methods and machine learning methods is a great topic that opens a wide of wondering and research. I like the book and I thank you for your progress to improve the research area of machine learning and deep learning applications. Abdulaziz Almalaq Practitioner The book covers all needed procedures for a time series forecasting. My expectations are satisfied. Thank you. Onur Copur Business Data Analyst Directed to solving practical problems; pragmatism is precisely what I was looking for. Thanks Jason for the valuable insights! David Morton de Lachapelle Chief Scientific Officer

The book is very comprehensive, yet, it is organized in a way that allows quick browsing and finding of the deep learning angle that is right for my project. Before any subject is completed, a simple fully working example is provided both in the PDF and in the folder. I followed the code with the debugger and learned types, shapes and value of variables. Jason is very helpful and responsive, always willing to answer questions if anything is not fully understood, sending me to additional tutorials (he wrote plenty and all of them taught me a lot about deep learning) or even writing some additional text to make something clearer. Thank you Jason for helping me become a deep learning expert. Or Bennatan Program Manager This book is a great read regardless of your skill level of the python language. The tutorials are easy to understand and start of off with beginner topics all the way through to the very advanced. 25 chapters of progressive learning examples, 574 pages of amazing content, a must-have item for the programmer’s library. Casey Condran Systems Specialist

You're Not Alone in Choosing Machine Learning Mastery

Trusted by Over 47,354 Practitioners

...including employees from companies like:

...students and faculty from universities like:

and many thousands more...

Absolutely No Risk with...

100% Money Back Guarantee

Plus, as you should expect of any great product on the market, every Machine Learning Mastery Ebook

comes with the surest sign of confidence: my gold-standard 100% money-back guarantee.

100% Money-Back Guarantee

If you're not happy with your purchase of any of the Machine Learning Mastery Ebooks,

just email me within 90 days of buying, and I'll give you your money back ASAP.

No waiting. No questions asked. No risk.

…it’s time to take the next step.

Bring Deep Learning to Your Time Series Project NOW! Choose Your Package:

Basic Package You will get the Ebook: Deep Learning for Time Series Forecasting (including bonus source code) Buy Now for $37 (a great deal!)

Deep Learning Bundle TOP SELLER You get the 7-Ebook set: Deep Learning With Python Deep Learning for Computer Vision Deep Learning for Natural Language Processing Deep Learning for Time Series Forecasting Generative Adversarial Networks with Python Long Short-Term Memory Networks with Python Better Deep Learning (includes all bonus source code) Buy Now for $187 That's $269.00 of Value! (You get a 30.48% discount)

Super Bundle BEST VALUE You get the complete 18-Ebook set: Statistics Methods for Machine Learning Linear Algebra for Machine Learning Probability for Machine Learning Master Machine Learning Algorithms ML Algorithms From Scratch Machine Learning Mastery With Weka Machine Learning Mastery With R Machine Learning Mastery With Python Imbalanced Classification With Python Time Series Forecasting With Python Deep Learning With Python Deep Learning for CV Deep Learning for NLP Deep Learning for Time Series Forecasting Generative Adversarial Networks with Python Better Deep Learning LSTM Networks With Python XGBoost With Python (includes all bonus source code) Buy Now for $447 That's $646.00 of Value! (You save a massive $199.00)

All prices are in US Dollars (USD).

(1) Click the button. (2) Enter your details. (3) Download immediately.

Secure Payment Processing With SSL Encryption

Are you a Student, Teacher or Retiree? Contact me about a discount.

Do you have any Questions? See the FAQ.

What Are Skills in Machine Learning Worth?

Your boss asks you:

Hey, can you build a predictive model for this?

Imagine you had the skills and confidence to say:

""YES!""

...and follow through .

I have been there. It feels great!

How much is that worth to you?

The industry is demanding skills in machine learning.

The market wants people that can deliver results, not write academic papers.

Business knows what these skills are worth and are paying sky-high starting salaries.

A Data Scientists Salary Begins at:

$100,000 to $150,000.

A Machine Learning Engineers Salary is Even Higher.

What Are Your Alternatives?

You made it this far.

You're ready to take action.

But, what are your alternatives? What options are there?

(1) A Theoretical Textbook for $100+

...it's boring, math-heavy and you'll probably never finish it.

(2) An On-site Boot Camp for $10,000+

...it's full of young kids, you must travel and it can take months.

(3) A Higher Degree for $100,000+

...it's expensive, takes years, and you'll be an academic.

OR...

For the Hands-On Skills You Get...

And the Speed of Results You See...

And the Low Price You Pay...

Machine Learning Mastery Ebooks are

Amazing Value!

And they work. That's why I offer the money-back guarantee.

You're A Professional

The field moves quickly,

...how long can you wait? You think you have all the time in the world, but... New methods are devised and algorithms change .

. New books get released and prices increase .

. New graduates come along and jobs get filled . Right Now is the Best Time to make your start.

Bottom-up is Slow and Frustrating,

...don't you want a faster way? Can you really go on another day, week or month... Scraping ideas and code from incomplete posts .

. Skimming theory and insight from short videos .

. Parsing Greek letters from academic textbooks . Targeted Training is your Shortest Path to a result.

Professionals Stay On Top Of Their Field

Get The Training You Need!

You don't want to fall behind or miss the opportunity.

Frequently Asked Questions

Previous Next

Do you have another question?

Please contact me."
302;machinelearningmastery.com;https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/;2019-04-02;How to Normalize, Center, and Standardize Image Pixels in Keras;"# example of using ImageDataGenerator to normalize images

from keras . datasets import mnist

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Conv2D

from keras . layers import MaxPooling2D

from keras . layers import Dense

from keras . layers import Flatten

from keras . preprocessing . image import ImageDataGenerator

# load dataset

( trainX , trainY ) , ( testX , testY ) = mnist . load_data ( )

# reshape dataset to have a single channel

width , height , channels = trainX . shape [ 1 ] , trainX . shape [ 2 ] , 1

trainX = trainX . reshape ( ( trainX . shape [ 0 ] , width , height , channels ) )

testX = testX . reshape ( ( testX . shape [ 0 ] , width , height , channels ) )

# one hot encode target values

trainY = to_categorical ( trainY )

testY = to_categorical ( testY )

# confirm scale of pixels

print ( 'Train min=%.3f, max=%.3f' % ( trainX . min ( ) , trainX . max ( ) ) )

print ( 'Test min=%.3f, max=%.3f' % ( testX . min ( ) , testX . max ( ) ) )

# create generator (1.0/255.0 = 0.003921568627451)

datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 )

# prepare an iterators to scale images

train_iterator = datagen . flow ( trainX , trainY , batch_size = 64 )

test_iterator = datagen . flow ( testX , testY , batch_size = 64 )

print ( 'Batches train=%d, test=%d' % ( len ( train_iterator ) , len ( test_iterator ) ) )

# confirm the scaling works

batchX , batchy = train_iterator . next ( )

print ( 'Batch shape=%s, min=%.3f, max=%.3f' % ( batchX . shape , batchX . min ( ) , batchX . max ( ) ) )

# define model

model = Sequential ( )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , input_shape = ( width , height , channels ) ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 64 , activation = 'relu' ) )

model . add ( Dense ( 10 , activation = 'softmax' ) )

# compile model

model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )

# fit model with generator

model . fit_generator ( train_iterator , steps_per_epoch = len ( train_iterator ) , epochs = 5 )

# evaluate model

_ , acc = model . evaluate_generator ( test_iterator , steps = len ( test_iterator ) , verbose = 0 )"
303;news.mit.edu;http://news.mit.edu/2020/david-rand-0412;;Playing a new tune;"What’s it like being a professor at the MIT Sloan School of Management? David Rand has an answer you might not expect.

“Being an academic is like being in a punk-rock band,” says Rand, the Erwin H. Schell Professor in MIT Sloan’s Marketing Group.

Oh? How so?

“The short version is, in both cases, you start by trying to come up with a new idea that nobody’s used before,” explains Rand, who studies human behavior, cooperation, and social networks. “In academia, it’s a good research idea, and in music it’s a cool riff or melody. Then you take that kernel and spend a lot of time developing it into this cohesive whole that you try to make a perfect as possible.”

Sounds reasonable. What happens next?

“Once that’s done, you have to capture it in a way you can share with other people, which is either writing the paper, or recording the song,” continues Rand, who also has a joint appointment with MIT's Department of Brain and Cognitive Sciences. “That’s always the most painful part. It’s always tempting to just start new projects or songs, rather than putting in the work to finish the recording you’ve already created, but you have to do it.”

Rand spent years playing guitar and bass in punk-rock bands when he was younger, but has generated hits of a different kind more recently, as a professor who writes innovative academic papers on social phenomena such as cooperation and the spread of misinformation on social media. Much of Rand’s work explores what happens when people’s behavior is guided by intuitive thinking or a more deliberative mode of cognition. With that framework in mind, he seeks to understand what decisions people will make in social settings, like whether to pay costs to help others, what news to believe and share online, and whom to vote for.

Rand is now running the academic version of a recording studio, if you will. He is the director of MIT’s Human Cooperation Laboratory and co-director of MIT’s Applied Cooperation Team — settings where he acts like a record producer, collaborating with other scholars to help them pursue their own research ideas.

For his distinctive body of research, Rand joined MIT with tenure in 2018, feeling that his work was “very MIT, very Sloan” in its emphases on networks and real-world impact.

“I visited, and once I saw what people are doing, I said, ‘This is great,’” Rand recalls. “There are so many connections between my interests and what people are working on in the marketing group, in other groups at Sloan, and across MIT generally.”

Holidays in the sun

Rand grew up in Ithaca, New York, where his father was an applied math professor at Cornell University. As an undergraduate at Cornell himself, Rand majored in computational biology. That helped him become interested in evolutionary biology — including questions about how cooperation and altruism fit in a framework of evolutionary competition.

But academia wasn’t the only thing that got Rand interested in cooperation — so did playing in punk bands, which for him included traveling down from Cornell to Florida and other places on winter break and in the summers.

“I had grown up with the basic understanding that people were by nature selfish, although my parents would say, ‘You didn’t get that from us,’” Rand reflects. “I guess it was growing up in the 1980s.”

Still, he continues, while touring with his band, “we had so many experiences of total strangers being nice to us and helping us. The parents of fans, every show, would be like, ‘Here’s some random band from New York, you can sleep on the floor, we’ll make you breakfast.’ Our van broke down and a mechanic helped us for free because he felt bad for us. It transformed my idea of human nature. At least, under the right circumstances, people can be very prosocial.”

A couple years after college, Rand was accepted into Harvard University’s PhD program in systems biology, although without, he says, a firm grasp of what he wanted to study. However, taking a class on evolutionary game theory, Rand recounts, “I fell in love with the prisoner’s dilemma,” the classic problem in which two prisoners can collectively benefit the most by cooperating, but individually benefit by pursuing their own self-interests.

Rand starting doing experiments about the prisoner’s dilemma, motivated by a simple question concerning such situations — “What do people actually do?” — and has never really stopped. Since 2008, he has co-authored over 100 peer-reviewed papers about cooperation, altruism, and the spread of ideas and behavior in networks.

Cambridge calling

In 2009, Rand earned his PhD from Harvard, and, after some postdoctoral fellowships, joined the Yale University faculty in 2013. He earned tenure from Yale in 2018. The same year, he received his job offer from MIT Sloan and joined the Institute, attracted in part by the opportunities for interdisciplinary research.

“I think there are tons of opportunities for real innovation that come from combining approaches from different disciplines,” Rand says.

However, he notes, just because interdisciplinary research may sound appealing doesn’t mean it is easy.

“I feel one of the benefits of my [academic] trajectory is I am multilingual in a scientific sense,” Rand says. “Lots of people talk about how interdisciplinarity is cool. But the thing that makes interdisciplinarity hard is that in each discipline, if you’re raised in it, you learn how to talk in that discipline’s dialect, making it surprisingly hard to collaborate with people outside of the discipline. But … I’ve spent a lot of time in each of the relevant disciplines, to learn those disciplinary languages. One of my graduates students said to me, ‘Oh yeah, you’re code-switching.’”

At MIT, Rand has been collaborating extensively with professor of political science Adam Berinsky, who also runs experiments about political misinformation. Working with a collection of graduate students from across MIT, they have been investigating misinformation and social media.

In one project, conducted in coordination with Facebook, they have been testing whether social media platforms can survey their users to crowdsource fact-checking.

“The problem with professional fact-checking is that is doesn’t scale,” explains Rand. “It’s much easier to create misleading content than it is to fact-check it, so professional fact-checkers simply can’t keep up. But it may be that averaging the responses of many laypeople can approximate the conclusions of professionals, thanks to the ‘wisdom of crowds.’”

In another, they found that video was not much more persuasive than text. “There is a great deal of panic right now about how AI can be used to make ‘deepfakes,’ very convincingly doctored videos,” Rand says. “But our work suggests that these concerns may be a bit premature. It’s really important to do empirical tests about what matters and what approaches will be effective, rather than just going off our intuitions.”

Wherever his academic work takes him, Rand likes to circle back to music as a source of inspiration.

“To me the essence of punk rock is saying, ‘Let me think about this for myself,’” Rand says. “Let me not be bound by social norms and conventions. … That’s what I try to do in my research. It’s the punk-rock approach to social science.”

Moreover, Rand concludes, in academia, thinking about one’s research as art helps shape it for the better.

“If the reason that I’m doing this is that I’m trying to make something beautiful, I’m like a perfectionist, with a good level of perfectionism in my science,” he says. “It’s not tempting to cut corners or be sloppy when you’re doing it for yourself, because then it makes things unsatisfying. You want results that are as true, and therefore as beautiful, as possible.”"
304;machinelearningmastery.com;http://machinelearningmastery.com/load-csv-machine-learning-data-weka/;2016-06-22;How To Load CSV Machine Learning Data in Weka;"Tweet Share Share

Last Updated on August 22, 2019

You must be able to load your data before you can start modeling it.

In this post you will discover how you can load your CSV dataset in Weka. After reading this post, you will know:

About the ARFF file format and how it is the default way to represent data in Weka.

How to load a CSV file in the Weka Explorer and save it in ARFF format.

How to load a CSV file in the ArffViewer tool and save it in ARFF format.

This tutorial assumes that you already have Weka installed.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

How to Talk About Data in Weka

Machine learning algorithms are primarily designed to work with arrays of numbers.

This is called tabular or structured data because it is how data looks in a spreadsheet, comprised of rows and columns.

Weka has a specific computer science centric vocabulary when describing data:

Instance : A row of data is called an instance, as in an instance or observation from the problem domain.

: A row of data is called an instance, as in an instance or observation from the problem domain. Attribute: A column of data is called a feature or attribute, as in feature of the observation.

Each attribute can have a different type, for example:

Real for numeric values like 1.2.

for numeric values like 1.2. Integer for numeric values without a fractional part like 5.

for numeric values without a fractional part like 5. Nominal for categorical data like “dog” and “cat”.

for categorical data like “dog” and “cat”. String for lists of words, like this sentence.

On classification problems, the output variable must be nominal. For regression problems, the output variable must be real.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Data in Weka

Weka prefers to load data in the ARFF format.

ARFF is an acronym that stands for Attribute-Relation File Format. It is an extension of the CSV file format where a header is used that provides metadata about the data types in the columns.

For example, the first few lines of the classic iris flowers dataset in CSV format looks as follows:

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 1 2 3 4 5 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa

The same file in ARFF format looks as follows:

@RELATION iris @ATTRIBUTE sepallength REAL @ATTRIBUTE sepalwidth REAL @ATTRIBUTE petallength REAL @ATTRIBUTE petalwidth REAL @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @RELATION iris @ATTRIBUTE sepallength REAL @ATTRIBUTE sepalwidth REAL @ATTRIBUTE petallength REAL @ATTRIBUTE petalwidth REAL @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa

You can see that directives start with the at symbol (@) and that there is one for the name of the dataset (e.g. @RELATION iris), there is a directive to define the name and datatype of each attribute (e.g. @ATTRIBUTE sepallength REAL) and there is a directive to indicate the start of the raw data (e.g. @DATA).

Lines in an ARFF file that start with a percentage symbol (%) indicate a comment.

Values in the raw data section that have a question mark symbol (?) indicate an unknown or missing value. The format supports numeric and categorical values as in the iris example above, but also supports dates and string values.

Depending on your installation of Weka, you may or may not have some default datasets in your Weka installation directory under the data/ subdirectory. These default datasets distributed with Weka are in the ARFF format and have the .arff file extension.

Load CSV Files in the ARFF-Viewer

Your data is not likely to be in ARFF format.

In fact, it is much more likely to be in Comma Separated Value (CSV) format. This is a simple format where data is laid out in a table of rows and columns and a comma is used to separate the values on a row. Quotes may also be used to surround values, especially if the data contains strings of text with spaces.

The CSV format is easily exported from Microsoft Excel, so once you can get your data into Excel, you can easily convert it to CSV format.

Weka provides a handy tool to load CSV files and save them in ARFF. You only need to do this once with your dataset.

Using the steps below you can convert your dataset from CSV format to ARFF format and use it with the Weka workbench. If you do not have a CSV file handy, you can use the iris flowers dataset. Download the file from the UCI Machine Learning repository (direct link) and save it to your current working directory as iris.csv.

1. Start the Weka chooser.

2. Open the ARFF-Viewer by clicking “Tools” in the menu and select “ArffViewer”.

3. You will be presented with an empty ARFF-Viewer window.

4. Open your CSV file in the ARFF-Viewer by clicking the “File” menu and select “Open”. Navigate to your current working directory. Change the “Files of Type:” filter to “CSV data files (*.csv)”. Select your file and click the “Open” button.

5. You should see a sample of your CSV file loaded into the ARFF-Viewer.

6. Save your dataset in ARFF format by clicking the “File” menu and selecting “Save as…”. Enter a filename with a .arff extension and click the “Save” button.

You can now load your saved .arff file directly into Weka.

Note, the ARFF-Viewer provides options for modifying your dataset before saving. For example you can change values, change the name of attributes and change their data types.

It is highly recommended that you specify the names of each attribute as this will help with analysis of your data later. Also, make sure that the data types of each attribute are correct.

Load CSV Files in the Weka Explorer

You can also load your CSV files directly in the Weka Explorer interface.

This is handy if you are in a hurry and want to quickly test out an idea.

This section shows you how you can load your CSV file in the Weka Explorer interface. You can use the iris dataset again, to practice if you do not have a CSV dataset to load.

1. Start the Weka GUI Chooser.

2. Launch the Weka Explorer by clicking the “Explorer” button.

3. Click the “Open file…” button.

4. Navigate to your current working directory. Change the “Files of Type” to “CSV data files (*.csv)”. Select your file and click the “Open” button.

You can work with the data directly. You can also save your dataset in ARFF format by clicking he “Save” button and typing a filename.

Use Excel for Other File Formats

If you have data in another format, load it in Microsoft Excel first.

It is common to get data in another format such as CSV using a different delimiter or fixed width fields. Excel has powerful tools for loading tabular data in a variety of formats. Use these tools and first load your data into Excel.

Once you have loaded your data into Excel, you can export it into CSV format. You can then work with it in Weka, either directly or by first converting it to ARFF format.

Resources

Below are some additional resources that you may find useful when working with CSV data in Weka.

Summary

In this post you discovered how to load your CSV data into Weka for machine learning.

Specifically, you learned:

About the ARFF file format and how Weka uses it to represent datasets for machine learning.

How to load your CSV data using ARFF-Viewer and save it into ARFF format.

How to load your CSV data directly in the Weka Explorer and use it for modeling.

Do you have any questions about loading data in Weka or about this post? Ask your questions in the comments and I will do my best to answer.

Discover Machine Learning Without The Code! Develop Your Own Models in Minutes ...with just a few a few clicks Discover how in my new Ebook:

Machine Learning Mastery With Weka Covers self-study tutorials and end-to-end projects like:

Loading data, visualization, build models, tuning, and much more... Finally Bring The Machine Learning To Your Own Projects Skip the Academics. Just Results. See What's Inside"
305;news.mit.edu;http://news.mit.edu/2016/scene-at-mit-puppy-love-1026;;Scene at MIT: Puppy love;"Members of the MIT Puppy Lab brought their puppy love to the portico of Building 10 with unseasonably warm temperatures near 80 degrees Fahrenheit on Wednesday, Oct. 19.

Puppy Lab founder Stephanie Ku ’14, a graduate student in the Harvard-MIT Health Sciences and Technology program, says the dogs were an instant success when Puppy Lab launched in May: “The first day, we were in the Lobby 10 Green Room, as I call it, and the temperature rose by like 5 to 10 degrees because there were so many people,” Ku says. “That’s not ideal in one way because no one wants to be in a crowded environment — you can barely touch the dogs — but at the same time, it’s an awesome thing because it shows us that there is a high demand and people were really interested.”

When Ku got her own dog, Wingnut, last year, she began to think about how students miss their dogs at home and find to be very helpful the MIT Libraries' Furry First Fridays, as well as therapy dogs brought in around finals. “MIT’s a hard place, and we all know that, but we all love it as well, and we just want to make sure that we support the students who are here, and the staff and faculty, because they also have stress,” Ku says.

“I thought it might be nice to have dogs more consistently available and to try to reach out to the dog community that’s already at MIT instead of relying on external therapy dog organizations,” she explains.

Around the same time, the MIT MindHandHeart Initiative was seeking grant proposals for campus wellness activities, and Ku’s plan for the Puppy Lab was supported with a $1,000 award. “MindHandHeart has also provided moral support and any kind of support that I really need if I run into a problem with logistics or with publicity,” Ku says.

Ku worked with MIT librarian Ellen Finnie, who is an instructor with the therapy group Dog B.O.N.E.S., to get eight new dogs certified as therapy dogs for the spring launch. “She’s been really vital in that process,” Ku says. “So right now we’re kind of in parallel programs: The Furry First Fridays program still runs on the first Friday of the month, and Puppy Lab does its own weekly programs on Wednesdays.”

Based on a survey she conducted, Ku says, “It seems like Puppy Lab is something a lot of people really wanted and really appreciate. I’ve gotten overwhelmingly positive feedback overall.” Next, she’d like to make the Puppy Lab an official student group.

Submitted by: Denis Paiste/Materials Processing Center | Photo by: Denis Paiste.

Have a creative photo of campus life you'd like to share? Submit it to Scene at MIT."
306;news.mit.edu;http://news.mit.edu/2020/mit-energy-economics-class-inspires-students-pursue-clean-energy-careers-0413;;Energy economics class inspires students to pursue clean energy careers;"Jing Li, an assistant professor of applied economics in the MIT Sloan School of Management, stands at the front of the classroom and encourages her undergraduate students to dig deeper. “Why was this a good idea?” she prompts. “How did people come up with these numbers?”

It’s the second-to-last day of class, and the students in 15.0201/14.43 (Economics of Energy, Innovation, and Sustainability) are discussing their teams’ results and the logic behind the decisions they made in the Electricity Strategy Game — a main feature of this elective.

“[With] so much magic,” a student quips in response to Li’s question, to a chorus of laughter.

The real magic, they all know, is in Li’s approach to teaching: She holds her students accountable for their conclusions and throws them head-first into challenging problems to help them confidently engage with the complexities of energy economics.

“She didn’t baby us with tiny data sets. She gave us the real deal,” says Wilbur Li, a senior computer science major and mechanical engineering minor (no relation to Jing Li). He initially took the class to round out his fall semester schedule, unsure if he would keep it due to a rigorous class load. However, just a couple of weeks into the semester, he was sold on the class.

“It’s one of those classes at MIT that isn’t really a requirement for anyone, but it’s a class that only draws people who are genuinely interested in the subject area,” he says. “That made for really good discussions. You could tell that people were interested beyond an academic sense.”

15.0201/14.43, a part of MITEI’s interdisciplinary Energy Studies minor, is a relatively new course. The class, which is also offered as graduate-level course 15.020, made its debut in the spring 2019 semester and was developed to expand the energy economics offerings at MIT. Part of the motivation for creating 15.0201/14.43 stemmed from the fact that Professor Christopher Knittel’s course, 15.037/15.038 (Energy Economics and Policy), is consistently in high demand, without enough supply to accommodate interested students.

“Professor Knittel and I have positioned our two courses so that someone who wants to get a taste of energy economics could take either one and come away with a good mental map of the field, but also that someone who is very serious about a future career in energy would find it useful to take both,” says Li.

Li’s class focuses on innovation and employs environmental economics principles and business cases to explore the development and adoption of new technology, and business strategies related to sustainability.

“The class has been particularly attractive to students who are interested in the energy landscape, such as how energy markets impact and relate to local environmental issues and how to provide energy to parts of the globe that currently lack access to affordable or reliable energy,” she says. “It has also appealed to students interested in applied microeconomics.”

In addition to crunching large data sets and bringing in guest speakers, such as Paul Joskow, the Elizabeth and James Killian Professor of Economics Emeritus and chair of MIT’s Department of Economics, a major element of the class — and a runaway favorite of many of the students — is the Electricity Strategy Game. The game was created by professors Severin Borenstein and James Bushnell for the University of California at Berkley’s Haas School of Business.

The game is designed to replicate the world of deregulated wholesale electricity markets. Players are divided into firms and utilize electricity generation portfolios, based on actual portfolios of the largest generation firms in the California market, to compete in a sequence of daily electricity spot markets, in which commodities are traded for immediate delivery. Each portfolio contains differing generation technologies (thermal, nuclear, and hydro), with varying operating costs. Spot market conditions vary from hour to hour and day to day. Players must develop strategies to deploy their assets over a sequence of spot markets while accounting for the cost structure of their portfolio, varying levels of hourly electricity demand, and strategies of other players. The game is conducted in six rounds, with the second half of the game taking into account carbon permits. Winners are determined by the financial performance of their firm and an evaluation of the logic of the firm’s actions, which the teams describe in a series of memos to Li.

“I loved the Electricity Strategy Game! It was really fun to have to figure out how to predict demand and then how to price supply accordingly,” says Anupama Phatak, a junior mechanical engineering major and economics minor. “The bid for portfolios was also a really cool process. I put a lot of time and effort into understanding the game and developing a strategy, so it made the process all the more rewarding when my team won.”

Wilbur Li echoed Phatak’s enthusiasm. “My favorite part of the game was definitely the auction — it was the most exciting part,” he says. “Every single group did research on their own to figure out what sort of bidding prices they wanted for each piece of property [power plants] — and when we showed up, every single group had wildly different final prices for what we bid on the plants.”

For Isaac Perper, a senior mechanical engineering and computer science double major and economics minor, the value of the game was in getting a glimpse of how energy portfolios would play out in real-life auctions. “We all had different portfolios, so I think that was the most interesting part. We got to see differences between coal, hydro, and gas plants and the different price points at which they are profitable. I think the auction mirrored what you would expect in a real market,” he says.

Many of the students who took 14.43 (Economics of Energy, Innovation, and Sustainability) are making it their mission to apply the lessons learned from the class to their career goals. The class helped inspire Wilbur Li to pursue a career in cleantech product development, such as working on smart meters or more efficient transportation for wind turbine blades.

“A class like 14.43 definitely helps with understanding how the products that are being worked on can be scaled in terms of figuring out which players in the economy would want to pick up and utilize a product,” he says. “It has given me a deeper understanding of how technology scales on a market level, as well as how to understand and account for the target impact of those technologies.”

Phatak says that the class has made her more conscious of the adverse environmental consequences of products such as palm oil. “I now understand that even the smallest ingredient in our everyday products can have negative impacts around the world that I might not even see,” she says. Because of the topics covered in Li’s course, Phatak is now actively pursuing internships in sustainability.

Perper shared that the class opened his eyes to a lot of inefficiencies that exist in the energy market today. Indeed, he says that his life’s goal is to help to solve some of those inefficiencies. “Going into this class, I had kind of thought that we have our different electricity producers and some pollute more than others, but in terms of the actual market structure and how electricity is distributed, paid for, and expanded into developing areas, all of those things were more complicated and inefficient than I had expected,” he says. When he returns to MIT in the fall to pursue his master’s degree in computer science and electrical engineering, Perper will be thinking more about the bigger questions in terms of energy policy and technology.

Li says she hopes that students come away from 14.43 with “more questions than answers,” as well as a honed sense of which questions are worth spending time to answer. She also aims for her students to leave with the knowledge that sustainability and energy touch every organization in some way.

“Whatever kind of organization you are a part of and the role you take in that organization — investor, manager, employee, customer, voter — you can contribute to the sustainability goals of your organization with your ideas, voice, and actions,” she says."
307;news.mit.edu;http://news.mit.edu/2020/pandemic-health-response-economic-recovery-0401;;The data speak: Stronger pandemic response yields better economic recovery;"The research described in this article has been published as a working paper but has not yet been peer-reviewed by experts in the field.

With much of the U.S. in shutdown mode to limit the spread of the Covid-19 disease, a debate has sprung up about when the country might “reopen” commerce, to limit economic fallout from the pandemic. But as a new study co-authored by an MIT economist shows, taking care of public health first is precisely what generates a stronger economic rebound later.

The study, using data from the flu pandemic that swept the U.S. in 1918-1919, finds cities that acted more emphatically to limit social and civic interactions had more economic growth following the period of restrictions.

Indeed, cities that implemented social-distancing and other public health interventions just 10 days earlier than their counterparts saw a 5 percent relative increase in manufacturing employment after the pandemic ended, through 1923. Similarly, an extra 50 days of social distancing was worth a 6.5 percent increase in manufacturing employment, in a given city.

“We find no evidence that cities that acted more aggressively in public health terms performed worse in economic terms,” says Emil Verner, an assistant professor in the MIT Sloan School of Management and co-author of a new paper detailing the findings. “If anything, the cities that acted more aggressively performed better.”

With that in mind, he observes, the idea of a “trade-off” between public health and economic activity does not hold up to scrutiny; places that are harder hit by a pandemic are unlikely to rebuild their economic capacities as quickly, compared to areas that are more intact.

“It casts doubt on the idea there is a trade-off between addressing the impact of the virus, on the one hand, and economic activity, on the other hand, because the pandemic itself is so destructive for the economy,” Verner says.

The study, “Pandemics Depress the Economy, Public Health Interventions Do Not: Evidence from the 1918 Flu,” was posted to the Social Science Research Network as a working paper on March 26. In addition to Verner, the co-authors are Sergio Correia, an economist with the U.S. Federal Reserve, and Stephen Luck, an economist with the Federal Reserve Bank of New York.

Evaluating economic consequences

To conduct the research, the three scholars examined mortality statistics from the U.S. Centers for Disease Control (CDC), historical economic data from the U.S. Census Bureau, and banking statistics compiled by finance economist Mark D. Flood, using the “Annual Reports of the Comptroller of Currency,” a government publication.

As Verner notes, the researchers were motivated to investigate the 1918-1919 flu pandemic to see what lessons from it might be applicable to the current crisis.

“The genesis of the study is that we’re interested in what the expected economic impacts of today’s coronavirus are going to be, and what is the right way to think about the economic consequences of the public health and social distancing interventions we’re seeing all around the world,” Verner says.

Scholars have known that the varying use of “nonpharmaceutical interventions,” or social-distancing measures, correlated to varying health outcomes across cities in 1918 and 1919. When that pandemic hit, U.S. cities that shut down schools earlier, such as St. Louis, fared better against the flu than places implementing shutdowns later, such as Philadelphia. The current study extends that framework to economic activity.

Quite a bit like today, social distancing measures back then included school and theater closures, bans on public gatherings, and restricted business activity.

“The nonpharmaceutical interventions that were implemented in 1918 interestingly resemble many of the policies that are being used today to reduce the spread of Covid-19,” Verner says.

Overall, the study indicates, the economic impact of the pandemic was severe. Using state-level data, the researchers find an 18 percent drop in manufacturing output through 1923, well after the last wave of the flu hit in 1919.

Looking at the effect across 43 cities, however, the researchers found significantly different economic outcomes, linked to different social distancing policies. The best-performing cities included Oakland, California; Omaha, Nebraska; Portland, Oregon; and Seattle, which all enforced above average duration and intensity of social distancing in 1918. Cities that instituted significantly fewer than days of social distancing in 1918, and saw manufacturing struggle afterward, include Philadelphia; St. Paul, Minnesota; and Lowell, Massachusetts.

“What we find is that areas that were more severely affected in the 1918 flu pandemic see a sharp and persistent decline in a number of measures of economic activity, including manufacturing employment, manufacturing output, bank loans, and the stock of consumer durables,” Verner says.

Banking issues

As far as banking goes, the study included banking write-downs as an indicator of economic health, because “banks were recognizing losses from loans that households and businesses were defaulting on, due to the economic disruption caused by the pandemic,” Verner says.

The researchers found that in Albany, New York; Birmingham, Alabama; Boston; and Syracuse, New York — all of which also had below average days of social distancing in 1918 — the banking sector struggled more than anywhere else in the country.

As the authors note in the paper, the economic struggles that followed the 1918-1919 flu pandemic reduced the ability of firms to manufacture goods — but the reduction in employment meant that people had less purchasing power as well.

“The evidence that we have in our paper … suggests that the pandemic creates both a supply-side problem and a demand-side problem,” Verner notes.

As Verner readily acknowledges, the composition of the U.S. economy has evolved since 1918-1919, with relatively less manufacturing today and relatively more activity in services. The 1918-1919 pandemic was also especially deadly for prime working-age adults, making its economic impact particularly severe. Still, the economists think the dynamics of the previous pandemic are readily applicable to our ongoing crisis.

“The structure of the economy is of course different,” Verner notes. However, he adds, “While one shouldn’t extrapolate too directly from history, we can learn some of the lessons that may be relevant to us today.” First among those lessons, he emphasizes: “Pandemic economics are different than normal economics.”"
308;news.mit.edu;http://news.mit.edu/2020/mirrored-chip-could-enable-handheld-dark-field-microscopes-0224;;Mirrored chip could enable handheld dark-field microscopes;"Do a Google search for dark-field images, and you’ll discover a beautifully detailed world of microscopic organisms set in bright contrast to their midnight-black backdrops. Dark-field microscopy can reveal intricate details of translucent cells and aquatic organisms, as well as faceted diamonds and other precious stones that would otherwise appear very faint or even invisible under a typical bright-field microscope.

Scientists generate dark-field images by fitting standard microscopes with often costly components to illumate the sample stage with a hollow, highly angled cone of light. When a translucent sample is placed under a dark-field microscope, the cone of light scatters off the sample’s features to create an image of the sample on the microscope’s camera, in bright contrast to the dark background.

Now, engineers at MIT have developed a small, mirrored chip that helps to produce dark-field images, without dedicated expensive components. The chip is slightly larger than a postage stamp and as thin as a credit card. When placed on a microscope’s stage, the chip emits a hollow cone of light that can be used to generate detailed dark-field images of algae, bacteria, and similarly translucent tiny objects.

Credit: Cecile Chazot

The new optical chip can be added to standard microscopes as an affordable, downsized alternative to conventional dark-field components. The chip may also be fitted into hand-held microscopes to produce images of microorganisms in the field.

“Imagine you’re a marine biologist,” says Cecile Chazot, a graduate student in MIT’s Department of Materials Science and Engineering. “You normally have to bring a big bucket of water into the lab to analyze. If the sample is bad, you have to go back out to collect more samples. If you have a hand-held, dark-field microscope, you can check a drop in your bucket while you’re out at sea, to see if you can go home or if you need a new bucket.”

Chazot is the lead author of a paper detailing the team’s new design, published today in the journal Nature Photonics. Her co-authors are Sara Nagelberg, Igor Coropceanu, Kurt Broderick, Yunjo Kim, Moungi Bawendi, Peter So, and Mathias Kolle of MIT, along with Christopher Rowlands at Imperial College London and Maik Scherer of Papierfabrik Louisenthal GmbH in Germany.

Forever fluorescent

In an ongoing effort, members of Kolle’s lab are designing materials and devices that exhibit long-lasting “structural colors” that do not rely on dyes or pigmentation. Instead, they employ nano- and microscale structures that reflect and scatter light much like tiny prisms or soap bubbles. They can therefore appear to change colors depending on how their structures are arranged or manipulated.

Structural color can be seen in the iridescent wings of beetles and butterflies, the feathers of birds, as well as fish scales and some flower petals. Inspired by examples of structural color in nature, Kolle has been investigating various ways to manipulate light from a microscopic, structural perspective.

As part of this effort, he and Chazot designed a small, three-layered chip that they originally intended to use as a miniature laser. The middle layer functions as the chip’s light source, made from a polymer infused with quantum dots — tiny nanoparticles that emit light when excited with fluorescent light. Chazot likens this layer to a glowstick bracelet, where the reaction of two chemicals creates the light; except here no chemical reaction is needed — just a bit of blue light will make the quantum dots shine in bright orange and red colors.

“In glowsticks, eventually these chemicals stop emitting light,” Chazot says. “But quantum dots are stable. If you were to make a bracelet with quantum dots, they would be fluorescent for a very long time.”

Over this light-generating layer, the researchers placed a Bragg mirror — a structure made from alternating nanoscale layers of transparent materials, with distinctly different refractive indices, meaning the degrees to which the layers reflect incoming light.

The Bragg mirror, Kolle says, acts as a sort of “gatekeeper” for the photons that are emitted by the quantum dots. The arrangement and thicknesses of the mirror’s layers is such that it lets photons escape up and out of the chip, but only if the light arrives at the mirror at high angles. Light arriving at lower angles is bounced back down into the chip.

The researchers added a third feature below the light-generating layer to recycle the photons initially rejected by the Bragg mirror. This third layer is molded out of solid, transparent epoxy coated with a reflective gold film and resembles a miniature egg crate, pocked with small wells, each measuring about 4 microns in diameter.

Chazot lined this surface with a thin layer of highly reflective gold — an optical arrangement that acts to catch any light that reflects back down from the Bragg mirror, and ping-pong that light back up, likely at a new angle that the mirror would let through. The design for this third layer was inspired by the microscopic scale structure in the wings of the Papilio butterfly.

“The butterfly’s wing scales feature really intriguing egg crate-like structures with a Bragg mirror lining, which gives them their iridescent color,” Chazot says.

An optical shift

The researchers originally designed the chip as an array of miniature laser sources, thinking that its three layers could work together to create tailored laser emission patterns.

“The initial project was to build an assembly of individually switchable coupled microscale lasing cavities,” says Kolle, associate professor of mechanical engineering at MIT. “But when Cecile made the first surfaces we realized that they had a very interesting emission profile, even without the lasing.”

When Chazot had looked at the chip under a microscope, she noticed something curious: The chip emitted photons only at high angles forming a hollow cone of light. Turns out, the Bragg mirror had just the right layer thicknesses to only let photons pass through when they came at the mirror with a certain (high) angle.

“Once we saw this hollow cone of light, we wondered: ‘Could this device be useful for something?’” Chazot says. “And the answer was: Yes!”

As it turns out, they had incorporated the capabilities of multiple expensive, bulky dark-field microscope components into a single small chip.

Chazot and her colleagues used well-established theoretical optical concepts to model the chip’s optical properties to optimize its performance for this newly found task. They fabricated multiple chips, each producing a hollow cone of light with a tailored angular profile.

“Regardless of the microscope you’re using, among all these tiny little chips, one will work with your objective,” Chazot says.

To test the chips, the team collected samples of seawater as well as nonpathogenic strains of the bacteria E. coli, and placed each sample on a chip that they set on the platform of a standard bright-field microscope. With this simple setup, they were able to produce clear and detailed dark-field images of individual bacterial cells, as well as microorganisms in seawater, which were close to invisible under bright-field illumination.

In the near future these dark-field illumination chips could be mass-produced and tailored for even simple, high school-grade microscopes, to enable imaging of low-contrast, translucent biological samples. In combination with other work in Kolle’s lab, the chips may also be incorporated into miniaturized dark-field imaging devices for point-of-care diagnostics and bioanalytical applications in the field.

“This is a wonderful story of discovery based innovation that has the potential for widespread impact in science and education through outfitting garden-variety microscopes with this technology,” says James Burgess, program manager for the Institute for Soldier Nanotechnologies, Army Research Office. “Additionally, the ability to obtain superior contrast in imaging of biological and inorganic materials under optical magnification could be incorporated into systems for identification of new biological threats and toxins in Army Medical Center laboratories and on the battlefield.”

This research was supported, in part, by the National Science Foundation, the U.S. Army Research Office, and the National Institutes of Health."
309;machinelearningmastery.com;https://machinelearningmastery.com/how-to-study-machine-learning-algorithms/;2014-10-21;How to Study Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

Algorithms make up a big part of machine learning.

You select and apply machine learning algorithms to build a model from your data, select features, combine the predictions from multiple models and even evaluate the capabilities of a given model.

In this post you will review 5 different approaches that you can use to study machine learning algorithms.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

1. List Machine Learning Algorithms

There are a lot of machine learning algorithms and it can feel overwhelming.

Even defining what a machine learning algorithm is, can be tricky.

A great place to start out is to make your own lists of algorithms. Start a text file, word document or spreadsheet and list algorithm names. Also list the general category or categories to which each algorithm belongs.

This simple tactic can help you build familiarity with the different types and classes of algorithms available. Later as you get more experienced, lists like this can prompt you and give you ideas of different methods to spot check on your problem.

Some examples of algorithm lists to get you started include:

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

2. Apply Machine Learning Algorithms

Machine Learning algorithm do not exist in isolation, they are best understood when applied to a dataset.

Apply algorithms to problems to understand them. Practice applied machine learning. It sounds simple, but you will be amazed at the number of people paralyzed to make this small step from theory into action.

This may mean working on a problem that matters to you, a competition dataset or a classical machine learning dataset.

Use a machine learning platform like Weka, R or scikit-learn to get access to many machine learning algorithms.

Start to build up an intuition for different types of algorithms, such as decision trees and support vector machines. Think about their required preconditions and the effects the parameters have on results.

Build up confidence in applying different algorithms. You should be spot checking algorithms on your problems.

3. Describe Machine Learning Algorithms

The next step in understanding a machine learning algorithm is to explore what is already understood about the algorithm.

This could be done before you apply the algorithm, but I think it is valuable to have a working intuition of the algorithm in action as context before diving into the algorithm description.

You can research an algorithm. This includes locating and reading the primary sources where the algorithm was first described as well as authoritative interpretations of the algorithm in textbooks and review papers.

Conference papers, competition results and even forms and Q&A websites can help you better understand the best practices and usage heuristics for an algorithm.

As you are researching an algorithm, build up a description. I like to use a well-defined algorithm description template.

You can continue to add to this template you discover more about an algorithm. You can add references, list the pseudocode for the algorithm and list best practices and usage heuristics.

This is a valuable techniques and you can build up your own mini-encyclopedia of algorithm descriptions for your own reference (for example, see Clever Algorithms for 45 algorithm recipes).

For more information on the template that I use, check out the post “How to Learn a Machine Learning Algorithm“.

4. Implement Machine Learning Algorithms

Implementing machine learning algorithms is an excellent way to get a concrete understanding of how an algorithm works.

There are many micro-decisions that have to be made when implementing an algorithm. Some of these decision points are exposed with algorithm configuration parameters, but many are not.

By implementing an algorithm yourself you will get a feeling for just how to customize the algorithm and choose what to expose and what decision points to fix in place.

Implementing algorithms from scratch will help you understand the mathematical descriptions and extensions of an algorithm. This may sound counter-intuitive. The mathematical descriptions are idealized and often provide a snap-shot description of a given processes within an algorithm. Once you translate them into code, the implications of those descriptions may be a lot more obvious.

You can leverage tutorials and open source implementations of algorithms to help you get through those difficult parts.

Note that a “my first implementation” of an algorithm will be less scalable and more fragile than a production grade implementation you may find in a machine learning tool or library.

Take a look at my tutorial for implementing k-nearest neighbors from scratch in Python.

You may also be interested in my post “How to Implement a Machine Learning Algorithm“.

5. Experiment On Machine Learning Algorithms

Experimenting on machine learning algorithms is the best way to understand them.

You need to act like the scientist and study a running machine learning algorithm like a complex system.

You need to control variables, use standardized datasets that are well understood and explore the cause and effect relationships of the parameters on the results.

Understanding the effects of the parameters will help you better configure the algorithm on problems in the future. Understanding the behavior of the algorithm under different circumstances will help you better scale and adapt the method to new and different problem domains in the future.

Many machine learning algorithms are stochastic in nature and resist more classical methods of algorithm analysis. They often require empirical investigation and probabilistic description to be understood.

Summary

In this post you discovered five ways to study and learn about machine learning algorithms.

They where:

List Machine Learning Algorithms Apply Machine Learning Algorithms Describe Machine Learning Algorithms Implement Machine Learning Algorithms Experiment On Machine Learning Algorithms

Have you used any of these methods to learn a machine learning algorithm?

Perhaps you have your own approach to studying machine learning algorithms? Share it in the comments below.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
310;machinelearningmastery.com;https://machinelearningmastery.com/why-learn-probability-for-machine-learning/;2019-09-10;5 Reasons to Learn Probability for Machine Learning;"Tweet Share Share

Last Updated on November 8, 2019

Probability is a field of mathematics that quantifies uncertainty.

It is undeniably a pillar of the field of machine learning, and many recommend it as a prerequisite subject to study prior to getting started. This is misleading advice, as probability makes more sense to a practitioner once they have the context of the applied machine learning process in which to interpret it.

In this post, you will discover why machine learning practitioners should study probabilities to improve their skills and capabilities.

After reading this post, you will know:

Not everyone should learn probability; it depends where you are in your journey of learning machine learning.

Many algorithms are designed using the tools and techniques from probability, such as Naive Bayes and Probabilistic Graphical Models.

The maximum likelihood framework that underlies the training of many machine learning algorithms comes from the field of probability.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Overview

This tutorial is divided into seven parts; they are:

Reasons to NOT Learn Probability Class Membership Requires Predicting a Probability Some Algorithms Are Designed Using Probability Models Are Trained Using a Probabilistic Framework Models Can Be Tuned With a Probabilistic Framework Probabilistic Measures Are Used to Evaluate Model Skill One More Reason

Reasons to NOT Learn Probability

Before we go through the reasons that you should learn probability, let’s start off by taking a small look at the reason why you should not.

I think you should not study probability if you are just getting started with applied machine learning.

It’s not required . Having an appreciation for the abstract theory that underlies some machine learning algorithms is not required in order to use machine learning as a tool to solve problems.

. Having an appreciation for the abstract theory that underlies some machine learning algorithms is not required in order to use machine learning as a tool to solve problems. It’s slow . Taking months to years to study an entire related field before starting machine learning will delay you achieving your goals of being able to work through predictive modeling problems.

. Taking months to years to study an entire related field before starting machine learning will delay you achieving your goals of being able to work through predictive modeling problems. It’s a huge field. Not all of probability is relevant to theoretical machine learning, let alone applied machine learning.

I recommend a breadth-first approach to getting started in applied machine learning.

I call this the results-first approach. It is where you start by learning and practicing the steps for working through a predictive modeling problem end-to-end (e.g. how to get results) with a tool (such as scikit-learn and Pandas in Python).

This process then provides the skeleton and context for progressively deepening your knowledge, such as how algorithms work and, eventually, the math that underlies them.

After you know how to work through a predictive modeling problem, let’s look at why you should deepen your understanding of probability.

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Class Membership Requires Predicting a Probability

Classification predictive modeling problems are those where an example is assigned a given label.

An example that you may be familiar with is the iris flowers dataset where we have four measurements of a flower and the goal is to assign one of three different known species of iris flower to the observation.

We can model the problem as directly assigning a class label to each observation.

Input : Measurements of a flower.

: Measurements of a flower. Output: One iris species.

A more common approach is to frame the problem as a probabilistic class membership, where the probability of an observation belonging to each known class is predicted.

Input : Measurements of a flower.

: Measurements of a flower. Output: Probability of membership to each iris species.

Framing the problem as a prediction of class membership simplifies the modeling problem and makes it easier for a model to learn. It allows the model to capture ambiguity in the data, which allows a process downstream, such as the user to interpret the probabilities in the context of the domain.

The probabilities can be transformed into a crisp class label by choosing the class with the largest probability. The probabilities can also be scaled or transformed using a probability calibration process.

This choice of a class membership framing of the problem interpretation of the predictions made by the model requires a basic understanding of probability.

2. Models Are Designed Using Probability

There are algorithms that are specifically designed to harness the tools and methods from probability.

These range from individual algorithms, like Naive Bayes algorithm, which is constructed using Bayes Theorem with some simplifying assumptions.

The linear regression algorithm can be seen as a probabilistic model that minimizes the mean squared error of predictions, and the logistic regression algorithm can be seen as a probabilistic model that minimizes the negative log likelihood of predicting the positive class label.

Linear Regression

Logistic Regression

It also extends to whole fields of study, such as probabilistic graphical models, often called graphical models or PGM for short, and designed around Bayes Theorem.

A notable graphical model is Bayesian Belief Networks or Bayes Nets, which are capable of capturing the conditional dependencies between variables.

3. Models Are Trained With Probabilistic Frameworks

Many machine learning models are trained using an iterative algorithm designed under a probabilistic framework.

Some examples of general probabilsitic modeling frameworks are:

Perhaps the most common is the framework of maximum likelihood estimation, sometimes shorted as MLE. This is a framework for estimating model parameters (e.g. weights) given observed data.

This is the framework that underlies the ordinary least squares estimate of a linear regression model and the log loss estimate for logistic regression.

The expectation-maximization algorithm, or EM for short, is an approach for maximum likelihood estimation often used for unsupervised data clustering, e.g. estimating k means for k clusters, also known as the k-Means clustering algorithm.

For models that predict class membership, maximum likelihood estimation provides the framework for minimizing the difference or divergence between an observed and predicted probability distribution. This is used in classification algorithms like logistic regression as well as deep learning neural networks.

It is common to measure this difference in probability distribution during training using entropy, e.g. via cross-entropy. Entropy, and differences between distributions measured via KL divergence, and cross-entropy are from the field of information theory that directly build upon probability theory. For example, entropy is calculated directly as the negative log of the probability.

As such, these tools from information theory such as minimising cross-entropy loss can be seen as another probabilistic framework for model estimation.

Minimum Cross-Entropy Loss Estimation

4. Models Are Tuned With a Probabilistic Framework

It is common to tune the hyperparameters of a machine learning model, such as k for kNN or the learning rate in a neural network.

Typical approaches include grid searching ranges of hyperparameters or randomly sampling hyperparameter combinations.

Bayesian optimization is a more efficient to hyperparameter optimization that involves a directed search of the space of possible configurations based on those configurations that are most likely to result in better performance. As its name suggests, the approach was devised from and harnesses Bayes Theorem when sampling the space of possible configurations.

For more on Bayesian optimization, see the tutorial:

5. Models Are Evaluated With Probabilistic Measures

For those algorithms where a prediction of probabilities is made, evaluation measures are required to summarize the performance of the model.

There are many measures used to summarize the performance of a model based on predicted probabilities. Common examples include:

Log Loss (also called cross-entropy).

Brier Score, and the Brier Skill Score

For more on metrics for evaluating predicted probabilities, see the tutorial:

For binary classification tasks where a single probability score is predicted, Receiver Operating Characteristic, or ROC, curves can be constructed to explore different cut-offs that can be used when interpreting the prediction that, in turn, result in different trade-offs. The area under the ROC curve, or ROC AUC, can also be calculated as an aggregate measure. A related method that couses on the positive class is the Precision-Recall Curve and area under curve.

ROC Curve and ROC AUC

Precision-Recall Curve and AUC

For more on these curves and when to use them see the tutorial:

Choice and interpretation of these scoring methods require a foundational understanding of probability theory.

One More Reason

If I could give one more reason, it would be: Because it is fun.

Seriously.

Learning probability, at least the way I teach it with practical examples and executable code, is a lot of fun. Once you can see how the operations work on real data, it is hard to avoid developing a strong intuition for a subject that is often quite unintuitive.

Do you have more reasons why it is critical for an intermediate machine learning practitioner to learn probability?

Let me know in the comments below.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Posts

Articles

Summary

In this post, you discovered why, as a machine learning practitioner, you should deepen your understanding of probability.

Specifically, you learned:

Not everyone should learn probability; it depends where you are in your journey of learning machine learning.

Many algorithms are designed using the tools and techniques from probability, such as Naive Bayes and Probabilistic Graphical Models.

The maximum likelihood framework that underlies the training of many machine learning algorithms comes from the field of probability.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Probability for Machine Learning! Develop Your Understanding of Probability ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Probability for Machine Learning It provides self-study tutorials and end-to-end projects on:

Bayes Theorem, Bayesian Optimization, Distributions, Maximum Likelihood, Cross-Entropy, Calibrating Models

and much more... Finally Harness Uncertainty in Your Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
311;news.mit.edu;http://news.mit.edu/2020/ten-mit-awarded-2020-paul-and-daisy-soros-fellowships-new-americans-0414;;Ten from MIT awarded 2020 Paul and Daisy Soros Fellowships for New Americans;"Ten MIT students and alumni are among the 30 recipients of this year’s Paul and Daisy Soros Fellowships for New Americans. The MIT-affiliated winners are Pelkins Mbacham Ajanoh, Sanath Devalapurkar, Mohamed Ismail, Connie Liu, Mark Aurel Nagy, Jin Park, Pooja Reddy, Riana Shah, Anthony Tabet, and Jason Ku Wang. They were selected from a pool of over 2,200 applicants.

As Soros winner Pooja Reddy notes, “I could not have asked for a better environment to grow than MIT. I was always surrounded by other immigrants and children of immigrants in all my classes. … With a real emphasis on education, my professors always showed so much compassion, which made me feel seen as a person. This drove me to persevere and learn more.”

The P.D. Soros Fellowship provides up to $90,000 in funding for graduate studies. Interested students should contact Kim Benard, assistant dean of distinguished fellowships. The deadline for this year’s application is Oct. 29, 2020.

Pelkins Mbacham Ajanoh ’18

Pelkins Mbacham Ajanoh graduated from MIT in 2018 with a BS in mechanical engineering. The Soros fellowship will fund his graduate studies at Harvard University where he will earn dual MBA and MS in engineering sciences degrees.

Ajanoh was born and raised in Limbe, Cameroon. He lost his father at age 13, and his mother subsequently immigrated to the US to financially support her children. After graduating high school and receiving the top score on Cameroon’s national exam, Ajanoh joined his mother in Texas, earned an associate’s degree at a community college, and enrolled at University of Texas at Arlington. After learning of MIT’s need-blind admissions policy, he applied to MIT and was accepted as a transfer student.

At MIT, Ajanoh became interested in the topic of creating economic opportunity in vulnerable communities through entrepreneurship, which led him to found CassVita, an agribusiness that converts cassava into shelf-stable flour. CassVita empowers over 300 farmers in Cameroon and its products are sold in over 30 supermarkets locally and internationally. In recognition for his work at MIT, Ajanoh was awarded the Albert G. Hill Prize and the Suzanne Berger Award for Future Global Leaders.

Sanath Devalapurkar

Sanath Devalapurkar will graduate from MIT in May 2020 with a BS in mathematics and a minor in physics. His Soros award will support his doctoral studies in mathematics at Harvard University.

Devalapurkar was born in Adoni, India, and lived in several different countries and U.S. states while growing up. After graduating high school in Los Angeles, he matriculated at MIT at age 16. Shortly after arriving at MIT, Devalapurkar began sitting in on graduate-level courses in mathematics, which fueled his passion and curiosity for the field. He is particularly interested in algebraic topology and algebraic geometry, subfields of math, and quantum field theory in physics.

Devalapurkar credits his interests to his parents’ unwavering support, his mentors in high school, and to Professor Haynes Miller and postdoc Jeremy Hahn in the MIT Department of Mathematics. During his time at MIT, Devalapurkar has worked on projects at the Emory Research Experiences for Undergraduates (REU), MIT’s Summer Program for Undergraduate Research, and the University of Chicago REU, all of which have helped reinforce his enthusiasm for math.

Mohamed Ismail

A PhD student in building technology in the MIT Department of Architecture, Mohamed Ismail is researching the application of structural optimization to the alleviation of housing insecurity in the Global South. Born to Sudanese parents who immigrated to the U.S. for educational opportunities, Ismail moved with his family to the Philippines when he was eight.

In the Philippines, Ismail witnessed how environmental issues are in fact human rights issues, which led him to environmental activism. He returned to the U.S. for college to learn how the built environment could improve societal well-being rather than harm it. Ismail received his bachelor’s degree in civil and structural engineering at Duke University before receiving his Master of Architecture at the University of Virginia. After graduating, he became a faculty lecturer at the UVA School of Architecture, teaching parametric structural design and digital workflows to architecture students.

At MIT, Ismail was an MIT Tata Center fellow, working with the Digital Structures research group to design low-cost, low-carbon structural components for housing in developing economies. Following his PhD, Ismail hopes to enrich the design profession with new methods that integrate structural performance into the architectural design process.

Connie Liu ‘16

An engineer turned educator turned nonprofit founder, Connie Liu graduated from MIT in 2016 with a BS in mechanical engineering. She was born in San Diego, California, the youngest of three children, to parents who had emigrated from China.

Growing up, Liu had a strong interest in science and social impact. At MIT, she focused on developing assistive technologies for people with disabilities. Seeing the impact her inventions had on real people inspired her interest in educating and empowering youth to create ideas that could make a change.

After graduating MIT, Liu relocated back to California to become a high school teacher, leading classes in such topics as smart wearables and design engineering for social good. Two years later, Liu founded Project Invent, a national nonprofit that teaches high school students throughout the U.S. how to invent technologies that can make a difference.

For her work on Project Invent, Liu has been recognized on the Forbes 30 Under 30 list and as a Westly Prize winner. While continuing to serve as executive director for Project Invent, Liu is now pursuing an MBA at the Stanford Graduate School of Business.

Mark Aurel Nagy

Mark Aurel Nagy was born in Budapest to a Chinese mother and Hungarian father. Although he immigrated to the U.S. soon after, Nagy and his siblings spent summers abroad under the care of extended family while his parents worked full time to make ends meet.

At Brown University, Nagy found himself drawn to the complexity of the brain, an interest that only deepened when he lost his Hungarian grandmother to neurodegenerative illness. After completing his BS with honors in neuroscience and physics, Nagy enrolled in the MD/PhD program in the Harvard-MIT Program in Health Sciences and Technology.

At Harvard, Nagy completed his neuroscience PhD in Professor Michael Greenberg's lab. His dissertation work employed next-generation sequencing-based assays to understand how sensory experience shapes neuronal function. Nagy has also been developing a company that leverages approaches created during his PhD to engineer better viral vectors for gene therapy of neurological disorders.

Nagy is currently completing his MD studies. As a practicing physician-scientist, he hopes to make lasting improvements to patient care through scientific advancement and, as a gay person of color, increase visibility for underrepresented minorities in the sciences and medicine.

Jin Park

Jin Park is pursuing a MD/PhD in the Harvard-MIT Program in Health Sciences and Technology. Born in Seoul, South Korea, he came to the U.S. at the age of 7, settling in the immigrant community of Flushing, Queens.

Growing up as an undocumented immigrant, Park and his parents — a restaurant line cook and a beauty salon worker — had limited access to health care and would often forgo medical treatment. These experiences imparted Jin with a deep conviction that health care should be a right in the U.S.

Park graduated from Harvard with a degree in molecular and cellular biology, winning the Hoopes Prize for one of his two senior theses. He has several pending publications from his research in the laboratory of MIT professor Tyler Jacks. Outside of the classroom, Park taught immigrants preparing for their naturalization exam through the Phillips Brooks House Association’s Chinatown Citizenship Program, eventually becoming the director of the program.

Being a DACA recipient has played a pivotal role in Park’s life. He has provided public testimony to the Judiciary Committee of the House of Representatives in support of legislation for Dreamers. He has written for CNN, The New York Times, The Boston Globe, and the Chronicle of Higher Education, and is a plaintiff to litigation being heard in the federal judiciary which counters arguments that seek to end the DACA program. In 2019, Park became the first DACA recipient to be selected as a Rhodes Scholar.

Pooja Reddy

Born in Boston, to Indian immigrant parents, Pooja Reddy is a senior majoring in materials science and engineering. After graduating MIT in May, she will begin a PhD in materials science and engineering at Stanford University.

Growing up, Reddy loved to draw and paint. When her family moved back to India, she found that her creativity was not encouraged at school and that there were often lower expectations for women than men. Reddy returned to the U.S. for high school where she dived back into art and creative expression. Between her experiences in India and attending high school in a majority white district, Reddy resolved to defy expectations based on race and gender.

At MIT, Reddy has used her voice and leadership positions to support others. She has been active as a teaching assistant, and as a peer mentor in her major and dorm. She has continued her creative pursuits through metalsmithing, and by running the MIT Art Club.

Reddy discovered a passion for solid-state physics and device technology through her work with Professor Geoffrey Beach in the MIT Laboratory of Nanomagnetism and Spin Dynamics. Her long-term goal is to use materials science to create new materials and devices for information technology.

Riana Shah

Riana Shah is a concurrent MBA/MPA candidate at MIT Sloan School of Management and Harvard Kennedy School. She is a Legatum Fellow for Entrepreneurship at MIT and the cofounder of Ethix.AI, an AI upskilling program that incorporates critical thinking about ethics and bias in algorithm development. At Harvard, Shah is a Zuckerman Fellow at the Center for Public Leadership and is part of the From Harvard Square to the Oval Office Program, which prepares promising female candidates to run for office.

Born in Ahmedabad, India, Shah was 14 when she moved to Queens, New York, with her mother and younger sister. As an undergraduate at Swarthmore College, Shah founded Independent Thought and Social Action International (ITSA), an education reform nonprofit that redesigns schools to teach students critical thinking and social innovation skills. Subsequently, Shah spent several years in technology, venture capital, management consulting and innovation strategy working with public and private institutions.

Shah’s work has been featured in the Huffington Post, Women of Influence, and Google's Generation In Project, and she is the podcast host of Venture Vignettes. She has spoken at the United Nations, the U.S. State Department, and the International Forum on Child Welfare.

Anthony Tabet

Anthony Tabet is an MIT PhD student in chemical engineering who is creating brain-machine interfaces that can be used to study or treat brain tumors like glioblastoma. After completing his studies, he hopes to start an academic research lab focused on translational technologies to improve human health for the most challenging-to-treat diseases.

Tabet’s parents fled to the countryside during the Lebanese civil war that nearly killed his entire paternal family. He was born in a town outside of Beirut and later immigrated with his family to Minneapolis. At age 16, Tabet began his undergraduate studies at the University of Minnesota. He majored in chemical engineering, studying polymeric materials for energy and biomedical applications. A Goldwater Scholar, Tabet received an Amgen Scholarship to fund his summer research at Stanford University. After graduating, he completed an MPhil in chemistry at Cambridge University in the UK as a Churchill Scholar.

Tabet is passionate about translating research ideas from the lab into commercialized technologies. While living in Minneapolis, he became frustrated with the barriers that entrepreneurs face in starting companies in the Midwest. In response, he cofounded the company CoCreateX to streamline how scientists and engineers find resources, capital, and community.

Jason Ku Wang

Jason Ku Wang is a first-year MD student in the Harvard-MIT Program in Health Sciences and Technology. Born in Memphis, Tennessee, Wang is the son of immigrants from China’s Hubei Province who came to the U.S. following the Cultural Revolution. Wang was sent to live with his grandparents in Wuhan, China, for the first four years of his life before rejoining his parents in the U.S. and eventually settling in Los Angeles.

Wang’s interest in medicine was inspired by his father’s journey from rural barefoot doctor to U.S. physician. As an undergraduate at Stanford University, Wang also discovered a love for computer science and statistics and delved deeper into the intersection of computation, biology, and medicine. He is the author of 12 journal publications, including six as first author, and received the Stanford Deans’ Award for Academic Achievement.

During college, Wang became interested in technology development. He interned in data and software engineering at Facebook and Tableau, and cofounded Stanford’s interdisciplinary health care hackathon (Health++). After graduating Stanford, Wang spent a year at Tsinghua University in Beijing as a Schwarzman Scholar, and completed a public affairs internship at Johnson and Johnson. As a physician, he hopes to further explore how computational advancements can democratize access to high-quality health care.

Note: This article has been updated to include Jin Park as an additional fellow from MIT, bringing the total to 10 for this year."
312;news.mit.edu;http://news.mit.edu/2020/mit-opens-sean-collier-care-center-for-patients-with-covid-19-0415;;MIT opens the Sean Collier Care Center for patients with Covid-19;"Throughout the course of the Covid-19 emergency, MIT has worked in partnership with its neighbors in the City of Cambridge, whenever possible, to provide medical supplies, equipment, and services to the larger community. In this spirit of collaboration, MIT has opened the Sean Collier Care Center, a fully licensed 75-bed facility for patients with Covid-19.

Located on the MIT campus and named for fallen MIT Police Officer Sean Collier, who was killed in the line of duty on April 18, 2013, the center will provide care for members of the MIT community and individuals from the broader Cambridge community. The center will be funded by MIT and staffed by MIT Medical. It is designed to alleviate the anticipated hospital bed shortage in the Commonwealth of Massachusetts as cases of Covid-19 approach peak levels in the coming weeks. It will focus exclusively on patients who would benefit from “eyes on” clinical care but are at very low risk for requiring ventilators or other intensive care.

“We are proud to help our neighbors in Cambridge by creating the Sean Collier Care Center,” says MIT Medical Director Cecilia Stuopis. “With this facility, we hope to do our part to ease some of the strain that our fellow health care facilities are feeling at this time.”

Eligible patients from the Cambridge community will be referred to the new center by clinicians at Mount Auburn Hospital and other local ambulatory care centers. Patients must transfer directly from one of these partner organizations to come to the new facility.

“Creating this center on such short notice was an incredible team effort, bringing together partners from emergency management, student life, athletics, finance, facilities, space planning, and many others,” says Stuopis. “It has been a deeply rewarding experience and is the epitome of what ‘One MIT’ truly means.”"
313;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-multilayer-perceptron-models-for-time-series-forecasting/;2018-11-08;How to Develop Multilayer Perceptron Models for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Multilayer Perceptrons, or MLPs for short, can be applied to time series forecasting.

A challenge with using MLPs for time series forecasting is in the preparation of the data. Specifically, lag observations must be flattened into feature vectors.

In this tutorial, you will discover how to develop a suite of MLP models for a range of standard time series forecasting problems.

The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem.

In this tutorial, you will discover how to develop a suite of Multilayer Perceptron models for a range of standard time series forecasting problems.

After completing this tutorial, you will know:

How to develop MLP models for univariate time series forecasting.

How to develop MLP models for multivariate time series forecasting.

How to develop MLP models for multi-step time series forecasting.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into four parts; they are:

Univariate MLP Models Multivariate MLP Models Multi-Step MLP Models Multivariate Multi-Step MLP Models

Univariate MLP Models

Multilayer Perceptrons, or MLPs for short, can be used to model univariate time series forecasting problems.

Univariate time series are a dataset comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence.

This section is divided into two parts; they are:

Data Preparation MLP Model

Data Preparation

Before a univariate series can be modeled, it must be prepared.

The MLP model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn.

Consider a given univariate sequence:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.

X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ... 1 2 3 4 5 X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ...

The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.

# split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on our small contrived dataset above.

The complete example is listed below.

# univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example splits the univariate series into six samples where each sample has three input time steps and one output time step.

[10 20 30] 40 [20 30 40] 50 [30 40 50] 60 [40 50 60] 70 [50 60 70] 80 [60 70 80] 90 1 2 3 4 5 6 [10 20 30] 40 [20 30 40] 50 [30 40 50] 60 [40 50 60] 70 [50 60 70] 80 [60 70 80] 90

Now that we know how to prepare a univariate series for modeling, let’s look at developing an MLP model that can learn the mapping of inputs to outputs.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

MLP Model

A simple MLP model has a single hidden layer of nodes, and an output layer used to make a prediction.

We can define an MLP for univariate time series forecasting as follows.

# define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_steps)) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_steps ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

Important in the definition is the shape of the input; that is what the model expects as input for each sample in terms of the number of time steps.

The number of time steps as input is the number we chose when preparing our dataset as an argument to the split_sequence() function.

The input dimension for each sample is specified in the input_dim argument on the definition of first hidden layer. Technically, the model will view each time step as a separate feature instead of separate time steps.

We almost always have multiple samples, therefore, the model will expect the input component of training data to have the dimensions or shape:

[samples, features] 1 [samples, features]

Our split_sequence() function in the previous section outputs the X with the shape [samples, features] ready to use for modeling.

The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or ‘mse‘, loss function.

Once the model is defined, we can fit it on the training dataset.

# fit model model.fit(X, y, epochs=2000, verbose=0) 1 2 # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 )

After the model is fit, we can use it to make a prediction.

We can predict the next value in the sequence by providing the input:

[70, 80, 90] 1 [70, 80, 90]

And expecting the model to predict something like:

[100] 1 [100]

The model expects the input shape to be two-dimensional with [samples, features], therefore, we must reshape the single input sample before making the prediction, e.g with the shape [1, 3] for 1 sample and 3 time steps used as input features.

# demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps ) ) yhat = model . predict ( x_input , verbose = 0 )

We can tie all of this together and demonstrate how to develop an MLP for univariate time series forecasting and make a single prediction.

# univariate mlp example from numpy import array from keras.models import Sequential from keras.layers import Dense # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_steps)) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # univariate mlp example from numpy import array from keras . models import Sequential from keras . layers import Dense # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps = 3 # split into samples X , y = split_sequence ( raw_seq , n_steps ) # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_steps ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

Your results may vary given the stochastic nature of the algorithm; try running the example a few times.

We can see that the model predicts the next value in the sequence.

[[100.0109]] 1 [[100.0109]]

Multivariate MLP Models

Multivariate time series data means data where there is more than one observation for each time step.

There are two main models that we may require with multivariate time series data; they are:

Multiple Input Series. Multiple Parallel Series.

Let’s take a look at each in turn.

Multiple Input Series

A problem may have two or more parallel input time series and an output time series that is dependent on the input time series.

The input time series are parallel because each series has an observation at the same time step.

We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series.

# define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) 1 2 3 4 # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] )

We can reshape these three arrays of data as a single dataset where each row is a time step and each column is a separate time series. This is a standard way of storing parallel time series in a CSV file.

# convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) 1 2 3 4 5 6 # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) )

The complete example is listed below.

# multivariate data preparation from numpy import array from numpy import hstack # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) print(dataset) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # multivariate data preparation from numpy import array from numpy import hstack # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) print ( dataset )

Running the example prints the dataset with one row per time step and one column for each of the two input and one output parallel time series.

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

As with the univariate time series, we must structure these data into samples with input and output samples.

We need to split the data into samples maintaining the order of observations across the two input sequences.

If we chose three input time steps, then the first sample would look as follows:

Input:

10, 15 20, 25 30, 35 1 2 3 10, 15 20, 25 30, 35

Output:

65 1 65

That is, the first three time steps of each parallel series are provided as input to the model and the model associates this with the value in the output series at the third time step, in this case 65.

We can see that, in transforming the time series into input/output samples to train the model, that we will have to discard some values from the output time series where we do not have values in the input time series at prior time steps. In turn, the choice of the size of the number of input time steps will have an important effect on how much of the training data is used.

We can define a function named split_sequences() that will take a dataset as we have defined it with rows for time steps and columns for parallel series and return input/output samples.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can test this function on our dataset using three time steps for each input time series as input.

The complete example is listed below.

# multivariate data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # multivariate data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the X and y components.

We can see that the X component has a three-dimensional structure.

The first dimension is the number of samples, in this case 7. The second dimension is the number of time steps per sample, in this case 3, the value specified to the function. Finally, the last dimension specifies the number of parallel time series or the number of variables, in this case 2 for the two parallel series.

We can then see that the input and output for each sample is printed, showing the three time steps for each of the two input series and the associated output for each sample.

(7, 3, 2) (7,) [[10 15] [20 25] [30 35]] 65 [[20 25] [30 35] [40 45]] 85 [[30 35] [40 45] [50 55]] 105 [[40 45] [50 55] [60 65]] 125 [[50 55] [60 65] [70 75]] 145 [[60 65] [70 75] [80 85]] 165 [[70 75] [80 85] [90 95]] 185 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 (7, 3, 2) (7,) [[10 15] [20 25] [30 35]] 65 [[20 25] [30 35] [40 45]] 85 [[30 35] [40 45] [50 55]] 105 [[40 45] [50 55] [60 65]] 125 [[50 55] [60 65] [70 75]] 145 [[60 65] [70 75] [80 85]] 165 [[70 75] [80 85] [90 95]] 185

Before we can fit an MLP on this data, we must flatten the shape of the input samples.

MLPs require that the shape of the input portion of each sample is a vector. With a multivariate input, we will have multiple vectors, one for each time step.

We can flatten the temporal structure of each input sample, so that:

[[10 15] [20 25] [30 35]] 1 2 3 [[10 15] [20 25] [30 35]]

Becomes:

[10, 15, 20, 25, 30, 35] 1 [10, 15, 20, 25, 30, 35]

First, we can calculate the length of each input vector as the number of time steps multiplied by the number of features or time series. We can then use this vector size to reshape the input.

# flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) 1 2 3 # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) )

We can now define an MLP model for the multivariate input where the vector length is used for the input dimension argument.

# define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_input)) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_input ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

When making a prediction, the model expects three time steps for two input time series.

We can predict the next value in the output series proving the input values of:

80, 85 90, 95 100, 105 1 2 3 80, 85 90, 95 100, 105

The shape of the 1 sample with 3 time steps and 2 variables would be [1, 3, 2]. We must again reshape this to be 1 sample with a vector of 6 elements or [1, 6]

We would expect the next value in the sequence to be 100 + 105 or 205.

# demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x_input = x_input.reshape((1, n_input)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x_input = x_input . reshape ( ( 1 , n_input ) ) yhat = model . predict ( x_input , verbose = 0 )

The complete example is listed below.

# multivariate mlp example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) # define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_input)) model.add(Dense(1)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x_input = x_input.reshape((1, n_input)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # multivariate mlp example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) ) # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_input ) ) model . add ( Dense ( 1 ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x_input = x_input . reshape ( ( 1 , n_input ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[[205.04436]] 1 [[205.04436]]

There is another more elaborate way to model the problem.

Each input series can be handled by a separate MLP and the output of each of these submodels can be combined before a prediction is made for the output sequence.

We can refer to this as a multi-headed input MLP model. It may offer more flexibility or better performance depending on the specifics of the problem that are being modeled.

This type of model can be defined in Keras using the Keras functional API.

First, we can define the first input model as an MLP with an input layer that expects vectors with n_steps features.

# first input model visible1 = Input(shape=(n_steps,)) dense1 = Dense(100, activation='relu')(visible1) 1 2 3 # first input model visible1 = Input ( shape = ( n_steps , ) ) dense1 = Dense ( 100 , activation = 'relu' ) ( visible1 )

We can define the second input submodel in the same way.

# second input model visible2 = Input(shape=(n_steps,)) dense2 = Dense(100, activation='relu')(visible2) 1 2 3 # second input model visible2 = Input ( shape = ( n_steps , ) ) dense2 = Dense ( 100 , activation = 'relu' ) ( visible2 )

Now that both input submodels have been defined, we can merge the output from each model into one long vector, which can be interpreted before making a prediction for the output sequence.

# merge input models merge = concatenate([dense1, dense2]) output = Dense(1)(merge) 1 2 3 # merge input models merge = concatenate ( [ dense1 , dense2 ] ) output = Dense ( 1 ) ( merge )

We can then tie the inputs and outputs together.

model = Model(inputs=[visible1, visible2], outputs=output) 1 model = Model ( inputs = [ visible1 , visible2 ] , outputs = output )

The image below provides a schematic for how this model looks, including the shape of the inputs and outputs of each layer.

This model requires input to be provided as a list of two elements, where each element in the list contains data for one of the submodels.

In order to achieve this, we can split the 3D input data into two separate arrays of input data: that is from one array with the shape [7, 3, 2] to two 2D arrays with the shape [7, 3]

# separate input data X1 = X[:, :, 0] X2 = X[:, :, 1] 1 2 3 # separate input data X1 = X [ : , : , 0 ] X2 = X [ : , : , 1 ]

These data can then be provided in order to fit the model.

# fit model model.fit([X1, X2], y, epochs=2000, verbose=0) 1 2 # fit model model . fit ( [ X1 , X2 ] , y , epochs = 2000 , verbose = 0 )

Similarly, we must prepare the data for a single sample as two separate two-dimensional arrays when making a single one-step prediction.

x_input = array([[80, 85], [90, 95], [100, 105]]) x1 = x_input[:, 0].reshape((1, n_steps)) x2 = x_input[:, 1].reshape((1, n_steps)) 1 2 3 x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x1 = x_input [ : , 0 ] . reshape ( ( 1 , n_steps ) ) x2 = x_input [ : , 1 ] . reshape ( ( 1 , n_steps ) )

We can tie all of this together; the complete example is listed below.

# multivariate mlp example from numpy import array from numpy import hstack from keras.models import Model from keras.layers import Input from keras.layers import Dense from keras.layers.merge import concatenate # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # separate input data X1 = X[:, :, 0] X2 = X[:, :, 1] # first input model visible1 = Input(shape=(n_steps,)) dense1 = Dense(100, activation='relu')(visible1) # second input model visible2 = Input(shape=(n_steps,)) dense2 = Dense(100, activation='relu')(visible2) # merge input models merge = concatenate([dense1, dense2]) output = Dense(1)(merge) model = Model(inputs=[visible1, visible2], outputs=output) model.compile(optimizer='adam', loss='mse') # fit model model.fit([X1, X2], y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([[80, 85], [90, 95], [100, 105]]) x1 = x_input[:, 0].reshape((1, n_steps)) x2 = x_input[:, 1].reshape((1, n_steps)) yhat = model.predict([x1, x2], verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # multivariate mlp example from numpy import array from numpy import hstack from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers . merge import concatenate # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # separate input data X1 = X [ : , : , 0 ] X2 = X [ : , : , 1 ] # first input model visible1 = Input ( shape = ( n_steps , ) ) dense1 = Dense ( 100 , activation = 'relu' ) ( visible1 ) # second input model visible2 = Input ( shape = ( n_steps , ) ) dense2 = Dense ( 100 , activation = 'relu' ) ( visible2 ) # merge input models merge = concatenate ( [ dense1 , dense2 ] ) output = Dense ( 1 ) ( merge ) model = Model ( inputs = [ visible1 , visible2 ] , outputs = output ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( [ X1 , X2 ] , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 80 , 85 ] , [ 90 , 95 ] , [ 100 , 105 ] ] ) x1 = x_input [ : , 0 ] . reshape ( ( 1 , n_steps ) ) x2 = x_input [ : , 1 ] . reshape ( ( 1 , n_steps ) ) yhat = model . predict ( [ x1 , x2 ] , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[[206.05022]] 1 [[206.05022]]

Multiple Parallel Series

An alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each.

For example, given the data from the previous section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may want to predict the value for each of the three time series for the next time step.

This might be referred to as multivariate forecasting.

Again, the data must be split into input/output samples in order to train a model.

The first sample of this dataset would be:

Input:

10, 15, 25 20, 25, 45 30, 35, 65 1 2 3 10, 15, 25 20, 25, 45 30, 35, 65

Output:

40, 45, 85 1 40, 45, 85

The split_sequences() function below will split multiple parallel time series with rows for time steps and one series per column into the required input/output shape.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this on the contrived problem; the complete example is listed below.

# multivariate output data prep from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # multivariate output data prep from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared X and y components.

The shape of X is three-dimensional, including the number of samples (6), the number of time steps chosen per sample (3), and the number of parallel time series or features (3).

The shape of y is two-dimensional as we might expect for the number of samples (6) and the number of time variables per sample to be predicted (3).

Then, each of the samples is printed showing the input and output components of each sample.

(6, 3, 3) (6, 3) [[10 15 25] [20 25 45] [30 35 65]] [40 45 85] [[20 25 45] [30 35 65] [40 45 85]] [ 50 55 105] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [ 60 65 125] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [ 70 75 145] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [ 80 85 165] [[ 60 65 125] [ 70 75 145] [ 80 85 165]] [ 90 95 185] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (6, 3, 3) (6, 3) [[10 15 25] [20 25 45] [30 35 65]] [40 45 85] [[20 25 45] [30 35 65] [40 45 85]] [ 50 55 105] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [ 60 65 125] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [ 70 75 145] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [ 80 85 165] [[ 60 65 125] [ 70 75 145] [ 80 85 165]] [ 90 95 185]

We are now ready to fit an MLP model on this data.

As with the previous case of multivariate input, we must flatten the three dimensional structure of the input data samples to a two dimensional structure of [samples, features], where lag observations are treated as features by the model.

# flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) 1 2 3 # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) )

The model output will be a vector, with one element for each of the three different time series.

n_output = y.shape[1] 1 n_output = y . shape [ 1 ]

We can now define our model, using the flattened vector length for the input layer and the number of time series as the vector length when making a prediction.

# define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_input)) model.add(Dense(n_output)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_input ) ) model . add ( Dense ( n_output ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

We can predict the next value in each of the three parallel series by providing an input of three time steps for each series.

70, 75, 145 80, 85, 165 90, 95, 185 1 2 3 70, 75, 145 80, 85, 165 90, 95, 185

The shape of the input for making a single prediction must be 1 sample, 3 time steps and 3 features, or [1, 3, 3]. Again, we can flatten this to [1, 6] to meet the expectations of the model.

We would expect the vector output to be:

[100, 105, 205] 1 [100, 105, 205]

# demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_input)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_input ) ) yhat = model . predict ( x_input , verbose = 0 )

We can tie all of this together and demonstrate an MLP for multivariate output time series forecasting below.

# multivariate output mlp example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) n_output = y.shape[1] # define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_input)) model.add(Dense(n_output)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_input)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # multivariate output mlp example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) ) n_output = y . shape [ 1 ] # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_input ) ) model . add ( Dense ( n_output ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_input ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[[100.95039 107.541306 206.81033 ]] 1 [[100.95039 107.541306 206.81033 ]]

As with multiple input series, there is another, more elaborate way to model the problem.

Each output series can be handled by a separate output MLP model.

We can refer to this as a multi-output MLP model. It may offer more flexibility or better performance depending on the specifics of the problem that is being modeled.

This type of model can be defined in Keras using the Keras functional API.

First, we can define the input model as an MLP with an input layer that expects flattened feature vectors.

# define model visible = Input(shape=(n_input,)) dense = Dense(100, activation='relu')(visible) 1 2 3 # define model visible = Input ( shape = ( n_input , ) ) dense = Dense ( 100 , activation = 'relu' ) ( visible )

We can then define one output layer for each of the three series that we wish to forecast, where each output submodel will forecast a single time step.

# define output 1 output1 = Dense(1)(dense) # define output 2 output2 = Dense(1)(dense) # define output 2 output3 = Dense(1)(dense) 1 2 3 4 5 6 # define output 1 output1 = Dense ( 1 ) ( dense ) # define output 2 output2 = Dense ( 1 ) ( dense ) # define output 2 output3 = Dense ( 1 ) ( dense )

We can then tie the input and output layers together into a single model.

# tie together model = Model(inputs=visible, outputs=[output1, output2, output3]) model.compile(optimizer='adam', loss='mse') 1 2 3 # tie together model = Model ( inputs = visible , outputs = [ output1 , output2 , output3 ] ) model . compile ( optimizer = 'adam' , loss = 'mse' )

To make the model architecture clear, the schematic below clearly shows the three separate output layers of the model and the input and output shapes of each layer.

When training the model, it will require three separate output arrays per sample.

We can achieve this by converting the output training data that has the shape [7, 3] to three arrays with the shape [7, 1].

# separate output y1 = y[:, 0].reshape((y.shape[0], 1)) y2 = y[:, 1].reshape((y.shape[0], 1)) y3 = y[:, 2].reshape((y.shape[0], 1)) 1 2 3 4 # separate output y1 = y [ : , 0 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y2 = y [ : , 1 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y3 = y [ : , 2 ] . reshape ( ( y . shape [ 0 ] , 1 ) )

These arrays can be provided to the model during training.

# fit model model.fit(X, [y1,y2,y3], epochs=2000, verbose=0) 1 2 # fit model model . fit ( X , [ y1 , y2 , y3 ] , epochs = 2000 , verbose = 0 )

Tying all of this together, the complete example is listed below.

# multivariate output mlp example from numpy import array from numpy import hstack from keras.models import Model from keras.layers import Input from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len(sequences)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps = 3 # convert into input/output X, y = split_sequences(dataset, n_steps) # flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) # separate output y1 = y[:, 0].reshape((y.shape[0], 1)) y2 = y[:, 1].reshape((y.shape[0], 1)) y3 = y[:, 2].reshape((y.shape[0], 1)) # define model visible = Input(shape=(n_input,)) dense = Dense(100, activation='relu')(visible) # define output 1 output1 = Dense(1)(dense) # define output 2 output2 = Dense(1)(dense) # define output 2 output3 = Dense(1)(dense) # tie together model = Model(inputs=visible, outputs=[output1, output2, output3]) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, [y1,y2,y3], epochs=2000, verbose=0) # demonstrate prediction x_input = array([[70,75,145], [80,85,165], [90,95,185]]) x_input = x_input.reshape((1, n_input)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # multivariate output mlp example from numpy import array from numpy import hstack from keras . models import Model from keras . layers import Input from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the dataset if end_ix > len ( sequences ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps = 3 # convert into input/output X , y = split_sequences ( dataset , n_steps ) # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) ) # separate output y1 = y [ : , 0 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y2 = y [ : , 1 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) y3 = y [ : , 2 ] . reshape ( ( y . shape [ 0 ] , 1 ) ) # define model visible = Input ( shape = ( n_input , ) ) dense = Dense ( 100 , activation = 'relu' ) ( visible ) # define output 1 output1 = Dense ( 1 ) ( dense ) # define output 2 output2 = Dense ( 1 ) ( dense ) # define output 2 output3 = Dense ( 1 ) ( dense ) # tie together model = Model ( inputs = visible , outputs = [ output1 , output2 , output3 ] ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , [ y1 , y2 , y3 ] , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] , [ 90 , 95 , 185 ] ] ) x_input = x_input . reshape ( ( 1 , n_input ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example prepares the data, fits the model, and makes a prediction.

[array([[100.86121]], dtype=float32), array([[105.14738]], dtype=float32), array([[205.97507]], dtype=float32)] 1 2 3 [array([[100.86121]], dtype=float32), array([[105.14738]], dtype=float32), array([[205.97507]], dtype=float32)]

Multi-Step MLP Models

In practice, there is little difference to the MLP model in predicting a vector output that represents different output variables (as in the previous example) or a vector output that represents multiple time steps of one variable.

Nevertheless, there are subtle and important differences in the way the training data is prepared. In this section, we will demonstrate the case of developing a multi-step forecast model using a vector model.

Before we look at the specifics of the model, let’s first look at the preparation of data for multi-step forecasting.

Data Preparation

As with one-step forecasting, a time series used for multi-step time series forecasting must be split into samples with input and output components.

Both the input and output components will be comprised of multiple time steps and may or may not have the same number of steps.

For example, given the univariate time series:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We could use the last three time steps as input and forecast the next two time steps.

The first sample would look as follows:

Input:

[10, 20, 30] 1 [10, 20, 30]

Output:

[40, 50] 1 [40, 50]

The split_sequence() function below implements this behavior and will split a given univariate time series into samples with a specified number of input and output time steps.

# split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on the small contrived dataset.

The complete example is listed below.

# multi-step data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # split into samples X, y = split_sequence(raw_seq, n_steps_in, n_steps_out) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # multi-step data preparation from numpy import array # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # split into samples X , y = split_sequence ( raw_seq , n_steps_in , n_steps_out ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example splits the univariate series into input and output time steps and prints the input and output components of each.

[10 20 30] [40 50] [20 30 40] [50 60] [30 40 50] [60 70] [40 50 60] [70 80] [50 60 70] [80 90] 1 2 3 4 5 [10 20 30] [40 50] [20 30 40] [50 60] [30 40 50] [60 70] [40 50 60] [70 80] [50 60 70] [80 90]

Now that we know how to prepare data for multi-step forecasting, let’s look at an MLP model that can learn this mapping.

Vector Output Model

The MLP can output a vector directly that can be interpreted as a multi-step forecast.

This approach was seen in the previous section were one time step of each output time series was forecasted as a vector.

With the number of input and output steps specified in the n_steps_in and n_steps_out variables, we can define a multi-step time-series forecasting model.

# define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_steps_in)) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') 1 2 3 4 5 # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_steps_in ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' )

The model can make a prediction for a single sample. We can predict the next two steps beyond the end of the dataset by providing the input:

[70, 80, 90] 1 [70, 80, 90]

We would expect the predicted output to be:

[100, 110] 1 [100, 110]

As expected by the model, the shape of the single sample of input data when making the prediction must be [1, 3] for the 1 sample and 3 time steps (features) of the input and the single feature.

# demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps_in)) yhat = model.predict(x_input, verbose=0) 1 2 3 4 # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps_in ) ) yhat = model . predict ( x_input , verbose = 0 )

Tying all of this together, the MLP for multi-step forecasting with a univariate time series is listed below.

# univariate multi-step vector-output mlp example from numpy import array from keras.models import Sequential from keras.layers import Dense # split a univariate sequence into samples def split_sequence(sequence, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len(sequence): break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # split into samples X, y = split_sequence(raw_seq, n_steps_in, n_steps_out) # define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_steps_in)) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([70, 80, 90]) x_input = x_input.reshape((1, n_steps_in)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # univariate multi-step vector-output mlp example from numpy import array from keras . models import Sequential from keras . layers import Dense # split a univariate sequence into samples def split_sequence ( sequence , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the sequence if out_end_ix > len ( sequence ) : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix : out_end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence raw_seq = [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # split into samples X , y = split_sequence ( raw_seq , n_steps_in , n_steps_out ) # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_steps_in ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ 70 , 80 , 90 ] ) x_input = x_input . reshape ( ( 1 , n_steps_in ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example forecasts and prints the next two time steps in the sequence.

[[102.572365 113.88405 ]] 1 [[102.572365 113.88405 ]]

Multivariate Multi-Step MLP Models

In the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting.

It is possible to mix and match the different types of MLP models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging, particularly in preparing the data and defining the shape of inputs and outputs for the model.

In this section, we will look at short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically:

Multiple Input Multi-Step Output. Multiple Parallel Input and Multi-Step Output.

Perhaps the biggest stumbling block is in the preparation of data, so this is where we will focus our attention.

Multiple Input Multi-Step Output

There are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series.

For example, consider our multivariate time series from a prior section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may use three prior time steps of each of the two input time series to predict two time steps of the output time series.

Input:

10, 15 20, 25 30, 35 1 2 3 10, 15 20, 25 30, 35

Output:

65 85 1 2 65 85

The split_sequences() function below implements this behavior.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this on our contrived dataset. The complete example is listed below.

# multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared training data.

We can see that the shape of the input portion of the samples is three-dimensional, comprised of six samples, with three time steps and two variables for the two input time series.

The output portion of the samples is two-dimensional for the six samples and the two time steps for each sample to be predicted.

The prepared samples are then printed to confirm that the data was prepared as we specified.

(6, 3, 2) (6, 2) [[10 15] [20 25] [30 35]] [65 85] [[20 25] [30 35] [40 45]] [ 85 105] [[30 35] [40 45] [50 55]] [105 125] [[40 45] [50 55] [60 65]] [125 145] [[50 55] [60 65] [70 75]] [145 165] [[60 65] [70 75] [80 85]] [165 185] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (6, 3, 2) (6, 2) [[10 15] [20 25] [30 35]] [65 85] [[20 25] [30 35] [40 45]] [ 85 105] [[30 35] [40 45] [50 55]] [105 125] [[40 45] [50 55] [60 65]] [125 145] [[50 55] [60 65] [70 75]] [145 165] [[60 65] [70 75] [80 85]] [165 185]

We can now develop an MLP model for multi-step predictions using a vector output.

The complete example is listed below.

# multivariate multi-step mlp example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out-1 # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) # flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) # define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_input)) model.add(Dense(n_steps_out)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([[70, 75], [80, 85], [90, 95]]) x_input = x_input.reshape((1, n_input)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # multivariate multi-step mlp example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out - 1 # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : - 1 ] , sequences [ end_ix - 1 : out_end_ix , - 1 ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) ) # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_input ) ) model . add ( Dense ( n_steps_out ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 70 , 75 ] , [ 80 , 85 ] , [ 90 , 95 ] ] ) x_input = x_input . reshape ( ( 1 , n_input ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example fits the model and predicts the next two time steps of the output sequence beyond the dataset.

We would expect the next two steps to be [185, 205].

It is a challenging framing of the problem with very little data, and the arbitrarily configured version of the model gets close.

[[186.53822 208.41725]] 1 [[186.53822 208.41725]]

Multiple Parallel Input and Multi-Step Output

A problem with parallel time series may require the prediction of multiple time steps of each time series.

For example, consider our multivariate time series from a prior section:

[[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 [[ 10 15 25] [ 20 25 45] [ 30 35 65] [ 40 45 85] [ 50 55 105] [ 60 65 125] [ 70 75 145] [ 80 85 165] [ 90 95 185]]

We may use the last three time steps from each of the three time series as input to the model and predict the next time steps of each of the three time series as output.

The first sample in the training dataset would be the following.

Input:

10, 15, 25 20, 25, 45 30, 35, 65 1 2 3 10, 15, 25 20, 25, 45 30, 35, 65

Output:

40, 45, 85 50, 55, 105 1 2 40, 45, 85 50, 55, 105

The split_sequences() function below implements this behavior.

# split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on the small contrived dataset.

The complete example is listed below.

# multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) print(X.shape, y.shape) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # multivariate multi-step data preparation from numpy import array from numpy import hstack # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) print ( X . shape , y . shape ) # summarize the data for i in range ( len ( X ) ) : print ( X [ i ] , y [ i ] )

Running the example first prints the shape of the prepared training dataset.

We can see that both the input (X) and output (Y) elements of the dataset are three dimensional for the number of samples, time steps, and variables or parallel time series respectively.

The input and output elements of each series are then printed side by side so that we can confirm that the data was prepared as we expected.

(5, 3, 3) (5, 2, 3) [[10 15 25] [20 25 45] [30 35 65]] [[ 40 45 85] [ 50 55 105]] [[20 25 45] [30 35 65] [40 45 85]] [[ 50 55 105] [ 60 65 125]] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [[ 60 65 125] [ 70 75 145]] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [[ 70 75 145] [ 80 85 165]] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [[ 80 85 165] [ 90 95 185]] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (5, 3, 3) (5, 2, 3) [[10 15 25] [20 25 45] [30 35 65]] [[ 40 45 85] [ 50 55 105]] [[20 25 45] [30 35 65] [40 45 85]] [[ 50 55 105] [ 60 65 125]] [[ 30 35 65] [ 40 45 85] [ 50 55 105]] [[ 60 65 125] [ 70 75 145]] [[ 40 45 85] [ 50 55 105] [ 60 65 125]] [[ 70 75 145] [ 80 85 165]] [[ 50 55 105] [ 60 65 125] [ 70 75 145]] [[ 80 85 165] [ 90 95 185]]

We can now develop an MLP model to make multivariate multi-step forecasts.

In addition to flattening the shape of the input data, as we have in prior examples, we must also flatten the three-dimensional structure of the output data. This is because the MLP model is only capable of taking vector inputs and outputs.

# flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) # flatten output n_output = y.shape[1] * y.shape[2] y = y.reshape((y.shape[0], n_output)) 1 2 3 4 5 6 # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) ) # flatten output n_output = y . shape [ 1 ] * y . shape [ 2 ] y = y . reshape ( ( y . shape [ 0 ] , n_output ) )

The complete example is listed below.

# multivariate multi-step mlp example from numpy import array from numpy import hstack from keras.models import Sequential from keras.layers import Dense # split a multivariate sequence into samples def split_sequences(sequences, n_steps_in, n_steps_out): X, y = list(), list() for i in range(len(sequences)): # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len(sequences): break # gather input and output parts of the pattern seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90]) in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95]) out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))]) # convert to [rows, columns] structure in_seq1 = in_seq1.reshape((len(in_seq1), 1)) in_seq2 = in_seq2.reshape((len(in_seq2), 1)) out_seq = out_seq.reshape((len(out_seq), 1)) # horizontally stack columns dataset = hstack((in_seq1, in_seq2, out_seq)) # choose a number of time steps n_steps_in, n_steps_out = 3, 2 # convert into input/output X, y = split_sequences(dataset, n_steps_in, n_steps_out) # flatten input n_input = X.shape[1] * X.shape[2] X = X.reshape((X.shape[0], n_input)) # flatten output n_output = y.shape[1] * y.shape[2] y = y.reshape((y.shape[0], n_output)) # define model model = Sequential() model.add(Dense(100, activation='relu', input_dim=n_input)) model.add(Dense(n_output)) model.compile(optimizer='adam', loss='mse') # fit model model.fit(X, y, epochs=2000, verbose=0) # demonstrate prediction x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]]) x_input = x_input.reshape((1, n_input)) yhat = model.predict(x_input, verbose=0) print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # multivariate multi-step mlp example from numpy import array from numpy import hstack from keras . models import Sequential from keras . layers import Dense # split a multivariate sequence into samples def split_sequences ( sequences , n_steps_in , n_steps_out ) : X , y = list ( ) , list ( ) for i in range ( len ( sequences ) ) : # find the end of this pattern end_ix = i + n_steps_in out_end_ix = end_ix + n_steps_out # check if we are beyond the dataset if out_end_ix > len ( sequences ) : break # gather input and output parts of the pattern seq_x , seq_y = sequences [ i : end_ix , : ] , sequences [ end_ix : out_end_ix , : ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y ) # define input sequence in_seq1 = array ( [ 10 , 20 , 30 , 40 , 50 , 60 , 70 , 80 , 90 ] ) in_seq2 = array ( [ 15 , 25 , 35 , 45 , 55 , 65 , 75 , 85 , 95 ] ) out_seq = array ( [ in_seq1 [ i ] + in_seq2 [ i ] for i in range ( len ( in_seq1 ) ) ] ) # convert to [rows, columns] structure in_seq1 = in_seq1 . reshape ( ( len ( in_seq1 ) , 1 ) ) in_seq2 = in_seq2 . reshape ( ( len ( in_seq2 ) , 1 ) ) out_seq = out_seq . reshape ( ( len ( out_seq ) , 1 ) ) # horizontally stack columns dataset = hstack ( ( in_seq1 , in_seq2 , out_seq ) ) # choose a number of time steps n_steps_in , n_steps_out = 3 , 2 # convert into input/output X , y = split_sequences ( dataset , n_steps_in , n_steps_out ) # flatten input n_input = X . shape [ 1 ] * X . shape [ 2 ] X = X . reshape ( ( X . shape [ 0 ] , n_input ) ) # flatten output n_output = y . shape [ 1 ] * y . shape [ 2 ] y = y . reshape ( ( y . shape [ 0 ] , n_output ) ) # define model model = Sequential ( ) model . add ( Dense ( 100 , activation = 'relu' , input_dim = n_input ) ) model . add ( Dense ( n_output ) ) model . compile ( optimizer = 'adam' , loss = 'mse' ) # fit model model . fit ( X , y , epochs = 2000 , verbose = 0 ) # demonstrate prediction x_input = array ( [ [ 60 , 65 , 125 ] , [ 70 , 75 , 145 ] , [ 80 , 85 , 165 ] ] ) x_input = x_input . reshape ( ( 1 , n_input ) ) yhat = model . predict ( x_input , verbose = 0 ) print ( yhat )

Running the example fits the model and predicts the values for each of the three time steps for the next two time steps beyond the end of the dataset.

We would expect the values for these series and time steps to be as follows:

90, 95, 185 100, 105, 205 1 2 90, 95, 185 100, 105, 205

We can see that the model forecast gets reasonably close to the expected values.

[[ 91.28376 96.567 188.37575 100.54482 107.9219 208.108 ] 1 [[ 91.28376 96.567 188.37575 100.54482 107.9219 208.108 ]

Summary

In this tutorial, you discovered how to develop a suite of Multilayer Perceptron, or MLP, models for a range of standard time series forecasting problems.

Specifically, you learned:

How to develop MLP models for univariate time series forecasting.

How to develop MLP models for multivariate time series forecasting.

How to develop MLP models for multi-step time series forecasting.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
314;machinelearningmastery.com;http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/;2013-12-22;How to Define Your Machine Learning Problem;"Tweet Share Share

Last Updated on June 7, 2016

The first step in any project is defining your problem. You can use the most powerful and shiniest algorithms available, but the results will be meaningless if you are solving the wrong problem.

In this post you will learn the process for thinking deeply about your problem before you get started. This is unarguably the most important aspect of applying machine learning.

Problem Definition Framework

I use a simple framework when defining a new problem to address with machine learning. The framework helps me to quickly understand the elements and motivation for the problem and whether machine learning is suitable or not.

The framework involves answering three questions to varying degrees of thoroughness:

Step 1 : What is the problem?

: What is the problem? Step 2 : Why does the problem need to be solved?

: Why does the problem need to be solved? Step 3: How would I solve the problem?

Step 1: What is the Problem

The first step is defining the problem. I use a number of tactics to collect this information.

Informal description

Describe the problem as though you were describing it to a friend or colleague. This can provide a great starting point for highlighting areas that you might need to fill. It also provides the basis for a one sentence description you can use to share your understanding of the problem.

For example: I need a program that will tell me which tweets will get retweets.

Formalism

In a previous blog post defining machine learning you learned about Tom Mitchell’s machine learning formalism. Here it is again to refresh your memory.

A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

Use this formalism to define the T, P, and E for your problem.

For example:

Task (T): Classify a tweet that has not been published as going to get retweets or not.

(T): Classify a tweet that has not been published as going to get retweets or not. Experience (E): A corpus of tweets for an account where some have retweets and some do not.

(E): A corpus of tweets for an account where some have retweets and some do not. Performance (P): Classification accuracy, the number of tweets predicted correctly out of all tweets considered as a percentage.

Assumptions

Create a list of assumptions about the problem and it’s phrasing. These may be rules of thumb and domain specific information that you think will get you to a viable solution faster.

It can be useful to highlight questions that can be tested against real data because breakthroughs and innovation occur when assumptions and best practice are demonstrated to be wrong in the face of real data. It can also be useful to highlight areas of the problem specification that may need to be challenged, relaxed or tightened.

For example:

The specific words used in the tweet matter to the model.

The specific user that retweets does not matter to the model.

The number of retweets may matter to the model.

Older tweets are less predictive than more recent tweets.

Similar problems

What other problems have you seen or can you think of that are like the problem you are trying to solve? Other problems can inform the problem you are trying to solve by highlighting limitations in your phrasing of the problem such as time dimensions and conceptual drift (where the concept being modeled changes over time). Other problems can also point to algorithms and data transformations that could be adopted to spot check performance.

For example: A related problem would be email spam discrimination that uses text messages as input data and needs binary classification decision.

Step 2: Why does the the problem need to be solved?

The second step is to think deeply about why you want or need the problem solved.

Motivation

Consider your motivation for solving the problem. What need will be fulfilled when the problem is solved?

For example, you may be solving the problem as a learning exercise. This is useful to clarify as you can decide that you don’t want to use the most suitable method to solve the problem, but instead you want to explore methods that you are not familiar with in order to learn new skills.

Alternatively, you may need to solve the problem as part of a duty at work, ultimately to keep your job.

Solution Benefits

Consider the benefits of having the problem solved. What capabilities does it enable?

It is important to be clear on the benefits of the problem being solved to ensure that you capitalize on them. These benefits can be used to sell the project to colleagues and management to get buy in and additional time or budget resources.

If it benefits you personally, then be clear on what those benefits are and how you will know when you have got them. For example, if it’s a tool or utility, then what will you be able to do with that utility that you can’t do now and why is that meaningful to you?

Solution Use

Consider how the solution to the problem will be used and what type of lifetime you expect the solution to have. As programmers we often think the work is done as soon as the program is written, but really the project is just beginning it’s maintenance lifetime.

The way the solution will be used will influence the nature and requirements of the solution you adopt.

Consider whether you are looking to write a report to present results or you want to operationalize the solution. If you want to operationalize the solution, consider the functional and nonfunctional requirements you have for a solution, just like a software project.

Step 3: How would I solve the problem?

In this third and final step of the problem definition, explore how you would solve the problem manually.

List out step-by-step what data you would collect, how you would prepare it and how you would design a program to solve the problem. This may include prototypes and experiments you would need to perform which are a gold mine because they will highlight questions and uncertainties you have about the domain that could be explored.

This is a powerful tool. It can highlight problems that actually can be solved satisfactorily using a manually implemented solution. It also flushes out important domain knowledge that has been trapped up until now like where the data is actually stored, what types of features would be useful and many other details.

Collect all of these details as they occur to you and update the previous sections of the problem definition. Especially the assumptions and rules of thumb.

We have considered a manually specified solution before when describing complex problems in why machine learning matters.

Summary

In this post you learned the value of being clear on the problem you are solving. You discovered a three step framework for defining your problem with practical tactics at at step:

Step 1: What is the problem? Describe the problem informally and formally and list assumptions and similar problems.

Describe the problem informally and formally and list assumptions and similar problems. Step 2: Why does the problem need to be solve? List your motivation for solving the problem, the benefits a solution provides and how the solution will be used.

List your motivation for solving the problem, the benefits a solution provides and how the solution will be used. Step 3: How would I solve the problem? Describe how the problem would be solved manually to flush domain knowledge.

How do you define your problem for machine learning? Have you used any of the above tactics and if so, what were your experiences? Leave a comment."
315;machinelearningmastery.com;https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/;2019-07-09;A Tour of Generative Adversarial Network Models;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are deep learning architecture generative models that have seen wide success.

There are thousands of papers on GANs and many hundreds of named-GANs, that is, models with a defined name that often includes “GAN“, such as DCGAN, as opposed to a minor extension to the method. Given the vast size of the GAN literature and number of models, it can be, at the very least, confusing and frustrating as to know what GAN models to focus on.

In this post, you will discover the Generative Adversarial Network models that you need to know to establish a useful and productive foundation in the field.

After reading this post, you will know:

The foundation GAN models that provide the basis for the field of study.

The extension GAN models that build upon what works and lead the way for more advanced models.

The advanced GAN models that push the limits of the architecture and achieve impressive results.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Foundation Generative Adversarial Network (GAN) Deep Convolutional Generative Adversarial Network (DCGAN)

Extensions Conditional Generative Adversarial Network (cGAN) Information Maximizing Generative Adversarial Network (InfoGAN) Auxiliary Classifier Generative Adversarial Network (AC-GAN) Stacked Generative Adversarial Network (StackGAN) Context Encoders Pix2Pix

Advanced Wasserstein Generative Adversarial Network (WGAN) Cycle-Consistent Generative Adversarial Network (CycleGAN) Progressive Growing Generative Adversarial Network (Progressive GAN) Style-Based Generative Adversarial Network (StyleGAN) Big Generative Adversarial Network (BigGAN)



Foundation Generative Adversarial Networks

This section summarizes the foundational GAN models from which most, if not all, other GANs build upon.

Generative Adversarial Network (GAN)

The Generative Adversarial Network architecture and first empirical demonstration of the approach was described in the 2014 paper by Ian Goodfellow, et al. titled “Generative Adversarial Networks.”

The paper describes the architecture succinctly involving a generator model that takes as input points from a latent space and generates an image, and a discriminator model that classifies images as either real (from the dataset) or fake (output by the generator).

We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake.

— Generative Adversarial Networks, 2014.

The models are comprised of fully connected layers (MLPs) with ReLU activations in the generator and maxout activations in the discriminator and was applied to standard image datasets such as MNIST and CIFAR-10.

We trained adversarial nets as a range of datasets including MNIST, the Toronto Face Database (TFD), and CIFAR-10. The generator nets used a mixture of rectifier linear activations and sigmoid activations, while the discriminator net used maxout activations. Dropout was applied in training the discriminator net.

— Generative Adversarial Networks, 2014.

Deep Convolutional Generative Adversarial Network (DCGAN)

The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model.

We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.

— Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, 2015.

The DCGAN is important because it suggested the constraints on the model required to effectively develop high-quality generator models in practice. This architecture, in turn, provided the basis for the rapid development of a large number of GAN extensions and applications.

We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings.

— Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, 2015.

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Generative Adversarial Network Extensions

This section summarizes named GAN models that provide some of the more common or widely used discrete extensions to the GAN model architecture or training process.

Conditional Generative Adversarial Network (cGAN)

The conditional generative adversarial network, or cGAN for short, is an extension to the GAN architecture that makes use of information in addition to the image as input both to the generator and the discriminator models. For example, if class labels are available, they can be used as input.

Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information y. y could be any kind of auxiliary information, such as class labels or data from other modalities. We can perform the conditioning by feeding y into the both the discriminator and generator as additional input layer.

— Conditional Generative Adversarial Nets, 2014.

Information Maximzing Generative Adversarial Network (InfoGAN)

The info generative adversarial network, or InfoGAN for short, is an extension to the GAN that attempts to structure the input or latent space for the generator. Specifically, the goal is to add specific semantic meaning to the variables in the latent space.

… , when generating images from the MNIST dataset, it would be ideal if the model automatically chose to allocate a discrete random variable to represent the numerical identity of the digit (0-9), and chose to have two additional continuous variables that represent the digit’s angle and thickness of the digit’s stroke.

— InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, 2016.

This is achieved by separating points in the latent space into both noise and latent codes. The latent codes are then used to condition or control specific semantic properties in the generated image.

… rather than using a single unstructured noise vector, we propose to decompose the input noise vector into two parts: (i) z, which is treated as source of incompressible noise; (ii) c, which we will call the latent code and will target the salient structured semantic features of the data distribution

— InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, 2016.

Auxiliary Classifier Generative Adversarial Network (AC-GAN)

The auxiliary classifier generative adversarial network, or AC-GAN, is an extension to the GAN that both changes the generator to be class conditional as with the cGAN, and adds an additional or auxiliary model to the discriminator that is trained to reconstruct the class label.

… we introduce a model that combines both strategies for leveraging side information. That is, the model proposed below is class conditional, but with an auxiliary decoder that is tasked with reconstructing class labels.

— Conditional Image Synthesis With Auxiliary Classifier GANs, 2016.

This architecture means that the discriminator both predicts the likelihood of the image given the class label and the class label given the image.

The discriminator gives both a probability distribution over sources and a probability distribution over the class labels, P(S | X), P(C | X) = D(X).

— Conditional Image Synthesis With Auxiliary Classifier GANs, 2016.

Stacked Generative Adversarial Network (StackGAN)

The stacked generative adversarial network, or StackGAN, is an extension to the GAN to generate images from text using a hierarchical stack of conditional GAN models.

… we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256×256 photo-realistic images conditioned on text descriptions.

— StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks, 2016.

The architecture is comprised of a series of text- and image-conditional GAN models. The first level generator (Stage-I GAN) is conditioned on text and generates a low-resolution image. The second level generator (Stage-II GaN) is conditioned both on the text and on the low-resolution image output by the first level and outputs a high-resolution image.

Low-resolution images are first generated by our Stage-I GAN. On the top of our Stage-I GAN, we stack Stage-II GAN to generate realistic high-resolution (e.g., 256×256) images conditioned on Stage-I results and text descriptions. By conditioning on the Stage-I result and the text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details for the object

— StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks, 2016.

Context Encoders

The Context Encoders model is an encoder-decoder model for conditional image generation trained using the adversarial approach devised for GANs. Although it is not referred to in the paper as a GAN model, it has many GAN features.

By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings.

— Context Encoders: Feature Learning by Inpainting, 2016.

The model is trained with a joint-loss that combines both the adversarial loss of generator and discriminator models and the reconstruction loss that calculates the vector norm distance between the predicted and expected output image.

When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output.

— Context Encoders: Feature Learning by Inpainting, 2016.

Pix2Pix

The pix2pix model is an extension of the GAN for image-conditional image generation, referred to as the task image-to-image translation. A U-Net model architecture is used in the generator model, and a PatchGAN model architecture is used as the discriminator model.

Our method also differs from the prior works in several architectural choices for the generator and discriminator. Unlike past work, for our generator we use a “U-Net”-based architecture, and for our discriminator we use a convolutional “PatchGAN” classifier, which only penalizes structure at the scale of image patches.

— Image-to-Image Translation with Conditional Adversarial Networks, 2016.

The loss for the generator model is updated to also include the vector distance from the target output image.

The discriminator’s job remains unchanged, but the generator is tasked to not only fool the discriminator but also to be near the ground truth output in an L2 sense. We also explore this option, using L1 distance rather than L2 as L1 encourages less blurring.

— Image-to-Image Translation with Conditional Adversarial Networks, 2016.

Advanced Generative Adversarial Networks

This section lists those GAN models that have recently led to surprising or impressive results, building upon prior GAN extensions.

These models mostly focus on developments that allow for the generation of large photorealistic images.

Wasserstein Generative Adversarial Network (WGAN)

The Wasserstein generative adversarial network, or WGAN for short, is an extension to the GAN that changes the training procedure to update the discriminator model, now called a critic, many more times than the generator model for each iteration.

The critic is updated to output a real-value (linear activation) instead of a binary prediction with a sigmoid activation, and the critic and generator models are both trained using “Wasserstein loss,” which is the average of the product of real and predicted values from the critic, designed to provide linear gradients that are useful for updating the model.

The discriminator learns very quickly to distinguish between fake and real, and as expected provides no reliable gradient information. The critic, however, can’t saturate, and converges to a linear function that gives remarkably clean gradients everywhere. The fact that we constrain the weights limits the possible growth of the function to be at most linear in different parts of the space, forcing the optimal critic to have this behaviour.

— Wasserstein GAN, 2017.

In addition, the weights of the critic model are clipped to keep them small, e.g. a bounding box of [-0.01. 0.01].

In order to have parameters w lie in a compact space, something simple we can do is clamp the weights to a fixed box (say W = [−0.01, 0.01]l ) after each gradient update.

— Wasserstein GAN, 2017.

Cycle-Consistent Generative Adversarial Network (CycleGAN)

The cycle-consistent generative adversarial network, or CycleGAN for short, is an extension to the GAN for image-to-image translation without paired image data. That means that examples of the target image are not required as is the case with conditional GANs, such as Pix2Pix.

… for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples.

— Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, 2017.

Their approach seeks “cycle consistency” such that image translation from one domain to another is reversible, meaning it forms a consistent cycle of translation.

… we exploit the property that translation should be “cycle consistent”, in the sense that if we translate, e.g., a sentence from English to French, and then translate it back from French to English, we should arrive back at the original sentence

— Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, 2017.

This is achieved by having two generator models: one for translation X to Y and another for reconstructing X given Y. In turn, the architecture has two discriminator models.

… our model includes two mappings G : X -> Y and F : Y -> X. In addition, we introduce two adversarial discriminators DX and DY , where DX aims to distinguish between images {x} and translated images {F(y)}; in the same way, DY aims to discriminate between {y} and {G(x)}.

— Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, 2017.

Progressive Growing Generative Adversarial Network (Progressive GAN)

The progressive growing generative adversarial network, or Progressive GAN for short, is a change to the architecture and training of GAN models that involves progressively increasing the model depth during the training process.

The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality …

— Progressive Growing of GANs for Improved Quality, Stability, and Variation, 2017.

This is achieved by keeping the generator and discriminator symmetric in depth during training and adding layers step-wise, much like the greedy layer-wise pretraining technique in the early developing of deep neural networks, except weights in prior layers are not frozen.

We use generator and discriminator networks that are mirror images of each other and always grow in synchrony. All existing layers in both networks remain trainable throughout the training process. When new layers are added to the networks, we fade them in smoothly …

— Progressive Growing of GANs for Improved Quality, Stability, and Variation, 2017.

Big Generative Adversarial Network (BigGAN)

The big generative adversarial network, or BigGAN for short, is an approach that demonstrates how high-quality output images can be created by scaling up existing class-conditional GAN models.

We demonstrate that GANs benefit dramatically from scaling, and train models with two to four times as many parameters and eight times the batch size compared to prior art.

— Large Scale GAN Training for High Fidelity Natural Image Synthesis, 2018.

The model architecture is based on a collection of best practices across a wide range of GAN models and extensions. Further improvements are achieved through systematic experimentation.

A “truncation trick” is used where points are sampled from a truncated Gaussian latent space at generation time that is different from the untruncated distribution at training time.

Remarkably, our best results come from using a different latent distribution for sampling than was used in training. Taking a model trained with z ∼ N (0, I) and sampling z from a truncated normal (where values which fall outside a range are resampled to fall inside that range) immediately provides a boost

— Large Scale GAN Training for High Fidelity Natural Image Synthesis, 2018.

Style-Based Generative Adversarial Network (StyleGAN)

The style-based generative adversarial network, or StyleGAN for short, is an extension of the generator that allows the latent code to be used as input at different points of the model to control features of the generated image.

… we re-design the generator architecture in a way that exposes novel ways to control the image synthesis process. Our generator starts from a learned constant input and adjusts the “style” of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales.

— A Style-Based Generator Architecture for Generative Adversarial Networks, 2018.

Instead of taking the point in the latent space as input, the point is fed through a deep embedding network before being provided as input at multiple points in a generator model. In addition, noise is also added along with the output from the embedding network.

Traditionally the latent code is provided to the generator through an input layer […] We depart from this design by omitting the input layer altogether and starting from a learned constant instead. Given a latent code z in the input latent space Z, a non-linear mapping network f : Z -> W first produces w ∈ W.

— A Style-Based Generator Architecture for Generative Adversarial Networks, 2018.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Foundation Papers

Extension Papers

Advanced Papers

Articles

Summary

In this post, you discovered the Generative Adversarial Network models that you need to know to establish a useful and productive foundation in the field

Specifically, you learned:

The foundation GAN models that provide the basis for the field of study.

The extension GAN models that build upon what works and lead the way for more advanced models.

The advanced GAN models that push the limits of the architecture and achieve impressive results.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Generative Adversarial Networks Today! Develop Your GAN Models in Minutes ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Generative Adversarial Networks with Python It provides self-study tutorials and end-to-end projects on:

DCGAN, conditional GANs, image translation, Pix2Pix, CycleGAN

and much more... Finally Bring GAN Models to your Vision Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
316;machinelearningmastery.com;https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/;2018-12-16;How to Avoid Overfitting in Deep Learning Neural Networks;"Tweet Share Share

Last Updated on August 6, 2019

Training a deep neural network that can generalize well to new data is a challenging problem.

A model with too little capacity cannot learn the problem, whereas a model with too much capacity can learn it too well and overfit the training dataset. Both cases result in a model that does not generalize well.

A modern approach to reducing generalization error is to use a larger model that may be required to use regularization during training that keeps the weights of the model small. These techniques not only reduce overfitting, but they can also lead to faster optimization of the model and better overall performance.

In this post, you will discover the problem of overfitting when training neural networks and how it can be addressed with regularization methods.

After reading this post, you will know:

Underfitting can easily be addressed by increasing the capacity of the network, but overfitting requires the use of specialized techniques.

Regularization methods like weight decay provide an easy way to control overfitting for large neural network models.

A modern recommendation for regularization is to use early stopping with dropout and a weight constraint.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

The Problem of Model Generalization and Overfitting Reduce Overfitting by Constraining Model Complexity Methods for Regularization Regularization Recommendations

The Problem of Model Generalization and Overfitting

The objective of a neural network is to have a final model that performs well both on the data that we used to train it (e.g. the training dataset) and the new data on which the model will be used to make predictions.

The central challenge in machine learning is that we must perform well on new, previously unseen inputs — not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.

— Page 110, Deep Learning, 2016.

We require that the model learn from known examples and generalize from those known examples to new examples in the future. We use methods like a train/test split or k-fold cross-validation only to estimate the ability of the model to generalize to new data.

Learning and also generalizing to new cases is hard.

Too little learning and the model will perform poorly on the training dataset and on new data. The model will underfit the problem. Too much learning and the model will perform well on the training dataset and poorly on new data, the model will overfit the problem. In both cases, the model has not generalized.

Underfit Model . A model that fails to sufficiently learn the problem and performs poorly on a training dataset and does not perform well on a holdout sample.

. A model that fails to sufficiently learn the problem and performs poorly on a training dataset and does not perform well on a holdout sample. Overfit Model . A model that learns the training dataset too well, performing well on the training dataset but does not perform well on a hold out sample.

. A model that learns the training dataset too well, performing well on the training dataset but does not perform well on a hold out sample. Good Fit Model. A model that suitably learns the training dataset and generalizes well to the old out dataset.

A model fit can be considered in the context of the bias-variance trade-off.

An underfit model has high bias and low variance. Regardless of the specific samples in the training data, it cannot learn the problem. An overfit model has low bias and high variance. The model learns the training data too well and performance varies widely with new unseen examples or even statistical noise added to examples in the training dataset.

In order to generalize well, a system needs to be sufficiently powerful to approximate the target function. If it is too simple to fit even the training data then generalization to new data is also likely to be poor. […] An overly complex system, however, may be able to approximate the data in many different ways that give similar errors and is unlikely to choose the one that will generalize best …

— Page 241, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

We can address underfitting by increasing the capacity of the model. Capacity refers to the ability of a model to fit a variety of functions; more capacity, means that a model can fit more types of functions for mapping inputs to outputs. Increasing the capacity of a model is easily achieved by changing the structure of the model, such as adding more layers and/or more nodes to layers.

Because an underfit model is so easily addressed, it is more common to have an overfit model.

An overfit model is easily diagnosed by monitoring the performance of the model during training by evaluating it on both a training dataset and on a holdout validation dataset. Graphing line plots of the performance of the model during training, called learning curves, will show a familiar pattern.

For example, line plots of the loss (that we seek to minimize) of the model on train and validation datasets will show a line for the training dataset that drops and may plateau and a line for the validation dataset that drops at first, then at some point begins to rise again.

As training progresses, the generalization error may decrease to a minimum and then increase again as the network adapts to idiosyncrasies of the training data.

— Page 250, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

A learning curve plot tells the story of the model learning the problem until a point at which it begins overfitting and its ability to generalize to the unseen validation dataset begins to get worse.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Reduce Overfitting by Constraining Model Complexity

There are two ways to approach an overfit model:

Reduce overfitting by training the network on more examples. Reduce overfitting by changing the complexity of the network.

A benefit of very deep neural networks is that their performance continues to improve as they are fed larger and larger datasets. A model with a near-infinite number of examples will eventually plateau in terms of what the capacity of the network is capable of learning.

A model can overfit a training dataset because it has sufficient capacity to do so. Reducing the capacity of the model reduces the likelihood of the model overfitting the training dataset, to a point where it no longer overfits.

The capacity of a neural network model, it’s complexity, is defined by both it’s structure in terms of nodes and layers and the parameters in terms of its weights. Therefore, we can reduce the complexity of a neural network to reduce overfitting in one of two ways:

Change network complexity by changing the network structure (number of weights). Change network complexity by changing the network parameters (values of weights).

In the case of neural networks, the complexity can be varied by changing the number of adaptive parameters in the network. This is called structural stabilization. […] The second principal approach to controlling the complexity of a model is through the use of regularization which involves the addition of a penalty term to the error function.

— Page 332, Neural Networks for Pattern Recognition, 1995.

For example, the structure could be tuned such as via grid search until a suitable number of nodes and/or layers is found to reduce or remove overfitting for the problem. Alternately, the model could be overfit and pruned by removing nodes until it achieves suitable performance on a validation dataset.

It is more common to instead constrain the complexity of the model by ensuring the parameters (weights) of the model remain small. Small parameters suggest a less complex and, in turn, more stable model that is less sensitive to statistical fluctuations in the input data.

Large weighs tend to cause sharp transitions in the [activation] functions and thus large changes in output for small changes in inputs.

— Page 269, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

It is more common to focus on methods that constrain the size of the weights in a neural network because a single network structure can be defined that is under-constrained, e.g. has a much larger capacity than is required for the problem, and regularization can be used during training to ensure that the model does not overfit. In such cases, performance can even be better as the additional capacity can be focused on better learning generalizable concepts in the problem.

Techniques that seek to reduce overfitting (reduce generalization error) by keeping network weights small are referred to as regularization methods. More specifically, regularization refers to a class of approaches that add additional information to transform an ill-posed problem into a more stable well-posed problem.

A problem is said to be ill-posed if small changes in the given information cause large changes in the solution. This instability with respect to the data makes solutions unreliable because small measurement errors or uncertainties in parameters may be greatly magnified and lead to wildly different responses. […] The idea behind regularization is to use supplementary information to restate an ill-posed problem in a stable form.

— Page 266, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

Regularization methods are so widely used to reduce overfitting that the term “regularization” may be used for any method that improves the generalization error of a neural network model.

Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization.

— Page 120, Deep Learning, 2016.

Regularization Methods for Neural Networks

The simplest and perhaps most common regularization method is to add a penalty to the loss function in proportion to the size of the weights in the model.

Weight Regularization (weight decay): Penalize the model during training based on the magnitude of the weights.

This will encourage the model to map the inputs to the outputs of the training dataset in such a way that the weights of the model are kept small. This approach is called weight regularization or weight decay and has proven very effective for decades for both simpler linear models and neural networks.

A simple alternative to gathering more data is to reduce the size of the model or improve regularization, by adjusting hyperparameters such as weight decay coefficients …

— Page 427, Deep Learning, 2016.

Below is a list of five of the most common additional regularization methods.

Activity Regularization: Penalize the model during training base on the magnitude of the activations. Weight Constraint: Constrain the magnitude of weights to be within a range or below a limit. Dropout: Probabilistically remove inputs during training. Noise: Add statistical noise to inputs during training. Early Stopping: Monitor model performance on a validation set and stop training when performance degrades.

Most of these methods have been demonstrated (or proven) to approximate the effect of adding a penalty to the loss function.

Each method approaches the problem differently, offering benefits in terms of a mixture of generalization performance, configurability, and/or computational complexity.

Regularization Recommendations

This section outlines some recommendations for using regularization methods for deep learning neural networks.

You should always consider using regularization, unless you have a very large dataset, e.g. big-data scale.

Unless your training set contains tens of millions of examples or more, you should include some mild forms of regularization from the start.

— Page 426, Deep Learning, 2016.

A good general recommendation is to design a neural network structure that is under-constrained and to use regularization to reduce the likelihood of overfitting.

… controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, … in practical deep learning scenarios, we almost always do find—that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately.

— Page 229, Deep Learning, 2016.

Early stopping should almost universally be used in addition to a method to keep weights small during training.

Early stopping should be used almost universally.

— Page 426, Deep Learning, 2016.

Some more specific recommendations include:

Classical : use early stopping and weight decay (L2 weight regularization).

: use early stopping and weight decay (L2 weight regularization). Alternate : use early stopping and added noise with a weight constraint.

: use early stopping and added noise with a weight constraint. Modern: use early stopping and dropout, in addition to a weight constraint.

These recommendations would suit Multilayer Perceptrons and Convolutional Neural Networks.

Some recommendations for recurrent neural nets include:

Classical : use early stopping with added weight noise and a weight constraint such as maximum norm.

: use early stopping with added weight noise and a weight constraint such as maximum norm. Modern: use early stopping with a backpropagation-through-time-aware version of dropout and a weight constraint.

There are no silver bullets when it comes to regularization and systematic experimentation is strongly encouraged.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Articles

Summary

In this post, you discovered the problem of overfitting when training neural networks and how it can be addressed with regularization methods.

Specifically, you learned:

Underfitting can easily be addressed by increasing the capacity of the network, but overfitting requires the use of specialized techniques.

Regularization methods like weight decay provide an easy way to control overfitting for large neural network models.

A modern recommendation for regularization is to use early stopping with dropout and a weight constraint.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
317;machinelearningmastery.com;https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/;2018-01-02;How to Configure an Encoder-Decoder Model for Neural Machine Translation;"Tweet Share Share

Last Updated on August 7, 2019

The encoder-decoder architecture for recurrent neural networks is achieving state-of-the-art results on standard machine translation benchmarks and is being used in the heart of industrial translation services.

The model is simple, but given the large amount of data required to train it, tuning the myriad of design decisions in the model in order get top performance on your problem can be practically intractable. Thankfully, research scientists have used Google-scale hardware to do this work for us and provide a set of heuristics for how to configure the encoder-decoder model for neural machine translation and for sequence prediction generally.

In this post, you will discover the details of how to best configure an encoder-decoder recurrent neural network for neural machine translation and other natural language processing tasks.

After reading this post, you will know:

The Google study that investigated each model design decision in the encoder-decoder model to isolate their effects.

The results and recommendations for design decisions like word embeddings, encoder and decoder depth, and attention mechanisms.

A set of base model design decisions that can be used as a starting point on your own sequence-to-sequence projects.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Encoder-Decoder Model for Neural Machine Translation

The Encoder-Decoder architecture for recurrent neural networks is displacing classical phrase-based statistical machine translation systems for state-of-the-art results.

As evidence, by their 2016 paper “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,” Google now uses the approach in their core of their Google Translate service.

A problem with this architecture is that the models are large, in turn requiring very large datasets on which to train. This has the effect of model training taking days or weeks and requiring computational resources that are generally very expensive. As such, little work has been done on the impact of different design choices on the model and their impact on model skill.

This problem is addressed explicitly by Denny Britz, et al. in their 2017 paper “Massive Exploration of Neural Machine Translation Architectures.” In the paper, they design a baseline model for a standard English-to-German translation task and enumerate a suite of different model design choices and describe their impact on the skill of the model. They claim that the complete set of experiments consumed more than 250,000 GPU compute hours, which is impressive, to say the least.

We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures.

In this post, we will look at some of the findings from this paper that we can use to tune our own neural machine translation models, as well as sequence-to-sequence models in general.

For more background on the Encoder-Decoder architecture and the attention mechanism, see the posts:

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

Baseline Model

We can start-off by describing the baseline model used as the starting point for all experiments.

A baseline model configuration was chosen such that the model would perform reasonably well on the translation task.

Embedding: 512-dimensions

RNN Cell: Gated Recurrent Unit or GRU

Encoder: Bidirectional

Encoder Depth: 2-layers (1 layer in each direction)

Decoder Depth: 2-layers

Attention: Bahdanau-style

Optimizer: Adam

Dropout: 20% on input

Each experiment started with the baseline model and varied one element in an attempt to isolate the impact of the design decision on the model skill, in this case, BLEU scores.

Embedding Size

A word-embedding is used to represent words input to the encoder.

This is a distributed representation where each word is mapped to a fixed-sized vector of continuous values. The benefit of this approach is that different words with similar meaning will have a similar representation.

This distributed representation is often learned while fitting the model on the training data. The embedding size defines the length of the vectors used to represent words. It is generally believed that a larger dimensionality will result in a more expressive representation, and in turn, better skill.

Interestingly, the results show that the largest size tested did achieve the best results, but the benefit of increasing the size was minor overall.

[results show] that 2048-dimensional embeddings yielded the overall best result, they only did so by a small margin. Even small 128-dimensional embeddings performed surprisingly well, while converging almost twice as quickly.

Recommendation: Start with a small embedding, such as 128, perhaps increase the size later for a minor lift in skill.

RNN Cell Type

There are generally three types of recurrent neural network cells that are commonly used:

Simple RNN.

Long Short-Term Memory or LSTM.

Gated Recurrent Unit or GRU.

The LSTM was developed to address the vanishing gradient problem of the Simple RNN that limited the training of deep RNNs. The GRU was developed in an attempt to simplify the LSTM.

Results showed that both the GRU and LSTM were significantly better than the Simple RNN, but the LSTM was generally better overall.

In our experiments, LSTM cells consistently outperformed GRU cells

Recommendation: Use LSTM RNN units in your model.

Encoder-Decoder Depth

Generally, deeper networks are believed to achieve better performance than shallow networks.

The key is to find a balance between network depth, model skill, and training time. This is because we generally do not have infinite resources to train very deep networks if the benefit to skill is minor.

The authors explore the depth of both the encoder and decoder models and the impact on model skill.

When it comes to encoders, it was found that depth did not have a dramatic impact on skill and more surprisingly, a 1-layer unidirectional model performs only slightly worse than a 4-layer unidirectional configuration. A two-layer bidirectional encoder performed slightly better than other configurations tested.

We found no clear evidence that encoder depth beyond two layers is necessary.

Recommendation: Use a 1-layer bidirectional encoder and extend to 2 bidirectional layers for a small lift in skill.

A similar story was seen when it came to decoders. The skill between decoders with 1, 2, and 4 layers was different by a small amount where a 4-layer decoder was slightly better. An 8-layer decoder did not converge under the test conditions.

On the decoder side, deeper models outperformed shallower ones by a small margin.

Recommendation: Use a 1-layer decoder as a starting point and use a 4-layer decoder for better results.

Direction of Encoder Input

The order of the sequence of source text can be provided to the encoder a number of ways:

Forward or as-normal.

Reversed.

Both forward and reversed at the same time.

The authors explored the impact of the order of the input sequence on model skill comparing various unidirectional and bidirectional configurations.

Generally, they confirmed previous findings that a reversed sequence is better than a forward sequence and that bidirectional is slightly better than a reversed sequence.

… bidirectional encoders generally outperform unidirectional encoders, but not by a large margin. The encoders with reversed source consistently outperform their non-reversed counterparts.

Recommendation: Use a reversed order input sequence or move to bidirectional for a small lift in model skill.

Attention Mechanism

A problem with the naive Encoder-Decoder model is that the encoder maps the input to a fixed-length internal representation from which the decoder must produce the entire output sequence.

Attention is an improvement to the model that allows the decoder to “pay attention” to different words in the input sequence as it outputs each word in the output sequence.

The authors look at a few variations on simple attention mechanisms. The results show that having attention results in dramatically better performance than not having attention.

While we did expect the attention-based models to significantly outperform those without an attention mechanism, we were surprised by just how poorly the [no attention] models fared.

The simple weighted average style attention described by Bahdanau, et al. in their 2015 paper “Neural machine translation by jointly learning to align and translate” was found to perform the best.

Recommendation: Use attention and prefer the Bahdanau-style weighted average style attention.

Inference

It is common in neural machine translation systems to use a beam-search to sample the probabilities for the words in the sequence output by the model.

The wider the beam width, the more exhaustive the search, and, it is believed, the better the results.

The results showed that a modest beam-width of 3-5 performed the best, which could be improved only very slightly through the use of length penalties. The authors generally recommend tuning the beam width on each specific problem.

We found that a well-tuned beam search is crucial to achieving good results, and that it leads to consistent gains of more than one BLEU point.

Recommendation: Start with a greedy search (beam=1) and tune based on your problem.

Final Model

The authors pull together their findings into a single “best model” and compare the results of this model to other well-performing models and state-of-the-art results.

The specific configurations of this model are summarized in the table below, taken from the paper. These parameters may be taken as a good or best starting point when developing your own encoder-decoder model for an NLP application.

The results of the system were shown to be impressive and achieve skill close to state-of-the-art with a simpler model, which was not the goal of the paper.

… we do show that through careful hyperparameter tuning and good initialization, it is possible to achieve state-of-the-art performance on standard WMT benchmarks

Importantly, the authors provide all of their code as an open source project called tf-seq2seq. Because two of the authors were members of the Google Brain residency program, their work was announced on the Google Research blog with the title “Introducing tf-seq2seq: An Open Source Sequence-to-Sequence Framework in TensorFlow“, 2017.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this post, you discovered how to best configure an encoder-decoder recurrent neural network for neural machine translation and other natural language processing tasks.

Specifically, you learned:

The Google study that investigated each model design decision in the encoder-decoder model to isolate their effects.

The results and recommendations for design decisions like word embeddings, encoder and decoder depth, and attention mechanisms.

A set of base model design decisioning that can be used as a starting point on your own sequence to sequence projects.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside"
318;machinelearningmastery.com;https://machinelearningmastery.com/types-of-classification-in-machine-learning/;2020-04-07;4 Types of Classification Tasks in Machine Learning;"# example of binary classification task

from numpy import where

from collections import Counter

from sklearn . datasets import make_blobs

from matplotlib import pyplot

# define dataset

X , y = make_blobs ( n_samples = 1000 , centers = 2 , random_state = 1 )

# summarize dataset shape

print ( X . shape , y . shape )

# summarize observations by class label

counter = Counter ( y )

print ( counter )

# summarize first few examples

for i in range ( 10 ) :

print ( X [ i ] , y [ i ] )

# plot the dataset and color the by class label

for label , _ in counter . items ( ) :

row_ix = where ( y == label ) [ 0 ]

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( label ) )

pyplot . legend ( )"
319;machinelearningmastery.com;https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/;2019-04-11;How to Configure Image Data Augmentation in Keras;"Tweet Share Share

Last Updated on July 5, 2019

Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.

Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.

The Keras deep learning neural network library provides the capability to fit models using image data augmentation via the ImageDataGenerator class.

In this tutorial, you will discover how to use image data augmentation when training deep learning neural networks.

After completing this tutorial, you will know:

Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.

Image data augmentation is supported in the Keras deep learning library via the ImageDataGenerator class.

How to use shift, flip, brightness, and zoom image data augmentation.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Update May/2019 : Fixed data type for pixel values when plotting.

: Fixed data type for pixel values when plotting. Update Jun/2019: Fixed small typo in API example (thanks Georgios).

Tutorial Overview

This tutorial is divided into eight parts; they are:

Image Data Augmentation Sample Image Image Augmentation With ImageDataGenerator Horizontal and Vertical Shift Augmentation Horizontal and Vertical Flip Augmentation Random Rotation Augmentation Random Brightness Augmentation Random Zoom Augmentation

Image Data Augmentation

The performance of deep learning neural networks often improves with the amount of data available.

Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.

Image data augmentation is perhaps the most well-known type of data augmentation and involves creating transformed versions of images in the training dataset that belong to the same class as the original image.

Transforms include a range of operations from the field of image manipulation, such as shifts, flips, zooms, and much more.

The intent is to expand the training dataset with new, plausible examples. This means, variations of the training set images that are likely to be seen by the model. For example, a horizontal flip of a picture of a cat may make sense, because the photo could have been taken from the left or right. A vertical flip of the photo of a cat does not make sense and would probably not be appropriate given that the model is very unlikely to see a photo of an upside down cat.

As such, it is clear that the choice of the specific data augmentation techniques used for a training dataset must be chosen carefully and within the context of the training dataset and knowledge of the problem domain. In addition, it can be useful to experiment with data augmentation methods in isolation and in concert to see if they result in a measurable improvement to model performance, perhaps with a small prototype dataset, model, and training run.

Modern deep learning algorithms, such as the convolutional neural network, or CNN, can learn features that are invariant to their location in the image. Nevertheless, augmentation can further aid in this transform invariant approach to learning and can aid the model in learning features that are also invariant to transforms such as left-to-right to top-to-bottom ordering, light levels in photographs, and more.

Image data augmentation is typically only applied to the training dataset, and not to the validation or test dataset. This is different from data preparation such as image resizing and pixel scaling; they must be performed consistently across all datasets that interact with the model.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Sample Image

We need a sample image to demonstrate standard data augmentation techniques.

In this tutorial, we will use a photograph of a bird titled “Feathered Friend” by AndYaDontStop, released under a permissive license.

Download the image and save it in your current working directory with the filename ‘bird.jpg‘.

Image Augmentation With ImageDataGenerator

The Keras deep learning library provides the ability to use data augmentation automatically when training a model.

This is achieved by using the ImageDataGenerator class.

First, the class may be instantiated and the configuration for the types of data augmentation are specified by arguments to the class constructor.

A range of techniques are supported, as well as pixel scaling methods. We will focus on five main types of data augmentation techniques for image data; specifically:

Image shifts via the width_shift_range and height_shift_range arguments.

Image flips via the horizontal_flip and vertical_flip arguments.

Image rotations via the rotation_range argument

Image brightness via the brightness_range argument.

Image zoom via the zoom_range argument.

For example, an instance of the ImageDataGenerator class can be constructed.

... # create data generator datagen = ImageDataGenerator() 1 2 3 . . . # create data generator datagen = ImageDataGenerator ( )

Once constructed, an iterator can be created for an image dataset.

The iterator will return one batch of augmented images for each iteration.

An iterator can be created from an image dataset loaded in memory via the flow() function; for example:

... # load image dataset X, y = ... # create iterator it = datagen.flow(X, y) 1 2 3 4 5 . . . # load image dataset X , y = . . . # create iterator it = datagen . flow ( X , y )

Alternately, an iterator can be created for an image dataset located on disk in a specified directory, where images in that directory are organized into subdirectories according to their class.

... # create iterator it = datagen.flow_from_directory(X, y, ...) 1 2 3 . . . # create iterator it = datagen . flow_from_directory ( X , y , . . . )

Once the iterator is created, it can be used to train a neural network model by calling the fit_generator() function.

The steps_per_epoch argument must specify the number of batches of samples comprising one epoch. For example, if your original dataset has 10,000 images and your batch size is 32, then a reasonable value for steps_per_epoch when fitting a model on the augmented data might be ceil(10,000/32), or 313 batches.

# define model model = ... # fit model on the augmented dataset model.fit_generator(it, steps_per_epoch=313, ...) 1 2 3 4 # define model model = . . . # fit model on the augmented dataset model . fit_generator ( it , steps_per_epoch = 313 , . . . )

The images in the dataset are not used directly. Instead, only augmented images are provided to the model. Because the augmentations are performed randomly, this allows both modified images and close facsimiles of the original images (e.g. almost no augmentation) to be generated and used during training.

A data generator can also be used to specify the validation dataset and the test dataset. Often, a separate ImageDataGenerator instance is used that may have the same pixel scaling configuration (not covered in this tutorial) as the ImageDataGenerator instance used for the training dataset, but would not use data augmentation. This is because data augmentation is only used as a technique for artificially extending the training dataset in order to improve model performance on an unaugmented dataset.

Now that we are familiar with how to use the ImageDataGenerator, let’s look at some specific data augmentation techniques for image data.

We will demonstrate each technique standalone by reviewing examples of images after they have been augmented. This is a good practice and is recommended when configuring your data augmentation. It is also common to use a range of augmentation techniques at the same time when training. We have isolated the techniques to one per section for demonstration purposes only.

Horizontal and Vertical Shift Augmentation

A shift to an image means moving all pixels of the image in one direction, such as horizontally or vertically, while keeping the image dimensions the same.

This means that some of the pixels will be clipped off the image and there will be a region of the image where new pixel values will have to be specified.

The width_shift_range and height_shift_range arguments to the ImageDataGenerator constructor control the amount of horizontal and vertical shift respectively.

These arguments can specify a floating point value that indicates the percentage (between 0 and 1) of the width or height of the image to shift. Alternately, a number of pixels can be specified to shift the image.

Specifically, a value in the range between no shift and the percentage or pixel value will be sampled for each image and the shift performed, e.g. [0, value]. Alternately, you can specify a tuple or array of the min and max range from which the shift will be sampled; for example: [-100, 100] or [-0.5, 0.5].

The example below demonstrates a horizontal shift with the width_shift_range argument between [-200,200] pixels and generates a plot of generated images to demonstrate the effect.

# example of horizontal shift image augmentation from numpy import expand_dims from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array from keras.preprocessing.image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img('bird.jpg') # convert to numpy array data = img_to_array(img) # expand dimension to one sample samples = expand_dims(data, 0) # create image data augmentation generator datagen = ImageDataGenerator(width_shift_range=[-200,200]) # prepare iterator it = datagen.flow(samples, batch_size=1) # generate samples and plot for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # generate batch of images batch = it.next() # convert to unsigned integers for viewing image = batch[0].astype('uint8') # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # example of horizontal shift image augmentation from numpy import expand_dims from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from keras . preprocessing . image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img ( 'bird.jpg' ) # convert to numpy array data = img_to_array ( img ) # expand dimension to one sample samples = expand_dims ( data , 0 ) # create image data augmentation generator datagen = ImageDataGenerator ( width_shift_range = [ - 200 , 200 ] ) # prepare iterator it = datagen . flow ( samples , batch_size = 1 ) # generate samples and plot for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # generate batch of images batch = it . next ( ) # convert to unsigned integers for viewing image = batch [ 0 ] . astype ( 'uint8' ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example creates the instance of ImageDataGenerator configured for image augmentation, then creates the iterator. The iterator is then called nine times in a loop and each augmented image is plotted.

We can see in the plot of the result that a range of different randomly selected positive and negative horizontal shifts was performed and the pixel values at the edge of the image are duplicated to fill in the empty part of the image created by the shift.

Below is the same example updated to perform vertical shifts of the image via the height_shift_range argument, in this case specifying the percentage of the image to shift as 0.5 the height of the image.

# example of vertical shift image augmentation from numpy import expand_dims from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array from keras.preprocessing.image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img('bird.jpg') # convert to numpy array data = img_to_array(img) # expand dimension to one sample samples = expand_dims(data, 0) # create image data augmentation generator datagen = ImageDataGenerator(height_shift_range=0.5) # prepare iterator it = datagen.flow(samples, batch_size=1) # generate samples and plot for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # generate batch of images batch = it.next() # convert to unsigned integers for viewing image = batch[0].astype('uint8') # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # example of vertical shift image augmentation from numpy import expand_dims from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from keras . preprocessing . image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img ( 'bird.jpg' ) # convert to numpy array data = img_to_array ( img ) # expand dimension to one sample samples = expand_dims ( data , 0 ) # create image data augmentation generator datagen = ImageDataGenerator ( height_shift_range = 0.5 ) # prepare iterator it = datagen . flow ( samples , batch_size = 1 ) # generate samples and plot for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # generate batch of images batch = it . next ( ) # convert to unsigned integers for viewing image = batch [ 0 ] . astype ( 'uint8' ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example creates a plot of images augmented with random positive and negative vertical shifts.

We can see that both horizontal and vertical positive and negative shifts probably make sense for the chosen photograph, but in some cases, the replicated pixels at the edge of the image may not make sense to a model.

Note that other fill modes can be specified via “fill_mode” argument.

Horizontal and Vertical Flip Augmentation

An image flip means reversing the rows or columns of pixels in the case of a vertical or horizontal flip respectively.

The flip augmentation is specified by a boolean horizontal_flip or vertical_flip argument to the ImageDataGenerator class constructor. For photographs like the bird photograph used in this tutorial, horizontal flips may make sense, but vertical flips would not.

For other types of images, such as aerial photographs, cosmology photographs, and microscopic photographs, perhaps vertical flips make sense.

The example below demonstrates augmenting the chosen photograph with horizontal flips via the horizontal_flip argument.

# example of horizontal flip image augmentation from numpy import expand_dims from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array from keras.preprocessing.image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img('bird.jpg') # convert to numpy array data = img_to_array(img) # expand dimension to one sample samples = expand_dims(data, 0) # create image data augmentation generator datagen = ImageDataGenerator(horizontal_flip=True) # prepare iterator it = datagen.flow(samples, batch_size=1) # generate samples and plot for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # generate batch of images batch = it.next() # convert to unsigned integers for viewing image = batch[0].astype('uint8') # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # example of horizontal flip image augmentation from numpy import expand_dims from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from keras . preprocessing . image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img ( 'bird.jpg' ) # convert to numpy array data = img_to_array ( img ) # expand dimension to one sample samples = expand_dims ( data , 0 ) # create image data augmentation generator datagen = ImageDataGenerator ( horizontal_flip = True ) # prepare iterator it = datagen . flow ( samples , batch_size = 1 ) # generate samples and plot for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # generate batch of images batch = it . next ( ) # convert to unsigned integers for viewing image = batch [ 0 ] . astype ( 'uint8' ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example creates a plot of nine augmented images.

We can see that the horizontal flip is applied randomly to some images and not others.

Random Rotation Augmentation

A rotation augmentation randomly rotates the image clockwise by a given number of degrees from 0 to 360.

The rotation will likely rotate pixels out of the image frame and leave areas of the frame with no pixel data that must be filled in.

The example below demonstrates random rotations via the rotation_range argument, with rotations to the image between 0 and 90 degrees.

# example of random rotation image augmentation from numpy import expand_dims from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array from keras.preprocessing.image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img('bird.jpg') # convert to numpy array data = img_to_array(img) # expand dimension to one sample samples = expand_dims(data, 0) # create image data augmentation generator datagen = ImageDataGenerator(rotation_range=90) # prepare iterator it = datagen.flow(samples, batch_size=1) # generate samples and plot for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # generate batch of images batch = it.next() # convert to unsigned integers for viewing image = batch[0].astype('uint8') # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # example of random rotation image augmentation from numpy import expand_dims from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from keras . preprocessing . image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img ( 'bird.jpg' ) # convert to numpy array data = img_to_array ( img ) # expand dimension to one sample samples = expand_dims ( data , 0 ) # create image data augmentation generator datagen = ImageDataGenerator ( rotation_range = 90 ) # prepare iterator it = datagen . flow ( samples , batch_size = 1 ) # generate samples and plot for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # generate batch of images batch = it . next ( ) # convert to unsigned integers for viewing image = batch [ 0 ] . astype ( 'uint8' ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example generates examples of the rotated image, showing in some cases pixels rotated out of the frame and the nearest-neighbor fill.

Random Brightness Augmentation

The brightness of the image can be augmented by either randomly darkening images, brightening images, or both.

The intent is to allow a model to generalize across images trained on different lighting levels.

This can be achieved by specifying the brightness_range argument to the ImageDataGenerator() constructor that specifies min and max range as a float representing a percentage for selecting a brightening amount.

Values less than 1.0 darken the image, e.g. [0.5, 1.0], whereas values larger than 1.0 brighten the image, e.g. [1.0, 1.5], where 1.0 has no effect on brightness.

The example below demonstrates a brightness image augmentation, allowing the generator to randomly darken the image between 1.0 (no change) and 0.2 or 20%.

# example of brighting image augmentation from numpy import expand_dims from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array from keras.preprocessing.image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img('bird.jpg') # convert to numpy array data = img_to_array(img) # expand dimension to one sample samples = expand_dims(data, 0) # create image data augmentation generator datagen = ImageDataGenerator(brightness_range=[0.2,1.0]) # prepare iterator it = datagen.flow(samples, batch_size=1) # generate samples and plot for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # generate batch of images batch = it.next() # convert to unsigned integers for viewing image = batch[0].astype('uint8') # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # example of brighting image augmentation from numpy import expand_dims from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from keras . preprocessing . image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img ( 'bird.jpg' ) # convert to numpy array data = img_to_array ( img ) # expand dimension to one sample samples = expand_dims ( data , 0 ) # create image data augmentation generator datagen = ImageDataGenerator ( brightness_range = [ 0.2 , 1.0 ] ) # prepare iterator it = datagen . flow ( samples , batch_size = 1 ) # generate samples and plot for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # generate batch of images batch = it . next ( ) # convert to unsigned integers for viewing image = batch [ 0 ] . astype ( 'uint8' ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example shows the augmented images with varying amounts of darkening applied.

Random Zoom Augmentation

A zoom augmentation randomly zooms the image in and either adds new pixel values around the image or interpolates pixel values respectively.

Image zooming can be configured by the zoom_range argument to the ImageDataGenerator constructor. You can specify the percentage of the zoom as a single float or a range as an array or tuple.

If a float is specified, then the range for the zoom will be [1-value, 1+value]. For example, if you specify 0.3, then the range will be [0.7, 1.3], or between 70% (zoom in) and 130% (zoom out).

The zoom amount is uniformly randomly sampled from the zoom region for each dimension (width, height) separately.

The zoom may not feel intuitive. Note that zoom values less than 1.0 will zoom the image in, e.g. [0.5,0.5] makes the object in the image 50% larger or closer, and values larger than 1.0 will zoom the image out by 50%, e.g. [1.5, 1.5] makes the object in the image smaller or further away. A zoom of [1.0,1.0] has no effect.

The example below demonstrates zooming the image in, e.g. making the object in the photograph larger.

# example of zoom image augmentation from numpy import expand_dims from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array from keras.preprocessing.image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img('bird.jpg') # convert to numpy array data = img_to_array(img) # expand dimension to one sample samples = expand_dims(data, 0) # create image data augmentation generator datagen = ImageDataGenerator(zoom_range=[0.5,1.0]) # prepare iterator it = datagen.flow(samples, batch_size=1) # generate samples and plot for i in range(9): # define subplot pyplot.subplot(330 + 1 + i) # generate batch of images batch = it.next() # convert to unsigned integers for viewing image = batch[0].astype('uint8') # plot raw pixel data pyplot.imshow(image) # show the figure pyplot.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # example of zoom image augmentation from numpy import expand_dims from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from keras . preprocessing . image import ImageDataGenerator from matplotlib import pyplot # load the image img = load_img ( 'bird.jpg' ) # convert to numpy array data = img_to_array ( img ) # expand dimension to one sample samples = expand_dims ( data , 0 ) # create image data augmentation generator datagen = ImageDataGenerator ( zoom_range = [ 0.5 , 1.0 ] ) # prepare iterator it = datagen . flow ( samples , batch_size = 1 ) # generate samples and plot for i in range ( 9 ) : # define subplot pyplot . subplot ( 330 + 1 + i ) # generate batch of images batch = it . next ( ) # convert to unsigned integers for viewing image = batch [ 0 ] . astype ( 'uint8' ) # plot raw pixel data pyplot . imshow ( image ) # show the figure pyplot . show ( )

Running the example generates examples of the zoomed image, showing a random zoom in that is different on both the width and height dimensions that also randomly changes the aspect ratio of the object in the image.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Posts

API

Articles

Summary

In this tutorial, you discovered how to use image data augmentation when training deep learning neural networks.

Specifically, you learned:

Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.

Image data augmentation is supported in the Keras deep learning library via the ImageDataGenerator class.

How to use shift, flip, brightness, and zoom image data augmentation.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning Models for Vision Today! Develop Your Own Vision Models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Computer Vision It provides self-study tutorials on topics like:

classification, object detection (yolo and rcnn), face recognition (vggface and facenet), data preparation and much more... Finally Bring Deep Learning to your Vision Projects Skip the Academics. Just Results. See What's Inside"
320;machinelearningmastery.com;https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/;2019-09-19;Discrete Probability Distributions for Machine Learning;"Tweet Share Share

Last Updated on February 10, 2020

The probability for a discrete random variable can be summarized with a discrete probability distribution.

Discrete probability distributions are used in machine learning, most notably in the modeling of binary and multi-class classification problems, but also in evaluating the performance for binary classification models, such as the calculation of confidence intervals, and in the modeling of the distribution of words in text for natural language processing.

Knowledge of discrete probability distributions is also required in the choice of activation functions in the output layer of deep learning neural networks for classification tasks and selecting an appropriate loss function.

Discrete probability distributions play an important role in applied machine learning and there are a few distributions that a practitioner must know about.

In this tutorial, you will discover discrete probability distributions used in machine learning.

After completing this tutorial, you will know:

The probability of outcomes for discrete random variables can be summarized using discrete probability distributions.

A single binary outcome has a Bernoulli distribution, and a sequence of binary outcomes has a Binomial distribution.

A single categorical outcome has a Multinoulli distribution, and a sequence of categorical outcomes has a Multinomial distribution.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into five parts; they are:

Discrete Probability Distributions Bernoulli Distribution Binomial Distribution Multinoulli Distribution Multinomial Distribution

Discrete Probability Distributions

A random variable is the quantity produced by a random process.

A discrete random variable is a random variable that can have one of a finite set of specific outcomes. The two types of discrete random variables most commonly used in machine learning are binary and categorical.

Binary Random Variable : x in {0, 1}

: x in {0, 1} Categorical Random Variable: x in {1, 2, …, K}.

A binary random variable is a discrete random variable where the finite set of outcomes is in {0, 1}. A categorical random variable is a discrete random variable where the finite set of outcomes is in {1, 2, …, K}, where K is the total number of unique outcomes.

Each outcome or event for a discrete random variable has a probability.

The relationship between the events for a discrete random variable and their probabilities is called the discrete probability distribution and is summarized by a probability mass function, or PMF for short.

For outcomes that can be ordered, the probability of an event equal to or less than a given value is defined by the cumulative distribution function, or CDF for short. The inverse of the CDF is called the percentage-point function and will give the discrete outcome that is less than or equal to a probability.

PMF : Probability Mass Function, returns the probability of a given outcome.

: Probability Mass Function, returns the probability of a given outcome. CDF : Cumulative Distribution Function, returns the probability of a value less than or equal to a given outcome.

: Cumulative Distribution Function, returns the probability of a value less than or equal to a given outcome. PPF: Percent-Point Function, returns a discrete value that is less than or equal to the given probability.

There are many common discrete probability distributions.

The most common are the Bernoulli and Multinoulli distributions for binary and categorical discrete random variables respectively, and the Binomial and Multinomial distributions that generalize each to multiple independent trials.

Binary Random Variable : Bernoulli Distribution

: Bernoulli Distribution Sequence of a Binary Random Variable : Binomial Distribution

: Binomial Distribution Categorical Random Variable : Multinoulli Distribution

: Multinoulli Distribution Sequence of a Categorical Random Variable: Multinomial Distribution

In the following sections, we will take a closer look at each of these distributions in turn.

There are additional discrete probability distributions that you may want to explore, including the Poisson Distribution and the Discrete Uniform Distribution.

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Bernoulli Distribution

The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome as either a 0 or 1.

x in {0, 1}

A “Bernoulli trial” is an experiment or case where the outcome follows a Bernoulli distribution. The distribution and the trial are named after the Swiss mathematician Jacob Bernoulli.

Some common examples of Bernoulli trials include:

The single flip of a coin that may have a heads (0) or a tails (1) outcome.

A single birth of either a boy (0) or a girl (1).

A common example of a Bernoulli trial in machine learning might be a binary classification of a single example as the first class (0) or the second class (1).

The distribution can be summarized by a single variable p that defines the probability of an outcome 1. Given this parameter, the probability for each event can be calculated as follows:

P(x=1) = p

P(x=0) = 1 – p

In the case of flipping a fair coin, the value of p would be 0.5, giving a 50% probability of each outcome.

Binomial Distribution

The repetition of multiple independent Bernoulli trials is called a Bernoulli process.

The outcomes of a Bernoulli process will follow a Binomial distribution. As such, the Bernoulli distribution would be a Binomial distribution with a single trial.

Some common examples of Bernoulli processes include:

A sequence of independent coin flips.

A sequence of independent births.

The performance of a machine learning algorithm on a binary classification problem can be analyzed as a Bernoulli process, where the prediction by the model on an example from a test set is a Bernoulli trial (correct or incorrect).

The Binomial distribution summarizes the number of successes k in a given number of Bernoulli trials n, with a given probability of success for each trial p.

We can demonstrate this with a Bernoulli process where the probability of success is 30% or P(x=1) = 0.3 and the total number of trials is 100 (k=100).

We can simulate the Bernoulli process with randomly generated cases and count the number of successes over the given number of trials. This can be achieved via the binomial() NumPy function. This function takes the total number of trials and probability of success as arguments and returns the number of successful outcomes across the trials for one simulation.

# example of simulating a binomial process and counting success from numpy.random import binomial # define the parameters of the distribution p = 0.3 k = 100 # run a single simulation success = binomial(k, p) print('Total Success: %d' % success) 1 2 3 4 5 6 7 8 # example of simulating a binomial process and counting success from numpy . random import binomial # define the parameters of the distribution p = 0.3 k = 100 # run a single simulation success = binomial ( k , p ) print ( 'Total Success: %d' % success )

We would expect that 30 cases out of 100 would be successful given the chosen parameters (k * p or 100 * 0.3).

A different random sequence of 100 trials will result each time the code is run, so your specific results will differ. Try running the example a few times.

In this case, we can see that we get slightly less than the expected 30 successful trials.

Total Success: 28 1 Total Success: 28

We can calculate the moments of this distribution, specifically the expected value or mean and the variance using the binom.stats() SciPy function.

# calculate moments of a binomial distribution from scipy.stats import binom # define the parameters of the distribution p = 0.3 k = 100 # calculate moments mean, var, _, _ = binom.stats(k, p, moments='mvsk') print('Mean=%.3f, Variance=%.3f' % (mean, var)) 1 2 3 4 5 6 7 8 # calculate moments of a binomial distribution from scipy . stats import binom # define the parameters of the distribution p = 0.3 k = 100 # calculate moments mean , var , _ , _ = binom . stats ( k , p , moments = 'mvsk' ) print ( 'Mean=%.3f, Variance=%.3f' % ( mean , var ) )

Running the example reports the expected value of the distribution, which is 30, as we would expect, as well as the variance of 21, which if we calculate the square root, gives us the standard deviation of about 4.5.

Mean=30.000, Variance=21.000 1 Mean=30.000, Variance=21.000

We can use the probability mass function to calculate the likelihood of different numbers of successful outcomes for a sequence of trials, such as 10, 20, 30, to 100.

We would expect 30 successful outcomes to have the highest probability.

# example of using the pmf for the binomial distribution from scipy.stats import binom # define the parameters of the distribution p = 0.3 k = 100 # define the distribution dist = binom(k, p) # calculate the probability of n successes for n in range(10, 110, 10): print('P of %d success: %.3f%%' % (n, dist.pmf(n)*100)) 1 2 3 4 5 6 7 8 9 10 # example of using the pmf for the binomial distribution from scipy . stats import binom # define the parameters of the distribution p = 0.3 k = 100 # define the distribution dist = binom ( k , p ) # calculate the probability of n successes for n in range ( 10 , 110 , 10 ) : print ( 'P of %d success: %.3f%%' % ( n , dist . pmf ( n ) * 100 ) )

Running the example defines the binomial distribution and calculates the probability for each number of successful outcomes in [10, 100] in groups of 10.

The probabilities are multiplied by 100 to give percentages, and we can see that 30 successful outcomes has the highest probability at about 8.6%.

P of 10 success: 0.000% P of 20 success: 0.758% P of 30 success: 8.678% P of 40 success: 0.849% P of 50 success: 0.001% P of 60 success: 0.000% P of 70 success: 0.000% P of 80 success: 0.000% P of 90 success: 0.000% P of 100 success: 0.000% 1 2 3 4 5 6 7 8 9 10 P of 10 success: 0.000% P of 20 success: 0.758% P of 30 success: 8.678% P of 40 success: 0.849% P of 50 success: 0.001% P of 60 success: 0.000% P of 70 success: 0.000% P of 80 success: 0.000% P of 90 success: 0.000% P of 100 success: 0.000%

Given the probability of success is 30% for one trial, we would expect that a probability of 50 or fewer successes out of 100 trials to be close to 100%. We can calculate this with the cumulative distribution function, demonstrated below.

# example of using the cdf for the binomial distribution from scipy.stats import binom # define the parameters of the distribution p = 0.3 k = 100 # define the distribution dist = binom(k, p) # calculate the probability of <=n successes for n in range(10, 110, 10): print('P of %d success: %.3f%%' % (n, dist.cdf(n)*100)) 1 2 3 4 5 6 7 8 9 10 # example of using the cdf for the binomial distribution from scipy . stats import binom # define the parameters of the distribution p = 0.3 k = 100 # define the distribution dist = binom ( k , p ) # calculate the probability of <=n successes for n in range ( 10 , 110 , 10 ) : print ( 'P of %d success: %.3f%%' % ( n , dist . cdf ( n ) * 100 ) )

Running the example prints each number of successes in [10, 100] in groups of 10 and the probability of achieving that many success or less over 100 trials.

As expected, after 50 successes or less covers 99.999% of the successes expected to happen in this distribution.

P of 10 success: 0.000% P of 20 success: 1.646% P of 30 success: 54.912% P of 40 success: 98.750% P of 50 success: 99.999% P of 60 success: 100.000% P of 70 success: 100.000% P of 80 success: 100.000% P of 90 success: 100.000% P of 100 success: 100.000% 1 2 3 4 5 6 7 8 9 10 P of 10 success: 0.000% P of 20 success: 1.646% P of 30 success: 54.912% P of 40 success: 98.750% P of 50 success: 99.999% P of 60 success: 100.000% P of 70 success: 100.000% P of 80 success: 100.000% P of 90 success: 100.000% P of 100 success: 100.000%

Multinoulli Distribution

The Multinoulli distribution, also called the categorical distribution, covers the case where an event will have one of K possible outcomes.

x in {1, 2, 3, …, K}

It is a generalization of the Bernoulli distribution from a binary variable to a categorical variable, where the number of cases K for the Bernoulli distribution is set to 2, K=2.

A common example that follows a Multinoulli distribution is:

A single roll of a die that will have an outcome in {1, 2, 3, 4, 5, 6}, e.g. K=6.

A common example of a Multinoulli distribution in machine learning might be a multi-class classification of a single example into one of K classes, e.g. one of three different species of the iris flower.

The distribution can be summarized with K variables from p1 to pK, each defining the probability of a given categorical outcome from 1 to K, and where all probabilities sum to 1.0.

P(x=1) = p1

P(x=2) = p1

P(x=3) = p3

…

P(x=K) = pK

In the case of a single roll of a die, the probabilities for each value would be 1/6, or about 0.166 or about 16.6%.

Multinomial Distribution

The repetition of multiple independent Multinoulli trials will follow a multinomial distribution.

The multinomial distribution is a generalization of the binomial distribution for a discrete variable with K outcomes.

An example of a multinomial process includes a sequence of independent dice rolls.

A common example of the multinomial distribution is the occurrence counts of words in a text document, from the field of natural language processing.

A multinomial distribution is summarized by a discrete random variable with K outcomes, a probability for each outcome from p1 to pK, and n successive trials.

We can demonstrate this with a small example with 3 categories (K=3) with equal probability (p=33.33%) and 100 trials.

Firstly, we can use the multinomial() NumPy function to simulate 100 independent trials and summarize the number of times that the event resulted in each of the given categories. The function takes both the number of trials and the probabilities for each category as a list.

The complete example is listed below.

# example of simulating a multinomial process from numpy.random import multinomial # define the parameters of the distribution p = [1.0/3.0, 1.0/3.0, 1.0/3.0] k = 100 # run a single simulation cases = multinomial(k, p) # summarize cases for i in range(len(cases)): print('Case %d: %d' % (i+1, cases[i])) 1 2 3 4 5 6 7 8 9 10 # example of simulating a multinomial process from numpy . random import multinomial # define the parameters of the distribution p = [ 1.0 / 3.0 , 1.0 / 3.0 , 1.0 / 3.0 ] k = 100 # run a single simulation cases = multinomial ( k , p ) # summarize cases for i in range ( len ( cases ) ) : print ( 'Case %d: %d' % ( i + 1 , cases [ i ] ) )

We would expect each category to have about 33 events.

Running the example reports each case and the number of events.

A different random sequence of 100 trials will result each time the code is run, so your specific results will differ. Try running the example a few times.

In this case, we see a spread of cases as high as 37 and as low as 30.

Case 1: 37 Case 2: 33 Case 3: 30 1 2 3 Case 1: 37 Case 2: 33 Case 3: 30

We might expect the idealized case of 100 trials to result in 33, 33, and 34 cases for events 1, 2 and 3 respectively.

We can calculate the probability of this specific combination occurring in practice using the probability mass function or multinomial.pmf() SciPy function.

The complete example is listed below.

# calculate the probability for a given number of events of each type from scipy.stats import multinomial # define the parameters of the distribution p = [1.0/3.0, 1.0/3.0, 1.0/3.0] k = 100 # define the distribution dist = multinomial(k, p) # define a specific number of outcomes from 100 trials cases = [33, 33, 34] # calculate the probability for the case pr = dist.pmf(cases) # print as a percentage print('Case=%s, Probability: %.3f%%' % (cases, pr*100)) 1 2 3 4 5 6 7 8 9 10 11 12 13 # calculate the probability for a given number of events of each type from scipy . stats import multinomial # define the parameters of the distribution p = [ 1.0 / 3.0 , 1.0 / 3.0 , 1.0 / 3.0 ] k = 100 # define the distribution dist = multinomial ( k , p ) # define a specific number of outcomes from 100 trials cases = [ 33 , 33 , 34 ] # calculate the probability for the case pr = dist . pmf ( cases ) # print as a percentage print ( 'Case=%s, Probability: %.3f%%' % ( cases , pr* 100 ) )

Running the example reports the probability of less than 1% for the idealized number of cases of [33, 33, 34] for each event type.

Case=[33, 33, 34], Probability: 0.813% 1 Case=[33, 33, 34], Probability: 0.813%

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

API

Articles

Summary

In this tutorial, you discovered discrete probability distributions used in machine learning.

Specifically, you learned:

The probability of outcomes for discrete random variables can be summarized using discrete probability distributions.

A single binary outcome has a Bernoulli distribution, and a sequence of binary outcomes has a Binomial distribution.

A single categorical outcome has a Multinoulli distribution, and a sequence of categorical outcomes has a Multinomial distribution.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Get a Handle on Probability for Machine Learning! Develop Your Understanding of Probability ...with just a few lines of python code ...with just a few lines of python code Discover how in my new Ebook:

Probability for Machine Learning It provides self-study tutorials and end-to-end projects on:

Bayes Theorem, Bayesian Optimization, Distributions, Maximum Likelihood, Cross-Entropy, Calibrating Models

and much more... Finally Harness Uncertainty in Your Projects Skip the Academics. Just Results. Skip the Academics. Just Results. See What's Inside"
321;machinelearningmastery.com;http://machinelearningmastery.com/evaluate-machine-learning-algorithms-with-r/;2016-01-31;How to Evaluate Machine Learning Algorithms with R;"# Linear Discriminant Analysis

set . seed ( seed )

fit . lda < - train ( diabetes ~ . , data = dataset , method = ""lda"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control )

# Logistic Regression

set . seed ( seed )

fit . glm < - train ( diabetes ~ . , data = dataset , method = ""glm"" , metric = metric , trControl = control )

# GLMNET

set . seed ( seed )

fit . glmnet < - train ( diabetes ~ . , data = dataset , method = ""glmnet"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control )

# SVM Radial

set . seed ( seed )

fit . svmRadial < - train ( diabetes ~ . , data = dataset , method = ""svmRadial"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control , fit = FALSE )

# kNN

set . seed ( seed )

fit . knn < - train ( diabetes ~ . , data = dataset , method = ""knn"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control )

# Naive Bayes

set . seed ( seed )

fit . nb < - train ( diabetes ~ . , data = dataset , method = ""nb"" , metric = metric , trControl = control )

# CART

set . seed ( seed )

fit . cart < - train ( diabetes ~ . , data = dataset , method = ""rpart"" , metric = metric , trControl = control )

# C5.0

set . seed ( seed )

fit . c50 < - train ( diabetes ~ . , data = dataset , method = ""C5.0"" , metric = metric , trControl = control )

# Bagged CART

set . seed ( seed )

fit . treebag < - train ( diabetes ~ . , data = dataset , method = ""treebag"" , metric = metric , trControl = control )

# Random Forest

set . seed ( seed )

fit . rf < - train ( diabetes ~ . , data = dataset , method = ""rf"" , metric = metric , trControl = control )

# Stochastic Gradient Boosting (Generalized Boosted Modeling)

set . seed ( seed )"
322;news.mit.edu;http://news.mit.edu/2020/can-financial-disclosure-climate-risk-accelerate-climate-action-0416;;Can financial disclosure of climate risk accelerate climate action?;"The Covid-19 pandemic could be a dry run for future impacts of climate change, with challenging and unprecedented situations requiring rapid and aggressive responses worldwide. A proactive approach to climate change aimed at minimizing such impacts will inevitably involve significant cuts in greenhouse gas (GHG) emissions and investment in more resilient infrastructure. Although current global mitigation and adaptation efforts are proceeding slowly, one emerging strategy could serve as an accelerant: the financial disclosure of climate risk by companies. Such disclosure, if practiced more widely and consistently, could lower the risks of climate change by redirecting investments away from GHG-emitting activities and pinpointing infrastructure that needs to be made more resilient.

Toward that end, the MIT Joint Program on the Science and Policy of Global Change engaged dozens of decision-makers in the financial sector and industry in a two-hour panel discussion on climate-related financial risk. Held as a Zoom meeting on March 26 and facilitated by joint program Co-Director Ronald Prinn, the discussion featured six finance and economics experts from the Bank of England, the Bank of Canada, HSBC, BP, and MIT. Panelists described how their organization has been affected by climate-related financial risk and the steps it’s taking to address it, how climate and economic scenarios could be useful in better understanding climate-related financial risks, and potential research that an institution like MIT could pursue to advance the state of knowledge in this area.

Organizational impacts and responses

Physical risks — potential losses due to more frequent and severe weather driven by climate change — and transition risks — potential losses due to a rapid transition to a low-carbon economy — pose significant economic threats to financial institutions and industrial firms. Those represented on the panel have taken notice and are mounting systemic responses.

Theresa Löber, head of the Bank of England’s Climate Hub, noted that the bank has taken a lead role in ensuring that financial firms develop an enhanced approach to managing the financial risks of climate change. Each institution under its control is required to appoint a senior representative who’s accountable to the bank, incorporate physical and transition risk into its existing risk management framework, perform scenario analyses, and properly disclose climate risks. The largest firms must also undergo a climate stress test.

A climate focus is also prominent at the Bank of Canada, as part of its mandate to promote economic stability. The bank participates in a network of central banks focused on greening the financial system through an exchange of ideas on how best to assess climate-related risk, and conducts its own studies of different climate and economic scenarios.

“Generally, what we’re finding is that there’s a tradeoff between physical and transition risks depending on the pathway you look at,” said Craig Johnston, senior economist at the Bank of Canada. “If we do nothing [to reduce emissions], we see very limited transition risks, but the highest level of physical risks. On the other side of things, a rapid transformation toward a low-carbon economy has the highest transition risks, but it does mitigate physical risks to some degree.”

Guided by the actions of central banks and evolving market forces, private banks and firms in other sectors are taking climate-related financial disclosure seriously.

Alan Smith, global head of risk strategy at HSBC, observed that every kind of risk the financial institution faces is affected by climate change, so that the issue now informs all of the company’s activities.

Spencer Dale, group chief economist at BP, observed that as a major oil and gas company facing a global energy transition to low-carbon sources, BP sees the issue of financial disclosure of climate risk as having less to do with the firm’s financial risks and more to do with its core purpose and structure. A key consideration is how products sold by BP can be consistent with a companywide goal of achieving net-zero emissions by 2050. While carbon offsets, carbon capture technology, and tree planting could be part of the solution, the company’s main challenge will be to shift the business to zero-carbon products.

Best practices for using scenarios in climate-related financial risk assessment

All six panelists saw an important role for scenarios — projections of how the climate and economy are likely to evolve under different climate policies and rates of energy technology market penetration — in enabling financial institutions and businesses to assess climate-related financial risk. There was general agreement that scenarios should not be seen as predictions, but instead as a range of plausible potential outcomes with varying levels of uncertainty.

Recognizing the inherent uncertainty of any single scenario, Dale noted that BP has surveyed the hundreds of scenarios in the Intergovernmental Panel on Climate Change database to help understand the range of possibilities for different new technologies and energy sources to contribute to a net-zero emissions solution. On the other hand, Löber observed that the financial community was having difficulty sorting through the many scenarios now available, leading the bank to limit its stress test to three reference scenarios representing a wide range of potential climate policy action.

“What we’re trying to test here is a combination of physical and transition risks across these different scenarios,” said Löber. “We’re trying to be as prescriptive as we can be so financial firms can process this, but also so that we can aggregate the results to understand how the system as a whole is exposed to these risks.”

Panelists differed on the utility of different kinds of scenarios. Some maintained that finer detail in the types of quantities projected in a scenario would help make it more relevant; others expressed concern about the challenges of making robust projections at that level of detail. Another concern was that models generating scenarios failed to account for potential tipping points — climatic or economic disruptions — that could dramatically alter projections. Finally, various participants highlighted the challenge of reconciling long-range climate and economic scenarios with shorter-term company-level concerns.

New research directions for MIT and beyond

Panelists and participants alike encouraged more research on climate-related financial disclosure at MIT and similar institutions, including ongoing upgrades of publicly available climate and economic models developed by the joint program.

Henry Jacoby, founding co-director (emeritus) of the MIT Joint Program and William F. Pounds, professor emeritus of management at the MIT Sloan School of Management, noted that a 2018 workshop convened by MIT Vice President for Research Maria Zuber yielded a report and two broad directions for research. One was to simply use MIT’s existing models and develop a richer set of scenarios more targeted to the financial risk and disclosure problem. The other was to enrich these models with variables and sectors more relevant to the concerns of the finance community.

To produce relevant results, John Parsons, senior lecturer in risk management and corporate finance at MIT Sloan, recommended that MIT complement its existing global economic modeling with parallel research into the more detailed microeconomic analysis that is needed to calculate financial exposure. The idea, he said, was not necessarily to put everything into a single model, but rather to focus on aspects of each scenario that relate to the financial risks to which individual companies are exposed.

BP’s Spencer Dale suggested an alternative emphasis for future research. “For us, the most valuable thing that MIT and others can do is not to produce more and more precise, specific, prescriptive scenarios,” he said, “but rather to help us better understand the range of uncertainty and confidence levels associated with existing scenarios.”

The discussion came to a close with two main questions: How might the joint program structure a research program at MIT so that it could continue to work closely with decision-makers who would ultimately use these scenarios, and how would such a program be funded? In the coming months, John Reilly, co-director of the MIT Joint Program and a senior lecturer at MIT Sloan, committed the program to help coordinate an effort among MIT, finance, and industry experts to address these questions and develop a detailed action plan."
323;machinelearningmastery.com;https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/;2019-07-11;How to Code the GAN Training Algorithm and Loss Functions;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

# gan training algorithm def train_gan ( generator , discriminator , dataset , latent_dim , n_epochs , n_batch ) : # calculate the number of batches per epoch batches_per_epoch = int ( len ( dataset ) / n_batch ) # calculate the number of training iterations n_steps = batches_per_epoch * n_epochs # gan training algorithm for i in range ( n_steps ) : # generate points in the latent space z = randn ( latent_dim * n_batch ) # reshape into a batch of inputs for the network z = z . reshape ( n_batch , latent_dim ) # generate fake images fake = generator . predict ( z ) # select a batch of random real images ix = randint ( 0 , len ( dataset ) , n_batch ) # retrieve real images real = dataset [ ix ] # update weights of the discriminator model # ... # generate points in the latent space z = randn ( latent_dim * n_batch ) # reshape into a batch of inputs for the network z = z . reshape ( n_batch , latent_dim ) # generate fake images fake = generator . predict ( z ) # classify as real or fake result = discriminator . predict ( fake ) # update weights of the generator model # ..."
324;news.mit.edu;http://news.mit.edu/2020/computational-thinking-class-enables-students-engage-covid-19-response-0407;;Computational thinking class enables students to engage in Covid-19 response;"When an introductory computational science class, which is open to the general public, was repurposed to study the Covid-19 pandemic this spring, the instructors saw student registration rise from 20 students to nearly 300.

Introduction to Computational Thinking (6.S083/18.S190), which applies data science, artificial intelligence, and mathematical models using the Julia programming language developed at MIT, was introduced in the fall as a pilot half-semester class. It was launched as part of the MIT Stephen A. Schwarzman College of Computing’s computational thinking program and spearheaded by Department of Mathematics Professor Alan Edelman and Visiting Professor David P. Sanders. They very quickly were able to fast-track the curriculum to focus on applications to Covid-19 responses; students were equally fast in jumping on board.

“Everyone at MIT wants to contribute,” says Edelman. “While we at the Julia Lab are doing research in building tools for scientists, Dave and I thought it would be valuable to teach the students about some of the fundamentals related to computation for drug development, disease models, and such.”

The course is offered through MIT’s Department of Electronic Engineering and Computer Science and the Department of Mathematics. “This course opens a trove of opportunities to use computation to better understand and contain the Covid-19 pandemic,” says MIT Computer Science and Artificial Intelligence Laboratory Director Daniela Rus.

The fall version of the class had a maximum enrollment of 20 students, but the spring class has ballooned to nearly 300 students in one weekend, almost all from MIT. “We’ve had a tremendous response,” Edelman says. “This definitely stressed the MIT sign-up systems in ways that I could not have imagined.”

Sophomore Shinjini Ghosh, majoring in computer science and linguistics, says she was initially drawn to the class to learn Julia, “but also to develop the skills to do further computational modeling and conduct research on the spread and possible control of Covid-19.”

""There's been a lot of misinformation about the epidemiology and statistical modeling of the coronavirus,” adds sophomore Raj Movva, a computer science and biology major. “I think this class will help clarify some details, and give us a taste of how one might actually make predictions about the course of a pandemic.""

Edelman says that he has always dreamed of an interdisciplinary modern class that would combine the machine learning and AI of a “data-driven” world, the modern software and systems possibilities that Julia allows, and the physical models, differential equations, and scientific machine learning of the “physical world.”

He calls this class “a natural outgrowth of Julia Lab's research, and that of the general cooperative open-source Julia community.” For years, this online community collaborates to create tools to speed up the drug approval process, aid in scientific machine learning and differential equations, and predict infectious disease transmission. “The lectures are open to the world, following the great MIT tradition of MIT open courses,” says Edelman.

So when MIT turned to virtual learning to de-densify campus, the transition to an online, remotely taught version of the class was not too difficult for Edelman and Sanders.

""Even though we have run open remote learning courses before, it's never the same as being able to see the live audience in front of you,” says Edelman. “However, MIT students ask such great questions in the Zoom chat, so that it remains as intellectually invigorating as ever.""

Sanders, a Marcos Moshinsky research fellow currently on leave as a professor at the National University of Mexico, is working on techniques for accelerating global optimization. Involved with the Julia Lab since 2014, Sanders has worked with Edelman on various teaching, research, and outreach projects related to Julia, and his YouTube tutorials have reached over 100,000 views. “His videos have often been referred to as the best way to learn the Julia language,” says Edelman.

Edelman will also be enlisting some help from Philip, his family’s Corgi who until recently had been a frequent wanderer of MIT’s halls and classrooms. “Philip is a well-known Julia expert whose image has been classified many times by Julia’s AI Systems,” says Edelman. “Students are always happy when Philip participates in the online classes.”"
325;machinelearningmastery.com;https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/;2018-10-23;How to Grid Search SARIMA Hyperparameters for Time Series Forecasting;"# grid search sarima hyperparameters for monthly car sales dataset

from math import sqrt

from multiprocessing import cpu_count

from joblib import Parallel

from joblib import delayed

from warnings import catch_warnings

from warnings import filterwarnings

from statsmodels . tsa . statespace . sarimax import SARIMAX

from sklearn . metrics import mean_squared_error

from pandas import read_csv

# one-step sarima forecast

def sarima_forecast ( history , config ) :

order , sorder , trend = config

# define model

model = SARIMAX ( history , order = order , seasonal_order = sorder , trend = trend , enforce_stationarity = False , enforce_invertibility = False )

# fit model

model_fit = model . fit ( disp = False )

# make one step forecast

yhat = model_fit . predict ( len ( history ) , len ( history ) )

return yhat [ 0 ]

# root mean squared error or rmse

def measure_rmse ( actual , predicted ) :

return sqrt ( mean_squared_error ( actual , predicted ) )

# split a univariate dataset into train/test sets

def train_test_split ( data , n_test ) :

return data [ : - n_test ] , data [ - n_test : ]

# walk-forward validation for univariate data

def walk_forward_validation ( data , n_test , cfg ) :

predictions = list ( )

# split dataset

train , test = train_test_split ( data , n_test )

# seed history with training dataset

history = [ x for x in train ]

# step over each time-step in the test set

for i in range ( len ( test ) ) :

# fit model and make forecast for history

yhat = sarima_forecast ( history , cfg )

# store forecast in list of predictions

predictions . append ( yhat )

# add actual observation to history for the next loop

history . append ( test [ i ] )

# estimate prediction error

error = measure_rmse ( test , predictions )

return error

# score a model, return None on failure

def score_model ( data , n_test , cfg , debug = False ) :

result = None

# convert config to a key

key = str ( cfg )

# show all warnings and fail on exception if debugging

if debug :

result = walk_forward_validation ( data , n_test , cfg )

else :

# one failure during model validation suggests an unstable config

try :

# never show warnings when grid searching, too noisy

with catch_warnings ( ) :

filterwarnings ( ""ignore"" )

result = walk_forward_validation ( data , n_test , cfg )

except :

error = None

# check for an interesting result

if result is not None :

print ( ' > Model[%s] %.3f' % ( key , result ) )

return ( key , result )

# grid search configs

def grid_search ( data , cfg_list , n_test , parallel = True ) :

scores = None

if parallel :

# execute configs in parallel

executor = Parallel ( n_jobs = cpu_count ( ) , backend = 'multiprocessing' )

tasks = ( delayed ( score_model ) ( data , n_test , cfg ) for cfg in cfg_list )

scores = executor ( tasks )

else :

scores = [ score_model ( data , n_test , cfg ) for cfg in cfg_list ]

# remove empty results

scores = [ r for r in scores if r [ 1 ] != None ]

# sort configs by error, asc

scores . sort ( key = lambda tup : tup [ 1 ] )

return scores

# create a set of sarima configs to try

def sarima_configs ( seasonal = [ 0 ] ) :

models = list ( )

# define config lists

p_params = [ 0 , 1 , 2 ]

d_params = [ 0 , 1 ]

q_params = [ 0 , 1 , 2 ]

t_params = [ 'n' , 'c' , 't' , 'ct' ]

P_params = [ 0 , 1 , 2 ]

D_params = [ 0 , 1 ]

Q_params = [ 0 , 1 , 2 ]

m_params = seasonal

# create config instances

for p in p_params :

for d in d_params :

for q in q_params :

for t in t_params :

for P in P_params :

for D in D_params :

for Q in Q_params :

for m in m_params :

cfg = [ ( p , d , q ) , ( P , D , Q , m ) , t ]

models . append ( cfg )

return models

if __name__ == '__main__' :

# load dataset

series = read_csv ( 'monthly-car-sales.csv' , header = 0 , index_col = 0 )

data = series . values

print ( data . shape )

# data split

n_test = 12

# model configs

cfg_list = sarima_configs ( seasonal = [ 0 , 6 , 12 ] )

# grid search

scores = grid_search ( data , cfg_list , n_test )

print ( 'done' )

# list top 3 configs

for cfg , error in scores [ : 3 ] :"
326;news.mit.edu;http://news.mit.edu/2019/mit-sounding-explores-far-reaching-musical-frontiers-0927;;MIT Sounding 2019-20 explores far-reaching musical frontiers;"Now in its eighth year, 2019-20 MIT Sounding presents another season of wide-ranging musical offerings that have found a vibrant home at MIT.

“The program feeds the hunger of a diverse audience for music at MIT,” says Evan Ziporyn, faculty director of the MIT Center for Art, Science and Technology (CAST) and curator of the series. “We try to give students a sense of exploration, while also developing a larger-scale dialogue with local audiences.”

The eclectic journey continues with Boston premieres of music from New York, Czechia, and Nepal, as well as returning artists who have wowed local audiences and who continue to push new musical boundaries. Add a septet of turntable artists, a multimedia score by Tod Machover, and a virtual reality-enhanced, dataset-driven “space opera” by artist Matthew Ritchie, and you have an abundant season of MIT Sounding.

Glenn Branca: New York’s enfant terrible

The year started with a bang with “Branca Lives: The Glenn Branca Ensemble/Ambient Orchestra,"" an all-too-rare performance of music by the proto-punk legend, who passed away in 2018.

“Branca’s symphonies for multiple guitars — sometimes up to 100 at a time — were Brutalism in musical form,” says Ziporyn. “He embraced the energy of noise, distortion, and feedback, but in a carefully organized way, activating overtones and microtones to create amazing, almost hallucinogenic textures. He was thinking orchestrally, building out from the sound of the electric guitar rather than from classical instruments. Then he began to write for acoustic orchestra and found ways to get the same effects.”

“Branca Lives"" presents the composer’s eponymous guitar ensemble, led by his longtime concertmaster and collaborator, Reg Bloor. Their set will include Branca’s “The Light (for David),” a tribute to David Bowie. Ziporyn and the Ambient Orchestra will open the concert with Boston premieres of two of Branca’s rarely performed orchestral works — “Symphony No. 14 (2,000,000,000 Light Years from Home)” and “Freeform.”

“It’s brilliant and surprising music that deserves to be known,” adds Ziporyn.

Lochan Rijal shares music of Nepal

Despite an ever-shrinking global culture, many musical traditions remain overlooked, including the music of Nepal. “काँचो आवाज (Raw Sounds),” a program that celebrates Nepal’s unique musical heritage, seeks to address that oversight.

“काँचो आवाज (Raw Sounds)” features Lochan Rijal, the award-winning Nepali multi-instrumentalist singer and songwriter, performing new and traditional compositions based on his own musical narrative of everyday life in Nepal. The head of Kathmandu University’s Department of Music, Rijal will play the sarangi, a traditional short-necked fiddle, and the Gandharva lute arbaja, recently discovered in Rijal’s research in Nepal.

During his residency, Rijal will discuss a temple restoration project and Nepal’s musical traditions in a public lecture.

Iva Bittová with MITSO

Legendary Czech vocalist/violinist Iva Bittová is a familiar force of nature at MIT, having performed with the improvisational trio EVIYAN, and collaborated with the Festival Jazz Ensemble and Pilobolus Dance for MIT One World.

Bittová returns this October as composer to launch the MIT Symphony Orchestra’s (MITSO) 2019-20 season in “The Heart is a Bell.” The concert pairs two pieces by 20th century Czech female composers: Bittová’s “Zvon” and Vítězslava Kapralova’s “Suita Rustica.” Composed 75 years apart, both works draw on Czech and Slovak folk culture, seen through a modern lens.

At once personal and avant-garde, “Zvon” features Bittova’s voice, jazz combo, elements of world music and cabaret, and improvisation by members of the orchestra. “We’re widening the orchestral landscape,” says Ziporyn, who steps in as acting MITSO director this academic year.

Additional projects and performances

What happens when seven DJs gather, challenged to make music together rather than as solo acts? Audiences will find out this January, in “the wave function collapses.” The unique program features harbanger (pronounced “harbinger”), a turntable septet with visiting artists Harry Allen and DJ Rob Swift, known for their work with Public Enemy and The Source magazine. “The wave function collapses” is the culmination of a two-week workshop facilitated by Eran Egozy, professor of the practice in music technology at MIT and co-founder and CTO of Harmonix Music Systems. The 2020 Independent Activities Period (IAP) offering includes two courses: a history of DJ culture by hip hop activist and “Media Assassin” Harry Allen, and hands-on DJ instruction by DJ Rob Swift.

Virtuoso violinist Johnny Gandelsman performed Johann Sebastian Bach’s ""Sonatas and Partitas"" as part of MIT Sounding’s 2015 season. The adventurous soloist returns this spring to perform “Bach’s Cello Suites” on the violin — which can be challenging, given the two instruments’ very different voicings. But this isn’t reinvention for its own sake, says Ziporyn. It’s simply “to get the most from the music, in an enthralling way.”

This March brings composer Tod Machover’s ""City Symphonies"" to Boston for the first time. Rich in visuals and sense of place, “Moving Images: MITSO and Film” is part of the MIT Symphony Orchestra’s 2019-20 season. “It’s time to present this music on Tod’s home turf,” notes Ziporyn, who will conduct the ensemble. Audiences can expect a unique evening of music and film, including work developed by Machover and his team in the Opera of the Future group at the MIT Media Lab.

The season closes with a new transmedia work, “The Invisible College,” created by 2018–20 Dasha Zhukova Distinguished Visiting Artist Matthew Ritchie. The project refers to the multitude of interactions and collaborations that take place behind the scenes within the university, and brings together a multidisciplinary team of MIT artists, faculty, and students. Based on datasets representing scales of the universe — from nanoparticles to dark energy — “The Invisible College” encompasses a site-specific installation, virtual reality experience, and a May “Dark Energy: A Space Opera,” a collaboration between Ritchie, Ziporyn, and Christine Southworth."
327;news.mit.edu;http://news.mit.edu/2020/proteins-cytokine-storms-covid-19-0416;;Proteins may halt the severe cytokine storms seen in Covid-19 patients;"One of the defining features of Covid-19 is the excessive immune response that can occur in severe cases. This burst of immune overreaction, also called a cytokine storm, damages the lungs and can be fatal.

A team of MIT researchers has developed specialized proteins, similar in structure to antibodies, that they believe could soak up these excess cytokines.

“The idea is that they can be injected into the body and bind to the excessive cytokines as generated by the cytokine storm, removing the excessive cytokines and alleviating the symptoms from the infection,” says Rui Qing, an MIT research scientist who is one of the senior authors of the study.

The researchers have reported their initial findings in the journal Quarterly Review of Biophysics (QRB) Discovery, and they now hope to begin testing their proteins in human cells and in animal models of cytokine release and coronavirus infection.

Shuguang Zhang, a principal research scientist in the MIT Media Lab’s Laboratory of Molecular Architecture, is also a senior author of the paper. Shilei Hao, a visiting scientist at MIT, is the lead author of the study, and David Jin, CEO and president of Avalon GloboCare, is also an author.

A molecular sponge

The researchers’ work on blocking cytokine storms grew out of a project that Zhang began 10 years ago to develop modified versions of membrane-embedded proteins. These proteins are usually difficult to study because once they are extracted from the cell membrane, they only maintain their structure if they are suspended in special types of detergents.

After working on the problem for several years, Zhang and Qing developed a method for modifying the hydrophobic regions of these proteins, making them soluble in water and much easier to study. Their method, called the QTY code, calls for replacing some hydrophobic amino acids with hydrophilic amino acids that have similar structures. Leucine is converted to glutamine, isoleucine and valine are converted to threonine, and phenylalanine is converted to tyrosine.

Following the development of the QTY code, Jin approached Zhang’s lab with the idea of designing water-soluble versions of proteins known as cytokine receptors. These receptors are found on the surface of immune cells, where they bind to cytokines — signaling proteins that stimulate inflammation and other immune responses.

Jin believed that proteins that mimic these cytokine receptors could help combat cytokine storms, which can be produced by viral or bacterial infections, including HIV and hepatitis. They can also occur as a side effect of cancer immunotherapy.

In April 2019, Zhang’s team set out to design proteins that could sop up these excess cytokines like a sponge. To do that, they used the QTY code to make water-soluble versions of cytokine receptors. When proteins are soluble in water, they can travel efficiently through the human bloodstream, while the original, hydrophobic versions of the proteins would likely stick to cells that they encountered.

The researchers also attached an antibody segment called the Fc region to their water-soluble receptor proteins. This region helps to further stabilize the proteins in the bloodstream, and makes them less likely to be attacked by the immune system.

The researchers designed proteins that mimic six different cytokine receptors, which can bind to cytokines such as interferon and interleukin, as well as a class of cytokines called chemokines. In laboratory tests of the proteins’ binding strength, the researchers found that their modified proteins were able to bind to cytokines with similar strength as naturally occurring cytokine receptors.

“The cytokine receptors that we designed will soak up the majority of the excessive cytokines that are released during the cytokine storm,” Jin says.

Driven by curiosity

In March, when evidence began to suggest that the SARS-CoV-2 virus was inducing cytokine storms in some patients, the researchers realized that the receptor proteins they had designed might be able to help. They decided to quickly publish the results they have generated so far, and they are now planning to do additional tests in human cells and in animal models of Covid-19 infection.

The potential usefulness of this approach underscores the importance of “curiosity-driven research,” Zhang says.

“As it turns out, our research initiated in April 2019 is directly relevant to the treatment of Covid-19 infected patients,” he says. “Curiosity-driven, or even proactive research often leads to preparedness, which is key to preventing future disasters.”

The researchers have filed for patents on the proteins that they designed, as well as on their overall approach to creating water-soluble cytokine receptors. They hope to license the technology quickly and to collaborate with pharmaceutical and biotech companies who can help to move it toward clinical trials.

“Obviously this approach will need further animal studies, and potentially human clinical studies,” Jin says. “But we have confidence that this discovery will contribute to clinical applications to address viral diseases that involve cytokine storms.”

The research was funded primarily by Avalon GloboCare, and also by a fellowship from the China Scholarship Council and Chongqing University, China."
328;machinelearningmastery.com;http://machinelearningmastery.com/grid-search-arima-hyperparameters-with-python/;2017-01-17;How to Grid Search ARIMA Model Hyperparameters with Python;"import warnings

from pandas import read_csv

from pandas import datetime

from statsmodels . tsa . arima_model import ARIMA

from sklearn . metrics import mean_squared_error

# evaluate an ARIMA model for a given order (p,d,q)

def evaluate_arima_model ( X , arima_order ) :

# prepare training dataset

train_size = int ( len ( X ) * 0.66 )

train , test = X [ 0 : train_size ] , X [ train_size : ]

history = [ x for x in train ]

# make predictions

predictions = list ( )

for t in range ( len ( test ) ) :

model = ARIMA ( history , order = arima_order )

model_fit = model . fit ( disp = 0 )

yhat = model_fit . forecast ( ) [ 0 ]

predictions . append ( yhat )

history . append ( test [ t ] )

# calculate out of sample error

error = mean_squared_error ( test , predictions )

return error

# evaluate combinations of p, d and q values for an ARIMA model

def evaluate_models ( dataset , p_values , d_values , q_values ) :

dataset = dataset . astype ( 'float32' )

best_score , best_cfg = float ( ""inf"" ) , None

for p in p_values :

for d in d_values :

for q in q_values :

order = ( p , d , q )

try :

mse = evaluate_arima_model ( dataset , order )

if mse < best_score :

best_score , best_cfg = mse , order

print ( 'ARIMA%s MSE=%.3f' % ( order , mse ) )

except :

continue

print ( 'Best ARIMA%s MSE=%.3f' % ( best_cfg , best_score ) )

# load dataset

def parser ( x ) :

return datetime . strptime ( '190' + x , '%Y-%m' )

series = read_csv ( 'shampoo-sales.csv' , header = 0 , parse_dates = [ 0 ] , index_col = 0 , squeeze = True , date_parser = parser )

# evaluate parameters

p_values = [ 0 , 1 , 2 , 4 , 6 , 8 , 10 ]

d_values = range ( 0 , 3 )

q_values = range ( 0 , 3 )

warnings . filterwarnings ( ""ignore"" )"
329;towardsdatascience.com;https://towardsdatascience.com/what-5-years-of-a-relationships-messages-look-like-45921155e3f2?source=collection_home---4------0-----------------------;2020-04-16;What 5 Years of a Relationship’s Messages Look Like;"Of course, modern messaging is more than just the words you exchange (there’s a reason the Oxford Dictionary word-of-the-year in 2015 was an emoji). Surprisingly, the initial scrape of the Telegram HTML actually preserved the original emoji.

Native Python does not cope with these characters, however there’s a neat little package (imaginatively called ‘emoji’) that allows you to identify, count, and decode emojis in strings.

If we investigate the ‘non-text’ messages, we can see that emojis are an especially important communication method for my girlfriend. I, on the other hand, add value by sharing photos and links (generally from Reddit), and stickers (a feature that Telegram was relatively early to introduce).

We can attempt to see how volumes of these non-text messages have changed over time using the same rolling average trick. However, beyond an early 2018 spike in Stickers (when we worked out that you could download custom sets — truly, a gamechaging moment), there aren’t many discernable patterns.

Instead, let’s build a graph showing the accumilation of these non-text messages by type. To do this, we can employ NumPy’s cumsum() method, which will take a cumulative sum of a series. Supposing we have a dataframe whose index is a date range, and each column describes the number of messages of each type sent on that day:

#Create a list of message types

required_cols = ['Emoji','Photo','Sticker','Link','Voice message','Animation'] #Create a new dataframe of just these types

df_types_cum = df_types[required_cols] #Iterate through the columns and replace

#each with the cumulative sum version

for i in required_cols:

df_types_cum[i] = np.cumsum(df_types_cum[i]) #Use Pandas' in-built plot method to show it

df_types_cum.plot.area(figsize=(15,6), lw=0)

This produces the following chart.

Again, we can see the introduction and uptake of stickers (the amber segment), as well as an acceleration in the use of emojis from 2018 onwards."
330;news.mit.edu;http://news.mit.edu/2020/three-mit-students-named-2020-goldwater-scholars-0409;;Katie Collins, Vaishnavi Phadnis, and Vaibhavi Shah named 2020-21 Goldwater Scholars;"MIT students Katie Collins, Vaishnavi Phadnis, and Vaibhavi Shah have been selected to receive a Barry Goldwater Scholarship for the 2020-21 academic year. Over 5,000 college students from across the United States were nominated for the scholarships, from which only 396 recipients were selected based on academic merit.

The Goldwater scholarships have been conferred since 1989 by the Barry Goldwater Scholarship and Excellence in Education Foundation. These scholarships have supported undergraduates who go on to become leading scientists, engineers, and mathematicians in their respective fields. All of the 2020-21 Goldwater Scholars intend to obtain a doctorate in their area of research, including the three MIT recipients.

Katie Collins, a third-year majoring in brain and cognitive sciences with minors in computer science and biomedical engineering, got involved with research in high school, when she worked on computational models of metabolic networks and synthetic gene networks in the lab of Department of Electrical Engineering and Computer Science Professor Timothy Lu at MIT. It was this project that led her to realize how challenging it is to model and analyze complex biological networks. She also learned that machine learning can provide a path for exploring these networks and understanding human diseases. This realization has coursed a scientific path for Collins that is equally steeped in computer science and human biology.

Over the past few years, Collins has become increasingly interested in the human brain, particularly what machine learning can learn from human common-sense reasoning and the way brains process sparse, noisy data. “I aim to develop novel computational algorithms to analyze complex, high-dimensional data in biomedicine, as well as advance modelling paradigms to improve our understanding of human cognition,” explains Collins. In his letter of recommendation, Professor Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences and one of Collins’ mentors, wrote, “It is very difficult to imagine a better candidate for the Goldwater fellowship.” Collins plans to pursue a PhD studying machine learning or computational neuroscience and to one day run her own lab. “I hope to become a professor, leading a research program at the interface of computer science and cognitive neuroscience.”

Vaishnavi Phadnis, a second-year majoring in computer science and molecular biology, sees molecular and cellular biology as the bridge between chemistry and life, and she’s been enthralled with understanding that bridge since 7th grade, when she learned about the chemical basis of the cell. Phadnis spent two years working in a cancer research lab while still in high school, an experience which convinced her that research was not just her passion but also her future. “In my first week at MIT, I approached Professor Robert Weinberg, and I’ve been grateful to do research in his lab ever since,” she says.

“Vaishnavi’s exuberance makes her a joy to have in the lab,” wrote Weinberg, who is the Daniel Ludwig Professor in the Department of Biology. Phadnis is investigating ferroptosis, a recently discovered, iron-dependent form of cell death that may be relevant in neurodegeneration and also a potential strategy for targeting highly aggressive cancer cells. “She is a phenomenon who has vastly exceeded our expectations of the powers of someone her age,” Weinberg says. Phadnis is thankful to Weinberg and all the scientific mentors, both past and present, that have inspired her along her research path. Deciphering the mechanisms behind fundamental cellular processes and exploring their application in human diseases is something Phadnis plans to continue doing in her future as a physician-scientist after pursuing an MD/PhD. “I hope to devote most of my time to leading my own research group, while also practicing medicine,” she says.

Vaibhavi Shah, a third-year studying biological engineering with a minor in science, technology and society, spent a lot of time in high school theorizing ways to tackle major shortcomings in medicine and science with the help of technology. “When I came to college, I was able to bring some of these ideas to fruition,” she says, working with both the Big Data in Radiology Group at the University of California at San Francisco and the lab of Professor Mriganka Sur, the Newton Professor of Neuroscience in the Department of Brain and Cognitive Sciences.

Shah is particularly interested in integrating innovative research findings with traditional clinical practices. According to her, technology, like computer vision algorithms, can be adopted to diagnose diseases such as Alzheimer’s, allowing patients to start appropriate treatments earlier. “This is often harder to do at smaller, rural institutions that may not always have a specialist present,” says Shah, and algorithms can help fill that gap. One of aims of Shah’s research is to improve the efficiency and equitability of physician decision-making. “My ultimate goal is to improve patient outcomes, and I aim to do this by tackling emerging scientific questions in machine learning and artificial intelligence at the forefront of neurology,” she says. The clinic is a place Shah expects to be in the future after obtaining her physician-scientist training, saying, “I hope to a practicing neurosurgeon and clinical investigator.”

The Barry Goldwater Scholarship and Excellence in Education Program was established by Congress in 1986 to honor Senator Barry Goldwater, a soldier and statesman who served the country for 56 years. Awardees receive scholarships of up to $7,500 a year to cover costs related to tuition, room and board, fees, and books."
331;machinelearningmastery.com;https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/;2019-01-24;Understand the Impact of Learning Rate on Neural Network Performance;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76

# study of patience for the learning rate drop schedule on the blobs problem from sklearn . datasets import make_blobs from keras . layers import Dense from keras . models import Sequential from keras . optimizers import SGD from keras . utils import to_categorical from keras . callbacks import Callback from keras . callbacks import ReduceLROnPlateau from keras import backend from matplotlib import pyplot # monitor the learning rate class LearningRateMonitor ( Callback ) : # start of training def on_train_begin ( self , logs = { } ) : self . lrates = list ( ) # end of each training epoch def on_epoch_end ( self , epoch , logs = { } ) : # get and store the learning rate optimizer = self . model . optimizer lrate = float ( backend . get_value ( self . model . optimizer . lr ) ) self . lrates . append ( lrate ) # prepare train and test dataset def prepare_data ( ) : # generate 2d classification dataset X , y = make_blobs ( n_samples = 1000 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 ) # one hot encode output variable y = to_categorical ( y ) # split into train and test n_train = 500 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] return trainX , trainy , testX , testy # fit a model and plot learning curve def fit_model ( trainX , trainy , testX , testy , patience ) : # define model model = Sequential ( ) model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) # compile model opt = SGD ( lr = 0.01 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] ) # fit model rlrp = ReduceLROnPlateau ( monitor = 'val_loss' , factor = 0.1 , patience = patience , min_delta = 1E - 7 ) lrm = LearningRateMonitor ( ) history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 200 , verbose = 0 , callbacks = [ rlrp , lrm ] ) return lrm . lrates , history . history [ 'loss' ] , history . history [ 'accuracy' ] # create line plots for a series def line_plots ( patiences , series ) : for i in range ( len ( patiences ) ) : pyplot . subplot ( 220 + ( i + 1 ) ) pyplot . plot ( series [ i ] ) pyplot . title ( 'patience=' + str ( patiences [ i ] ) , pad = - 80 ) pyplot . show ( ) # prepare dataset trainX , trainy , testX , testy = prepare_data ( ) # create learning curves for different patiences patiences = [ 2 , 5 , 10 , 15 ] lr_list , loss_list , acc_list , = list ( ) , list ( ) , list ( ) for i in range ( len ( patiences ) ) : # fit model and plot learning curves for a patience lr , loss , acc = fit_model ( trainX , trainy , testX , testy , patiences [ i ] ) lr_list . append ( lr ) loss_list . append ( loss ) acc_list . append ( acc ) # plot learning rates line_plots ( patiences , lr_list ) # plot loss line_plots ( patiences , loss_list ) # plot accuracy line_plots ( patiences , acc_list )"
332;machinelearningmastery.com;http://machinelearningmastery.com/feature-selection-with-the-caret-r-package/;2014-09-21;Feature Selection with the Caret R Package;"# ensure results are repeatable

set . seed ( 7 )

# load the library

library ( mlbench )

library ( caret )

# load the dataset

data ( PimaIndiansDiabetes )

# prepare training scheme

control < - trainControl ( method = ""repeatedcv"" , number = 10 , repeats = 3 )

# train the model

model < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""lvq"" , preProcess = ""scale"" , trControl = control )

# estimate variable importance

importance < - varImp ( model , scale = FALSE )

# summarize importance

print ( importance )

# plot importance"
333;machinelearningmastery.com;https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/;2019-02-26;How to use Learning Curves to Diagnose Machine Learning Model Performance;"Tweet Share Share

Last Updated on August 6, 2019

A learning curve is a plot of model learning performance over experience or time.

Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally. The model can be evaluated on the training dataset and on a hold out validation dataset after each update during training and plots of the measured performance can created to show learning curves.

Reviewing learning curves of models during training can be used to diagnose problems with learning, such as an underfit or overfit model, as well as whether the training and validation datasets are suitably representative.

In this post, you will discover learning curves and how they can be used to diagnose the learning and generalization behavior of machine learning models, with example plots showing common learning problems.

After reading this post, you will know:

Learning curves are plots that show changes in learning performance over time in terms of experience.

Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.

Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Learning Curves Diagnosing Model Behavior Diagnosing Unrepresentative Datasets

Learning Curves in Machine Learning

Generally, a learning curve is a plot that shows time or experience on the x-axis and learning or improvement on the y-axis.

Learning curves (LCs) are deemed effective tools for monitoring the performance of workers exposed to a new task. LCs provide a mathematical representation of the learning process that takes place as task repetition occurs.

— Learning curve models and applications: Literature review and research directions, 2011.

For example, if you were learning a musical instrument, your skill on the instrument could be evaluated and assigned a numerical score each week for one year. A plot of the scores over the 52 weeks is a learning curve and would show how your learning of the instrument has changed over time.

Learning Curve: Line plot of learning (y-axis) over experience (x-axis).

Learning curves are widely used in machine learning for algorithms that learn (optimize their internal parameters) incrementally over time, such as deep learning neural networks.

The metric used to evaluate learning could be maximizing, meaning that better scores (larger numbers) indicate more learning. An example would be classification accuracy.

It is more common to use a score that is minimizing, such as loss or error whereby better scores (smaller numbers) indicate more learning and a value of 0.0 indicates that the training dataset was learned perfectly and no mistakes were made.

During the training of a machine learning model, the current state of the model at each step of the training algorithm can be evaluated. It can be evaluated on the training dataset to give an idea of how well the model is “learning.” It can also be evaluated on a hold-out validation dataset that is not part of the training dataset. Evaluation on the validation dataset gives an idea of how well the model is “generalizing.”

Train Learning Curve : Learning curve calculated from the training dataset that gives an idea of how well the model is learning.

: Learning curve calculated from the training dataset that gives an idea of how well the model is learning. Validation Learning Curve: Learning curve calculated from a hold-out validation dataset that gives an idea of how well the model is generalizing.

It is common to create dual learning curves for a machine learning model during training on both the training and validation datasets.

In some cases, it is also common to create learning curves for multiple metrics, such as in the case of classification predictive modeling problems, where the model may be optimized according to cross-entropy loss and model performance is evaluated using classification accuracy. In this case, two plots are created, one for the learning curves of each metric, and each plot can show two learning curves, one for each of the train and validation datasets.

Optimization Learning Curves : Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.

: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss. Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.

Now that we are familiar with the use of learning curves in machine learning, let’s look at some common shapes observed in learning curve plots.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Diagnosing Model Behavior

The shape and dynamics of a learning curve can be used to diagnose the behavior of a machine learning model and in turn perhaps suggest at the type of configuration changes that may be made to improve learning and/or performance.

There are three common dynamics that you are likely to observe in learning curves; they are:

Underfit.

Overfit.

Good Fit.

We will take a closer look at each with examples. The examples will assume that we are looking at a minimizing metric, meaning that smaller relative scores on the y-axis indicate more or better learning.

Underfit Learning Curves

Underfitting refers to a model that cannot learn the training dataset.

Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set.

— Page 111, Deep Learning, 2016.

An underfit model can be identified from the learning curve of the training loss only.

It may show a flat line or noisy values of relatively high loss, indicating that the model was unable to learn the training dataset at all.

An example of this is provided below and is common when the model does not have a suitable capacity for the complexity of the dataset.

An underfit model may also be identified by a training loss that is decreasing and continues to decrease at the end of the plot.

This indicates that the model is capable of further learning and possible further improvements and that the training process was halted prematurely.

A plot of learning curves shows underfitting if:

The training loss remains flat regardless of training.

The training loss continues to decrease until the end of training.

Overfit Learning Curves

Overfitting refers to a model that has learned the training dataset too well, including the statistical noise or random fluctuations in the training dataset.

… fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely.

— Page 22, An Introduction to Statistical Learning: with Applications in R, 2013.

The problem with overfitting, is that the more specialized the model becomes to training data, the less well it is able to generalize to new data, resulting in an increase in generalization error. This increase in generalization error can be measured by the performance of the model on the validation dataset.

This is an example of overfitting the data, […]. It is an undesirable situation because the fit obtained will not yield accurate estimates of the response on new observations that were not part of the original training data set.

— Page 24, An Introduction to Statistical Learning: with Applications in R, 2013.

This often occurs if the model has more capacity than is required for the problem, and, in turn, too much flexibility. It can also occur if the model is trained for too long.

A plot of learning curves shows overfitting if:

The plot of training loss continues to decrease with experience.

The plot of validation loss decreases to a point and begins increasing again.

The inflection point in validation loss may be the point at which training could be halted as experience after that point shows the dynamics of overfitting.

The example plot below demonstrates a case of overfitting.

Good Fit Learning Curves

A good fit is the goal of the learning algorithm and exists between an overfit and underfit model.

A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.

The loss of the model will almost always be lower on the training dataset than the validation dataset. This means that we should expect some gap between the train and validation loss learning curves. This gap is referred to as the “generalization gap.”

A plot of learning curves shows a good fit if:

The plot of training loss decreases to a point of stability.

The plot of validation loss decreases to a point of stability and has a small gap with the training loss.

Continued training of a good fit will likely lead to an overfit.

The example plot below demonstrates a case of a good fit.

Diagnosing Unrepresentative Datasets

Learning curves can also be used to diagnose properties of a dataset and whether it is relatively representative.

An unrepresentative dataset means a dataset that may not capture the statistical characteristics relative to another dataset drawn from the same domain, such as between a train and a validation dataset. This can commonly occur if the number of samples in a dataset is too small, relative to another dataset.

There are two common cases that could be observed; they are:

Training dataset is relatively unrepresentative.

Validation dataset is relatively unrepresentative.

Unrepresentative Train Dataset

An unrepresentative training dataset means that the training dataset does not provide sufficient information to learn the problem, relative to the validation dataset used to evaluate it.

This may occur if the training dataset has too few examples as compared to the validation dataset.

This situation can be identified by a learning curve for training loss that shows improvement and similarly a learning curve for validation loss that shows improvement, but a large gap remains between both curves.

Unrepresentative Validation Dataset

An unrepresentative validation dataset means that the validation dataset does not provide sufficient information to evaluate the ability of the model to generalize.

This may occur if the validation dataset has too few examples as compared to the training dataset.

This case can be identified by a learning curve for training loss that looks like a good fit (or other fits) and a learning curve for validation loss that shows noisy movements around the training loss.

It may also be identified by a validation loss that is lower than the training loss. In this case, it indicates that the validation dataset may be easier for the model to predict than the training dataset.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Books

Papers

Posts

Articles

Summary

In this post, you discovered learning curves and how they can be used to diagnose the learning and generalization behavior of machine learning models.

Specifically, you learned:

Learning curves are plots that show changes in learning performance over time in terms of experience.

Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.

Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Better Deep Learning Models Today! Train Faster, Reduce Overftting, and Ensembles ...with just a few lines of python code Discover how in my new Ebook:

Better Deep Learning It provides self-study tutorials on topics like:

weight decay, batch normalization, dropout, model stacking and much more... Bring better deep learning to your projects! Skip the Academics. Just Results. See What's Inside"
334;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/;2019-06-30;How to Develop a GAN to Generate CIFAR10 Small Color Photographs;"# example of a dcgan on cifar10

from numpy import expand_dims

from numpy import zeros

from numpy import ones

from numpy import vstack

from numpy . random import randn

from numpy . random import randint

from keras . datasets . cifar10 import load_data

from keras . optimizers import Adam

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Reshape

from keras . layers import Flatten

from keras . layers import Conv2D

from keras . layers import Conv2DTranspose

from keras . layers import LeakyReLU

from keras . layers import Dropout

from matplotlib import pyplot

# define the standalone discriminator model

def define_discriminator ( in_shape = ( 32 , 32 , 3 ) ) :

model = Sequential ( )

# normal

model . add ( Conv2D ( 64 , ( 3 , 3 ) , padding = 'same' , input_shape = in_shape ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample

model . add ( Conv2D ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample

model . add ( Conv2D ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample

model . add ( Conv2D ( 256 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# classifier

model . add ( Flatten ( ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

return model

# define the standalone generator model

def define_generator ( latent_dim ) :

model = Sequential ( )

# foundation for 4x4 image

n_nodes = 256 * 4 * 4

model . add ( Dense ( n_nodes , input_dim = latent_dim ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Reshape ( ( 4 , 4 , 256 ) ) )

# upsample to 8x8

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# upsample to 16x16

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# upsample to 32x32

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# output layer

model . add ( Conv2D ( 3 , ( 3 , 3 ) , activation = 'tanh' , padding = 'same' ) )

return model

# define the combined generator and discriminator model, for updating the generator

def define_gan ( g_model , d_model ) :

# make weights in the discriminator not trainable

d_model . trainable = False

# connect them

model = Sequential ( )

# add generator

model . add ( g_model )

# add the discriminator

model . add ( d_model )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt )

return model

# load and prepare cifar10 training images

def load_real_samples ( ) :

# load cifar10 dataset

( trainX , _ ) , ( _ , _ ) = load_data ( )

# convert from unsigned ints to floats

X = trainX . astype ( 'float32' )

# scale from [0,255] to [-1,1]

X = ( X - 127.5 ) / 127.5

return X

# select real samples

def generate_real_samples ( dataset , n_samples ) :

# choose random instances

ix = randint ( 0 , dataset . shape [ 0 ] , n_samples )

# retrieve selected images

X = dataset [ ix ]

# generate 'real' class labels (1)

y = ones ( ( n_samples , 1 ) )

return X , y

# generate points in latent space as input for the generator

def generate_latent_points ( latent_dim , n_samples ) :

# generate points in the latent space

x_input = randn ( latent_dim * n_samples )

# reshape into a batch of inputs for the network

x_input = x_input . reshape ( n_samples , latent_dim )

return x_input

# use the generator to generate n fake examples, with class labels

def generate_fake_samples ( g_model , latent_dim , n_samples ) :

# generate points in latent space

x_input = generate_latent_points ( latent_dim , n_samples )

# predict outputs

X = g_model . predict ( x_input )

# create 'fake' class labels (0)

y = zeros ( ( n_samples , 1 ) )

return X , y

# create and save a plot of generated images

def save_plot ( examples , epoch , n = 7 ) :

# scale from [-1,1] to [0,1]

examples = ( examples + 1 ) / 2.0

# plot images

for i in range ( n * n ) :

# define subplot

pyplot . subplot ( n , n , 1 + i )

# turn off axis

pyplot . axis ( 'off' )

# plot raw pixel data

pyplot . imshow ( examples [ i ] )

# save plot to file

filename = 'generated_plot_e%03d.png' % ( epoch + 1 )

pyplot . savefig ( filename )

pyplot . close ( )

# evaluate the discriminator, plot generated images, save generator model

def summarize_performance ( epoch , g_model , d_model , dataset , latent_dim , n_samples = 150 ) :

# prepare real samples

X_real , y_real = generate_real_samples ( dataset , n_samples )

# evaluate discriminator on real examples

_ , acc_real = d_model . evaluate ( X_real , y_real , verbose = 0 )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( g_model , latent_dim , n_samples )

# evaluate discriminator on fake examples

_ , acc_fake = d_model . evaluate ( x_fake , y_fake , verbose = 0 )

# summarize discriminator performance

print ( '>Accuracy real: %.0f%%, fake: %.0f%%' % ( acc_real* 100 , acc_fake* 100 ) )

# save plot

save_plot ( x_fake , epoch )

# save the generator model tile file

filename = 'generator_model_%03d.h5' % ( epoch + 1 )

g_model . save ( filename )

# train the generator and discriminator

def train ( g_model , d_model , gan_model , dataset , latent_dim , n_epochs = 200 , n_batch = 128 ) :

bat_per_epo = int ( dataset . shape [ 0 ] / n_batch )

half_batch = int ( n_batch / 2 )

# manually enumerate epochs

for i in range ( n_epochs ) :

# enumerate batches over the training set

for j in range ( bat_per_epo ) :

# get randomly selected 'real' samples

X_real , y_real = generate_real_samples ( dataset , half_batch )

# update discriminator model weights

d_loss1 , _ = d_model . train_on_batch ( X_real , y_real )

# generate 'fake' examples

X_fake , y_fake = generate_fake_samples ( g_model , latent_dim , half_batch )

# update discriminator model weights

d_loss2 , _ = d_model . train_on_batch ( X_fake , y_fake )

# prepare points in latent space as input for the generator

X_gan = generate_latent_points ( latent_dim , n_batch )

# create inverted labels for the fake samples

y_gan = ones ( ( n_batch , 1 ) )

# update the generator via the discriminator's error

g_loss = gan_model . train_on_batch ( X_gan , y_gan )

# summarize loss on this batch

print ( '>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %

( i + 1 , j + 1 , bat_per_epo , d_loss1 , d_loss2 , g_loss ) )

# evaluate the model performance, sometimes

if ( i + 1 ) % 10 == 0 :

summarize_performance ( i , g_model , d_model , dataset , latent_dim )

# size of the latent space

latent_dim = 100

# create the discriminator

d_model = define_discriminator ( )

# create the generator

g_model = define_generator ( latent_dim )

# create the gan

gan_model = define_gan ( g_model , d_model )

# load image data

dataset = load_real_samples ( )

# train model"
335;machinelearningmastery.com;http://machinelearningmastery.com/introduction-python-deep-learning-library-keras/;2016-05-09;Introduction to Python Deep Learning with Keras;"Tweet Share Share

Last Updated on September 13, 2019

Two of the top numerical platforms in Python that provide the basis for Deep Learning research and development are Theano and TensorFlow.

Both are very powerful libraries, but both can be difficult to use directly for creating deep learning models.

In this post, you will discover the Keras Python library that provides a clean and convenient way to create a range of deep learning models on top of Theano or TensorFlow.

Discover how to develop deep learning models for a range of predictive modeling problems with just a few lines of code in my new book, with 18 step-by-step tutorials and 9 projects.

Let’s get started.

Update Oct/2016 : Updated for Keras 1.1.0, Theano 0.8.2 and TensorFlow 0.10.0.

: Updated for Keras 1.1.0, Theano 0.8.2 and TensorFlow 0.10.0. Update Sep/2019: Updated for Keras 2.2.5 API.

What is Keras?

Keras is a minimalist Python library for deep learning that can run on top of Theano or TensorFlow.

It was developed to make implementing deep learning models as fast and easy as possible for research and development.

It runs on Python 2.7 or 3.5 and can seamlessly execute on GPUs and CPUs given the underlying frameworks. It is released under the permissive MIT license.

Keras was developed and maintained by François Chollet, a Google engineer using four guiding principles:

Modularity : A model can be understood as a sequence or a graph alone. All the concerns of a deep learning model are discrete components that can be combined in arbitrary ways.

: A model can be understood as a sequence or a graph alone. All the concerns of a deep learning model are discrete components that can be combined in arbitrary ways. Minimalism : The library provides just enough to achieve an outcome, no frills and maximizing readability.

: The library provides just enough to achieve an outcome, no frills and maximizing readability. Extensibility : New components are intentionally easy to add and use within the framework, intended for researchers to trial and explore new ideas.

: New components are intentionally easy to add and use within the framework, intended for researchers to trial and explore new ideas. Python: No separate model files with custom file formats. Everything is native Python.

How to Install Keras

Keras is relatively straightforward to install if you already have a working Python and SciPy environment.

You must also have an installation of Theano or TensorFlow on your system already.

You can see installation instructions for both platforms here:

Keras can be installed easily using PyPI, as follows:

sudo pip install keras 1 sudo pip install keras

At the time of writing, the most recent version of Keras is version 2.2.5. You can check your version of Keras on the command line using the following snippet:

You can check your version of Keras on the command line using the following snippet:

python -c ""import keras; print keras.__version__"" 1 python -c ""import keras; print keras.__version__""

Running the above script you will see:

2.2.5 1 2.2.5

You can upgrade your installation of Keras using the same method:

sudo pip install --upgrade keras 1 sudo pip install --upgrade keras

Theano and TensorFlow Backends for Keras

Assuming you have both Theano and TensorFlow installed, you can configure the backend used by Keras.

The easiest way is by adding or editing the Keras configuration file in your home directory:

~/.keras/keras.json 1 ~/.keras/keras.json

Which has the format:

{ ""image_data_format"": ""channels_last"", ""backend"": ""tensorflow"", ""epsilon"": 1e-07, ""floatx"": ""float32"" } 1 2 3 4 5 6 { ""image_data_format"": ""channels_last"", ""backend"": ""tensorflow"", ""epsilon"": 1e-07, ""floatx"": ""float32"" }

In this configuration file you can change the “backend” property from “tensorflow” (the default) to “theano“. Keras will then use the configuration the next time it is run.

You can confirm the backend used by Keras using the following snippet on the command line:

python -c ""from keras import backend; print(backend.backend())"" 1 python -c ""from keras import backend; print(backend.backend())""

Running this with default configuration you will see:

Using TensorFlow backend. tensorflow 1 2 Using TensorFlow backend. tensorflow

You can also specify the backend to use by Keras on the command line by specifying the KERAS_BACKEND environment variable, as follows:

KERAS_BACKEND=theano python -c ""from keras import backend; print(backend.backend())"" 1 KERAS_BACKEND=theano python -c ""from keras import backend; print(backend.backend())""

Running this example prints:

Using Theano backend. theano 1 2 Using Theano backend. theano

Need help with Deep Learning in Python? Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Build Deep Learning Models with Keras

The focus of Keras is the idea of a model.

The main type of model is called a Sequence which is a linear stack of layers.

You create a sequence and add layers to it in the order that you wish for the computation to be performed.

Once defined, you compile the model which makes use of the underlying framework to optimize the computation to be performed by your model. In this you can specify the loss function and the optimizer to be used.

Once compiled, the model must be fit to data. This can be done one batch of data at a time or by firing off the entire model training regime. This is where all the compute happens.

Once trained, you can use your model to make predictions on new data.

We can summarize the construction of deep learning models in Keras as follows:

Define your model. Create a sequence and add layers. Compile your model. Specify loss functions and optimizers. Fit your model. Execute the model using data. Make predictions. Use the model to generate predictions on new data.

You can develop your first deep learning neural network in Keras with just a few lines of code. See this step-by-step Keras Tutorial:

Keras Resources

The list below provides some additional resources that you can use to learn more about Keras.

Summary

In this post, you discovered the Keras Python library for deep learning research and development.

You discovered that Keras is designed for minimalism and modularity allowing you to very quickly define deep learning models and run them on top of a Theano or TensorFlow backend.

Do you have any questions about Keras or this post? Ask your questions in the comments and I will do my best to answer them.

Develop Deep Learning Projects with Python! What If You Could Develop A Network in Minutes ...with just a few lines of Python Discover how in my new Ebook:

Deep Learning With Python It covers end-to-end projects on topics like:

Multilayer Perceptrons, Convolutional Nets and Recurrent Neural Nets, and more... Finally Bring Deep Learning To

Your Own Projects Skip the Academics. Just Results. See What's Inside"
336;news.mit.edu;http://news.mit.edu/2020/bridging-gap-between-human-and-machine-vision-0211;;Bridging the gap between human and machine vision;"Suppose you look briefly from a few feet away at a person you have never met before. Step back a few paces and look again. Will you be able to recognize her face? “Yes, of course,” you probably are thinking. If this is true, it would mean that our visual system, having seen a single image of an object such as a specific face, recognizes it robustly despite changes to the object’s position and scale, for example. On the other hand, we know that state-of-the-art classifiers, such as vanilla deep networks, will fail this simple test.

In order to recognize a specific face under a range of transformations, neural networks need to be trained with many examples of the face under the different conditions. In other words, they can achieve invariance through memorization, but cannot do it if only one image is available. Thus, understanding how human vision can pull off this remarkable feat is relevant for engineers aiming to improve their existing classifiers. It also is important for neuroscientists modeling the primate visual system with deep networks. In particular, it is possible that the invariance with one-shot learning exhibited by biological vision requires a rather different computational strategy than that of deep networks.

A new paper by MIT PhD candidate in electrical engineering and computer science Yena Han and colleagues in Nature Scientific Reports entitled “Scale and translation-invariance for novel objects in human vision” discusses how they study this phenomenon more carefully to create novel biologically inspired networks.

""Humans can learn from very few examples, unlike deep networks. This is a huge difference with vast implications for engineering of vision systems and for understanding how human vision really works,"" states co-author Tomaso Poggio — director of the Center for Brains, Minds and Machines (CBMM) and the Eugene McDermott Professor of Brain and Cognitive Sciences at MIT. ""A key reason for this difference is the relative invariance of the primate visual system to scale, shift, and other transformations. Strangely, this has been mostly neglected in the AI community, in part because the psychophysical data were so far less than clear-cut. Han's work has now established solid measurements of basic invariances of human vision.”

To differentiate invariance rising from intrinsic computation with that from experience and memorization, the new study measured the range of invariance in one-shot learning. A one-shot learning task was performed by presenting Korean letter stimuli to human subjects who were unfamiliar with the language. These letters were initially presented a single time under one specific condition and tested at different scales or positions than the original condition. The first experimental result is that — just as you guessed — humans showed significant scale-invariant recognition after only a single exposure to these novel objects. The second result is that the range of position-invariance is limited, depending on the size and placement of objects.

Next, Han and her colleagues performed a comparable experiment in deep neural networks designed to reproduce this human performance. The results suggest that to explain invariant recognition of objects by humans, neural network models should explicitly incorporate built-in scale-invariance. In addition, limited position-invariance of human vision is better replicated in the network by having the model neurons’ receptive fields increase as they are further from the center of the visual field. This architecture is different from commonly used neural network models, where an image is processed under uniform resolution with the same shared filters.

“Our work provides a new understanding of the brain representation of objects under different viewpoints. It also has implications for AI, as the results provide new insights into what is a good architectural design for deep neural networks,” remarks Han, CBMM researcher and lead author of the study.

Han and Poggio were joined by Gemma Roig and Gad Geiger in the work."
337;news.mit.edu;http://news.mit.edu/2020/artificial-intelligence-identifies-new-antibiotic-0220;;Artificial intelligence yields new antibiotic;"Using a machine-learning algorithm, MIT researchers have identified a powerful new antibiotic compound. In laboratory tests, the drug killed many of the world’s most problematic disease-causing bacteria, including some strains that are resistant to all known antibiotics. It also cleared infections in two different mouse models.

The computer model, which can screen more than a hundred million chemical compounds in a matter of days, is designed to pick out potential antibiotics that kill bacteria using different mechanisms than those of existing drugs.

“We wanted to develop a platform that would allow us to harness the power of artificial intelligence to usher in a new age of antibiotic drug discovery,” says James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering. “Our approach revealed this amazing molecule which is arguably one of the more powerful antibiotics that has been discovered.”

In their new study, the researchers also identified several other promising antibiotic candidates, which they plan to test further. They believe the model could also be used to design new drugs, based on what it has learned about chemical structures that enable drugs to kill bacteria.

“The machine learning model can explore, in silico, large chemical spaces that can be prohibitively expensive for traditional experimental approaches,” says Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL).

Barzilay and Collins, who are faculty co-leads for MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health (J-Clinic), are the senior authors of the study, which appears today in Cell. The first author of the paper is Jonathan Stokes, a postdoc at MIT and the Broad Institute of MIT and Harvard.

A new pipeline

Over the past few decades, very few new antibiotics have been developed, and most of those newly approved antibiotics are slightly different variants of existing drugs. Current methods for screening new antibiotics are often prohibitively costly, require a significant time investment, and are usually limited to a narrow spectrum of chemical diversity.

“We’re facing a growing crisis around antibiotic resistance, and this situation is being generated by both an increasing number of pathogens becoming resistant to existing antibiotics, and an anemic pipeline in the biotech and pharmaceutical industries for new antibiotics,” Collins says.

To try to find completely novel compounds, he teamed up with Barzilay, Professor Tommi Jaakkola, and their students Kevin Yang, Kyle Swanson, and Wengong Jin, who have previously developed machine-learning computer models that can be trained to analyze the molecular structures of compounds and correlate them with particular traits, such as the ability to kill bacteria.

The idea of using predictive computer models for “in silico” screening is not new, but until now, these models were not sufficiently accurate to transform drug discovery. Previously, molecules were represented as vectors reflecting the presence or absence of certain chemical groups. However, the new neural networks can learn these representations automatically, mapping molecules into continuous vectors which are subsequently used to predict their properties.

In this case, the researchers designed their model to look for chemical features that make molecules effective at killing E. coli. To do so, they trained the model on about 2,500 molecules, including about 1,700 FDA-approved drugs and a set of 800 natural products with diverse structures and a wide range of bioactivities.

Once the model was trained, the researchers tested it on the Broad Institute’s Drug Repurposing Hub, a library of about 6,000 compounds. The model picked out one molecule that was predicted to have strong antibacterial activity and had a chemical structure different from any existing antibiotics. Using a different machine-learning model, the researchers also showed that this molecule would likely have low toxicity to human cells.

This molecule, which the researchers decided to call halicin, after the fictional artificial intelligence system from “2001: A Space Odyssey,” has been previously investigated as possible diabetes drug. The researchers tested it against dozens of bacterial strains isolated from patients and grown in lab dishes, and found that it was able to kill many that are resistant to treatment, including Clostridium difficile, Acinetobacter baumannii, and Mycobacterium tuberculosis. The drug worked against every species that they tested, with the exception of Pseudomonas aeruginosa, a difficult-to-treat lung pathogen.

To test halicin’s effectiveness in living animals, the researchers used it to treat mice infected with A. baumannii, a bacterium that has infected many U.S. soldiers stationed in Iraq and Afghanistan. The strain of A. baumannii that they used is resistant to all known antibiotics, but application of a halicin-containing ointment completely cleared the infections within 24 hours.

Preliminary studies suggest that halicin kills bacteria by disrupting their ability to maintain an electrochemical gradient across their cell membranes. This gradient is necessary, among other functions, to produce ATP (molecules that cells use to store energy), so if the gradient breaks down, the cells die. This type of killing mechanism could be difficult for bacteria to develop resistance to, the researchers say.

“When you’re dealing with a molecule that likely associates with membrane components, a cell can’t necessarily acquire a single mutation or a couple of mutations to change the chemistry of the outer membrane. Mutations like that tend to be far more complex to acquire evolutionarily,” Stokes says.

In this study, the researchers found that E. coli did not develop any resistance to halicin during a 30-day treatment period. In contrast, the bacteria started to develop resistance to the antibiotic ciprofloxacin within one to three days, and after 30 days, the bacteria were about 200 times more resistant to ciprofloxacin than they were at the beginning of the experiment.

The researchers plan to pursue further studies of halicin, working with a pharmaceutical company or nonprofit organization, in hopes of developing it for use in humans.

Optimized molecules

After identifying halicin, the researchers also used their model to screen more than 100 million molecules selected from the ZINC15 database, an online collection of about 1.5 billion chemical compounds. This screen, which took only three days, identified 23 candidates that were structurally dissimilar from existing antibiotics and predicted to be nontoxic to human cells.

In laboratory tests against five species of bacteria, the researchers found that eight of the molecules showed antibacterial activity, and two were particularly powerful. The researchers now plan to test these molecules further, and also to screen more of the ZINC15 database.

The researchers also plan to use their model to design new antibiotics and to optimize existing molecules. For example, they could train the model to add features that would make a particular antibiotic target only certain bacteria, preventing it from killing beneficial bacteria in a patient’s digestive tract.

“This groundbreaking work signifies a paradigm shift in antibiotic discovery and indeed in drug discovery more generally,” says Roy Kishony, a professor of biology and computer science at Technion (the Israel Institute of Technology), who was not involved in the study. “Beyond in silica screens, this approach will allow using deep learning at all stages of antibiotic development, from discovery to improved efficacy and toxicity through drug modifications and medicinal chemistry.”

The research was funded by the Abdul Latif Jameel Clinic for Machine Learning in Health, the Defense Threat Reduction Agency, the Broad Institute, the DARPA Make-It Program, the Canadian Institutes of Health Research, the Canadian Foundation for Innovation, the Canada Research Chairs Program, the Banting Fellowships Program, the Human Frontier Science Program, the Pershing Square Foundation, the Swiss National Science Foundation, a National Institutes of Health Early Investigator Award, the National Science Foundation Graduate Research Fellowship Program, and a gift from Anita and Josh Bekenstein."
338;machinelearningmastery.com;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/;2019-08-11;A Tour of Machine Learning Algorithms;"Tweet Share Share

Last Updated on December 5, 2019

In this post, we will take a tour of the most popular machine learning algorithms.

It is useful to tour the main algorithms in the field to get a feeling of what methods are available.

There are so many algorithms that it can feel overwhelming when algorithm names are thrown around and you are expected to just know what they are and where they fit.

I want to give you two ways to think about and categorize the algorithms you may come across in the field.

The first is a grouping of algorithms by their learning style .

. The second is a grouping of algorithms by their similarity in form or function (like grouping similar animals together).

Both approaches are useful, but we will focus in on the grouping of algorithms by similarity and go on a tour of a variety of different algorithm types.

After reading this post, you will have a much better understanding of the most popular machine learning algorithms for supervised learning and how they are related.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Algorithms Grouped by Learning Style

There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data.

It is popular in machine learning and artificial intelligence textbooks to first consider the learning styles that an algorithm can adopt.

There are only a few main learning styles or learning models that an algorithm can have and we’ll go through them here with a few examples of algorithms and problem types that they suit.

This taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.

Let’s take a look at three different learning styles in machine learning algorithms:

1. Supervised Learning

Input data is called training data and has a known label or result such as spam/not-spam or a stock price at a time.

A model is prepared through a training process in which it is required to make predictions and is corrected when those predictions are wrong. The training process continues until the model achieves a desired level of accuracy on the training data.

Example problems are classification and regression.

Example algorithms include: Logistic Regression and the Back Propagation Neural Network.

2. Unsupervised Learning

Input data is not labeled and does not have a known result.

A model is prepared by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to systematically reduce redundancy, or it may be to organize data by similarity.

Example problems are clustering, dimensionality reduction and association rule learning.

Example algorithms include: the Apriori algorithm and K-Means.

3. Semi-Supervised Learning

Input data is a mixture of labeled and unlabelled examples.

There is a desired prediction problem but the model must learn the structures to organize the data as well as make predictions.

Example problems are classification and regression.

Example algorithms are extensions to other flexible methods that make assumptions about how to model the unlabeled data.

Overview of Machine Learning Algorithms

When crunching data to model business decisions, you are most typically using supervised and unsupervised learning methods.

A hot topic at the moment is semi-supervised learning methods in areas such as image classification where there are large datasets with very few labeled examples.

Algorithms Grouped By Similarity

Algorithms are often grouped by similarity in terms of their function (how they work). For example, tree-based methods, and neural network inspired methods.

I think this is the most useful way to group algorithms and it is the approach we will use here.

This is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories like Learning Vector Quantization that is both a neural network inspired method and an instance-based method. There are also categories that have the same name that describe the problem and the class of algorithm such as Regression and Clustering.

We could handle these cases by listing algorithms twice or by selecting the group that subjectively is the “best” fit. I like this latter approach of not duplicating algorithms to keep things simple.

In this section, we list many of the popular machine learning algorithms grouped the way we think is the most intuitive. The list is not exhaustive in either the groups or the algorithms, but I think it is representative and will be useful to you to get an idea of the lay of the land.

Please Note: There is a strong bias towards algorithms used for classification and regression, the two most prevalent supervised machine learning problems you will encounter.

If you know of an algorithm or a group of algorithms not listed, put it in the comments and share it with us. Let’s dive in.

Regression Algorithms

Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.

Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process.

The most popular regression algorithms are:

Ordinary Least Squares Regression (OLSR)

Linear Regression

Logistic Regression

Stepwise Regression

Multivariate Adaptive Regression Splines (MARS)

Locally Estimated Scatterplot Smoothing (LOESS)

Instance-based Algorithms

Instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model.

Such methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on the representation of the stored instances and similarity measures used between instances.

The most popular instance-based algorithms are:

k-Nearest Neighbor (kNN)

Learning Vector Quantization (LVQ)

Self-Organizing Map (SOM)

Locally Weighted Learning (LWL)

Support Vector Machines (SVM)

Regularization Algorithms

An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.

I have listed regularization algorithms separately here because they are popular, powerful and generally simple modifications made to other methods.

The most popular regularization algorithms are:

Ridge Regression

Least Absolute Shrinkage and Selection Operator (LASSO)

Elastic Net

Least-Angle Regression (LARS)

Decision Tree Algorithms

Decision tree methods construct a model of decisions made based on actual values of attributes in the data.

Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.

The most popular decision tree algorithms are:

Classification and Regression Tree (CART)

Iterative Dichotomiser 3 (ID3)

C4.5 and C5.0 (different versions of a powerful approach)

Chi-squared Automatic Interaction Detection (CHAID)

Decision Stump

M5

Conditional Decision Trees

Bayesian Algorithms

Bayesian methods are those that explicitly apply Bayes’ Theorem for problems such as classification and regression.

The most popular Bayesian algorithms are:

Naive Bayes

Gaussian Naive Bayes

Multinomial Naive Bayes

Averaged One-Dependence Estimators (AODE)

Bayesian Belief Network (BBN)

Bayesian Network (BN)

Clustering Algorithms

Clustering, like regression, describes the class of problem and the class of methods.

Clustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.

The most popular clustering algorithms are:

k-Means

k-Medians

Expectation Maximisation (EM)

Hierarchical Clustering

Association Rule Learning Algorithms

Association rule learning methods extract rules that best explain observed relationships between variables in data.

These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.

The most popular association rule learning algorithms are:

Apriori algorithm

Eclat algorithm

Artificial Neural Network Algorithms

Artificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks.

They are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.

Note that I have separated out Deep Learning from neural networks because of the massive growth and popularity in the field. Here we are concerned with the more classical methods.

The most popular artificial neural network algorithms are:

Perceptron

Multilayer Perceptrons (MLP)

Back-Propagation

Stochastic Gradient Descent

Hopfield Network

Radial Basis Function Network (RBFN)

Deep Learning Algorithms

Deep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation.

They are concerned with building much larger and more complex neural networks and, as commented on above, many methods are concerned with very large datasets of labelled analog data, such as image, text. audio, and video.

The most popular deep learning algorithms are:

Convolutional Neural Network (CNN)

Recurrent Neural Networks (RNNs)

Long Short-Term Memory Networks (LSTMs)

Stacked Auto-Encoders

Deep Boltzmann Machine (DBM)

Deep Belief Networks (DBN)

Dimensionality Reduction Algorithms

Like clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarize or describe data using less information.

This can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.

Principal Component Analysis (PCA)

Principal Component Regression (PCR)

Partial Least Squares Regression (PLSR)

Sammon Mapping

Multidimensional Scaling (MDS)

Projection Pursuit

Linear Discriminant Analysis (LDA)

Mixture Discriminant Analysis (MDA)

Quadratic Discriminant Analysis (QDA)

Flexible Discriminant Analysis (FDA)

Ensemble Algorithms

Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.

Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.

Boosting

Bootstrapped Aggregation (Bagging)

AdaBoost

Weighted Average (Blending)

Stacked Generalization (Stacking)

Gradient Boosting Machines (GBM)

Gradient Boosted Regression Trees (GBRT)

Random Forest

Other Machine Learning Algorithms

Many algorithms were not covered.

I did not cover algorithms from specialty tasks in the process of machine learning, such as:

Feature selection algorithms

Algorithm accuracy evaluation

Performance measures

Optimization algorithms

I also did not cover algorithms from specialty subfields of machine learning, such as:

Computational intelligence (evolutionary algorithms, etc.)

Computer Vision (CV)

Natural Language Processing (NLP)

Recommender Systems

Reinforcement Learning

Graphical Models

And more…

These may feature in future posts.

Further Reading on Machine Learning Algorithms

This tour of machine learning algorithms was intended to give you an overview of what is out there and some ideas on how to relate algorithms to each other.

I’ve collected together some resources for you to continue your reading on algorithms. If you have a specific question, please leave a comment.

Other Lists of Machine Learning Algorithms

There are other great lists of algorithms out there if you’re interested. Below are few hand selected examples.

How to Study Machine Learning Algorithms

Algorithms are a big part of machine learning. It’s a topic I am passionate about and write about a lot on this blog. Below are few hand selected posts that might interest you for further reading.

How to Run Machine Learning Algorithms

Sometimes you just want to dive into code. Below are some links you can use to run machine learning algorithms, code them up using standard libraries or implement them from scratch.

Final Word

I hope you have found this tour useful.

Please, leave a comment if you have any questions or ideas on how to improve the algorithm tour.

Update: Continue the discussion on HackerNews and reddit.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside"
339;news.mit.edu;http://news.mit.edu/2020/researching-from-home-picower-science-stays-strong-even-at-distance-0407;;Researching from home: Science stays social, even at a distance;"With all but a skeleton crew staying home from each lab to minimize the spread of Covid-19, scores of Picower Institute researchers are immersing themselves in the considerable amount of scientific work that can done away from the bench. With piles of data to analyze; plenty of manuscripts to write; new skills to acquire; and fresh ideas to conceive, share, and refine for the future, neuroscientists have full plates, even when they are away from their, well, plates. They are proving that science can remain social, even if socially distant.

Ever since the mandatory ramp down of on-campus research took hold March 20, for example, teams of researchers in the lab of Troy Littleton, the Menicon Professor of Neuroscience, have sharpened their focus on two data-analysis projects that are every bit as essential to their science as acquiring the data in the lab in the first place. Research scientist Yulia Akbergenova and graduate student Karen Cunningham, for example, are poring over a huge amount of imaging data showing how the strength of connections between neurons, or synapses, mature and how that depends on the molecular components at the site. Another team, comprised of Picower postdoc Suresh Jetti and graduate students Andres Crane and Nicole Aponte-Santiago, is analyzing another large dataset, this time of gene transcription, to learn what distinguishes two subclasses of motor neurons that form synapses of characteristically different strength.

Work is similarly continuing among researchers in the lab of Elly Nedivi, the William R. (1964) and Linda R. Young Professor of Neuroscience. Since heading home, Senior Research Support Associate Kendyll Burnell has been looking at microscope images tracking how inhibitory interneurons innervate the visual cortex of mice throughout their development. By studying the maturation of inhibition, the lab hopes to improve understanding of the role of inhibitory circuitry in the experience-dependent changes, or plasticity, and development of the visual cortex, she says. As she’s worked, her poodle Soma (named for the central body structure of a neuron) has been by her side.

Despite extra time with comforts of home, though, it’s clear that nobody wanted this current mode of socially distant science. For every lab, it’s tremendously disruptive and costly. But labs are finding many ways to make progress nonetheless.

“Although we are certainly hurting because our lab work is at a standstill, the Miller lab is fortunate to have a large library of multiple-electrode neurophysiological data,” says Picower Professor Earl Miller. “The datasets are very rich. As our hypotheses and analytical tools develop, we can keep going back to old data to ask new questions. We are taking advantage of the wet lab downtime to analyze data and write papers. We have three under review and are writing at least three more right now.”

Miller is inviting new collaborations regardless of the physical impediment of social distancing. A recent lab meeting held via the videoconferencing app Zoom included MIT Department of Brain and Cognitive Sciences Associate Professor Ila Fiete and her graduate student, Mikail Khona. The Miller lab has begun studying how neural rhythms move around the cortex and what that means for brain function. Khona presented models of how timing relationships affect those waves. While this kind of an interaction between labs of the Picower Institute and the McGovern Institute for Brain Research would normally have taken place in person in MIT’s Building 46, neither lab let the pandemic get in the way.

Similarly, the lab of Li-Huei Tsai, Picower Professor and director of the Picower Institute, has teamed up with that of Manolis Kellis, professor in the MIT Computer Science and Artificial Intelligence Laboratory. They’re forming several small squads of experimenters and computational experts to launch analyses of gene expression and other data to illuminate the fate of individual cell types like interneurons or microglia in the context of the Alzheimer’s disease-afflicted brain. Other teams are focusing on analyses of questions such as how pathology varies in brain samples carrying different degrees of genetic risk factors. These analyses will prove useful for stages all along the scientific process, Tsai says, from forming new hypotheses to wrapping up papers that are well underway.

Remote collaboration and communication are proving crucial to researchers in other ways, too, proving that online interactions, though distant, can be quite personally fulfilling.

Nicholas DiNapoli, a research engineer in the lab of Associate Professor Kwanghun Chung, is making the best of time away from the bench by learning about the lab’s computational pipeline for processing the enormous amounts of imaging data it generates. He’s also taking advantage of a new program within the lab in which Senior Computer Scientist Lee Kamentsky is teaching Python computer programming principles to anyone in the lab who wants to learn. The training occurs via Zoom two days a week.

As part of a crowded calendar of Zoom meetings, or “Zeetings” as the lab has begun to call them, Newton Professor Mriganka Sur says he makes sure to have one-to-one meetings with everyone in the lab. The team also has organized into small subgroups around different themes of the lab’s research.

But also, the lab has continued to maintain its cohesion by banding together informally creating novel work and social experiences.

Graduate student Ning Leow, for example, used Zoom to create a co-working session in which participants kept a video connection open for hours at a time, just to be in each other’s virtual presence while they worked. Among a group of Sur lab friends, she read a paper related to her thesis and did a substantial amount of data analysis. She also advised a colleague on an analysis technique via the connection.

“I’ve got to say that it worked out really well for me personally because I managed to get whatever I wanted to complete on my list done,” she says, “and there was also a sense of healthy accountability along with the sense of community.”

Whether in person or via an officially imposed distance, science is social. In that spirit, graduate student K. Guadalupe ""Lupe"" Cruz organized a collaborative art event via Zoom for female scientists in brain and cognitive sciences at MIT. She took a photo of Rosalind Franklin, the scientist whose work was essential for resolving the structure of DNA, and divided it into nine squares to distribute to the event attendees. Without knowing the full picture, everyone drew just their section, talking all the while about how the strange circumstances of Covid-19 have changed their lives. At the end, they stitched their squares together to reconstruct the image.

Examples abound of how Picower scientists, though mostly separate and apart, are still coming together to advance their research and to maintain the fabric of their shared experiences."
340;machinelearningmastery.com;https://machinelearningmastery.com/how-to-predict-room-occupancy-based-on-environmental-factors/;2018-08-28;How to Predict Room Occupancy Based on Environmental Factors;"from pandas import read_csv

from matplotlib import pyplot

# load all data

data1 = read_csv ( 'datatest.txt' , header = 0 , index_col = 1 , parse_dates = True , squeeze = True )

data2 = read_csv ( 'datatraining.txt' , header = 0 , index_col = 1 , parse_dates = True , squeeze = True )

data3 = read_csv ( 'datatest2.txt' , header = 0 , index_col = 1 , parse_dates = True , squeeze = True )

# determine the number of features

n_features = data1 . values . shape [ 1 ]

pyplot . figure ( )

for i in range ( 1 , n_features ) :

# specify the subpout

pyplot . subplot ( n_features , 1 , i )

# plot data from each set

pyplot . plot ( data1 . index , data1 . values [ : , i ] )

pyplot . plot ( data2 . index , data2 . values [ : , i ] )

pyplot . plot ( data3 . index , data3 . values [ : , i ] )

# add a readable name to the plot

pyplot . title ( data1 . columns [ i ] , y = 0.5 , loc = 'right' )"
341;machinelearningmastery.com;https://machinelearningmastery.com/suitability-long-short-term-memory-networks-time-series-forecasting/;2017-05-25;On the Suitability of Long Short-Term Memory Networks for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Long Short-Term Memory (LSTM) is a type of recurrent neural network that can learn the order dependence between items in a sequence.

LSTMs have the promise of being able to learn the context required to make predictions in time series forecasting problems, rather than having this context pre-specified and fixed.

Given the promise, there is some doubt as to whether LSTMs are appropriate for time series forecasting.

In this post, we will look at the application of LSTMs to time series forecasting by some of the leading developers of the technique.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

LSTM for Time Series Forecasting

We will take a closer look at a paper that seeks to explore the suitability of LSTMs for time series forecasting.

The paper is titled “Applying LSTM to Time Series Predictable through Time-Window Approaches” (get the PDF, Gers, Eck and Schmidhuber, published in 2001.

They start off by commenting that univariate time series forecasting problems are actually simpler than the types of problems traditionally used to demonstrate the capabilities of LSTMs.

Time series benchmark problems found in the literature … are often conceptually simpler than many tasks already solved by LSTM. They often do not require RNNs at all, because all relevant information about the next event is conveyed by a few recent events contained within a small time window.

The paper focuses on the application of LSTMs to two complex time series forecasting problems and contrasting the results of LSTMs to other types of neural networks.

The focus of the study are two classical time series problems:

Mackey-Glass Series

This is a contrived time series calculated from a differential equation.

For more information, see:

Chaotic Laser Data (Set A)

This is a series taken from a from a contest at the Santa Fe Institute.

Set A is defined as:

A clean physics laboratory experiment. 1,000 points of the fluctuations in a far-infrared laser, approximately described by three coupled nonlinear ordinary differential equations.

For more information, see:

Section 2 “The Competition” in The Future of Time Series, 1993.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Autoregression

An autoregression (AR) approach was used to model these problems.

This means that the next time step was taken as a function of some number of past (or lag) observations.

This is a common approach for classical statistical time series forecasting.

The LSTM is exposed to one input at a time with no fixed set of lag variables, as the windowed-multilayer Perceptron (MLP).

For more information on AR for time series, see the post:

Analysis of Results

Some of the more salient comments were in response to the poor results of the LSTMs on the Mackey-Glass Series problem.

First, they comment that increasing the learning capacity of the network did not help:

Increasing the number of memory blocks did not significantly improve the results.

This may have required a further increase in the number of training epochs. It is also possible that a stack of LSTMs may have improved results.

They comment that in order to do well on the Mackey-Glass Series, the LSTM is required to remember recent past observations, whereas the MLP is given this data explicitly.

The results for the AR-LSTM approach are clearly worse than the results for the time window approaches, for example with MLPs. The AR-LSTM network does not have access to the past as part of its input … [for the LSTM to do well] required remembering one or two events from the past, then using this information before over-writing the same memory cells.

They comment that in general, this poses more of a challenge for LSTMs and RNNs than it does for MLPs.

Assuming that any dynamic model needs all inputs from t-tau …, we note that the AR-RNN has to store all inputs from t-tau to t and overwrite them at the adequate time. This requires the implementation of a circular buffer, a structure quite difficult for an RNN to simulate.

Again, I can’t help but think that a much larger hidden layer (more memory units) and a much deeper network (stacked LSTMs) would be better suited to learn multiple past observations.

They later conclude the paper and discuss that based on the results, LSTMs may not be suited to AR type formulations of time series forecasting, at least when the lagged observations are close to the time being forecasted.

This is a fair conclusion given the LSTMs performance compared to MLPs on the tested univariate problems.

A time window based MLP outperformed the LSTM pure-AR approach on certain time series prediction benchmarks solvable by looking at a few recent inputs only. Thus LSTM’s special strength, namely, to learn to remember single events for very long, unknown time periods, was not necessary here. LSTM learned to tune into the fundamental oscillation of each series but was unable to accurately follow the signal.

They do highlight the LSTMs ability to learn oscillation behavior (e.g. cycles or seasonality).

Our results suggest to use LSTM only on tasks where traditional time window-based approaches must fail. LSTM’s ability to track slow oscillations in the chaotic signal may be applicable to cognitive domains such as rhythm detection in speech and music.

This is interesting, but perhaps not as useful, as such patterns are often explicitly removed wherever possible prior to forecasting. Nevertheless, it may highlight the possibility of LSTMs learning to forecast in the context of a non-stationary series.

Final Word

So, what does all of this mean?

Taken at face value, we may conclude that LSTMs are unsuitable for AR-based univariate time series forecasting. That we should turn first to MLPs with a fixed window and only to LSTMs if MLPs cannot achieve a good result.

This sounds fair.

I would argue a few points that should be considered before we write-off LSTMs for time series forecasting:

Consider more sophisticated data preparation, such as at least scaling and stationarity. If a cycle or trend is obvious, then it should be removed so that the model can focus on the underlying signal. That being said, the capability of LSTMs to perform well on non-stationary data as well or better than other methods is intriguing, but I would expect would be commensurate with an increase in required network capacity and training.

Consider the use of both larger models and hierarchical models (stacked LSTMs) to automatically learn (or “remember”) a larger temporal dependence. Larger models can learn more.

Consider fitting the model for much longer, e.g. thousands or hundreds of thousands of epochs, whilst making use of regularization techniques. LSTMs take a long time to learn complex dependencies.

I won’t point out that we can move beyond AR based models; it’s obvious, and AR models are a nice clean proving ground for LSTMs to consider and take on classical statistical methods like ARIMA and well performing neural nets like window MLPs.

I believe there’s great promise and opportunity for LSTMs applied to the problem of time series forecasting.

Do you agree?

Let me know in the comments below.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
342;machinelearningmastery.com;https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/;2018-10-30;Comparing Classical and Machine Learning Algorithms for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Machine learning and deep learning methods are often reported to be the key solution to all predictive modeling problems.

An important recent study evaluated and compared the performance of many classical and modern machine learning and deep learning methods on a large and diverse set of more than 1,000 univariate time series forecasting problems.

The results of this study suggest that simple classical methods, such as linear methods and exponential smoothing, outperform complex and sophisticated methods, such as decision trees, Multilayer Perceptrons (MLP), and Long Short-Term Memory (LSTM) network models.

These findings highlight the requirement to both evaluate classical methods and use their results as a baseline when evaluating any machine learning and deep learning methods for time series forecasting in order demonstrate that their added complexity is adding skill to the forecast.

In this post, you will discover the important findings of this recent study evaluating and comparing the performance of a classical and modern machine learning methods on a large and diverse set of time series forecasting datasets.

After reading this post, you will know:

Classical methods like ETS and ARIMA out-perform machine learning and deep learning methods for one-step forecasting on univariate datasets.

Classical methods like Theta and ARIMA out-perform machine learning and deep learning methods for multi-step forecasting on univariate datasets.

Machine learning and deep learning methods do not yet deliver on their promise for univariate time series forecasting, and there is much work to do.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Overview

Spyros Makridakis, et al. published a study in 2018 titled “Statistical and Machine Learning forecasting methods: Concerns and ways forward.”

In this post, we will take a close look at the study by Makridakis, et al. that carefully evaluated and compared classical time series forecasting methods to the performance of modern machine learning methods.

This post is divided into seven sections; they are:

Study Motivation Time Series Datasets Time Series Forecasting Methods Data Preparation One-Step Forecasting Results Multi-Step Forecasting Results Outcomes

Study Motivation

The goal of the study was to clearly demonstrate the capability of a suite of different machine learning methods as compared to classical time series forecasting methods on a very large and diverse collection of univariate time series forecasting problems.

The study was a response to the increasing number of papers and claims that machine learning and deep learning methods offer superior results for time series forecasting with little objective evidence.

Literally hundreds of papers propose new ML algorithms, suggesting methodological advances and accuracy improvements. Yet, limited objective evidence is available regarding their relative performance as a standard forecasting tool.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

The authors clearly lay out three issues with the flood of claims; they are:

Their conclusions are based on a few, or even a single time series, raising questions about the statistical significance of the results and their generalization.

The methods are evaluated for short-term forecasting horizons, often one-step-ahead, not considering medium and long-term ones.

No benchmarks are used to compare the accuracy of ML methods versus alternative ones.

As a response, the study includes eight classical methods and 10 machine learning methods evaluated using one-step and multiple-step forecasts across a collection of 1,045 monthly time series.

Although not definitive, the results are intended to be objective and robust.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Time Series Datasets

The time series datasets used in the study were drawn from the time series datasets used in the M3-Competition.

The M3-Competition was the third in a series of competitions that sought to discover exactly what algorithms perform well in practice on real time series forecasting problems. The results of the competition were published in the 2000 paper titled “The M3-Competition: Results, Conclusions and Implications.”

The datasets used in the competition were drawn from a wide range of industries and had a range of different time intervals, from hourly to annual.

The 3003 series of the M3-Competition were selected on a quota basis to include various types of time series data (micro, industry, macro, etc.) and different time intervals between successive observations (yearly, quarterly, etc.).

The table below, taken from the paper, provides a summary of the 3,003 datasets used in the competition.

The finding of the competition was that simpler time series forecasting methods outperform more sophisticated methods, including neural network models.

This study, the previous two M-Competitions and many other empirical studies have proven, beyond the slightest doubt, that elaborate theoretical constructs or more sophisticated methods do not necessarily improve post-sample forecasting accuracy, over simple methods, although they can better fit a statistical model to the available historical data.

— The M3-Competition: Results, Conclusions and Implications, 2000.

The more recent study that we are reviewing in this post that evaluate machine learning methods selected a subset of 1,045 time series with a monthly interval from those used in the M3 competition.

… evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

Time Series Forecasting Methods

The study evaluates the performance of eight classical (or simpler) methods and 10 machine learning methods.

… of eight traditional statistical methods and eight popular ML ones, […], plus two more that have become popular during recent years.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

The eight classical methods evaluated were as follows:

Naive 2, which is actually a random walk model adjusted for season.

Simple Exponential Smoothing.

Holt.

Damped exponential smoothing.

Average of SES, Holt, and Damped.

Theta method.

ARIMA, automatic.

ETS, automatic.

A total of eight machine learning methods were used in an effort to reproduce and compare to results presented in the 2010 paper “An Empirical Comparison of Machine Learning Models for Time Series Forecasting.”

They were:

Multi-Layer Perceptron (MLP)

Bayesian Neural Network (BNN)

Radial Basis Functions (RBF)

Generalized Regression Neural Networks (GRNN), also called kernel regression

K-Nearest Neighbor regression (KNN)

CART regression trees (CART)

Support Vector Regression (SVR)

Gaussian Processes (GP)

An additional two ‘modern‘ neural network algorithms were also added to the list given the recent rise in their adoption; they were:

Recurrent Neural Network (RNN)

Long Short-Term Memory (LSTM)

Data Preparation

A careful data preparation methodology was used, again, based on the methodology described in the 2010 paper “An Empirical Comparison of Machine Learning Models for Time Series Forecasting.”

In that paper, each time series was adjusted using a power transform, deseasonalized and detrended.

[…] before computing the 18 forecasts, they preprocessed the series in order to achieve stationarity in their mean and variance. This was done using the log transformation, then deseasonalization and finally scaling, while first differences were also considered for removing the component of trend.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

Inspired by these operations, variations of five different data transforms were applied for an MLP for one-step forecasting and their results were compared. The five transforms were:

Original data.

Box-Cox Power Transform.

Deseasonalizing the data.

Detrending the data.

All three transforms (power, deseasonalize, detrend).

Generally, it was found that the best approach was to apply a power transform and deseasonalize the data, and perhaps detrend the series as well.

The best combination according to sMAPE is number 7 (Box-Cox transformation, deseasonalization) while the best one according to MASE is number 10 (Box-Cox transformation, deseasonalization and detrending)

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

One-Step Forecasting Results

All models were evaluated using one-step time series forecasting.

Specifically, the last 18 time steps were used as a test set, and models were fit on all remaining observations. A separate one-step forecast was made for each of the 18 observations in the test set, presumably using a walk-forward validation method where true observations were used as input in order to make each forecast.

The forecasting model was developed using the first n – 18 observations, where n is the length of the series. Then, 18 forecasts were produced and their accuracy was evaluated compared to the actual values not used in developing the forecasting model.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

Reviewing the results, the MLP and BNN were found to achieve the best performance from all of the machine learning methods.

The results […] show that MLP and BNN outperform the remaining ML methods.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

A surprising result was that RNNs and LSTMs were found to perform poorly.

It should be noted that RNN is among the less accurate ML methods, demonstrating that research progress does not necessarily guarantee improvements in forecasting performance. This conclusion also applies in the performance of LSTM, another popular and more advanced ML method, which does not enhance forecasting accuracy too.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

Comparing the performance of all methods, it was found that the machine learning methods were all out-performed by simple classical methods, where ETS and ARIMA models performed the best overall.

This finding confirms the results from previous similar studies and competitions.

Multi-Step Forecasting Results

Multi-step forecasting involves predicting multiple steps ahead of the last known observation.

Three approaches to multi-step forecasting were evaluated for the machine learning methods; they were:

Iterative forecasting

Direct forecasting

Multi-neural network forecasting

The classical methods were found to outperform the machine learning methods again.

In this case, methods such as Theta, ARIMA, and a combination of exponential smoothing (Comb) were found to achieve the best performance.

In brief, statistical models seem to generally outperform ML methods across all forecasting horizons, with Theta, Comb and ARIMA being the dominant ones among the competitors according to both error metrics examined.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

Outcomes

The study provides important supporting evidence that classical methods may dominate univariate time series forecasting, at least on the types of forecasting problems evaluated.

The study demonstrates the worse performance and the increase in computational cost of machine learning and deep learning methods for univariate time series forecasting for both one-step and multi-step forecasts.

These findings strongly encourage the use of classical methods, such as ETS, ARIMA, and others as a first step before more elaborate methods are explored, and requires that the results from these simpler methods be used as a baseline in performance that more elaborate methods must clear in order to justify their usage.

It also highlights the need to not just consider the careful use of data preparation methods, but to actively test multiple different combinations of data preparation schemes for a given problem in order to discover what works best, even in the case of classical methods.

Machine learning and deep learning methods may still achieve better performance on specific univariate time series problems and should be evaluated.

The study does not look at more complex time series problems, such as those datasets with:

Complex irregular temporal structures.

Missing observations

Heavy noise.

Complex interrelationships between multiple variates.

The study concludes with an honest puzzlement at why machine learning methods perform so poorly in practice, given their impressive performance in other areas of artificial intelligence.

The most interesting question and greatest challenge is to find the reasons for their poor performance with the objective of improving their accuracy and exploiting their huge potential. AI learning algorithms have revolutionized a wide range of applications in diverse fields and there is no reason that the same cannot be achieved with the ML methods in forecasting. Thus, we must find how to be applied to improve their ability to forecast more accurately.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

Comments are made by the authors regarding LSTMs and RNNs, that are generally believed to be the deep learning approach for sequence prediction problems in general, and in this case their clearly poor performance in practice.

[…] one would expect RNN and LSTM, which are more advanced types of NNs, to be far more accurate than the ARIMA and the rest of the statistical methods utilized.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

They comment that LSTMs appear to be more suited at fitting or overfitting the training dataset rather than forecasting it.

Another interesting example could be the case of LSTM that compared to simpler NNs like RNN and MLP, report better model fitting but worse forecasting accuracy

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

There is work to do and machine learning methods and deep learning methods hold the promise of better learning time series data than classical statistical methods, and even doing so directly on the raw observations via automatic feature learning.

Given their ability to learn, ML methods should do better than simple benchmarks, like exponential smoothing. Accepting the problem is the first step in devising workable solutions and we hope that those in the field of AI and ML will accept the empirical findings and work to improve the forecasting accuracy of their methods.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

Further Reading

This section provides more resources on the topic if you are looking to go deeper.

Summary

In this post, you discovered the important findings of a recent study evaluating and comparing the performance of classical and modern machine learning methods on a large and diverse set of time series forecasting datasets.

Specifically, you learned:

Classical methods like ETS and ARIMA out-perform machine learning and deep learning methods for one-step forecasting on univariate datasets.

Classical methods like Theta and ARIMA out-perform machine learning and deep learning methods for multi-step forecasting on univariate datasets.

Machine learning and deep learning methods do not yet deliver on their promise for univariate time series forecasting and there is much work to do.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Time Series Today! Develop Your Own Forecasting models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Time Series Forecasting It provides self-study tutorials on topics like:

CNNs, LSTMs, Multivariate Forecasting, Multi-Step Forecasting and much more... Finally Bring Deep Learning to your Time Series Forecasting Projects Skip the Academics. Just Results. See What's Inside"
343;machinelearningmastery.com;http://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/;2016-07-27;Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras;"# LSTM with Variable Length Input Sequences to One Character Output

import numpy

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from keras . utils import np_utils

from keras . preprocessing . sequence import pad_sequences

from theano . tensor . shared_randomstreams import RandomStreams

# fix random seed for reproducibility

numpy . random . seed ( 7 )

# define the raw dataset

alphabet = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""

# create mapping of characters to integers (0-25) and the reverse

char_to_int = dict ( ( c , i ) for i , c in enumerate ( alphabet ) )

int_to_char = dict ( ( i , c ) for i , c in enumerate ( alphabet ) )

# prepare the dataset of input to output pairs encoded as integers

num_inputs = 1000

max_len = 5

dataX = [ ]

dataY = [ ]

for i in range ( num_inputs ) :

start = numpy . random . randint ( len ( alphabet ) - 2 )

end = numpy . random . randint ( start , min ( start + max_len , len ( alphabet ) - 1 ) )

sequence_in = alphabet [ start : end + 1 ]

sequence_out = alphabet [ end + 1 ]

dataX . append ( [ char_to_int [ char ] for char in sequence_in ] )

dataY . append ( char_to_int [ sequence_out ] )

print ( sequence_in , '->' , sequence_out )

# convert list of lists to array and pad sequences if needed

X = pad_sequences ( dataX , maxlen = max_len , dtype = 'float32' )

# reshape X to be [samples, time steps, features]

X = numpy . reshape ( X , ( X . shape [ 0 ] , max_len , 1 ) )

# normalize

X = X / float ( len ( alphabet ) )

# one hot encode the output variable

y = np_utils . to_categorical ( dataY )

# create and fit the model

batch_size = 1

model = Sequential ( )

model . add ( LSTM ( 32 , input_shape = ( X . shape [ 1 ] , 1 ) ) )

model . add ( Dense ( y . shape [ 1 ] , activation = 'softmax' ) )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

model . fit ( X , y , epochs = 500 , batch_size = batch_size , verbose = 2 )

# summarize performance of the model

scores = model . evaluate ( X , y , verbose = 0 )

print ( ""Model Accuracy: %.2f%%"" % ( scores [ 1 ] * 100 ) )

# demonstrate some model predictions

for i in range ( 20 ) :

pattern_index = numpy . random . randint ( len ( dataX ) )

pattern = dataX [ pattern_index ]

x = pad_sequences ( [ pattern ] , maxlen = max_len , dtype = 'float32' )

x = numpy . reshape ( x , ( 1 , max_len , 1 ) )

x = x / float ( len ( alphabet ) )

prediction = model . predict ( x , verbose = 0 )

index = numpy . argmax ( prediction )

result = int_to_char [ index ]

seq_in = [ int_to_char [ value ] for value in pattern ]"
344;news.mit.edu;http://news.mit.edu/2020/urine-sensor-test-detect-lung-tumors-0401;;New sensors could offer early detection of lung tumors;"People who are at high risk of developing lung cancer, such as heavy smokers, are routinely screened with computed tomography (CT), which can detect tumors in the lungs. However, this test has an extremely high rate of false positives, as it also picks up benign nodules in the lungs.

Researchers at MIT have now developed a new approach to early diagnosis of lung cancer: a urine test that can detect the presence of proteins linked to the disease. This kind of noninvasive test could reduce the number of false positives and help detect more tumors in the early stages of the disease.

Early detection is very important for lung cancer, as the five-year survival rates are at least six times higher in patients whose tumors are detected before they spread to distant locations in the body.

“If you look at the field of cancer diagnostics and therapeutics, there’s a renewed recognition of the importance of early cancer detection and prevention. We really need new technologies that are going to give us the capability to see cancer when we can intercept it and intervene early,” says Sangeeta Bhatia, who is the John and Dorothy Wilson Professor of Health Sciences and Technology and Electrical Engineering and Computer Science, and a member of MIT’s Koch Institute for Integrative Cancer Research and the Institute for Medical Engineering and Science.

Bhatia and her colleagues found that the new test, which is based on nanoparticles that can be injected or inhaled, could detect tumors as small as 2.8 cubic millimeters in mice.

Bhatia is the senior author of the study, which appears today in Science Translational Medicine. The paper’s lead authors are MIT and Harvard University graduate students Jesse Kirkpatrick and Ava Soleimany, and former MIT graduate student Andrew Warren, who is now an associate at Third Rock Ventures.

Targeting lung tumors

For several years, Bhatia’s lab has been developing nanoparticles that can detect cancer by interacting with enzymes called proteases. These enzymes help tumor cells to escape their original locations by cutting through proteins of the extracellular matrix.

To find those proteins, Bhatia created nanoparticles coated with peptides (short protein fragments) that are targeted by cancer-linked proteases. The particles accumulate at tumor sites, where the peptides are cleaved, releasing biomarkers that can then be detected in a urine sample.

Her lab has previously developed sensors for colon and ovarian cancer, and in their new study, the researchers wanted to apply the technology to lung cancer, which kills about 150,000 people in the United States every year. People who receive a CT screen and get a positive result often undergo a biopsy or other invasive test to search for lung cancer. In some cases, this procedure can cause complications, so a noninvasive follow-up test could be useful to determine which patients actually need a biopsy, Bhatia says.

“The CT scan is a good tool that can see a lot of things,” she says. “The problem with it is that 95 percent of what it finds is not cancer, and right now you have to biopsy too many patients who test positive.”

To customize their sensors for lung cancer, the researchers analyzed a database of cancer-related genes called the Cancer Genome Atlas and identified proteases that are abundant in lung cancer. They created a panel of 14 peptide-coated nanoparticles that could interact with these enzymes.

The researchers then tested the sensors in two different mouse models of cancer, both of which are engineered with genetic mutations that lead them to naturally develop lung tumors. To help prevent background noise that could come from other organs or the bloodstream, the researchers injected the particles directly into the airway.

Using these sensors, the researchers performed their diagnostic test at three time points: 5 weeks, 7.5 weeks, and 10.5 weeks after tumor growth began. To make the diagnoses more accurate, they used machine learning to train an algorithm to distinguish between data from mice that had tumors and mice that did not.

With this approach, the researchers found that they could accurately detect tumors in one of the mouse models as early as 7.5 weeks, when the tumors were only 2.8 cubic millimeters, on average. In the other strain of mice, tumors could be detected at 5 weeks. The sensors’ success rate was also comparable to or better than the success rate of CT scans performed at the same time points.

Reducing false positives

The researchers also found that the sensors have another important ability — they can distinguish between early-stage cancer and noncancerous inflammation of the lungs. Lung inflammation, common in people who smoke, is one of the reasons that CT scans produce so many false positives.

Bhatia envisions that the nanoparticle sensors could be used as a noninvasive diagnostic for people who get a positive result on a screening test, potentially eliminating the need for a biopsy. For use in humans, her team is working on a form of the particles that could be inhaled as a dry powder or through a nebulizer. Another possible application is using these sensors to monitor how well lung tumors respond to treatment, such as drugs or immunotherapies.

“A great next step would be to take this into patients who have known cancer, and are being treated, to see if they're on the right medicine,” Bhatia says.

She is also working on a version of the sensor that could be used to distinguish between viral and bacterial forms of pneumonia, which could help doctors to determine which patients need antibiotics and may even provide complementary information to nucleic acid tests like those being developed for Covid-19. Glympse Bio, a company co-founded by Bhatia, is also working on developing this approach to replace biopsy in the assessment of liver disease.

The research was funded by the Koch Institute Support (core) Grant from the National Cancer Institute, the National Institute of Environmental Health Sciences, the National Science Foundation, the Ludwig Center for Molecular Oncology at MIT, the Koch Institute’s Marble Center for Cancer Nanomedicine, the Koch Institute Frontier Research Program through a gift from Upstage Lung Cancer, and Johnson and Johnson."
345;news.mit.edu;http://news.mit.edu/2019/mit-policy-hackathon-connects-data-driven-problem-solvers-0521;;MIT Policy Hackathon connects data-driven problem solvers;"As the size, complexity, and interconnection of societal systems increase, these systems generate huge amounts of data that can lead to new insights. These data create an opportunity for policymakers aiming to address major societal challenges, provided they have the tools to understand the data and use them for better decision-making.

At a unique MIT event convened by MIT’s Technology and Policy Program (TPP), a part of the Institute for Data, Systems, and Society (IDSS), interdisciplinary teams analyzed data sets and created policy proposals to real challenges submitted by academic groups and local government. The student-run MIT Policy Hackathon gathered data analysts, engineers, scientists, domain experts, and policy specialists to look for creative, data-driven solutions addressing major societal issues.

“One of the goals of the hackathon is to show others the power of using technology and policy together to craft solutions to important societal problems,” says Becca Browder, a Policy Hackathon organizer and student in TPP. “I think the event achieved that goal.”

The hackathon teams worked over 48 hours on one of five challenges in the areas of climate, health, artificial intelligence and ethics, urban planning, and the future of work. The hackathon ended in a proposal pitch session to a panel of judges from academia, government, and industry.

In the climate challenge, sponsored by the City of Boston, teams examined precipitation data to help the city prepare for increased flooding due to climate change.

“The city is taking climate change very seriously,” says Charlie Jewell, director of planning and sustainability for the Boston Water and Sewer Commission. After mentoring and judging the climate challenge, Jewell said there was a “good give-and-take” to be had from partnering with local universities. “The organizers and participants all did such an unbelievable job. I got some great ideas from participants for looking at our rainfall data in different ways. They also showed what kind of data they needed and how we could get it.”

Hackathon participant Minghao Qiu, a student at IDSS in the Social and Engineering Systems doctoral program, also found the opportunity to work directly with stakeholders useful. “The interaction with the challenge sponsor helped me think about how to better communicate my research findings with policymakers in the future,” says Qiu, whose team GAMMDRYL also included TPP alumnus Arthur Yip SM ’14. GAMMDRYL won the climate challenge with a proposal recommending the city team up with a citizen science initiative that crowdsources rainfall data.

“I learned that it is often useful to help decision-makers to understand their data better,” Qiu says.

The overall winner of the hackathon was a team called Dream ER, who worked on the health challenge. This challenge, sponsored by Harvard School of Public Health graduate student Ahmed Mahmoud Abdelfattah, asked for ways to optimize emergency rooms by studying patient traffic and outcome data.

“By using creative visualization techniques, they simulated how their policy suggestions can result in an overall improvement in service efficiency,” Abdelfattah says of the winning team’s proposal. “Their proposal was also quite generalizable, meaning that those same methods they used to examine the data and simulate changes can be applied to other hospitals and other care settings.”

For the AI and ethics challenge, sponsored by the Berkman Klein Center for Internet and Society at Harvard University, teams worked to develop a resource, such as a visualization tool, to help nontechnical policy advocates understand different definitions of ""algorithmic fairness"" — especially in the context of criminal justice risk-assessment tools. Participants had access to data shared by journalists who evaluated COMPAS, a widely-used recidivism risk scoring tool.

The urban planning challenge, sponsored by the City of Boston’s Department of Innovation and Technology, tasked participants with assessing the impact of AirBnB on neighborhood economies and Boston’s affordable housing crisis, using the city’s short-term rental data. The future of work challenge, posed by the MIT Initiative on the Digital Economy (IDE), asked for a broad exploration of the potential for machine learning to automate tasks. Using a data set of work activities put together by researchers at MIT and Carnegie Mellon University, this challenge asked for policy proposals that help predict and prepare for the impact of machine learning automation on industries and workers.

This was the third MIT Policy Hackathon: an inaugural hackathon was held in spring 2018, and another was organized for Boston Hubweek in fall 2018. Students hope to make it a fixture of the program. “IDSS and TPP work on how policy and society interact with science and technology, and how we can use data to enhance policy,” Browder says. “These are also main goals of the hackathon, so there is strong strategic alignment between the event and the host organizations.”

TPP director Noelle Selin agrees. “TPP and IDSS are educating scientists, engineers, and leaders who can use the tools of data science as well as speak the language of policy,” says Selin, a professor in IDSS and Earth, Atmospheric, and Planetary Sciences. “We need this type of interdisciplinary thinking to tackle the most pressing challenges facing society.”"
346;news.mit.edu;http://news.mit.edu/2020/new-model-quantifies-impact-quarantine-measures-covid-19-spread-0416;;Model quantifies the impact of quarantine measures on Covid-19’s spread;"The research described in this article has been published on a preprint server but has not yet been peer-reviewed by scientific or medical experts.

Every day for the past few weeks, charts and graphs plotting the projected apex of Covid-19 infections have been splashed across newspapers and cable news. Many of these models have been built using data from studies on previous outbreaks like SARS or MERS. Now, a team of engineers at MIT has developed a model that uses data from the Covid-19 pandemic in conjunction with a neural network to determine the efficacy of quarantine measures and better predict the spread of the virus.

“Our model is the first which uses data from the coronavirus itself and integrates two fields: machine learning and standard epidemiology,” explains Raj Dandekar, a PhD candidate studying civil and environmental engineering. Together with George Barbastathis, professor of mechanical engineering, Dandekar has spent the past few months developing the model as part of the final project in class 2.168 (Learning Machines).

Most models used to predict the spread of a disease follow what is known as the SEIR model, which groups people into “susceptible,” “exposed,” “infected,” and “recovered.” Dandekar and Barbastathis enhanced the SEIR model by training a neural network to capture the number of infected individuals who are under quarantine, and therefore no longer spreading the infection to others.

The model finds that in places like South Korea, where there was immediate government intervention in implementing strong quarantine measures, the virus spread plateaued more quickly. In places that were slower to implement government interventions, like Italy and the United States, the “effective reproduction number” of Covid-19 remains greater than one, meaning the virus has continued to spread exponentially.

The machine learning algorithm shows that with the current quarantine measures in place, the plateau for both Italy and the United States will arrive somewhere between April 15-20. This prediction is similar to other projections like that of the Institute for Health Metrics and Evaluation.

“Our model shows that quarantine restrictions are successful in getting the effective reproduction number from larger than one to smaller than one,” says Barbastathis. “That corresponds to the point where we can flatten the curve and start seeing fewer infections.”

Quantifying the impact of quarantine

In early February, as news of the virus’ troubling infection rate started dominating headlines, Barbastathis proposed a project to students in class 2.168. At the end of each semester, students in the class are tasked with developing a physical model for a problem in the real world and developing a machine learning algorithm to address it. He proposed that a team of students work on mapping the spread of what was then simply known as “the coronavirus.”

“Students jumped at the opportunity to work on the coronavirus, immediately wanting to tackle a topical problem in typical MIT fashion,” adds Barbastathis.

One of those students was Dandekar. “The project really interested me because I got to apply this new field of scientific machine learning to a very pressing problem,” he says.

As Covid-19 started to spread across the globe, the scope of the project expanded. What had originally started as a project looking just at spread within Wuhan, China grew to also include the spread in Italy, South Korea, and the United States.

The duo started modeling the spread of the virus in each of these four regions after the 500th case was recorded. That milestone marked a clear delineation in how different governments implemented quarantine orders.

Armed with precise data from each of these countries, the research team took the standard SEIR model and augmented it with a neural network that learns how infected individuals under quarantine impact the rate of infection. They trained the neural network through 500 iterations so it could then teach itself how to predict patterns in the infection spread.

Using this model, the research team was able to draw a direct correlation between quarantine measures and a reduction in the effective reproduction number of the virus.

“The neural network is learning what we are calling the ‘quarantine control strength function,’” explains Dandekar. In South Korea, where strong measures were implemented quickly, the quarantine control strength function has been effective in reducing the number of new infections. In the United States, where quarantine measures have been slowly rolled out since mid-March, it has been more difficult to stop the spread of the virus.

Predicting the “plateau”

As the number of cases in a particular country decreases, the forecasting model transitions from an exponential regime to a linear one. Italy began entering this linear regime in early April, with the U.S. not far behind it.

The machine learning algorithm Dandekar and Barbastathis have developed predicted that the United States will start to shift from an exponential regime to a linear regime in the first week of April, with a stagnation in the infected case count likely between April 15 and April 20. It also suggests that the infection count will reach 600,000 in the United States before the rate of infection starts to stagnate.

“This is a really crucial moment of time. If we relax quarantine measures, it could lead to disaster,” says Barbastathis.

According to Barbastathis, one only has to look to Singapore to see the dangers that could stem from relaxing quarantine measures too quickly. While the team didn’t study Singapore’s Covid-19 cases in their research, the second wave of infection this country is currently experiencing reflects their model’s finding about the correlation between quarantine measures and infection rate.

“If the U.S. were to follow the same policy of relaxing quarantine measures too soon, we have predicted that the consequences would be far more catastrophic,” Barbastathis adds.

The team plans to share the model with other researchers in the hopes that it can help inform Covid-19 quarantine strategies that can successfully slow the rate of infection."
