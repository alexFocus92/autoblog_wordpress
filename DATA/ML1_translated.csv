;periodico;url;fecha;titulo;cuerpo;titulo_traducido;cuerpo_traducido
0;machinelearningmastery.com;https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/;2019-06-23;How to use the UpSampling2D and Conv2DTranspose Layers in Keras;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are an architecture for training generative models, such as deep convolutional neural networks for generating images.

The GAN architecture is comprised of both a generator and a discriminator model. The generator is responsible for creating new outputs, such as images, that plausibly could have come from the original dataset. The generator model is typically implemented using a deep convolutional neural network and results-specialized layers that learn to fill in features in an image rather than extract features from an input image.

Two common types of layers that can be used in the generator model are a upsample layer (UpSampling2D) that simply doubles the dimensions of the input and the transpose convolutional layer (Conv2DTranspose) that performs an inverse convolution operation.

In this tutorial, you will discover how to use UpSampling2D and Conv2DTranspose Layers in Generative Adversarial Networks when generating images.

After completing this tutorial, you will know:

Generative models in the GAN architecture are required to upsample input data in order to generate an output image.

The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.

The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Need for Upsampling in GANs

How to Use the Upsampling Layer

How to Use the Transpose Convolutional Layer

Need for Upsampling in Generative Adversarial Networks

Generative Adversarial Networks are an architecture for neural networks for training a generative model.

The architecture is comprised of a generator and a discriminator model, both of which are implemented as a deep convolutional neural network. The discriminator is responsible for classifying images as either real (from the domain) or fake (generated). The generator is responsible for generating new plausible examples from the problem domain.

The generator works by taking a random point from the latent space as input and outputting a complete image, in a one-shot manner.

A traditional convolutional neural network for image classification, and related tasks, will use pooling layers to downsample input images. For example, an average pooling or max pooling layer will reduce the feature maps from a convolutional by half on each dimension, resulting in an output that is one quarter the area of the input.

Convolutional layers themselves also perform a form of downsampling by applying each filter across the input images or feature maps; the resulting activations are an output feature map that is smaller because of the border effects. Often padding is used to counter this effect.

The generator model in a GAN requires an inverse operation of a pooling layer in a traditional convolutional layer. It needs a layer to translate from coarse salient features to a more dense and detailed output.

A simple version of an unpooling or opposite pooling layer is called an upsampling layer. It works by repeating the rows and columns of the input.

A more elaborate approach is to perform a backwards convolutional operation, originally referred to as a deconvolution, which is incorrect, but is more commonly referred to as a fractional convolutional layer or a transposed convolutional layer.

Both of these layers can be used on a GAN to perform the required upsampling operation to transform a small input into a large image output.

In the following sections, we will take a closer look at each and develop an intuition for how they work so that we can use them effectively in our GAN models.

How to Use the UpSampling2D Layer

Perhaps the simplest way to upsample an input is to double each row and column.

For example, an input image with the shape 2×2 would be output as 4×4.

1, 2 Input = (3, 4) 1, 1, 2, 2 Output = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4 1 2 3 4 5 6 7 1, 2 Input = (3, 4) 1, 1, 2, 2 Output = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4

Worked Example Using the UpSampling2D Layer

The Keras deep learning library provides this capability in a layer called UpSampling2D.

It can be added to a convolutional neural network and repeats the rows and columns provided as input in the output. For example:

... # define model model = Sequential() model.add(UpSampling2D()) 1 2 3 4 . . . # define model model = Sequential ( ) model . add ( UpSampling2D ( ) )

We can demonstrate the behavior of this layer with a simple contrived example.

First, we can define a contrived input image that is 2×2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input.

... # define input data X = asarray([[1, 2], [3, 4]]) # show input data for context print(X) 1 2 3 4 5 6 . . . # define input data X = asarray ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) # show input data for context print ( X )

Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.

... # reshape input data into one sample a sample with a channel X = X.reshape((1, 2, 2, 1)) 1 2 3 . . . # reshape input data into one sample a sample with a channel X = X . reshape ( ( 1 , 2 , 2 , 1 ) )

We can now define our model.

The model has only the UpSampling2D layer which takes 2×2 grayscale images as input directly and outputs the result of the upsampling operation.

... # define model model = Sequential() model.add(UpSampling2D(input_shape=(2, 2, 1))) # summarize the model model.summary() 1 2 3 4 5 6 . . . # define model model = Sequential ( ) model . add ( UpSampling2D ( input_shape = ( 2 , 2 , 1 ) ) ) # summarize the model model . summary ( )

We can then use the model to make a prediction, that is upsample a provided input image.

... # make a prediction with the model yhat = model.predict(X) 1 2 3 . . . # make a prediction with the model yhat = model . predict ( X )

The output will have four dimensions, like the input, therefore, we can convert it back to a 2×2 array to make it easier to review the result.

... # reshape output to remove channel to make printing easier yhat = yhat.reshape((4, 4)) # summarize output print(yhat) 1 2 3 4 5 . . . # reshape output to remove channel to make printing easier yhat = yhat . reshape ( ( 4 , 4 ) ) # summarize output print ( yhat )

Tying all of this together, the complete example of using the UpSampling2D layer in Keras is provided below.

# example of using the upsampling layer from numpy import asarray from keras.models import Sequential from keras.layers import UpSampling2D # define input data X = asarray([[1, 2], [3, 4]]) # show input data for context print(X) # reshape input data into one sample a sample with a channel X = X.reshape((1, 2, 2, 1)) # define model model = Sequential() model.add(UpSampling2D(input_shape=(2, 2, 1))) # summarize the model model.summary() # make a prediction with the model yhat = model.predict(X) # reshape output to remove channel to make printing easier yhat = yhat.reshape((4, 4)) # summarize output print(yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # example of using the upsampling layer from numpy import asarray from keras . models import Sequential from keras . layers import UpSampling2D # define input data X = asarray ( [ [ 1 , 2 ] , [ 3 , 4 ] ] ) # show input data for context print ( X ) # reshape input data into one sample a sample with a channel X = X . reshape ( ( 1 , 2 , 2 , 1 ) ) # define model model = Sequential ( ) model . add ( UpSampling2D ( input_shape = ( 2 , 2 , 1 ) ) ) # summarize the model model . summary ( ) # make a prediction with the model yhat = model . predict ( X ) # reshape output to remove channel to make printing easier yhat = yhat . reshape ( ( 4 , 4 ) ) # summarize output print ( yhat )

Running the example first creates and summarizes our 2×2 input data.

Next, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer has no parameters or model weights. This is because it is not learning anything; it is just doubling the input.

Finally, the model is used to upsample our input, resulting in a doubling of each row and column for our input data, as we expected.

[[1 2] [3 4]] _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= up_sampling2d_1 (UpSampling2 (None, 4, 4, 1) 0 ================================================================= Total params: 0 Trainable params: 0 Non-trainable params: 0 _________________________________________________________________ [[1. 1. 2. 2.] [1. 1. 2. 2.] [3. 3. 4. 4.] [3. 3. 4. 4.]] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [[1 2] [3 4]] _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= up_sampling2d_1 (UpSampling2 (None, 4, 4, 1) 0 ================================================================= Total params: 0 Trainable params: 0 Non-trainable params: 0 _________________________________________________________________ [[1. 1. 2. 2.] [1. 1. 2. 2.] [3. 3. 4. 4.] [3. 3. 4. 4.]]

By default, the UpSampling2D will double each input dimension. This is defined by the ‘size‘ argument that is set to the tuple (2,2).

You may want to use different factors on each dimension, such as double the width and triple the height. This could be achieved by se";Cómo usar las capas UpSampling2D y Conv2DTranspose en Keras;"Tweet Compartir Compartir Última actualización el 12 de julio de 2019 Las redes adversas generativas, o GAN, son una arquitectura para el entrenamiento de modelos generativos, como redes neuronales convolucionales profundas para generar imágenes. La arquitectura GAN se compone de un modelo generador y un modelo discriminador. El generador es responsable de crear nuevas salidas, como imágenes, que podrían provenir del conjunto de datos original. El modelo de generador se implementa típicamente usando una red neuronal convolucional profunda y capas especializadas en resultados que aprenden a completar características en una imagen en lugar de extraer características de una imagen de entrada. Dos tipos comunes de capas que se pueden usar en el modelo del generador son una capa de muestra ascendente (UpSampling2D) que simplemente duplica las dimensiones de la entrada y la capa convolucional de transposición (Conv2DTranspose) que realiza una operación de convolución inversa. En este tutorial, descubrirá cómo usar las capas UpSampling2D y Conv2DTranspose en redes adversas generativas al generar imágenes. Después de completar este tutorial, sabrá: Se requieren modelos generativos en la arquitectura GAN para muestrear datos de entrada para generar una imagen de salida. La capa Upsampling es una capa simple sin pesos que duplicará las dimensiones de entrada y puede usarse en un modelo generativo cuando es seguida por una capa convolucional tradicional. La capa convolucional de transposición es una capa convolucional inversa que aumentará la entrada de la muestra y aprenderá a completar los detalles durante el proceso de capacitación del modelo. Descubra cómo desarrollar DCGAN, GAN condicionales, Pix2Pix, CycleGAN y más con Keras en mi nuevo libro de GAN, con 29 tutoriales paso a paso y código fuente completo. Empecemos. Descripción general del tutorial Este tutorial se divide en tres partes; son: Necesidad de muestreo ascendente en GAN Cómo usar la capa de muestreo ascendente Cómo utilizar la capa convolucional transpuesta Necesidad de muestreo ascendente en redes adversas generativas Las redes adversas generativas son una arquitectura de redes neuronales para el entrenamiento de un modelo generativo. La arquitectura se compone de un modelo generador y discriminador, los cuales se implementan como una red neuronal convolucional profunda. El discriminador es responsable de clasificar las imágenes como reales (del dominio) o falsas (generadas). El generador es responsable de generar nuevos ejemplos plausibles del dominio del problema. El generador funciona tomando un punto aleatorio del espacio latente como entrada y produciendo una imagen completa, de una sola vez. Una red neuronal convolucional tradicional para la clasificación de imágenes, y tareas relacionadas, utilizará capas de agrupación para reducir las imágenes de entrada. Por ejemplo, una agrupación promedio o una capa de agrupación máxima reducirá a la mitad los mapas de entidades de un convolucional en cada dimensión, lo que da como resultado una salida que es un cuarto del área de la entrada. Las capas convolucionales también realizan una forma de muestreo descendente aplicando cada filtro a través de las imágenes de entrada o mapas de características; las activaciones resultantes son un mapa de características de salida que es más pequeño debido a los efectos de borde. A menudo se usa relleno para contrarrestar este efecto. El modelo de generador en una GAN requiere una operación inversa de una capa de agrupación en una capa convolucional tradicional. Necesita una capa para traducir de características sobresalientes gruesas a una salida más densa y detallada. Una versión simple de una capa de agrupación opuesta o sin agrupación se denomina capa de muestreo ascendente. Funciona repitiendo las filas y columnas de la entrada. Un enfoque más elaborado es realizar una operación convolucional hacia atrás, originalmente conocida como deconvolución, que es incorrecta, pero se conoce más comúnmente como una capa convolucional fraccional o una capa convolucional transpuesta. Ambas capas se pueden utilizar en una GAN para realizar la operación de muestreo ascendente necesaria para transformar una entrada pequeña en una salida de imagen grande. En las siguientes secciones, analizaremos más de cerca cada una y desarrollaremos una intuición de cómo funcionan para que podamos usarlas de manera efectiva en nuestros modelos GAN. Cómo usar la capa UpSampling2D Quizás la forma más simple de aumentar la muestra de una entrada es duplicar cada fila y columna. Por ejemplo, una imagen de entrada con la forma 2 × 2 se generará como 4 × 4. 1, 2 Entrada = (3, 4) 1, 1, 2, 2 Salida = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4 1 2 3 4 5 6 7 1 , 2 Entrada = (3, 4) 1, 1, 2, 2 Salida = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4 Ejemplo trabajado usando la capa UpSampling2D La profundidad de Keras la biblioteca de aprendizaje proporciona esta capacidad en una capa llamada UpSampling2D. Se puede agregar a una red neuronal convolucional y repite las filas y columnas proporcionadas como entrada en la salida. Por ejemplo: ... # define model model = Sequential () model.add (UpSampling2D ()) 1 2 3 4. . . # define modelo modelo = modelo secuencial (). add (UpSampling2D ()) Podemos demostrar el comportamiento de esta capa con un simple ejemplo artificial. Primero, podemos definir una imagen de entrada artificial que tenga 2 × 2 píxeles. Podemos usar valores específicos para cada píxel para que después del muestreo ascendente, podamos ver exactamente qué efecto tuvo la operación en la entrada. ... # define los datos de entrada X = una matriz ([[1, 2], [3, 4]]) # muestra los datos de entrada para la impresión de contexto (X) 1 2 3 4 5 6. . . # define los datos de entrada X = una matriz ([[1, 2], [3, 4]]) # muestra los datos de entrada para la impresión de contexto (X) Una vez que se define la imagen, debemos agregar una dimensión de canal (por ejemplo, escala de grises) y también una dimensión de muestra (por ejemplo, tenemos 1 muestra) para que podamos pasarla como entrada al modelo. ... # reforma los datos de entrada en una muestra, una muestra con un canal X = X. remodelar ((1, 2, 2, 1)) 1 2 3. . . # reforma los datos de entrada en una muestra, una muestra con un canal X = X. remodelar ((1, 2, 2, 1)) Ahora podemos definir nuestro modelo. El modelo solo tiene la capa UpSampling2D que toma imágenes de escala de grises 2 × 2 como entrada directa y genera el resultado de la operación de muestreo ascendente. ... # define model model = Sequential () model.add (UpSampling2D (input_shape = (2, 2, 1))) # resume el modelo model.summary () 1 2 3 4 5 6. . . # define modelo modelo = modelo secuencial (). add (UpSampling2D (input_shape = (2, 2, 1))) # resume el modelo del modelo. summary () Podemos usar el modelo para hacer una predicción, es decir, una muestra de una imagen de entrada proporcionada. ... # haz una predicción con el modelo yhat = model.predict (X) 1 2 3. . . # hacer una predicción con el modelo yhat = model. predecir (X) La salida tendrá cuatro dimensiones, como la entrada, por lo tanto, podemos convertirla nuevamente en una matriz de 2 × 2 para que sea más fácil revisar el resultado. ... # reforma la salida para eliminar el canal para facilitar la impresión yhat = yhat.reshape ((4, 4)) # resume la impresión de salida (yhat) 1 2 3 4 5. . . # cambiar la forma de salida para eliminar el canal para facilitar la impresión yhat = yhat. remodelar ((4, 4)) # resumir la impresión de salida (yhat) Al unir todo esto, a continuación se proporciona el ejemplo completo del uso de la capa UpSampling2D en Keras. # ejemplo de uso de la capa de muestreo ascendente de numpy import asarray from keras.models import Sequential from keras.layers import UpSampling2D # define datos de entrada X = asarray ([[1, 2], [3, 4]]) # muestra datos de entrada para impresión de contexto (X) # remodelar datos de entrada en una muestra una muestra con un canal X = X.reshape ((1, 2, 2, 1)) # define model model = Sequential () model.add (UpSampling2D (input_shape = ( 2, 2, 1))) # resuma el modelo model.summary () # haga una predicción con el modelo yhat = model.predict (X) # reforme la salida para eliminar el canal para facilitar la impresión yhat = yhat.reshape ((4 , 4)) # resumir la impresión de salida (yhat) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # ejemplo de uso de la capa de muestreo superior de una matriz de importación numpy de keras. Los modelos importan secuenciales de keras. las capas importan UpSampling2D # define los datos de entrada X = una matriz ([[1, 2], [3, 4]]) # muestra los datos de entrada para la impresión de contexto (X) # cambia la forma de los datos de entrada en una muestra una muestra con un canal X = X . remodelar ((1, 2, 2, 1)) # definir modelo modelo = modelo secuencial (). add (UpSampling2D (input_shape = (2, 2, 1))) # resume el modelo del modelo. summary () # realiza una predicción con el modelo yhat = model. predecir (X) # cambiar la forma de salida para eliminar el canal y facilitar la impresión yhat = yhat. remodelar ((4, 4)) # resumir la impresión de salida (yhat) Ejecutar el ejemplo primero crea y resume nuestros datos de entrada 2 × 2. A continuación, se resume el modelo. Podemos ver que generará un resultado de 4 × 4 como esperamos, y lo que es más importante, la capa no tiene parámetros ni pesos de modelo. Esto se debe a que no está aprendiendo nada; solo está duplicando la entrada. Finalmente, el modelo se usa para muestrear nuestra entrada, lo que resulta en una duplicación de cada fila y columna para nuestros datos de entrada, como esperábamos. [[1 2] [3 4]] _________________________________________________________________ Capa (tipo) Forma de salida Parámetro # =============================== ================================== up_sampling2d_1 (UpSampling2 (Ninguno, 4, 4, 1) 0 === ================================================== ============ Parámetros totales: 0 Parámetros entrenables: 0 Parámetros no entrenables: 0 _________________________________________________________________ [[1. 1. 2. 2.] [1. 1. 2. 2.] [3 3. 4. 4.] [3. 3. 4. 4.]] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [[1 2] [3 4]] _________________________________________________________________ Capa ( tipo) Parámetro de forma de salida # ============================================ ===================== up_sampling2d_1 (UpSampling2 (Ninguno, 4, 4, 1) 0 ================ ================================================= Total parámetros: 0 parámetros entrenables: 0 parámetros no entrenables: 0 _________________________________________________________________ [[1. 1. 2. 2.] [1. 1. 2. 2.] [3. 3. 4. 4.] [3. 3. 4. 4.]] B Por defecto, UpSampling2D duplicará cada dimensión de entrada. Esto se define por el argumento &#39;tamaño&#39; que se establece en la tupla (2,2). Es posible que desee utilizar diferentes factores en cada dimensión, como duplicar el ancho y triplicar la altura. Esto podría lograrse por sí mismo"
1;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/;2018-09-23;How to Develop RNN Models for Human Activity Recognition Time Series Classification;"# convlstm model

from numpy import mean

from numpy import std

from numpy import dstack

from pandas import read_csv

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import Dropout

from keras . layers import LSTM

from keras . layers import TimeDistributed

from keras . layers import ConvLSTM2D

from keras . utils import to_categorical

from matplotlib import pyplot

# load a single file as a numpy array

def load_file ( filepath ) :

dataframe = read_csv ( filepath , header = None , delim_whitespace = True )

return dataframe . values

# load a list of files and return as a 3d numpy array

def load_group ( filenames , prefix = '' ) :

loaded = list ( )

for name in filenames :

data = load_file ( prefix + name )

loaded . append ( data )

# stack group so that features are the 3rd dimension

loaded = dstack ( loaded )

return loaded

# load a dataset group, such as train or test

def load_dataset_group ( group , prefix = '' ) :

filepath = prefix + group + '/Inertial Signals/'

# load all 9 files as a single array

filenames = list ( )

# total acceleration

filenames += [ 'total_acc_x_' + group + '.txt' , 'total_acc_y_' + group + '.txt' , 'total_acc_z_' + group + '.txt' ]

# body acceleration

filenames += [ 'body_acc_x_' + group + '.txt' , 'body_acc_y_' + group + '.txt' , 'body_acc_z_' + group + '.txt' ]

# body gyroscope

filenames += [ 'body_gyro_x_' + group + '.txt' , 'body_gyro_y_' + group + '.txt' , 'body_gyro_z_' + group + '.txt' ]

# load input data

X = load_group ( filenames , filepath )

# load class output

y = load_file ( prefix + group + '/y_' + group + '.txt' )

return X , y

# load the dataset, returns train and test X and y elements

def load_dataset ( prefix = '' ) :

# load all train

trainX , trainy = load_dataset_group ( 'train' , prefix + 'HARDataset/' )

print ( trainX . shape , trainy . shape )

# load all test

testX , testy = load_dataset_group ( 'test' , prefix + 'HARDataset/' )

print ( testX . shape , testy . shape )

# zero-offset class values

trainy = trainy - 1

testy = testy - 1

# one hot encode y

trainy = to_categorical ( trainy )

testy = to_categorical ( testy )

print ( trainX . shape , trainy . shape , testX . shape , testy . shape )

return trainX , trainy , testX , testy

# fit and evaluate a model

def evaluate_model ( trainX , trainy , testX , testy ) :

# define model

verbose , epochs , batch_size = 0 , 25 , 64

n_timesteps , n_features , n_outputs = trainX . shape [ 1 ] , trainX . shape [ 2 ] , trainy . shape [ 1 ]

# reshape into subsequences (samples, time steps, rows, cols, channels)

n_steps , n_length = 4 , 32

trainX = trainX . reshape ( ( trainX . shape [ 0 ] , n_steps , 1 , n_length , n_features ) )

testX = testX . reshape ( ( testX . shape [ 0 ] , n_steps , 1 , n_length , n_features ) )

# define model

model = Sequential ( )

model . add ( ConvLSTM2D ( filters = 64 , kernel_size = ( 1 , 3 ) , activation = 'relu' , input_shape = ( n_steps , 1 , n_length , n_features ) ) )

model . add ( Dropout ( 0.5 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 100 , activation = 'relu' ) )

model . add ( Dense ( n_outputs , activation = 'softmax' ) )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( trainX , trainy , epochs = epochs , batch_size = batch_size , verbose = verbose )

# evaluate model

_ , accuracy = model . evaluate ( testX , testy , batch_size = batch_size , verbose = 0 )

return accuracy

# summarize scores

def summarize_results ( scores ) :

print ( scores )

m , s = mean ( scores ) , std ( scores )

print ( 'Accuracy: %.3f%% (+/-%.3f)' % ( m , s ) )

# run an experiment

def run_experiment ( repeats = 10 ) :

# load data

trainX , trainy , testX , testy = load_dataset ( )

# repeat experiment

scores = list ( )

for r in range ( repeats ) :

score = evaluate_model ( trainX , trainy , testX , testy )

score = score * 100.0

print ( '>#%d: %.3f' % ( r + 1 , score ) )

scores . append ( score )

# summarize results

summarize_results ( scores )

# run the experiment";Cómo desarrollar modelos RNN para la clasificación de series de tiempo de reconocimiento de actividad humana;"# modelo convlstm de numpy import media de numpy import std de numpy import dstack de pandas import read_csv de keras. Los modelos importan secuenciales de keras. Las capas importan densas de keras. las capas importan Flatten de keras. Las capas importan el abandono de keras. las capas importan LSTM de keras. Las capas importan TimeDistributed de keras. las capas importan ConvLSTM2D desde keras. utils import to_categorical desde matplotlib import pyplot # carga un solo archivo como una matriz numpy def load_file (filepath): dataframe = read_csv (filepath, header = None, delim_whitespace = True) devuelve el dataframe. valores # cargar una lista de archivos y devolver como una matriz numpy 3d def load_group (nombre de archivo, prefijo = &#39;&#39;): cargado = lista () para el nombre en los nombres de archivo: datos = carga_archivo (prefijo + nombre) cargado. anexar (datos) # grupo de pila para que las características sean la 3ra dimensión cargado = dstack (cargado) devolver cargado # cargar un grupo de conjunto de datos, como train o test def load_dataset_group (group, prefix = &#39;&#39;): filepath = prefix + group + &#39;/ Inertial Signals /&#39; # carga los 9 archivos como una sola matriz de nombres de archivos = list () # nombres de archivos de aceleración total + = [&#39;total_acc_x_&#39; + group + &#39;.txt&#39;, &#39;total_acc_y_&#39; + group + &#39;.txt&#39;, &#39; total_acc_z_ &#39;+ group +&#39; .txt &#39;] # nombres de archivos de aceleración corporal + = [&#39; body_acc_x_ &#39;+ group +&#39; .txt &#39;,&#39; body_acc_y_ &#39;+ group +&#39; .txt &#39;,&#39; body_acc_z_ &#39;+ group +&#39; .txt &#39;] # body gyroscope filenames + = [&#39;body_gyro_x_&#39; + group + &#39;.txt&#39;, &#39;body_gyro_y_&#39; + group + &#39;.txt&#39;, &#39;body_gyro_z_&#39; + group + &#39;.txt&#39;] # cargar datos de entrada X = load_group (nombre de archivo, ruta de archivo) # carga de salida de clase y = load_file (prefijo + grupo + &#39;/ y_&#39; + grupo + &#39;.txt&#39;) devuelve X, y # carga el conjunto de datos, devuelve tren y prueba elementos X e y def load_dataset (prefijo = &#39;&#39; ): # cargar todo el tren trainX, trainy = loa d_dataset_group (&#39;train&#39;, prefijo + &#39;HARDataset /&#39;) print (trainX. forma, entrenada. shape) # cargar todas las pruebas testX, testy = load_dataset_group (&#39;test&#39;, prefijo + &#39;HARDataset /&#39;) print (testX. shape, testy. shape) # valores de clase de compensación cero trainy = trainy - 1 testy = testy - 1 # one hot encode y trainy = to_categorical (trainy) testy = to_categorical (testy) print (trainX. shape, trainy. shape, testX. shape, testy. shape) return trainX, trainy, testX, testy # fit y evaluar un modelo def eval_model (trainX, trainy, testX, testy): # define el modelo detallado, epochs, batch_size = 0, 25, 64 n_timesteps, n_features, n_outputs = trainX. forma [1], trainX. forma [2], entrenada. shape [1] # remodelar en subsecuencias (muestras, pasos de tiempo, filas, cols, canales) n_steps, n_length = 4, 32 trainX = trainX. remodelar ((trainX. forma [0], n_steps, 1, n_length, n_features)) testX = testX. reshape ((testX. forma [0], n_steps, 1, n_length, n_features)) # define el modelo del modelo = Modelo secuencial (). modelo add (ConvLSTM2D (filtros = 64, kernel_size = (1, 3), activación = &#39;relu&#39;, input_shape = (n_steps, 1, n_length, n_features))). Añadir modelo (Dropout (0.5)). agregar modelo (Flatten ()). modelo add (Dense (100, activación = &#39;relu&#39;)). modelo add (Dense (n_outputs, activación = &#39;softmax&#39;)). compilar (pérdida = &#39;categorical_crossentropy&#39;, optimizador = &#39;adam&#39;, métricas = [&#39;precisión&#39;]) # ajustar el modelo de red. fit (trainX, trainy, epochs = epochs, batch_size = batch_size, verbose = verbose) # evaluar modelo _, precisión = modelo. evaluar (testX, testy, batch_size = batch_size, verbose = 0) devolver precisión # resumir puntajes def summaryize_results (puntajes): print (puntajes) m, s = promedio (puntajes), std (puntajes) imprimir (&#39;Precisión:% .3f %% (+ / -%. 3f) &#39;% (m, s)) # ejecutar un experimento def run_experiment (repeats = 10): # cargar datos trainX, trainy, testX, testy = load_dataset () # repetir puntajes de experimento = lista () para r en rango (repeticiones): puntaje = evaluación_modelo (trainX, trainy, testX, testy) puntaje = puntaje * 100.0 print (&#39;&gt; #% d:% .3f&#39;% (r + 1, puntaje)) puntajes. agregar (puntaje) # resumir resultados summaryize_results (puntajes) # ejecutar el experimento"
2;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-and-demonstrate-competence-with-deep-learning-for-computer-vision/;2019-03-10;How to Develop Competence With Deep Learning for Computer Vision;"Tweet Share Share

Last Updated on July 5, 2019

Computer vision is perhaps one area that has been most impacted by developments in deep learning.

It can be difficult to both develop and to demonstrate competence with deep learning for problems in the field of computer vision. It is not clear how to get started, what the most important techniques are, and the types of problems and projects that can best highlight the value that deep learning can bring to the field.

On approach is to systematically develop, and at the same time demonstrate competence with, data handling, modeling techniques, and application domains and present your results in a public portfolio of completed projects. This approach allows you to compound your skills from project to project. It also provides the basis for real projects that can be presented and discussed with prospective employers in order to demonstrate your capabilities.

In this post, you will discover how to develop and demonstrate competence in deep learning applied to problems in computer vision.

After reading this post, you will know:

Developing a portfolio of completed small projects can both be leveraged on new projects in the future and demonstrate your competence with deep learning for computer vision projects.

Projects can be kept small in scope, although they can still demonstrate a systematic approach to problem-solving and the development of skillful models.

A three-level competence framework can be followed that includes data handling competence, technique competence, and application competence.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Deep Learning for Computer Vision Develop a Portfolio of Small Projects Deep Learning for Computer Vision Competence Framework

Deep Learning for Computer Vision

Perhaps one domain that has been the most impacted by developments in deep learning is computer vision.

Computer vision is a subfield of artificial intelligence concerned with understanding data in images, such as photos and videos.

Computer vision tasks such as recognizing handwritten digits and objects in photographs were some of the early case studies demonstrating the capability of modern deep learning techniques achieving state-of-the-art results.

As a practitioner, you may wish to develop and demonstrate your skills with deep learning in computer vision.

This does assume a few things, such as:

You are familiar with applied machine learning, meaning that you are able to work through a predictive modeling project end-to-end and deliver a skillful model.

You are familiar with deep learning techniques, meaning that you know the difference between the main methods and when to use them.

This does not mean that you are an expert, only that you have a working knowledge and are able to wok through problems systematically.

As a machine learning or even deep learning practitioner, how can you show competence with computer vision applications?

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Develop a Portfolio of Small Projects

Competence with deep learning for computer vision can be developed and demonstrated using a project-based approach.

Specifically, the skills can be built and demonstrated incrementally by completing and presenting small projects that use deep learning techniques on computer vision problems.

This requires you to develop a portfolio of completed projects. A portfolio helps you in two specific ways:

Skill Development : The code and findings from the projects in the portfolio can be leveraged by you on future projects, accelerating your progress and allowing you to take on larger and more challenging projects.

: The code and findings from the projects in the portfolio can be leveraged by you on future projects, accelerating your progress and allowing you to take on larger and more challenging projects. Skill Demonstration: The public presentation of the projects provides a demonstration of your capabilities, providing the basis for discussion of APIs, model selection, and design decisions with prospective employers.

Projects can be focused on standard and publicly available computer vision datasets, such as those developed and hosted by academics or those used in machine learning competitions.

Projects can be completed in a systematic manner, including aspects such as clear problem definition, review of relevant literature and models, model development and tuning, and the presentation of results and findings in a report, notebook, or even slideshow presentation format.

Projects are small, meaning that they can be completed in a workday, perhaps spread over a number of nights and weekends. This is important as it limits the scope of the project to focus on workflow and delivering a skillful result, rather than developing a state-of-the-art result.

Deep Learning for Computer Vision Competence Framework

Projects can be selected carefully in such a way to both build in terms of challenge or complexity and in terms of leverage or skill development.

Below is a three-level framework for developing and demonstrating competence with deep learning for computer vision, intended for practitioners already familiar with the basics of applied machine learning and the basics of deep learning:

Level 1: Data Handling Competence . That you know how to load and manipulate image data.

. That you know how to load and manipulate image data. Level 2: Technique Competence . That you know how to define, fit, and tune convolutional neural networks.

. That you know how to define, fit, and tune convolutional neural networks. Level 3: Application Competence. That you can develop skillful deep learning models for common computer vision problems.

Level 1: Data Handling Competence

Data handling competence refers to the ability to load and transform data.

This includes basic data I/O operations such as loading and saving image or video data.

Most importantly, it involves using standard APIs to manipulate image data in ways that may be useful when preparing data for molding with deep learning neural networks.

Examples include:

Image resizing and interpolation.

Image blurring and sharpening.

Image affine transforms.

Image whitening and thresholding.

Data handling could be demonstrated with one of many image handling APIs, such as:

It may include the basic data handing capability of machine learning and deep learning libraries, such as:

What are your favorite image handling APIs in Python?

Let me know in the comments below.

Level 2: Technique Competence

Technique competence refers to the ability to use the specific deep learning models and methods that are used for computer vision problems.

This includes from a high-level the three main classes of methods:

Multilayer Perceptrons, or MLPs.

Convolutional Neural Networks, or CNNs.

Recurrent Neural Networks, such as the Long Short-Term Memory Network, or LSTM.

More specifically, this requires a demonstration of strong skills with how to configure and get the most of the layers used in a CNN, such as:

Convolutional Layers.

Pooling Layers.

Patterns of using Layers.

This may also include skill with some general classes of effective models, such as:

ImageNet CNNs such as AlexNet, VGG, ResNet, Inception, etc.

CNN-LSTMs, LSTM-CNNs, etc.

R-CNNs, YOLO, etc.

What are your favorite deep learning techniques for computer vision?

Let me know in the comments below.

Level 3: Application Competence

Application competence refers to the ability to work through a specific computer vision problem and use deep learning methods to deliver a skillful model.

A skillful model means a model that is capable of making predictions that have better performance than a naive baseline method. It does not mean achieving state-of-the-art results and replicating a model and results in a paper, although they are fine project ideas if they are within scope of a small project.

The project should be completed systematically, including most if not all of the following steps:

Problem Description. Describe the predictive modeling problem, including the domain and relevant background. Literature Review. Describe standard or common approaches to solving the problem using deep learning methods as described in seminal and/or recent research papers. Summarize Data. Describe the available data, including statistical summaries and data visualization. Evaluate Models. Spot-check a suite of model types, configurations, data preparation schemes, and more in order to narrow down what works well on the problem. Improve Performance. Improve the performance of the model or models that work well with hyperparameter tuning and perhaps ensemble methods. Present Results. Present the findings of the project.

A step before this process, step zero, might be to choose a publicly available dataset appropriate for the project.

The backbone of deep learning for computer vision is image classification, commonly referred to as image recognition or object detection. This involves predicting a class label given an image, often a photograph.

Problems of this type should be the focus.

Two standard computer vision datasets of this type include:

Classifying handwritten digits (e.g. MNIST and SVHN).

Classifying photos of objects (e.g. CIFAR-10 and CIFAR-100).

Classifying photos of faces (e.g. VGGFace2)

A related computer vision task is identifying the location of one or more objects within photographs, also referred to as object recognition or object localization or segmentation.

Object Recognition and Localization (e.g. COCO)

There are also tasks that involve";Cómo desarrollar competencia con el aprendizaje profundo para la visión por computadora;"Tweet Compartir Compartir Última actualización el 5 de julio de 2019 La visión por computadora es quizás un área que se ha visto más afectada por los desarrollos en el aprendizaje profundo. Puede ser difícil desarrollar y demostrar competencia con el aprendizaje profundo para problemas en el campo de la visión por computadora. No está claro cómo comenzar, cuáles son las técnicas más importantes y los tipos de problemas y proyectos que pueden resaltar mejor el valor que el aprendizaje profundo puede aportar al campo. El enfoque consiste en desarrollar sistemáticamente y, al mismo tiempo, demostrar competencia con el manejo de datos, las técnicas de modelado y los dominios de aplicación y presentar sus resultados en una cartera pública de proyectos completados. Este enfoque le permite combinar sus habilidades de un proyecto a otro. También proporciona la base para proyectos reales que pueden presentarse y discutirse con posibles empleadores para demostrar sus capacidades. En esta publicación, descubrirá cómo desarrollar y demostrar competencia en el aprendizaje profundo aplicado a problemas en la visión por computadora. Después de leer esta publicación, sabrá: el desarrollo de una cartera de pequeños proyectos completados se puede aprovechar en nuevos proyectos en el futuro y demostrar su competencia con el aprendizaje profundo para proyectos de visión por computadora. Los proyectos pueden tener un alcance pequeño, aunque aún pueden demostrar un enfoque sistemático para la resolución de problemas y el desarrollo de modelos hábiles. Se puede seguir un marco de competencia de tres niveles que incluye competencia en el manejo de datos, competencia técnica y competencia en la aplicación. Descubra cómo crear modelos para la clasificación de fotos, detección de objetos, reconocimiento de rostros y más en mi nuevo libro de visión por computadora, con 30 tutoriales paso a paso y código fuente completo. Empecemos. Descripción general Este tutorial se divide en tres partes; son: Aprendizaje profundo para la visión por computadora Desarrollar una cartera de proyectos pequeños Aprendizaje profundo para la visión por computadora Marco de competencia Aprendizaje profundo para la visión por computadora Quizás un dominio que ha sido más afectado por los desarrollos en el aprendizaje profundo es la visión por computadora. La visión por computadora es un subcampo de la inteligencia artificial relacionada con la comprensión de datos en imágenes, como fotos y videos. Las tareas de visión por computadora, como el reconocimiento de dígitos y objetos escritos a mano en fotografías, fueron algunos de los primeros estudios de casos que demostraron la capacidad de las técnicas modernas de aprendizaje profundo para lograr resultados de vanguardia. Como profesional, es posible que desee desarrollar y demostrar sus habilidades con un aprendizaje profundo en visión por computadora. Esto supone algunas cosas, como: Usted está familiarizado con el aprendizaje automático aplicado, lo que significa que puede trabajar a través de un proyecto de modelado predictivo de extremo a extremo y ofrecer un modelo hábil. Usted está familiarizado con las técnicas de aprendizaje profundo, lo que significa que conoce la diferencia entre los métodos principales y cuándo usarlos. Esto no significa que usted sea un experto, solo que tenga un conocimiento práctico y que pueda resolver problemas sistemáticamente. Como profesional del aprendizaje automático o incluso del aprendizaje profundo, ¿cómo puede demostrar su competencia con las aplicaciones de visión por computadora? ¿Quiere resultados con Deep Learning para visión artificial? Tome mi curso gratuito de 7 días por correo electrónico ahora (con código de muestra). Haga clic para registrarse y también obtenga una versión gratuita en PDF del libro. Descargue su mini-curso GRATUITO Desarrolle una cartera de proyectos pequeños La competencia con el aprendizaje profundo para la visión por computadora puede desarrollarse y demostrarse utilizando un enfoque basado en proyectos. Específicamente, las habilidades se pueden desarrollar y demostrar de manera incremental completando y presentando pequeños proyectos que utilizan técnicas de aprendizaje profundo en problemas de visión por computadora. Esto requiere que desarrolle una cartera de proyectos completados. Un portafolio lo ayuda de dos maneras específicas: Desarrollo de habilidades: el código y los resultados de los proyectos en el portafolio pueden ser aprovechados por usted en proyectos futuros, acelerando su progreso y permitiéndole asumir proyectos más grandes y desafiantes. : Usted puede aprovechar el código y los resultados de los proyectos en la cartera en proyectos futuros, acelerando su progreso y permitiéndole asumir proyectos más grandes y desafiantes. Demostración de habilidades: la presentación pública de los proyectos proporciona una demostración de sus capacidades, proporcionando la base para la discusión de API, selección de modelos y decisiones de diseño con posibles empleadores. Los proyectos pueden centrarse en conjuntos de datos de visión por computadora estándar y disponibles al público, como los desarrollados y alojados por académicos o los utilizados en competiciones de aprendizaje automático. Los proyectos se pueden completar de manera sistemática, incluidos aspectos como la definición clara del problema, la revisión de la literatura y los modelos relevantes, el desarrollo y ajuste del modelo, y la presentación de resultados y hallazgos en un informe, cuaderno o incluso presentación de diapositivas. Los proyectos son pequeños, lo que significa que pueden completarse en un día laboral, tal vez repartidos en varias noches y fines de semana. Esto es importante ya que limita el alcance del proyecto para enfocarse en el flujo de trabajo y entregar un resultado hábil, en lugar de desarrollar un resultado de última generación. Los proyectos de marco de aprendizaje profundo para la competencia de visión por computadora se pueden seleccionar cuidadosamente de tal manera que se construyan tanto en términos de desafío o complejidad como en términos de apalancamiento o desarrollo de habilidades. A continuación se muestra un marco de tres niveles para desarrollar y demostrar competencia con el aprendizaje profundo para la visión por computadora, destinado a profesionales que ya están familiarizados con los conceptos básicos del aprendizaje automático aplicado y los conceptos básicos del aprendizaje profundo: Nivel 1: Competencia en el manejo de datos. Que sabes cómo cargar y manipular datos de imágenes. . Que sabes cómo cargar y manipular datos de imágenes. Nivel 2: Competencia técnica. Que usted sabe cómo definir, ajustar y sintonizar redes neuronales convolucionales. . Que usted sabe cómo definir, ajustar y sintonizar redes neuronales convolucionales. Nivel 3: Competencia de aplicación. Que puede desarrollar modelos hábiles de aprendizaje profundo para problemas comunes de visión por computadora. Nivel 1: Competencia en el manejo de datos La competencia en el manejo de datos se refiere a la capacidad de cargar y transformar datos. Esto incluye operaciones básicas de E / S de datos, como cargar y guardar datos de imagen o video. Lo más importante es que implica el uso de API estándar para manipular datos de imágenes de formas que pueden ser útiles al preparar datos para moldear con redes neuronales de aprendizaje profundo. Los ejemplos incluyen: Cambio de tamaño de la imagen e interpolación. Imagen borrosa y nitidez. Imagen transformaciones afines. Blanqueamiento de imagen y umbral. El manejo de datos podría demostrarse con una de las muchas API de manejo de imágenes, tales como: Puede incluir la capacidad básica de manejo de datos de las bibliotecas de aprendizaje automático y aprendizaje profundo, tales como: ¿Cuáles son sus API de manejo de imágenes favoritas en Python? Déjame saber abajo en los comentarios. Nivel 2: Competencia técnica La competencia técnica se refiere a la capacidad de utilizar los modelos y métodos específicos de aprendizaje profundo que se utilizan para problemas de visión por computadora. Esto incluye desde un alto nivel las tres clases principales de métodos: Perceptrones multicapa, o MLP. Redes neuronales convolucionales, o CNN. Redes neuronales recurrentes, como la red de memoria a largo plazo o LSTM. Más específicamente, esto requiere una demostración de habilidades sólidas sobre cómo configurar y aprovechar al máximo las capas utilizadas en una CNN, como: Capas convolucionales. Agrupando Capas. Patrones de uso de capas. Esto también puede incluir habilidades con algunas clases generales de modelos efectivos, tales como: ImageNet CNN como AlexNet, VGG, ResNet, Inception, etc. CNN-LSTM, LSTM-CNN, etc. R-CNN, YOLO, etc. ¿Cuáles son tus técnicas de aprendizaje profundo favoritas para la visión por computadora? Déjame saber abajo en los comentarios. Nivel 3: Competencia de la aplicación La competencia de la aplicación se refiere a la capacidad de trabajar a través de un problema específico de visión por computadora y usar métodos de aprendizaje profundo para entregar un modelo hábil. Un modelo hábil significa un modelo que es capaz de hacer predicciones que tienen un mejor rendimiento que un método de referencia ingenuo. No significa lograr resultados de vanguardia y replicar un modelo y resultados en un documento, aunque son buenas ideas de proyectos si están dentro del alcance de un proyecto pequeño. El proyecto debe completarse sistemáticamente, incluyendo la mayoría, si no todos, los siguientes pasos: Descripción del problema. Describa el problema del modelado predictivo, incluido el dominio y los antecedentes relevantes. Revisión de literatura. Describa enfoques estándar o comunes para resolver el problema utilizando métodos de aprendizaje profundo como se describe en documentos de investigación recientes y / o seminales. Resumir datos. Describa los datos disponibles, incluidos resúmenes estadísticos y visualización de datos. Evaluar modelos. Verifique un conjunto de tipos de modelos, configuraciones, esquemas de preparación de datos y más para limitar lo que funciona bien en el problema. Mejorar el rendimiento. Mejore el rendimiento del modelo o modelos que funcionan bien con el ajuste de hiperparámetros y quizás con métodos de conjunto. Resultados actuales Presentar los resultados del proyecto. Un paso antes de este proceso, paso cero, podría ser elegir un conjunto de datos disponible públicamente apropiado para el proyecto. La columna vertebral del aprendizaje profundo para la visión por computadora es la clasificación de imágenes, comúnmente conocida como reconocimiento de imágenes o detección de objetos. Esto implica predecir una etiqueta de clase dada una imagen, a menudo una fotografía. Los problemas de este tipo deberían ser el foco. Dos conjuntos de datos estándar de visión por computadora de este tipo incluyen: Clasificación de dígitos escritos a mano (por ejemplo, MNIST y SVHN). Clasificación de fotos de objetos (por ejemplo, CIFAR-10 y CIFAR-100). Clasificación de fotos de caras (p. Ej., VGGFace2) Una tarea de visión por computadora relacionada es identificar la ubicación de uno o más objetos dentro de las fotografías, también conocido como reconocimiento de objetos o localización o segmentación de objetos. Reconocimiento y localización de objetos (por ejemplo, COCO) También hay tareas que involucran"
3;machinelearningmastery.com;https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/;2019-06-20;How to Implement GAN Hacks in Keras to Train Stable Models;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are challenging to train.

This is because the architecture involves both a generator and a discriminator model that compete in a zero-sum game. It means that improvements to one model come at the cost of a degrading of performance in the other model. The result is a very unstable training process that can often lead to failure, e.g. a generator that generates the same image all the time or generates nonsense.

As such, there are a number of heuristics or best practices (called “GAN hacks“) that can be used when configuring and training your GAN models. These heuristics are been hard won by practitioners testing and evaluating hundreds or thousands of combinations of configuration operations on a range of problems over many years.

Some of these heuristics can be challenging to implement, especially for beginners.

Further, some or all of them may be required for a given project, although it may not be clear which subset of heuristics should be adopted, requiring experimentation. This means a practitioner must be ready to implement a given heuristic with little notice.

In this tutorial, you will discover how to implement a suite of best practices or GAN hacks that you can copy-and-paste directly into your GAN project.

After reading this tutorial, you will know:

The best sources for practical heuristics or hacks when developing generative adversarial networks.

How to implement seven best practices for the deep convolutional GAN model architecture from scratch.

How to implement four additional best practices from Soumith Chintala’s GAN Hacks presentation and list.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Heuristics for Training Stable GANs Best Practices for Deep Convolutional GANs Downsample Using Strided Convolutions Upsample Using Strided Convolutions Use LeakyReLU Use Batch Normalization Use Gaussian Weight Initialization Use Adam Stochastic Gradient Descent Scale Images to the Range [-1,1] Soumith Chintala’s GAN Hacks Use a Gaussian Latent Space Separate Batches of Real and Fake Images Use Label Smoothing Use Noisy Labels

Heuristics for Training Stable GANs

GANs are difficult to train.

At the time of writing, there is no good theoretical foundation as to how to design and train GAN models, but there is established literature of heuristics, or “hacks,” that have been empirically demonstrated to work well in practice.

As such, there are a range of best practices to consider and implement when developing a GAN model.

Perhaps the two most important sources of suggested configuration and training parameters are:

Alec Radford, et al’s 2015 paper that introduced the DCGAN architecture. Soumith Chintala’s 2016 presentation and associated “GAN Hacks” list.

In this tutorial, we will explore how to implement the most important best practices from these two sources.

Best Practices for Deep Convolutional GANs

Perhaps one of the most important steps forward in the design and training of stable GAN models was the 2015 paper by Alec Radford, et al. titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.”

In the paper, they describe the Deep Convolutional GAN, or DCGAN, approach to GAN development that has become the de facto standard.

We will look at how to implement seven best practices for the DCGAN model architecture in this section.

1. Downsample Using Strided Convolutions

The discriminator model is a standard convolutional neural network model that takes an image as input and must output a binary classification as to whether it is real or fake.

It is standard practice with deep convolutional networks to use pooling layers to downsample the input and feature maps with the depth of the network.

This is not recommended for the DCGAN, and instead, they recommend downsampling using strided convolutions.

This involves defining a convolutional layer as per normal, but instead of using the default two-dimensional stride of (1,1) to change it to (2,2). This has the effect of downsampling the input, specifically halving the width and height of the input, resulting in output feature maps with one quarter the area.

The example below demonstrates this with a single hidden convolutional layer that uses downsampling strided convolutions by setting the ‘strides‘ argument to (2,2). The effect is the model will downsample the input from 64×64 to 32×32.

# example of downsampling with strided convolutions from keras.models import Sequential from keras.layers import Conv2D # define model model = Sequential() model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3))) # summarize model model.summary() 1 2 3 4 5 6 7 8 # example of downsampling with strided convolutions from keras . models import Sequential from keras . layers import Conv2D # define model model = Sequential ( ) model . add ( Conv2D ( 64 , kernel_size = ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 64 , 64 , 3 ) ) ) # summarize model model . summary ( )

Running the example shows the shape of the output of the convolutional layer, where the feature maps have one quarter of the area.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 32, 32, 64) 1792 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________

2. Upsample Using Strided Convolutions

The generator model must generate an output image given as input at a random point from the latent space.

The recommended approach for achieving this is to use a transpose convolutional layer with a strided convolution. This is a special type of layer that performs the convolution operation in reverse. Intuitively, this means that setting a stride of 2×2 will have the opposite effect, upsampling the input instead of downsampling it in the case of a normal convolutional layer.

By stacking a transpose convolutional layer with strided convolutions, the generator model is able to scale a given input to the desired output dimensions.

The example below demonstrates this with a single hidden transpose convolutional layer that uses upsampling strided convolutions by setting the ‘strides‘ argument to (2,2).

The effect is the model will upsample the input from 64×64 to 128×128.

# example of upsampling with strided convolutions from keras.models import Sequential from keras.layers import Conv2DTranspose # define model model = Sequential() model.add(Conv2DTranspose(64, kernel_size=(4,4), strides=(2,2), padding='same', input_shape=(64,64,3))) # summarize model model.summary() 1 2 3 4 5 6 7 8 # example of upsampling with strided convolutions from keras . models import Sequential from keras . layers import Conv2DTranspose # define model model = Sequential ( ) model . add ( Conv2DTranspose ( 64 , kernel_size = ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 64 , 64 , 3 ) ) ) # summarize model model . summary ( )

Running the example shows the shape of the output of the convolutional layer, where the feature maps have quadruple the area.

_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_transpose_1 (Conv2DTr (None, 128, 128, 64) 3136 ================================================================= Total params: 3,136 Trainable params: 3,136 Non-trainable params: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_transpose_1 (Conv2DTr (None, 128, 128, 64) 3136 ================================================================= Total params: 3,136 Trainable params: 3,136 Non-trainable params: 0 _________________________________________________________________

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

3. Use LeakyReLU

The rectified linear activation unit, or ReLU for short, is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less.

It has become a best practice when developing deep convolutional neural networks generally.

The best practice for GANs is to use a variation of the ReLU that allows some values less than zero and learns where the cut-off should be in each node. This is called the leaky rectified linear activation unit, or LeakyReLU for short.

A negative slope can be specified for the LeakyReLU and the default value of 0.2 is recommended.

Originally, ReLU was recommend for use in the generator model and LeakyReLU was recommended for use in the discriminator model, although more recently, the LeakyReLU is recommended in both models.

The example below demonstrates using the LeakyReLU with the default slope of 0.2 after a convolutional layer in a di";Cómo implementar hacks GAN en Keras para entrenar modelos estables;"Tweet Compartir Compartir Última actualización el 12 de julio de 2019 Las redes adversas generativas, o GAN, son difíciles de entrenar. Esto se debe a que la arquitectura involucra un modelo generador y un modelo discriminador que compiten en un juego de suma cero. Significa que las mejoras en un modelo tienen el costo de una degradación del rendimiento en el otro modelo. El resultado es un proceso de entrenamiento muy inestable que a menudo puede conducir al fracaso, por ejemplo, un generador que genera la misma imagen todo el tiempo o genera tonterías. Como tal, hay una serie de heurísticas o mejores prácticas (llamadas &quot;hacks GAN&quot;) que se pueden usar al configurar y entrenar sus modelos GAN. Los profesionales que prueban y evalúan cientos o miles de combinaciones de operaciones de configuración en una variedad de problemas durante muchos años han ganado estas heurísticas. Algunas de estas heurísticas pueden ser difíciles de implementar, especialmente para principiantes. Además, algunos o todos ellos pueden ser necesarios para un proyecto determinado, aunque puede que no esté claro qué subconjunto de heurísticas debe adoptarse, lo que requiere experimentación. Esto significa que un profesional debe estar listo para implementar una heurística dada con poco aviso. En este tutorial, descubrirá cómo implementar un conjunto de mejores prácticas o hacks GAN que puede copiar y pegar directamente en su proyecto GAN. Después de leer este tutorial, sabrá: Las mejores fuentes para la heurística práctica o los hacks al desarrollar redes de confrontación generativas. Cómo implementar siete mejores prácticas para la arquitectura convolucional profunda del modelo GAN desde cero. Cómo implementar cuatro mejores prácticas adicionales de la presentación y lista GAN Hacks de Soumith Chintala. Descubra cómo desarrollar DCGAN, GAN condicionales, Pix2Pix, CycleGAN y más con Keras en mi nuevo libro de GAN, con 29 tutoriales paso a paso y código fuente completo. Empecemos. Descripción general del tutorial Este tutorial se divide en tres partes; son: Heurística para el entrenamiento de GAN estables. Mejores prácticas para GAN convolucionales profundos. Muestra descendente Usar convoluciones estriadas. Upsample Usar convoluciones estriadas. Usar LeakyReLU. Usar normalización de lotes. Usar inicialización de peso gaussiano. Usar Adam Stochastic Gradient Descent Scale Images al rango [-1,1]. Hacks Use un espacio latente gaussiano Lotes separados de imágenes reales y falsas Use el suavizado de etiquetas Use etiquetas ruidosas Heurística para entrenar GAN estables Las GAN son difíciles de entrenar. Al momento de escribir, no hay una buena base teórica sobre cómo diseñar y entrenar modelos GAN, pero hay literatura establecida de heurística, o &quot;hacks&quot;, que se ha demostrado empíricamente que funciona bien en la práctica. Como tal, hay una serie de mejores prácticas para considerar e implementar al desarrollar un modelo GAN. Quizás las dos fuentes más importantes de configuración sugerida y parámetros de capacitación son: Alec Radford, et al. 2015, documento que introdujo la arquitectura DCGAN. La presentación de 2016 de Soumith Chintala y la lista asociada de &quot;GAN Hacks&quot;. En este tutorial, exploraremos cómo implementar las mejores prácticas más importantes de estas dos fuentes. Mejores prácticas para GANs convolucionales profundas Quizás uno de los pasos más importantes hacia adelante en el diseño y capacitación de modelos GAN estables fue el artículo de 2015 de Alec Radford, et al. titulado &quot;Aprendizaje de representación no supervisada con profundas redes adversas generativas convolucionales&quot;. En el documento, describen el enfoque Deep Convolutional GAN, o DCGAN, para el desarrollo de GAN que se ha convertido en el estándar de facto. En esta sección veremos cómo implementar siete mejores prácticas para la arquitectura del modelo DCGAN. 1. Reducción de la resolución utilizando convoluciones estriadas El modelo discriminador es un modelo de red neuronal convolucional estándar que toma una imagen como entrada y debe generar una clasificación binaria en cuanto a si es real o falsa. Es una práctica estándar con redes convolucionales profundas el uso de capas de agrupación para reducir la muestra de entrada y mapas de características con la profundidad de la red. Esto no se recomienda para DCGAN, y en su lugar, recomiendan reducir el muestreo utilizando convoluciones estriadas. Esto implica definir una capa convolucional según lo normal, pero en lugar de usar el paso bidimensional predeterminado de (1,1) para cambiarlo a (2,2). Esto tiene el efecto de reducir el muestreo de la entrada, específicamente reducir a la mitad el ancho y la altura de la entrada, lo que resulta en mapas de características de salida con una cuarta parte del área. El siguiente ejemplo demuestra esto con una sola capa convolucional oculta que utiliza convoluciones de reducción de muestreo reduciendo el argumento &#39;strides&#39; a (2,2). El efecto es que el modelo disminuirá la entrada de 64 × 64 a 32 × 32. # ejemplo de disminución de resolución con convoluciones en zancadas de keras.models import Sequential de keras.layers import Conv2D # define model model = Sequential () model.add (Conv2D (64, kernel_size = (3,3), strides = (2,2) , padding = &#39;same&#39;, input_shape = (64,64,3))) # resume el modelo model.summary () 1 2 3 4 5 6 7 8 # ejemplo de disminución de resolución con convoluciones zancadas de keras. Los modelos importan secuenciales de keras. importación de capas Conv2D # define modelo de modelo = modelo secuencial (). add (Conv2D (64, kernel_size = (3, 3), strides = (2, 2), padding = &#39;same&#39;, input_shape = (64, 64, 3))) # resume el modelo del modelo. summary () Al ejecutar el ejemplo, se muestra la forma de la salida de la capa convolucional, donde los mapas de entidades tienen una cuarta parte del área. _________________________________________________________________ Capa (tipo) Parámetro de forma de salida # ========================================= ======================== conv2d_1 (Conv2D) (Ninguno, 32, 32, 64) 1792 ============ ================================================== === Parámetros totales: 1,792 Parámetros entrenables: 1,792 Parámetros no entrenables: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 _________________________________________________________________ Capa (tipo) Parámetro de forma de salida # ============== ================================================== = conv2d_1 (Conv2D) (Ninguno, 32, 32, 64) 1792 =================================== ============================== Parámetros totales: 1,792 Parámetros entrenables: 1,792 Parámetros no entrenables: 0 _________________________________________________________________ 2. Muestra ascendente usando Strided Convoluciones El modelo generador debe generar una imagen de salida dada como entrada en un punto aleatorio desde el espacio latente. El enfoque recomendado para lograr esto es utilizar una capa convolucional de transposición con una convolución estriada. Este es un tipo especial de capa que realiza la operación de convolución en reversa. Intuitivamente, esto significa que establecer una zancada de 2 × 2 tendrá el efecto contrario, muestreando la entrada en lugar de reducirla en el caso de una capa convolucional normal. Al apilar una capa convolucional de transposición con convoluciones estriadas, el modelo generador puede escalar una entrada dada a las dimensiones de salida deseadas. El siguiente ejemplo demuestra esto con una sola capa convolucional de transposición oculta que usa convoluciones de muestreo ascendente al establecer el argumento &#39;strides&#39; en (2,2). El efecto es que el modelo aumentará la entrada de 64 × 64 a 128 × 128. # ejemplo de sobremuestreo con convoluciones estriadas de keras.models import Sequential de keras.layers import Conv2DTranspose # define model model = Sequential () model.add (Conv2DTranspose (64, kernel_size = (4,4), strides = (2,2) , padding = &#39;same&#39;, input_shape = (64,64,3))) # resume el modelo model.summary () 1 2 3 4 5 6 7 8 # ejemplo de muestreo ascendente con convoluciones estriadas de keras. Los modelos importan secuenciales de keras. importar capas Conv2DTranspose # define modelo modelo = modelo secuencial (). add (Conv2DTranspose (64, kernel_size = (4, 4), strides = (2, 2), padding = &#39;same&#39;, input_shape = (64, 64, 3))) # resume el modelo del modelo. summary () Ejecutar el ejemplo muestra la forma de la salida de la capa convolucional, donde los mapas de entidades han cuadruplicado el área. _________________________________________________________________ Capa (tipo) Parámetro de forma de salida # ========================================= ======================== conv2d_transpose_1 (Conv2DTr (Ninguno, 128, 128, 64) 3136 ============= ================================================== == Parámetros totales: 3,136 Parámetros entrenables: 3,136 Parámetros no entrenables: 0 _________________________________________________________________ 1 2 3 4 5 6 7 8 9 _________________________________________________________________ Capa (tipo) Parámetro de forma de salida # =============== ================================================== conv2d_transpose_1 (Conv2DTr (Ninguno, 128, 128, 64) 3136 ===================================== ============================ Parámetros totales: 3,136 Parámetros entrenables: 3,136 Parámetros no entrenables: 0 _________________________________________________________________ ¿Desea desarrollar GAN desde cero? Tome mi curso gratuito de 7 días por correo electrónico ahora (con código de muestra). Haga clic para inscribirse y también obtenga un PDF Eb gratis Buena versión del curso. Descargue su mini-curso GRATIS 3. Use LeakyReLU La unidad de activación lineal rectificada, o ReLU para abreviar, es un cálculo simple que devuelve el valor proporcionado como entrada directamente, o el valor 0.0 si la entrada es 0.0 o menos. Se ha convertido en una práctica recomendada cuando se desarrollan redes neuronales convolucionales profundas en general. La mejor práctica para las GAN es usar una variación de ReLU que permita algunos valores inferiores a cero y aprenda dónde debe estar el límite en cada nodo. Esto se llama la unidad de activación lineal rectificada con fugas, o LeakyReLU para abreviar. Se puede especificar una pendiente negativa para LeakyReLU y se recomienda el valor predeterminado de 0.2. Originalmente, ReLU se recomendaba para su uso en el modelo generador y LeakyReLU se recomendaba en el modelo discriminador, aunque más recientemente, se recomienda LeakyReLU en ambos modelos. El siguiente ejemplo demuestra el uso de LeakyReLU con la pendiente predeterminada de 0.2 después de una capa convolucional en un di"
4;machinelearningmastery.com;https://machinelearningmastery.com/how-to-evaluate-pixel-scaling-methods-for-image-classification/;2019-03-26;How to Evaluate Pixel Scaling Methods for Image Classification With CNNs;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113

# comparison of training-set based pixel scaling methods on MNIST from numpy import mean from numpy import std from matplotlib import pyplot from keras . datasets import mnist from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten # load train and test dataset def load_dataset ( ) : # load dataset ( trainX , trainY ) , ( testX , testY ) = mnist . load_data ( ) # reshape dataset to have a single channel width , height , channels = trainX . shape [ 1 ] , trainX . shape [ 2 ] , 1 trainX = trainX . reshape ( ( trainX . shape [ 0 ] , width , height , channels ) ) testX = testX . reshape ( ( testX . shape [ 0 ] , width , height , channels ) ) # one hot encode target values trainY = to_categorical ( trainY ) testY = to_categorical ( testY ) return trainX , trainY , testX , testY # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , input_shape = ( 28 , 28 , 1 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 64 , activation = 'relu' ) ) model . add ( Dense ( 10 , activation = 'softmax' ) ) # compile model model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] ) return model # normalize images def prep_normalize ( train , test ) : # convert from integers to floats train_norm = train . astype ( 'float32' ) test_norm = test . astype ( 'float32' ) # normalize to range 0-1 train_norm = train_norm / 255.0 test_norm = test_norm / 255.0 # return normalized images return train_norm , test_norm # center images def prep_center ( train , test ) : # convert from integers to floats train_cent = train . astype ( 'float32' ) test_cent = test . astype ( 'float32' ) # calculate statistics m = train_cent . mean ( ) # center datasets train_cent = train_cent - m test_cent = test_cent - m # return normalized images return train_cent , test_cent # standardize images def prep_standardize ( train , test ) : # convert from integers to floats train_stan = train . astype ( 'float32' ) test_stan = test . astype ( 'float32' ) # calculate statistics m = train_stan . mean ( ) s = train_stan . std ( ) # center datasets train_stan = ( train_stan - m ) / s test_stan = ( test_stan - m ) / s # return normalized images return train_stan , test_stan # repeated evaluation of model with data prep scheme def repeated_evaluation ( datapre_func , n_repeats = 10 ) : # prepare data trainX , trainY , testX , testY = load_dataset ( ) # repeated evaluation scores = list ( ) for i in range ( n_repeats ) : # define model model = define_model ( ) # prepare data prep_trainX , prep_testX = datapre_func ( trainX , testX ) # fit model model . fit ( prep_trainX , trainY , epochs = 5 , batch_size = 64 , verbose = 0 ) # evaluate model _ , acc = model . evaluate ( prep_testX , testY , verbose = 0 ) # store result scores . append ( acc ) print ( '> %d: %.3f' % ( i , acc * 100.0 ) ) return scores all_scores = list ( ) # normalization scores = repeated_evaluation ( prep_normalize ) print ( 'Normalization: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # center scores = repeated_evaluation ( prep_center ) print ( 'Centered: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # standardize scores = repeated_evaluation ( prep_standardize ) print ( 'Standardized: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # box and whisker plots of results pyplot . boxplot ( all_scores , labels = [ 'norm' , 'cent' , 'stan' ] ) pyplot . show ( )";Cómo evaluar los métodos de escala de píxeles para la clasificación de imágenes con CNN;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99100 101 102 103 104 105 106 107 108 109 110 111 112 113 # comparación de métodos de escalamiento de píxeles basados en el conjunto de entrenamiento en MNIST desde numpy import media desde numpy import std desde matplotlib import pyplot desde keras. los conjuntos de datos importan mnist de keras. utils import to_categorical de keras. Los modelos importan secuenciales de keras. las capas importan Conv2D desde keras. las capas importan MaxPooling2D de keras. Las capas importan densas de keras. las capas importan Flatten # load train y test dataset def load_dataset (): # load dataset (trainX, trainY), (testX, testY) = mnist. load_data () # reconfigura el dataset para que tenga un solo ancho de canal, altura, canales = trainX. forma [1], trainX. forma [2], 1 trainX = trainX. remodelar ((trainX. forma [0], ancho, altura, canales)) testX = testX. remodelar ((testX. forma [0], ancho, alto, canales)) # uno codificar en caliente valores objetivo trainY = to_categorical (trainY) testY = to_categorical (testY) return trainX, trainY, testX, testY # define cnn model def define_model ( ): modelo = modelo secuencial (). modelo add (Conv2D (32, (3, 3), activación = &#39;relu&#39;, input_shape = (28, 28, 1))). Añadir modelo (MaxPooling2D ((2, 2))). modelo add (Conv2D (64, (3, 3), activación = &#39;relu&#39;)). Añadir modelo (MaxPooling2D ((2, 2))). agregar modelo (Flatten ()). modelo add (Dense (64, activación = &#39;relu&#39;)). add (Dense (10, activación = &#39;softmax&#39;)) # compila el modelo del modelo. compilar (optimizador = &#39;adam&#39;, pérdida = &#39;categorical_crossentropy&#39;, métricas = [&#39;precisión&#39;]) devolver el modelo # normalizar imágenes def prep_normalize (tren, prueba): # convertir de enteros a flotadores train_norm = tren. astype (&#39;float32&#39;) test_norm = prueba. astype (&#39;float32&#39;) # normalize to range 0-1 train_norm = train_norm / 255.0 test_norm = test_norm / 255.0 # return normalized images return train_norm, test_norm # center images def prep_center (train, test): # convertir de enteros a flotadores train_cent = tren . astype (&#39;float32&#39;) test_cent = prueba. astype (&#39;float32&#39;) # calcular estadísticas m = train_cent. mean () # datasets centrales train_cent = train_cent - m test_cent = test_cent - m # devuelve imágenes normalizadas return train_cent, test_cent # estandariza imágenes def prep_standardize (train, test): # convierte de enteros a flotadores train_stan = train. astype (&#39;float32&#39;) test_stan = prueba. astype (&#39;float32&#39;) # calcular estadísticas m = train_stan. mean () s = train_stan. std () # datasets de centro train_stan = (train_stan - m) / s test_stan = (test_stan - m) / s # devuelve imágenes normalizadas return train_stan, test_stan # evaluación repetida del modelo con esquema de preparación de datos def repetido_evaluación (datapre_func, n_repeats = 10) : # preparar datos trainX, trainY, testX, testY = load_dataset () # puntajes de evaluación repetidos = list () para i en rango (n_repeats): # define modelo modelo = define_model () # prepara datos prep_trainX, prep_testX = datapre_func (trainX, testX) # modelo de modelo ajustado. fit (prep_trainX, trainY, epochs = 5, batch_size = 64, verbose = 0) # evaluar modelo _, acc = modelo. evalúe (prep_testX, testY, verbose = 0) # puntajes de resultados de la tienda. agregar (acc) print (&#39;&gt;% d:% .3f&#39;% (i, acc * 100.0)) devolver puntajes all_scores = list () # puntajes de normalización = repetida_evaluación (prep_normalize) print (&#39;Normalización:% .3f (%. 3f) &#39;% (media (puntajes), estándar (puntajes))) all_scores. agregar (puntajes) # puntajes centrales = evaluación_ repetida (centro de preparación) print (&#39;Centrado:% .3f (% .3f)&#39;% (promedio (puntajes), std (puntajes))) all_scores. agregar (puntajes) # estandarizar puntajes = evaluación_ repetida (prep_standardize) print (&#39;Estandarizado:% .3f (% .3f)&#39;% (promedio (puntajes), std (puntajes))) todos_puntos. agregar (puntajes) # box y bigotes gráficos de resultados pyplot. boxplot (all_scores, labels = [&#39;norma&#39;, &#39;cent&#39;, &#39;stan&#39;]) pyplot. show ( )"
5;machinelearningmastery.com;https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/;2017-06-06;The 5 Step Life-Cycle for Long Short-Term Memory Models in Keras;"# Example of LSTM to learn a sequence

from pandas import DataFrame

from pandas import concat

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# create sequence

length = 10

sequence = [ i / float ( length ) for i in range ( length ) ]

print ( sequence )

# create X/y pairs

df = DataFrame ( sequence )

df = concat ( [ df . shift ( 1 ) , df ] , axis = 1 )

df . dropna ( inplace = True )

# convert to LSTM friendly format

values = df . values

X , y = values [ : , 0 ] , values [ : , 1 ]

X = X . reshape ( len ( X ) , 1 , 1 )

# 1. define network

model = Sequential ( )

model . add ( LSTM ( 10 , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 ) )

# 2. compile network

model . compile ( optimizer = 'adam' , loss = 'mean_squared_error' )

# 3. fit network

history = model . fit ( X , y , epochs = 1000 , batch_size = len ( X ) , verbose = 0 )

# 4. evaluate network

loss = model . evaluate ( X , y , verbose = 0 )

print ( loss )

# 5. make predictions

predictions = model . predict ( X , verbose = 0 )";El ciclo de vida de 5 pasos para modelos de memoria a corto y largo plazo en Keras;"# Ejemplo de LSTM para aprender una secuencia de pandas import DataFrame de pandas import concat de keras. Los modelos importan secuenciales de keras. Las capas importan densas de keras. capas importar LSTM # crear secuencia longitud = 10 secuencia = [i / float (longitud) para i en rango (longitud)] imprimir (secuencia) # crear pares X / y df = DataFrame (secuencia) df = concat ([df. shift (1), df], eje = 1) df. dropna (inplace = True) # convertir a formato LSTM amigable valores = df. valores X, y = valores [:, 0], valores [:, 1] X = X. remodelar (len (X), 1, 1) # 1. definir modelo de red = modelo secuencial (). modelo add (LSTM (10, input_shape = (1, 1))). add (Dense (1)) # 2. compila el modelo de red. compilar (optimizador = &#39;adam&#39;, pérdida = &#39;mean_squared_error&#39;) # 3. ajustar el historial de red = modelo. fit (X, y, epochs = 1000, batch_size = len (X), verbose = 0) # 4. evaluar la pérdida de red = modelo. evaluar (X, y, detallado = 0) imprimir (pérdida) # 5. hacer predicciones predicciones = modelo. predecir (X, detallado = 0)"
6;machinelearningmastery.com;http://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/;2016-04-21;Bagging and Random Forest Ensemble Algorithms for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.

In this post you will discover the Bagging ensemble algorithm and the Random Forest algorithm for predictive modeling. After reading this post you will know about:

The bootstrap method for estimating statistical quantities from samples.

The Bootstrap Aggregation algorithm for creating multiple different models from a single training dataset.

The Random Forest algorithm that makes a small tweak to Bagging and results in a very powerful classifier.

This post was written for developers and assumes no background in statistics or mathematics. The post focuses on how the algorithm works and how to use it for predictive modeling problems.

If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Bootstrap Method

Before we get to Bagging, let’s take a quick look at an important foundation technique called the bootstrap.

The bootstrap is a powerful statistical method for estimating a quantity from a data sample. This is easiest to understand if the quantity is a descriptive statistic such as a mean or a standard deviation.

Let’s assume we have a sample of 100 values (x) and we’d like to get an estimate of the mean of the sample.

We can calculate the mean directly from the sample as:

mean(x) = 1/100 * sum(x)

We know that our sample is small and that our mean has error in it. We can improve the estimate of our mean using the bootstrap procedure:

Create many (e.g. 1000) random sub-samples of our dataset with replacement (meaning we can select the same value multiple times). Calculate the mean of each sub-sample. Calculate the average of all of our collected means and use that as our estimated mean for the data.

For example, let’s say we used 3 resamples and got the mean values 2.3, 4.5 and 3.3. Taking the average of these we could take the estimated mean of the data to be 3.367.

This process can be used to estimate other quantities like the standard deviation and even quantities used in machine learning algorithms, like learned coefficients.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Bootstrap Aggregation (Bagging)

Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.

An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.

Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees, like classification and regression trees (CART).

Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.

Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.

Let’s assume we have a sample dataset of 1000 instances (x) and we are using the CART algorithm. Bagging of the CART algorithm would work as follows.

Create many (e.g. 100) random sub-samples of our dataset with replacement. Train a CART model on each sample. Given a new dataset, calculate the average prediction from each model.

For example, if we had 5 bagged decision trees that made the following class predictions for a in input sample: blue, blue, red, blue and red, we would take the most frequent class and predict blue.

When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging.

The only parameters when bagging decision trees is the number of samples and hence the number of trees to include. This can be chosen by increasing the number of trees on run after run until the accuracy begins to stop showing improvement (e.g. on a cross validation test harness). Very large numbers of models may take a long time to prepare, but will not overfit the training data.

Just like the decision trees themselves, Bagging can be used for classification and regression problems.

Random Forest

Random Forests are an improvement over bagged decision trees.

A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions.

Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.

Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation.

It is a simple tweak. In CART, when selecting a split point, the learning algorithm is allowed to look through all variables and all variable values in order to select the most optimal split-point. The random forest algorithm changes this procedure so that the learning algorithm is limited to a random sample of features of which to search.

The number of features that can be searched at each split point (m) must be specified as a parameter to the algorithm. You can try different values and tune it using cross validation.

For classification a good default is: m = sqrt(p)

For regression a good default is: m = p/3

Where m is the number of randomly selected features that can be searched at a split point and p is the number of input variables. For example, if a dataset had 25 input variables for a classification problem, then:

m = sqrt(25)

m = 5

Estimated Performance

For each bootstrap sample taken from the training data, there will be samples left behind that were not included. These samples are called Out-Of-Bag samples or OOB.

The performance of each model on its left out samples when averaged can provide an estimated accuracy of the bagged models. This estimated performance is often called the OOB estimate of performance.

These performance measures are reliable test error estimate and correlate well with cross validation estimates.

Variable Importance

As the Bagged decision trees are constructed, we can calculate how much the error function drops for a variable at each split point.

In regression problems this may be the drop in sum squared error and in classification this might be the Gini score.

These drops in error can be averaged across all decision trees and output to provide an estimate of the importance of each input variable. The greater the drop when the variable was chosen, the greater the importance.

These outputs can help identify subsets of input variables that may be most or least relevant to the problem and suggest at possible feature selection experiments you could perform where some features are removed from the dataset.

Further Reading

Bagging is a simple technique that is covered in most introductory machine learning texts. Some examples are listed below.

Summary

In this post you discovered the Bagging ensemble machine learning algorithm and the popular variation called Random Forest. You learned:

How to estimate statistical quantities from a data sample.

How to combine the predictions from multiple high-variance models using bagging.

How to tweak the construction of decision trees when bagging to de-correlate their predictions, a technique called Random Forests.

Do you have any questions about this post or the Bagging or Random Forest Ensemble algorithms?

Leave a comment and ask your question and I will do my best to answer it.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside";Bagging y Algoritmos de conjunto de bosque aleatorio para el aprendizaje automático;Tweet Compartir Compartir Última actualización el 12 de agosto de 2019 Random Forest es uno de los algoritmos de aprendizaje automático más populares y potentes. Es un tipo de algoritmo de aprendizaje automático de conjunto llamado Bootstrap Aggregation o bagging. En esta publicación, descubrirá el algoritmo de conjunto de ensacado y el algoritmo de bosque aleatorio para el modelado predictivo. Después de leer esta publicación, conocerá: El método bootstrap para estimar cantidades estadísticas a partir de muestras. El algoritmo de agregación Bootstrap para crear múltiples modelos diferentes a partir de un único conjunto de datos de entrenamiento. El algoritmo Random Forest que hace un pequeño ajuste a Bagging y da como resultado un clasificador muy poderoso. Esta publicación fue escrita para desarrolladores y no asume antecedentes en estadística o matemáticas. La publicación se centra en cómo funciona el algoritmo y cómo usarlo para problemas de modelado predictivo. Si tiene alguna pregunta, deje un comentario y haré todo lo posible para responder. Descubra cómo funcionan los algoritmos de aprendizaje automático, incluidos kNN, árboles de decisión, ingenuos bayes, SVM, conjuntos y mucho más en mi nuevo libro, con 22 tutoriales y ejemplos en Excel. Empecemos. Método Bootstrap Antes de llegar a Bagging, echemos un vistazo rápido a una técnica de base importante llamada bootstrap. El bootstrap es un poderoso método estadístico para estimar una cantidad de una muestra de datos. Esto es más fácil de entender si la cantidad es una estadística descriptiva, como una media o una desviación estándar. Supongamos que tenemos una muestra de 100 valores (x) y nos gustaría obtener una estimación de la media de la muestra. Podemos calcular la media directamente de la muestra como: media (x) = 1/100 * suma (x) Sabemos que nuestra muestra es pequeña y que nuestra media tiene un error. Podemos mejorar la estimación de nuestra media utilizando el procedimiento bootstrap: cree muchas (por ejemplo, 1000) submuestras aleatorias de nuestro conjunto de datos con reemplazo (lo que significa que podemos seleccionar el mismo valor varias veces). Calcule la media de cada submuestra. Calcule el promedio de todos nuestros medios recopilados y úselos como nuestro promedio estimado para los datos. Por ejemplo, digamos que usamos 3 resamples y obtuvimos los valores medios 2.3, 4.5 y 3.3. Tomando el promedio de estos, podríamos tomar la media estimada de los datos en 3.367. Este proceso puede usarse para estimar otras cantidades como la desviación estándar e incluso las cantidades utilizadas en algoritmos de aprendizaje automático, como los coeficientes aprendidos. Obtenga su mapa mental de algoritmos GRATUITOS He creado un práctico mapa mental de más de 60 algoritmos organizados por tipo. Descárguelo, imprímalo y úselo. Descargue gratis También obtenga acceso exclusivo al algoritmo de aprendizaje automático del mini curso por correo electrónico. Bootstrap Aggregation (Bagging) Bootstrap Aggregation (o Bagging para abreviar), es un método de conjunto simple y muy poderoso. Un método de conjunto es una técnica que combina las predicciones de múltiples algoritmos de aprendizaje automático para hacer predicciones más precisas que cualquier modelo individual. La agregación de Bootstrap es un procedimiento general que puede usarse para reducir la varianza de aquellos algoritmos que tienen una alta varianza. Un algoritmo que tiene una alta varianza son los árboles de decisión, como los árboles de clasificación y regresión (CART). Los árboles de decisión son sensibles a los datos específicos sobre los que están capacitados. Si se modifican los datos de entrenamiento (por ejemplo, un árbol se entrena en un subconjunto de datos de entrenamiento), el árbol de decisión resultante puede ser bastante diferente y, a su vez, las predicciones pueden ser bastante diferentes. El ensacado es la aplicación del procedimiento Bootstrap a un algoritmo de aprendizaje automático de alta varianza, típicamente árboles de decisión. Supongamos que tenemos un conjunto de datos de muestra de 1000 instancias (x) y estamos usando el algoritmo CART. El ensacado del algoritmo CART funcionaría de la siguiente manera. Cree muchas (por ejemplo, 100) submuestras aleatorias de nuestro conjunto de datos con reemplazo. Entrene un modelo CART en cada muestra. Dado un nuevo conjunto de datos, calcule la predicción promedio de cada modelo. Por ejemplo, si tuviéramos 5 árboles de decisión en bolsas que hicieran las siguientes predicciones de clase para una muestra de entrada: azul, azul, rojo, azul y rojo, tomaríamos la clase más frecuente y predeciríamos azul. Cuando empaquetamos con árboles de decisión, nos preocupa menos que los árboles individuales sobreajusten los datos de capacitación. Por esta razón y para la eficiencia, los árboles de decisión individuales se cultivan en profundidad (por ejemplo, pocas muestras de entrenamiento en cada nodo de hoja del árbol) y los árboles no se podan. Estos árboles tendrán una alta varianza y un bajo sesgo. Estos son importantes para caracterizar los submodelos cuando se combinan predicciones usando el embolsado. Los únicos parámetros cuando se empaquetan los árboles de decisión es el número de muestras y, por lo tanto, el número de árboles a incluir. Esto se puede elegir aumentando el número de árboles en ejecución después de la ejecución hasta que la precisión comience a dejar de mostrar mejoría (por ejemplo, en un arnés de prueba de validación cruzada). Un gran número de modelos puede tomar mucho tiempo para prepararse, pero no sobreajustará los datos de entrenamiento. Al igual que los propios árboles de decisión, el ensacado puede usarse para problemas de clasificación y regresión. Bosque aleatorio Los bosques aleatorios son una mejora sobre los árboles de decisión en bolsas. Un problema con los árboles de decisión como CART es que son codiciosos. Eligen en qué variable dividir utilizando un algoritmo codicioso que minimiza el error. Como tal, incluso con Bagging, los árboles de decisión pueden tener muchas similitudes estructurales y, a su vez, tienen una alta correlación en sus predicciones. La combinación de predicciones de múltiples modelos en conjuntos funciona mejor si las predicciones de los submodelos no están correlacionadas o, en el mejor de los casos, están débilmente correlacionadas. El bosque aleatorio cambia el algoritmo de la forma en que se aprenden los subárboles para que las predicciones resultantes de todos los subárboles tengan menos correlación. Es un simple ajuste. En CART, al seleccionar un punto de división, el algoritmo de aprendizaje puede examinar todas las variables y todos los valores de las variables para seleccionar el punto de división más óptimo. El algoritmo de bosque aleatorio cambia este procedimiento para que el algoritmo de aprendizaje se limite a una muestra aleatoria de características en las que buscar. El número de características que se pueden buscar en cada punto de división (m) debe especificarse como un parámetro para el algoritmo. Puede probar diferentes valores y ajustarlo mediante validación cruzada. Para la clasificación, un buen valor predeterminado es: m = sqrt (p) Para la regresión, un buen valor predeterminado es: m = p / 3 Donde m es el número de entidades seleccionadas al azar que se pueden buscar en un punto dividido y p es el número de variables de entrada . Por ejemplo, si un conjunto de datos tenía 25 variables de entrada para un problema de clasificación, entonces: m = sqrt (25) m = 5 Rendimiento estimado Para cada muestra de arranque tomada de los datos de entrenamiento, quedarán muestras que no se incluyeron. Estas muestras se denominan muestras fuera de bolsa u OOB. El rendimiento de cada modelo en sus muestras omitidas cuando se promedia puede proporcionar una precisión estimada de los modelos empaquetados. Este rendimiento estimado a menudo se denomina estimación OOB del rendimiento. Estas medidas de rendimiento son estimaciones confiables de errores de prueba y se correlacionan bien con las estimaciones de validación cruzada. Importancia de las variables A medida que se construyen los árboles de decisión en bolsas, podemos calcular cuánto cae la función de error para una variable en cada punto de división. En problemas de regresión, esta puede ser la caída en el error de suma cuadrática y en la clasificación puede ser el puntaje de Gini. Estas caídas de error se pueden promediar en todos los árboles de decisión y salida para proporcionar una estimación de la importancia de cada variable de entrada. Cuanto mayor es la caída cuando se elige la variable, mayor es la importancia. Estas salidas pueden ayudar a identificar subconjuntos de variables de entrada que pueden ser más o menos relevantes para el problema y sugerir a los posibles experimentos de selección de características que podría realizar donde algunas características se eliminan del conjunto de datos. Leer más El ensacado es una técnica simple que se cubre en la mayoría de los textos introductorios de aprendizaje automático. Algunos ejemplos se enumeran a continuación. Resumen En esta publicación descubrió el algoritmo de aprendizaje automático del conjunto de ensacado y la variante popular llamada Bosque aleatorio. Aprendiste: Cómo estimar cantidades estadísticas a partir de una muestra de datos. Cómo combinar las predicciones de múltiples modelos de alta varianza utilizando embolsado. Cómo ajustar la construcción de los árboles de decisión al embolsar para descorrelacionar sus predicciones, una técnica llamada Bosques aleatorios. ¿Tiene alguna pregunta sobre esta publicación o sobre los algoritmos Bagging o Random Forest Ensemble? Deje un comentario y haga su pregunta y haré todo lo posible para responderla. ¡Descubra cómo funcionan los algoritmos de aprendizaje automático! Vea cómo funcionan los algoritmos en minutos ... con ejemplos simples y aritméticos. Descubra cómo en mi nuevo libro electrónico: Algoritmos de aprendizaje automático maestro. Cubre explicaciones y ejemplos de 10 algoritmos principales, como: Regresión lineal, vecinos más cercanos k, máquinas de vectores de soporte y mucho más ... Finalmente, retire el telón de los algoritmos de aprendizaje automático Omita los aspectos académicos. Solo resultados. Mira lo que hay dentro
7;machinelearningmastery.com;https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/;2020-01-07;Tour of Evaluation Metrics for Imbalanced Classification;"Tweet Share Share

Last Updated on January 14, 2020

A classifier is only as good as the metric used to evaluate it.

If you choose the wrong metric to evaluate your models, you are likely to choose a poor model, or in the worst case, be misled about the expected performance of your model.

Choosing an appropriate metric is challenging generally in applied machine learning, but is particularly difficult for imbalanced classification problems. Firstly, because most of the standard metrics that are widely used assume a balanced class distribution, and because typically not all classes, and therefore, not all prediction errors, are equal for imbalanced classification.

In this tutorial, you will discover metrics that you can use for imbalanced classification.

After completing this tutorial, you will know:

About the challenge of choosing metrics for classification, and how it is particularly difficult when there is a skewed class distribution.

How there are three main types of metrics for evaluating classifier models, referred to as rank, threshold, and probability.

How to choose a metric for imbalanced classification if you don’t know where to start.

Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more in my new book, with 30 step-by-step tutorials and full Python source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Challenge of Evaluation Metrics Taxonomy of Classifier Evaluation Metrics How to Choose an Evaluation Metric

Challenge of Evaluation Metrics

An evaluation metric quantifies the performance of a predictive model.

This typically involves training a model on a dataset, using the model to make predictions on a holdout dataset not used during training, then comparing the predictions to the expected values in the holdout dataset.

For classification problems, metrics involve comparing the expected class label to the predicted class label or interpreting the predicted probabilities for the class labels for the problem.

Selecting a model, and even the data preparation methods together are a search problem that is guided by the evaluation metric. Experiments are performed with different models and the outcome of each experiment is quantified with a metric.

Evaluation measures play a crucial role in both assessing the classification performance and guiding the classifier modeling.

— Classification Of Imbalanced Data: A Review, 2009.

There are standard metrics that are widely used for evaluating classification predictive models, such as classification accuracy or classification error.

Standard metrics work well on most problems, which is why they are widely adopted. But all metrics make assumptions about the problem or about what is important in the problem. Therefore an evaluation metric must be chosen that best captures what you or your project stakeholders believe is important about the model or predictions, which makes choosing model evaluation metrics challenging.

This challenge is made even more difficult when there is a skew in the class distribution. The reason for this is that many of the standard metrics become unreliable or even misleading when classes are imbalanced, or severely imbalanced, such as 1:100 or 1:1000 ratio between a minority and majority class.

In the case of class imbalances, the problem is even more acute because the default, relatively robust procedures used for unskewed data can break down miserably when the data is skewed.

— Page 187, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

For example, reporting classification accuracy for a severely imbalanced classification problem could be dangerously misleading. This is the case if project stakeholders use the results to draw conclusions or plan new projects.

In fact, the use of common metrics in imbalanced domains can lead to sub-optimal classification models and might produce misleading conclusions since these measures are insensitive to skewed domains.

— A Survey of Predictive Modelling under Imbalanced Distributions, 2015.

Importantly, different evaluation metrics are often required when working with imbalanced classification.

Unlike standard evaluation metrics that treat all classes as equally important, imbalanced classification problems typically rate classification errors with the minority class as more important than those with the majority class. As such performance metrics may be needed that focus on the minority class, which is made challenging because it is the minority class where we lack observations required to train an effective model.

The main problem of imbalanced data sets lies on the fact that they are often associated with a user preference bias towards the performance on cases that are poorly represented in the available data sample.

— A Survey of Predictive Modelling under Imbalanced Distributions, 2015.

Now that we are familiar with the challenge of choosing a model evaluation metric, let’s look at some examples of different metrics from which we might choose.

Want to Get Started With Imbalance Classification? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Taxonomy of Classifier Evaluation Metrics

There are tens of metrics to choose from when evaluating classifier models, and perhaps hundreds, if you consider all of the pet versions of metrics proposed by academics.

In order to get a handle on the metrics that you could choose from, we will use a taxonomy proposed by Cesar Ferri, et al. in their 2008 paper titled “An Experimental Comparison Of Performance Measures For Classification.” It was also adopted in the 2013 book titled “Imbalanced Learning” and I think proves useful.

We can divide evaluation metrics into three useful groups; they are:

Threshold Metrics Ranking Metrics Probability Metrics.

This division is useful because the top metrics used by practitioners for classifiers generally, and specifically imbalanced classification, fit into the taxonomy neatly.

Several machine learning researchers have identified three families of evaluation metrics used in the context of classification. These are the threshold metrics (e.g., accuracy and F-measure), the ranking methods and metrics (e.g., receiver operating characteristics (ROC) analysis and AUC), and the probabilistic metrics (e.g., root-mean-squared error).

— Page 189, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

Let’s take a closer look at each group in turn.

Threshold Metrics for Imbalanced Classification

Threshold metrics are those that quantify the classification prediction errors.

That is, they are designed to summarize the fraction, ratio, or rate of when a predicted class does not match the expected class in a holdout dataset.

Metrics based on a threshold and a qualitative understanding of error […] These measures are used when we want a model to minimise the number of errors.

— An Experimental Comparison Of Performance Measures For Classification, 2008.

Perhaps the most widely used threshold metric is classification accuracy.

Accuracy = Correct Predictions / Total Predictions

And the complement of classification accuracy called classification error.

Error = Incorrect Predictions / Total Predictions

Although widely used, classification accuracy is almost universally inappropriate for imbalanced classification. The reason is, a high accuracy (or low error) is achievable by a no skill model that only predicts the majority class.

For more on the failure of classification accuracy, see the tutorial:

For imbalanced classification problems, the majority class is typically referred to as the negative outcome (e.g. such as “no change” or “negative test result“), and the minority class is typically referred to as the positive outcome (e.g. “change” or “positive test result“).

Majority Class : Negative outcome, class 0.

: Negative outcome, class 0. Minority Class: Positive outcome, class 1.

Most threshold metrics can be best understood by the terms used in a confusion matrix for a binary (two-class) classification problem. This does not mean that the metrics are limited for use on binary classification; it is just an easy way to quickly understand what is being measured.

The confusion matrix provides more insight into not only the performance of a predictive model but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made. In this type of confusion matrix, each cell in the table has a specific and well-understood name, summarized as follows:

| Positive Prediction | Negative Prediction Positive Class | True Positive (TP) | False Negative (FN) Negative Class | False Positive (FP) | True Negative (TN) 1 2 3 | Positive Prediction | Negative Prediction Positive Class | True Positive (TP) | False Negative (FN) Negative Class | False Positive (FP) | True Negative (TN)

There are two groups of metrics that may be useful for imbalanced classification because they focus on one class; they are sensitivity-specificity and precision-recall.

Sensitivity-Specificity Metrics

Sensitivity refers to the true positive rate and summarizes how well the positive class was predicted.

Sensitivity = TruePositive / (TruePositive + FalseNegative)

Specificity is the complement to sensitivity, or the true negative rate, and summarises how well the negative class was predicted.

Specificity = TrueNegative / (FalsePositive + TrueNegative)

For imbalanced classification, the sensitivity might be more interesting than the specificity.

Sensitivity and Specificity can be combined into a single score that balances both concerns, called the geometric mean or G-Mean.

G-Mean = sqrt(Sensitivity * Specificity)

Precision-Recall Metrics

Precision summarizes the fraction of examples assigned the positive class that belong to the pos";Recorrido por las métricas de evaluación para la clasificación desequilibrada;"Tweet Compartir Compartir Última actualización el 14 de enero de 2020 Un clasificador es tan bueno como la métrica utilizada para evaluarlo. Si elige la métrica incorrecta para evaluar sus modelos, es probable que elija un modelo deficiente o, en el peor de los casos, se deje engañar por el rendimiento esperado de su modelo. Elegir una métrica adecuada es un desafío generalmente en el aprendizaje automático aplicado, pero es particularmente difícil para problemas de clasificación desequilibrados. En primer lugar, porque la mayoría de las métricas estándar que se utilizan ampliamente suponen una distribución de clase equilibrada, y porque normalmente no todas las clases, y por lo tanto, no todos los errores de predicción, son iguales para la clasificación desequilibrada. En este tutorial, descubrirá métricas que puede usar para la clasificación desequilibrada. Después de completar este tutorial, sabrá: Sobre el desafío de elegir métricas para la clasificación y cómo es particularmente difícil cuando hay una distribución de clases sesgada. Cómo hay tres tipos principales de métricas para evaluar los modelos de clasificadores, denominados rango, umbral y probabilidad. Cómo elegir una métrica para la clasificación desequilibrada si no sabe por dónde comenzar. Descubra SMOTE, clasificación de una clase, aprendizaje sensible a los costos, movimiento de umbral y mucho más en mi nuevo libro, con 30 tutoriales paso a paso y código fuente completo de Python. Empecemos. Descripción general del tutorial Este tutorial se divide en tres partes; son: Desafío de métricas de evaluación Taxonomía de clasificador Métricas de evaluación Cómo elegir una métrica de evaluación Desafío de métricas de evaluación Una métrica de evaluación cuantifica el desempeño de un modelo predictivo. Esto generalmente implica entrenar un modelo en un conjunto de datos, usar el modelo para hacer predicciones en un conjunto de datos de reserva no utilizado durante el entrenamiento, y luego comparar las predicciones con los valores esperados en el conjunto de datos de reserva. Para problemas de clasificación, las métricas implican comparar la etiqueta de clase esperada con la etiqueta de clase predicha o interpretar las probabilidades predichas para las etiquetas de clase para el problema. Seleccionar un modelo e incluso los métodos de preparación de datos juntos son un problema de búsqueda guiado por la métrica de evaluación. Los experimentos se realizan con diferentes modelos y el resultado de cada experimento se cuantifica con una métrica. Las medidas de evaluación desempeñan un papel crucial tanto en la evaluación del rendimiento de la clasificación como en la orientación del modelado del clasificador. - Clasificación de datos desequilibrados: una revisión, 2009. Existen métricas estándar que se utilizan ampliamente para evaluar los modelos predictivos de clasificación, como la precisión de la clasificación o el error de clasificación. Las métricas estándar funcionan bien en la mayoría de los problemas, por lo que se adoptan ampliamente. Pero todas las métricas hacen suposiciones sobre el problema o sobre lo que es importante en el problema. Por lo tanto, se debe elegir una métrica de evaluación que capture mejor lo que usted o las partes interesadas de su proyecto creen que es importante sobre el modelo o las predicciones, lo que dificulta la elección de las métricas de evaluación del modelo. Este desafío se hace aún más difícil cuando hay una distorsión en la distribución de la clase. La razón de esto es que muchas de las métricas estándar se vuelven poco confiables o incluso engañosas cuando las clases están desequilibradas o muy desequilibradas, como la proporción 1: 100 o 1: 1000 entre una clase minoritaria y mayoritaria. En el caso de los desequilibrios de clase, el problema es aún más grave porque los procedimientos predeterminados y relativamente robustos utilizados para los datos sin sesgar pueden descomponerse miserablemente cuando los datos están sesgados. - Página 187, Aprendizaje desequilibrado: Fundamentos, algoritmos y aplicaciones, 2013. Por ejemplo, informar la precisión de la clasificación para un problema de clasificación gravemente desequilibrado podría ser peligrosamente engañoso. Este es el caso si las partes interesadas del proyecto usan los resultados para sacar conclusiones o planificar nuevos proyectos. De hecho, el uso de métricas comunes en dominios desequilibrados puede conducir a modelos de clasificación subóptimos y puede producir conclusiones engañosas, ya que estas medidas son insensibles a los dominios sesgados. - Una encuesta de modelado predictivo bajo distribuciones desequilibradas, 2015. Es importante destacar que a menudo se requieren diferentes métricas de evaluación cuando se trabaja con una clasificación desequilibrada. A diferencia de las métricas de evaluación estándar que tratan a todas las clases como igualmente importantes, los problemas de clasificación desequilibrada generalmente califican los errores de clasificación con la clase minoritaria como más importantes que aquellos con la clase mayoritaria. Como tal, pueden ser necesarias métricas de rendimiento que se centren en la clase minoritaria, lo que se convierte en un desafío porque es la clase minoritaria donde carecemos de las observaciones necesarias para entrenar un modelo efectivo. El principal problema de los conjuntos de datos desequilibrados radica en el hecho de que a menudo están asociados con un sesgo de preferencia del usuario hacia el rendimiento en casos que están mal representados en la muestra de datos disponibles. - Una encuesta de modelado predictivo bajo distribuciones desequilibradas, 2015. Ahora que estamos familiarizados con el desafío de elegir una métrica de evaluación del modelo, veamos algunos ejemplos de diferentes métricas entre las que podríamos elegir. ¿Quiere comenzar con la clasificación de desequilibrio? Tome mi curso gratuito de 7 días por correo electrónico ahora (con código de muestra). Haga clic para registrarse y también obtenga una versión gratuita en PDF del libro. Descargue su taxonomía GRATUITA de mini-curso de métricas de evaluación de clasificadores Hay decenas de métricas para elegir al evaluar modelos de clasificadores, y tal vez cientos, si considera todas las versiones favoritas de métricas propuestas por académicos. Para poder manejar las métricas entre las que puede elegir, utilizaremos una taxonomía propuesta por Cesar Ferri, et al. en su artículo de 2008 titulado &quot;Una comparación experimental de medidas de rendimiento para la clasificación&quot;. También se adoptó en el libro de 2013 titulado &quot;Aprendizaje desequilibrado&quot; y creo que resulta útil. Podemos dividir las métricas de evaluación en tres grupos útiles; son: Métricas de umbral Métricas de clasificación Métricas de probabilidad. Esta división es útil porque las principales métricas utilizadas por los profesionales para los clasificadores en general, y la clasificación específicamente desequilibrada, encajan perfectamente en la taxonomía. Varios investigadores de aprendizaje automático han identificado tres familias de métricas de evaluación utilizadas en el contexto de la clasificación. Estas son las métricas de umbral (p. Ej., Precisión y medida F), los métodos de clasificación y las métricas (p. Ej., Análisis de características operativas del receptor (ROC) y AUC) y las métricas probabilísticas (p. Ej., Error cuadrático medio). - Página 189, Aprendizaje desequilibrado: Fundamentos, algoritmos y aplicaciones, 2013. Echemos un vistazo más de cerca a cada grupo. Métricas de umbral para clasificación desequilibrada Las métricas de umbral son aquellas que cuantifican los errores de predicción de clasificación. Es decir, están diseñados para resumir la fracción, relación o tasa de cuando una clase predicha no coincide con la clase esperada en un conjunto de datos de reserva. Métricas basadas en un umbral y una comprensión cualitativa del error [...] Estas medidas se utilizan cuando queremos un modelo para minimizar el número de errores. - Una comparación experimental de medidas de rendimiento para la clasificación, 2008. Quizás la métrica umbral más utilizada es la precisión de la clasificación. Precisión = Predicciones correctas / Predicciones totales Y el complemento de precisión de clasificación llamado error de clasificación. Error = Predicciones incorrectas / Predicciones totales Aunque se usa ampliamente, la precisión de la clasificación es casi universalmente inapropiada para la clasificación desequilibrada. La razón es que se puede lograr una alta precisión (o bajo error) mediante un modelo sin habilidad que solo predice la clase mayoritaria. Para obtener más información sobre el fracaso de la precisión de la clasificación, consulte el tutorial: Para problemas de clasificación desequilibrados, la clase mayoritaria generalmente se conoce como el resultado negativo (por ejemplo, &quot;sin cambio&quot; o &quot;resultado negativo de la prueba&quot;), y la clase minoritaria es típicamente referido como el resultado positivo (por ejemplo, &quot;cambio&quot; o &quot;resultado positivo de la prueba&quot;). Clase mayoritaria: resultado negativo, clase 0.: resultado negativo, clase 0. Clase minoritaria: resultado positivo, clase 1. La mayoría de las métricas de umbral pueden entenderse mejor por los términos utilizados en una matriz de confusión para un problema de clasificación binario (dos clases) . Esto no significa que las métricas estén limitadas para su uso en la clasificación binaria; Es solo una manera fácil de entender rápidamente lo que se está midiendo. La matriz de confusión proporciona más información no solo sobre el rendimiento de un modelo predictivo sino también sobre qué clases se predicen correctamente, cuáles incorrectamente y qué tipo de errores se están cometiendo. En este tipo de matriz de confusión, cada celda de la tabla tiene un nombre específico y bien entendido, resumido de la siguiente manera: | Predicción Positiva | Predicción negativa Clase positiva | Verdadero Positivo (TP) | Falso negativo (FN) Clase negativa | Falso positivo (FP) | Verdadero negativo (TN) 1 2 3 | Predicción Positiva | Predicción negativa Clase positiva | Verdadero Positivo (TP) | Falso negativo (FN) Clase negativa | Falso positivo (FP) | Verdadero negativo (TN) Hay dos grupos de métricas que pueden ser útiles para la clasificación desequilibrada porque se centran en una clase; son sensibilidad-especificidad y precisión-recuerdo. Métricas de sensibilidad-especificidad La sensibilidad se refiere a la tasa positiva verdadera y resume qué tan bien se predijo la clase positiva. Sensibilidad = TruePositive / (TruePositive + FalseNegative) La especificidad es el complemento de la sensibilidad, o la tasa negativa verdadera, y resume qué tan bien se predijo la clase negativa. Especificidad = TrueNegative / (FalsePositive + TrueNegative) Para una clasificación desequilibrada, la sensibilidad puede ser más interesante que la especificidad. La sensibilidad y la especificidad se pueden combinar en una sola puntuación que equilibra ambas preocupaciones, llamada media geométrica o media G. G-Mean = sqrt (Sensibilidad * Especificidad) Métricas de recuperación de precisión La precisión resume la fracción de ejemplos asignados a la clase positiva que pertenecen a la posición"
8;news.mit.edu;http://news.mit.edu/2020/how-to-stage-revolution-mit-history-class-0107;;How to stage a revolution;"Revolutions are monumental social upheavals that can remake whole nations, dismantling — often violently — old paradigms. But the stories of the epic struggles that leave their mark on the world’s history are frequently fragile, precarious, and idiosyncratic in their details, leaving some key questions only partially understood: Why and how do peoples overthrow their governments? Why do some revolutions succeed and others fail?

These are not simple questions, and, for 12 years, MIT students and faculty have set out to answer them in a survey course that spans centuries and continents.

Course 21H.001 (How to Stage a Revolution, or Revolutions for short) is an MIT history class that examines the roots, drivers, and complexities of how governments fall. Co-taught this past fall by three historians — History Section head Professor Jeffrey Ravel, Associate Professor Tanalís Padilla, and Lecturer Pouya Alimagham — the semester is divided into three parts, with each instructor covering, respectively, the French Revolution, the Mexican Revolution, and the Iranian Revolution.

During a mixture of lectures and breakout discussion sessions, students explore the causes, tactics, goals, and significant factors of each revolution, drawing insights from music, film, art, constitutions, declarations, and the writings of revolutionaries themselves.

A wide-angle approach

The topics covered this year span centuries, from the near-mythic French Revolution (1789–99) to the Mexican Revolution (1910-20) to events that have emerged in the students’ own lifetimes, such as the Arab Spring (2010-12). Alimagham brought the semester to a close with a focus on the Iranian Revolution; having students begin their exploration with the roots of American intervention in Iran the latter half of the 20th century, and tracking developments through to today’s western media narrative of the Sunni/Shia conflict.

“Revolutions are a surprisingly good way to learn about a culture,” says Quinn Bowers, a first-year student who took the opportunity to deepen his understanding of history as a parallel to his intended double major in mechanical engineering and aerospace engineering. “Revolutions draw attention to the values the culture holds. This class did a lot to dispel assumptions I didn’t even know I had.”

For another first-year student, Somaia Saba, the offering leapt out at her as she browsed the course catalog to plan her first semester at MIT. With an intended major in computation and cognition (Course 6.9), she was drawn to the class by a fascination with major political transformations, “especially because of the tense political climate in which we are currently living.”

The freedom and exploration in essay-writing was a transformative experience for Saba; essay prompts and writing assignments had never been her favorite aspect of the classroom. But, snagged by a brief mention in class about women’s roles during the Mexican Revolution, she found herself writing extensively on the subject, drawing on her personal attentiveness to women’s issues and roles in history.

“I did not realize the extent to which these issues mattered to me until [seeing the professor’s] comments on my essay.” She also notes that the class has given her ways of thinking and analyzing that allow her to be more engaged with current political events.

Ever-evolving

How to Stage a Revolution is also a chameleon course in that its subject matter fluxes from year to year depending on the expertise of the faculty instructors — a plan that allows a venerable course to cover any number of revolutionary histories. Two years ago, for instance, when Alimagham first taught the course, working alongside MIT historians Caley Horan and Malick Ghachem, the class consisted of modules on the Haitian Revolution, the American Civil War (as America’s second revolution), and the Iranian Revolution.

Not only is the course constantly transforming, Alimagham notes, but its three co-instructors are always adapting as well. “When you’re involved in a team-taught course that includes material in which you are not the primary expert, you evolve as an instructor. It keeps you on your toes.”

Ravel agrees: “One benefit of co-teaching is that we learn from each other. It’s a great conversation among the three of us.”

Ravel currently serves as the head of the MIT History Section, as president of the American Society for Eighteenth-Century Studies, and as a co-director for the Comédie-Française Registers Project, which is producing a collaborative, extensive history of one of France’s iconic theater groups. “Co-teaching reminds me of what it’s like to be a student again,” reflects Padilla. “It makes me more sensitive to how students are taking in information that, for me, is now second nature.”

Padilla is a historian of Latin America and a contributor to numerous publications and volumes surrounding the Mexican Revolution. Her current book project centers on how rural schoolteachers “went from being agents of state consolidation to activists against a government that increasingly abandoned its commitment to social justice.”

The technological contexts of revolutions

Like a number of other humanistic courses at MIT, How to Stage a Revolution is also a hands-on “maker class.” In addition to classroom lectures and discussion sessions, students produce posters on MIT’s Beaver Press, a student-built replica of the wooden, handset printing presses on which the great documents of the Renaissance, the Reformation, and the Scientific Revolution were printed.

Carving linoleum printing plates and inking them by hand, students use their academic understanding of various revolutions to design and produce colorful pro- and counter-revolutionary posters. In one print, the evocative image of a Mexican worker raises the Olympic rings between his hands like chains. In another, the guillotine stands ready with its victims nearby, indicating a mounting death toll, each head labeled respectively with Liberté, Egalité, and Fraternité.

Historic revolutionary narratives have a particular urgency in an MIT classroom: From the dissemination of revolutionary messages via an 18th century printing press to changing fuel technologies to the global social media that shaped the Arab Spring, the technological contexts of revolutions are intrinsic to understanding them.

“Whatever we end up doing in our post-MIT lives and careers will be in the context of complex, real-world problems,” says Bowers. “This class sheds light on some of the world’s most volatile problems.”

Story by MIT SHASS Communications

Editorial and design director: Emily Hiestand

Writer/reporter: Alison Lanier";Cómo organizar una revolución;"Las revoluciones son agitaciones sociales monumentales que pueden rehacer naciones enteras, desmantelando, a menudo violentamente, viejos paradigmas. Pero las historias de las luchas épicas que dejan su huella en la historia del mundo son con frecuencia frágiles, precarias e idiosincrásicas en sus detalles, dejando algunas preguntas clave que solo se entienden parcialmente: ¿por qué y cómo derrocan los pueblos a sus gobiernos? ¿Por qué algunas revoluciones tienen éxito y otras fallan? Estas no son preguntas simples y, durante 12 años, los estudiantes y el profesorado del MIT se han propuesto responderlas en un curso de encuesta que abarca siglos y continentes. El curso 21H.001 (Cómo organizar una revolución, o revoluciones para abreviar) es una clase de historia del MIT que examina las raíces, los factores y las complejidades de cómo caen los gobiernos. Este otoño pasado fue co-enseñado por tres historiadores: el profesor Jeffrey Ravel, jefe de la Sección de Historia, el profesor asociado Tanalís Padilla y el profesor Pouya Alimagham: el semestre se divide en tres partes, con cada instructor cubriendo, respectivamente, la Revolución Francesa, la Revolución Mexicana, y la revolución iraní. Durante una combinación de conferencias y sesiones de discusión, los estudiantes exploran las causas, tácticas, objetivos y factores significativos de cada revolución, extrayendo ideas de la música, el cine, el arte, las constituciones, las declaraciones y los escritos de los mismos revolucionarios. Un enfoque de gran angular Los temas cubiertos este año abarcan siglos, desde la casi mítica Revolución Francesa (1789-1799) hasta la Revolución Mexicana (1910-20) hasta los eventos que han surgido en la vida de los estudiantes, como el árabe Primavera (2010-12). Alimagham cerró el semestre con un enfoque en la revolución iraní; que los estudiantes comiencen su exploración con las raíces de la intervención estadounidense en Irán en la segunda mitad del siglo XX, y que sigan los desarrollos hasta la narrativa mediática actual del conflicto sunita / chiíta. &quot;Las revoluciones son una forma sorprendentemente buena de aprender sobre una cultura&quot;, dice Quinn Bowers, una estudiante de primer año que aprovechó la oportunidad para profundizar su comprensión de la historia como un paralelo a su doble especialidad en ingeniería mecánica e ingeniería aeroespacial. “Las revoluciones llaman la atención sobre los valores que posee la cultura. Esta clase hizo mucho para disipar supuestos que ni siquiera sabía que tenía ”. Para otra estudiante de primer año, Somaia Saba, la oferta saltó hacia ella mientras hojeaba el catálogo de cursos para planificar su primer semestre en el MIT. Con una especialización en computación y cognición (Curso 6.9), se sintió atraída a la clase por una fascinación por las grandes transformaciones políticas, &quot;especialmente debido al clima político tenso en el que vivimos actualmente&quot;. La libertad y la exploración en la redacción de ensayos fue una experiencia transformadora para Saba; Las indicaciones de ensayo y las tareas de escritura nunca habían sido su aspecto favorito del aula. Pero, enganchada por una breve mención en clase sobre los roles de las mujeres durante la Revolución Mexicana, se encontró escribiendo extensamente sobre el tema, aprovechando su atención personal a los problemas y roles de las mujeres en la historia. &quot;No me di cuenta de hasta qué punto me importaban estos problemas hasta que [al ver al profesor] comentaba mi ensayo&quot;. También señala que la clase le ha dado formas de pensar y analizar que le permiten estar más comprometida con los acontecimientos políticos actuales. La evolución constante de Cómo organizar una revolución también es un curso de camaleón, ya que su tema cambia de año en año dependiendo de la experiencia de los instructores de la facultad, un plan que permite un curso venerable para cubrir cualquier número de historias revolucionarias. Hace dos años, por ejemplo, cuando Alimagham enseñó el curso por primera vez, trabajando junto a los historiadores del MIT Caley Horan y Malick Ghachem, la clase consistía en módulos sobre la Revolución Haitiana, la Guerra Civil Estadounidense (como la segunda revolución de Estados Unidos) y la Revolución iraní. Alimagham señala que el curso no solo se está transformando constantemente, sino que sus tres coinstructores siempre se están adaptando también. “Cuando participas en un curso enseñado en equipo que incluye material en el que no eres el experto principal, evolucionas como instructor. Te mantiene alerta. Ravel está de acuerdo: “Un beneficio de la enseñanza conjunta es que aprendemos unos de otros. Es una gran conversación entre los tres &quot;. Ravel actualmente se desempeña como jefe de la Sección de Historia del MIT, como presidente de la Sociedad Americana de Estudios del Siglo XVIII y como codirector del Proyecto de Registros de la Comédie-Française, que está produciendo una extensa y colaborativa historia de uno de los estudios de Francia. grupos de teatro icónicos. &quot;La enseñanza conjunta me recuerda lo que es ser estudiante nuevamente&quot;, reflexiona Padilla. &quot;Me hace más sensible a la forma en que los estudiantes toman la información que, para mí, ahora es una segunda naturaleza&quot;. Padilla es historiador de América Latina y colaborador de numerosas publicaciones y volúmenes en torno a la Revolución Mexicana. Su proyecto de libro actual se centra en cómo los maestros de escuela rural &quot;pasaron de ser agentes de consolidación estatal a activistas contra un gobierno que abandonó cada vez más su compromiso con la justicia social&quot;. Los contextos tecnológicos de las revoluciones Al igual que otros cursos humanísticos en el MIT, Cómo organizar una revolución también es una práctica &quot;clase de creadores&quot;. Además de las conferencias en el aula y las sesiones de discusión, los estudiantes producen carteles en Beaver Press del MIT, una réplica construida por los estudiantes de las prensas de impresión de madera y teléfonos en las que se imprimieron los grandes documentos del Renacimiento, la Reforma y la Revolución Científica. Tallando placas de impresión de linóleo y entintando a mano, los estudiantes usan su conocimiento académico de varias revoluciones para diseñar y producir coloridos carteles pro y contrarrevolucionarios. En una impresión, la imagen evocadora de un trabajador mexicano levanta los anillos olímpicos entre sus manos como cadenas. En otra, la guillotina está lista con sus víctimas cercanas, lo que indica un número creciente de muertes, cada cabeza etiquetada respectivamente con Liberté, Egalité y Fraternité. Las narrativas revolucionarias históricas tienen una urgencia particular en un aula del MIT: desde la difusión de mensajes revolucionarios a través de una imprenta del siglo XVIII hasta el cambio de tecnologías de combustible a las redes sociales globales que dieron forma a la Primavera Árabe, los contextos tecnológicos de las revoluciones son intrínsecos para comprenderlos. &quot;Lo que terminemos haciendo en nuestras vidas y carreras posteriores al MIT será en el contexto de problemas complejos del mundo real&quot;, dice Bowers. &quot;Esta clase arroja luz sobre algunos de los problemas más volátiles del mundo&quot;. Historia de MIT SHASS Communications Editorial y directora de diseño: Emily Hiestand Escritora / reportera: Alison Lanier"
9;machinelearningmastery.com;http://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/;2016-09-01;Avoid Overfitting By Early Stopping With XGBoost In Python;"# plot learning curve

from numpy import loadtxt

from xgboost import XGBClassifier

from sklearn . model_selection import train_test_split

from sklearn . metrics import accuracy_score

from matplotlib import pyplot

# load data

dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" )

# split data into X and y

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# split data into train and test sets

X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.33 , random_state = 7 )

# fit model no training data

model = XGBClassifier ( )

eval_set = [ ( X_train , y_train ) , ( X_test , y_test ) ]

model . fit ( X_train , y_train , eval_metric = [ ""error"" , ""logloss"" ] , eval_set = eval_set , verbose = True )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )

print ( ""Accuracy: %.2f%%"" % ( accuracy * 100.0 ) )

# retrieve performance metrics

results = model . evals_result ( )

epochs = len ( results [ 'validation_0' ] [ 'error' ] )

x_axis = range ( 0 , epochs )

# plot log loss

fig , ax = pyplot . subplots ( )

ax . plot ( x_axis , results [ 'validation_0' ] [ 'logloss' ] , label = 'Train' )

ax . plot ( x_axis , results [ 'validation_1' ] [ 'logloss' ] , label = 'Test' )

ax . legend ( )

pyplot . ylabel ( 'Log Loss' )

pyplot . title ( 'XGBoost Log Loss' )

pyplot . show ( )

# plot classification error

fig , ax = pyplot . subplots ( )

ax . plot ( x_axis , results [ 'validation_0' ] [ 'error' ] , label = 'Train' )

ax . plot ( x_axis , results [ 'validation_1' ] [ 'error' ] , label = 'Test' )

ax . legend ( )

pyplot . ylabel ( 'Classification Error' )

pyplot . title ( 'XGBoost Classification Error' )";Evite el sobreajuste deteniéndose temprano con XGBoost en Python;"# trazar la curva de aprendizaje de numpy import loadtxt de xgboost import XGBClassifier de sklearn. model_selection import train_test_split de sklearn. métrica importar precision_score de matplotlib import pyplot # cargar datos conjunto de datos = loadtxt (&#39;pima-indians-diabetes.csv&#39;, delimiter = &quot;,&quot;) # dividir datos en X e y X = conjunto de datos [:, 0: 8] Y = conjunto de datos [:, 8] # dividir datos en conjuntos de entrenamiento y prueba X_train, X_test, y_train, y_test = train_test_split (X, Y, test_size = 0.33, random_state = 7) # modelo de ajuste sin modelo de datos de entrenamiento = XGBClassifier () eval_set = [( X_train, y_train), (X_test, y_test)] modelo. fit (X_train, y_train, eval_metric = [&quot;error&quot;, &quot;logloss&quot;], eval_set = eval_set, verbose = True) # realiza predicciones para los datos de prueba y_pred = modelo. predecir (X_test) predicciones = [round (valor) para el valor en y_pred] # evaluar predicciones exactitud = precision_score (y_test, predicciones) print (&quot;Precisión:% .2f %%&quot;% (exactitud * 100.0)) # recuperar resultados de métricas de rendimiento = modelo. evals_result () epochs = len (resultados [&#39;validation_0&#39;] [&#39;error&#39;]) x_axis = range (0, epochs) # plot log loss fig, ax = pyplot. subtramas () ax. plot (x_axis, resultados [&#39;validation_0&#39;] [&#39;logloss&#39;], label = &#39;Train&#39;) ax. plot (x_axis, resultados [&#39;validation_1&#39;] [&#39;logloss&#39;], label = &#39;Test&#39;) ax. leyenda () pyplot. Pylabot ylabel (&#39;pérdida de registro&#39;). título (&#39;XGBoost Log Loss&#39;) pyplot. show () # error de clasificación de la trama fig, ax = pyplot. subtramas () ax. plot (x_axis, resultados [&#39;validation_0&#39;] [&#39;error&#39;], label = &#39;Train&#39;) ax. plot (x_axis, resultados [&#39;validation_1&#39;] [&#39;error&#39;], label = &#39;Test&#39;) ax. leyenda () pyplot. ylabel (&#39;Error de clasificación&#39;) pyplot. title (&#39;Error de clasificación XGBoost&#39;)"
10;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/;2019-06-27;How to Develop a GAN for Generating MNIST Handwritten Digits;"# example of training a gan on mnist

from numpy import expand_dims

from numpy import zeros

from numpy import ones

from numpy import vstack

from numpy . random import randn

from numpy . random import randint

from keras . datasets . mnist import load_data

from keras . optimizers import Adam

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Reshape

from keras . layers import Flatten

from keras . layers import Conv2D

from keras . layers import Conv2DTranspose

from keras . layers import LeakyReLU

from keras . layers import Dropout

from matplotlib import pyplot

# define the standalone discriminator model

def define_discriminator ( in_shape = ( 28 , 28 , 1 ) ) :

model = Sequential ( )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = in_shape ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

return model

# define the standalone generator model

def define_generator ( latent_dim ) :

model = Sequential ( )

# foundation for 7x7 image

n_nodes = 128 * 7 * 7

model . add ( Dense ( n_nodes , input_dim = latent_dim ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Reshape ( ( 7 , 7 , 128 ) ) )

# upsample to 14x14

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# upsample to 28x28

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Conv2D ( 1 , ( 7 , 7 ) , activation = 'sigmoid' , padding = 'same' ) )

return model

# define the combined generator and discriminator model, for updating the generator

def define_gan ( g_model , d_model ) :

# make weights in the discriminator not trainable

d_model . trainable = False

# connect them

model = Sequential ( )

# add generator

model . add ( g_model )

# add the discriminator

model . add ( d_model )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt )

return model

# load and prepare mnist training images

def load_real_samples ( ) :

# load mnist dataset

( trainX , _ ) , ( _ , _ ) = load_data ( )

# expand to 3d, e.g. add channels dimension

X = expand_dims ( trainX , axis = - 1 )

# convert from unsigned ints to floats

X = X . astype ( 'float32' )

# scale from [0,255] to [0,1]

X = X / 255.0

return X

# select real samples

def generate_real_samples ( dataset , n_samples ) :

# choose random instances

ix = randint ( 0 , dataset . shape [ 0 ] , n_samples )

# retrieve selected images

X = dataset [ ix ]

# generate 'real' class labels (1)

y = ones ( ( n_samples , 1 ) )

return X , y

# generate points in latent space as input for the generator

def generate_latent_points ( latent_dim , n_samples ) :

# generate points in the latent space

x_input = randn ( latent_dim * n_samples )

# reshape into a batch of inputs for the network

x_input = x_input . reshape ( n_samples , latent_dim )

return x_input

# use the generator to generate n fake examples, with class labels

def generate_fake_samples ( g_model , latent_dim , n_samples ) :

# generate points in latent space

x_input = generate_latent_points ( latent_dim , n_samples )

# predict outputs

X = g_model . predict ( x_input )

# create 'fake' class labels (0)

y = zeros ( ( n_samples , 1 ) )

return X , y

# create and save a plot of generated images (reversed grayscale)

def save_plot ( examples , epoch , n = 10 ) :

# plot images

for i in range ( n * n ) :

# define subplot

pyplot . subplot ( n , n , 1 + i )

# turn off axis

pyplot . axis ( 'off' )

# plot raw pixel data

pyplot . imshow ( examples [ i , : , : , 0 ] , cmap = 'gray_r' )

# save plot to file

filename = 'generated_plot_e%03d.png' % ( epoch + 1 )

pyplot . savefig ( filename )

pyplot . close ( )

# evaluate the discriminator, plot generated images, save generator model

def summarize_performance ( epoch , g_model , d_model , dataset , latent_dim , n_samples = 100 ) :

# prepare real samples

X_real , y_real = generate_real_samples ( dataset , n_samples )

# evaluate discriminator on real examples

_ , acc_real = d_model . evaluate ( X_real , y_real , verbose = 0 )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( g_model , latent_dim , n_samples )

# evaluate discriminator on fake examples

_ , acc_fake = d_model . evaluate ( x_fake , y_fake , verbose = 0 )

# summarize discriminator performance

print ( '>Accuracy real: %.0f%%, fake: %.0f%%' % ( acc_real* 100 , acc_fake* 100 ) )

# save plot

save_plot ( x_fake , epoch )

# save the generator model tile file

filename = 'generator_model_%03d.h5' % ( epoch + 1 )

g_model . save ( filename )

# train the generator and discriminator

def train ( g_model , d_model , gan_model , dataset , latent_dim , n_epochs = 100 , n_batch = 256 ) :

bat_per_epo = int ( dataset . shape [ 0 ] / n_batch )

half_batch = int ( n_batch / 2 )

# manually enumerate epochs

for i in range ( n_epochs ) :

# enumerate batches over the training set

for j in range ( bat_per_epo ) :

# get randomly selected 'real' samples

X_real , y_real = generate_real_samples ( dataset , half_batch )

# generate 'fake' examples

X_fake , y_fake = generate_fake_samples ( g_model , latent_dim , half_batch )

# create training set for the discriminator

X , y = vstack ( ( X_real , X_fake ) ) , vstack ( ( y_real , y_fake ) )

# update discriminator model weights

d_loss , _ = d_model . train_on_batch ( X , y )

# prepare points in latent space as input for the generator

X_gan = generate_latent_points ( latent_dim , n_batch )

# create inverted labels for the fake samples

y_gan = ones ( ( n_batch , 1 ) )

# update the generator via the discriminator's error

g_loss = gan_model . train_on_batch ( X_gan , y_gan )

# summarize loss on this batch

print ( '>%d, %d/%d, d=%.3f, g=%.3f' % ( i + 1 , j + 1 , bat_per_epo , d_loss , g_loss ) )

# evaluate the model performance, sometimes

if ( i + 1 ) % 10 == 0 :

summarize_performance ( i , g_model , d_model , dataset , latent_dim )

# size of the latent space

latent_dim = 100

# create the discriminator

d_model = define_discriminator ( )

# create the generator

g_model = define_generator ( latent_dim )

# create the gan

gan_model = define_gan ( g_model , d_model )

# load image data

dataset = load_real_samples ( )

# train model";Cómo desarrollar una GAN para generar dígitos manuscritos MNIST;"# ejemplo de entrenamiento de un gan en mnist de numpy import expand_dims de numpy import ceros de numpy import unos de numpy import vstack de numpy. importación aleatoria randn de numpy. Randint de importación aleatoria de keras. conjuntos de datos mnist import load_data de keras. Los optimizadores importan a Adam de Keras. Los modelos importan secuenciales de keras. Las capas importan densas de keras. las capas importan Reformar desde keras. las capas importan Flatten de keras. las capas importan Conv2D desde keras. las capas importan Conv2DTranspose desde keras. las capas importan LeakyReLU desde keras. importación de capas Deserción desde matplotlib import pyplot # define el modelo discriminador independiente def define_discriminator (in_shape = (28, 28, 1)): modelo = modelo secuencial (). modelo add (Conv2D (64, (3, 3), strides = (2, 2), padding = &#39;same&#39;, input_shape = in_shape)). modelo add (LeakyReLU (alpha = 0.2)). agregar modelo (Dropout (0.4)). modelo add (Conv2D (64, (3, 3), strides = (2, 2), padding = &#39;same&#39;)). modelo add (LeakyReLU (alpha = 0.2)). agregar modelo (Dropout (0.4)). agregar modelo (Flatten ()). add (Dense (1, activación = &#39;sigmoide&#39;)) # compilar modelo opt = Adam (lr = 0.0002, beta_1 = 0.5) modelo. compilar (pérdida = &#39;binary_crossentropy&#39;, optimizer = opt, metrics = [&#39;precision&#39;]) return model # define el modelo de generador independiente def define_generator (latent_dim): model = Sequential () # fundación para la imagen 7x7 n_nodes = 128 * 7 * 7 modelo. modelo add (Dense (n_nodes, input_dim = latent_dim)). modelo add (LeakyReLU (alpha = 0.2)). add (Reshape ((7, 7, 128))) # upsample al modelo 14x14. modelo add (Conv2DTranspose (128, (4, 4), strides = (2, 2), padding = &#39;same&#39;)). add (LeakyReLU (alpha = 0.2)) # upsample al modelo 28x28. modelo add (Conv2DTranspose (128, (4, 4), strides = (2, 2), padding = &#39;same&#39;)). modelo add (LeakyReLU (alpha = 0.2)). add (Conv2D (1, (7, 7), activación = &#39;sigmoide&#39;, relleno = &#39;mismo&#39;)) devuelve el modelo # define el generador combinado y el modelo discriminador, para actualizar el generador def define_gan (g_model, d_model): # make pesos en el discriminador no entrenable d_model. entrenable = Falso # conectarlos modelo = Secuencial () # agregar modelo generador. add (g_model) # agrega el modelo discriminador. add (d_model) # compilar modelo opt = Adam (lr = 0.0002, beta_1 = 0.5) modelo. compile (loss = &#39;binary_crossentropy&#39;, optimizer = opt) devuelva el modelo # load y prepare imágenes de entrenamiento mnist def load_real_samples (): # load mnist dataset (trainX, _), (_, _) = load_data () # expand a 3d, por ejemplo, agregar canales dimensión X = expand_dims (trainX, axis = - 1) # convertir de ints sin signo a flotantes X = X. astype (&#39;float32&#39;) # escala de [0,255] a [0,1] X = X / 255.0 return X # seleccionar muestras reales def generate_real_samples (conjunto de datos, n_samples): # elegir instancias aleatorias ix = randint (0, conjunto de datos. forma [0], n_samples) # recupera las imágenes seleccionadas X = conjunto de datos [ix] # genera etiquetas de clase &#39;reales&#39; (1) y = ones ((n_samples, 1)) devuelve X, y # genera puntos en el espacio latente como entrada para el generator def generate_latent_points (latent_dim, n_samples): # genera puntos en el espacio latente x_input = randn (latent_dim * n_samples) # se transforma en un lote de entradas para la red x_input = x_input. reshape (n_samples, latent_dim) return x_input # usa el generador para generar n ejemplos falsos, con etiquetas de clase def generate_fake_samples (g_model, latent_dim, n_samples): # genera puntos en el espacio latente x_input = generate_latent_points (latent_dim, n_samples) # predice salidas X = g_model. predic (x_input) # crea etiquetas de clase &#39;falsas&#39; (0) y = ceros ((n_samples, 1)) devuelve X, y # crea y guarda un gráfico de imágenes generadas (escala de grises invertida) def save_plot (ejemplos, época, n = 10): # trama imágenes para i en rango (n * n): # define subplot pyplot. subplot (n, n, 1 + i) # apaga el pyplot del eje. axis (&#39;off&#39;) # plot pyplot de datos de píxeles sin procesar. imshow (ejemplos [i,:,:, 0], cmap = &#39;gray_r&#39;) # guardar el diagrama en el archivo filename = &#39;generate_plot_e% 03d.png&#39;% (epoch + 1) pyplot. savefig (nombre de archivo) pyplot. close () # evalúa el discriminador, traza las imágenes generadas, guarda el modelo del generador def summaryize_performance (epoch, g_model, d_model, dataset, latent_dim, n_samples = 100): # prepara muestras reales X_real, y_real = generate_real_samples (conjunto de datos, n_samples) # evalúa discriminador en ejemplos reales _, acc_real = d_model. evalúe (X_real, y_real, verbose = 0) # prepare ejemplos falsos x_fake, y_fake = generate_fake_samples (g_model, latent_dim, n_samples) # evalúe discriminador en ejemplos falsos _, acc_fake = d_model. evaluar (x_fake, y_fake, verbose = 0) # resumir el rendimiento del discriminador print (&#39;&gt; Precisión real:% .0f %%, fake:% .0f %%&#39;% (acc_real * 100, acc_fake * 100)) # save plot save_plot (x_fake, epoch) # guardar el archivo de mosaico del modelo del generador filename = &#39;generator_model_% 03d.h5&#39;% (epoch + 1) g_model. guardar (nombre de archivo) # entrenar el generador y discriminador def entrenar (g_model, d_model, gan_model, dataset, latent_dim, n_epochs = 100, n_batch = 256): bat_per_epo = int (dataset. shape [0] / n_batch) half_batch = int (n_batch / 2) # enumerar manualmente épocas para i en rango (n_epochs): # enumerar lotes sobre el conjunto de entrenamiento para j en rango (bat_per_epo): # obtener muestras &#39;reales&#39; seleccionadas al azar X_real, y_real = generate_real_samples (conjunto de datos, half_batch) # generar ejemplos &#39;falsos&#39; X_fake, y_fake = generate_fake_samples (g_model, latent_dim, half_batch) # crear conjunto de entrenamiento para el discriminador X, y = vstack ((X_real, X_fake)), vstack ((y_real, y_fake)) # actualizar discriminador modelo pesos d_loss , _ = d_model. train_on_batch (X, y) # prepara puntos en el espacio latente como entrada para el generador X_gan = generate_latent_points (latent_dim, n_batch) # crea etiquetas invertidas para las muestras falsas y_gan = ones ((n_batch, 1)) # actualiza el generador a través del discriminador error g_loss = gan_model. train_on_batch (X_gan, y_gan) # resume la pérdida en esta impresión por lotes (&#39;&gt;% d,% d /% d, d =%. 3f, g =%. 3f&#39;% (i + 1, j + 1, bat_per_epo, d_loss , g_loss)) # evalúa el rendimiento del modelo, a veces si (i + 1)% 10 == 0: summaryize_performance (i, g_model, d_model, dataset, latent_dim) # tamaño del espacio latente latent_dim = 100 # crea el discriminador d_model = define_discriminator () # crea el generador g_model = define_generator (latent_dim) # crea el gan gan_model = define_gan (g_model, d_model) # carga el dataset de datos de imagen = load_real_samples () # modelo de tren"
11;machinelearningmastery.com;https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/;2017-05-23;A Gentle Introduction to Long Short-Term Memory Networks by the Experts;"Tweet Share Share

Last Updated on February 20, 2020

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.

This is a behavior required in complex problem domains like machine translation, speech recognition, and more.

LSTMs are a complex area of deep learning. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional and sequence-to-sequence relate to the field.

In this post, you will get insight into LSTMs using the words of research scientists that developed the methods and applied them to new and important problems.

There are few that are better at clearly and precisely articulating both the promise of LSTMs and how they work than the experts that developed them.

We will explore key questions in the field of LSTMs using quotes from the experts, and if you’re interested, you will be able to dive into the original papers from which the quotes were taken.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

The Promise of Recurrent Neural Networks

Recurrent neural networks are different from traditional feed-forward neural networks.

This difference in the addition of complexity comes with the promise of new behaviors that the traditional methods cannot achieve.

Recurrent networks … have an internal state that can represent context information. … [they] keep information about past inputs for an amount of time that is not fixed a priori, but rather depends on its weights and on the input data. … A recurrent network whose inputs are not fixed but rather constitute an input sequence can be used to transform an input sequence into an output sequence while taking into account contextual information in a flexible way.

— Yoshua Bengio, et al., Learning Long-Term Dependencies with Gradient Descent is Difficult, 1994.

The paper defines 3 basic requirements of a recurrent neural network:

That the system be able to store information for an arbitrary duration.

That the system be resistant to noise (i.e. fluctuations of the inputs that are random or irrelevant to predicting a correct output).

That the system parameters be trainable (in reasonable time).

The paper also describes the “minimal task” for demonstrating recurrent neural networks.

Context is key.

Recurrent neural networks must use context when making predictions, but to this extent, the context required must also be learned.

… recurrent neural networks contain cycles that feed the network activations from a previous time step as inputs to the network to influence predictions at the current time step. These activations are stored in the internal states of the network which can in principle hold long-term temporal contextual information. This mechanism allows RNNs to exploit a dynamically changing contextual window over the input sequence history

— Hassim Sak, et al., Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling, 2014

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

LSTMs Deliver on the Promise

The success of LSTMs is in their claim to be one of the first implements to overcome the technical problems and deliver on the promise of recurrent neural networks.

Hence standard RNNs fail to learn in the presence of time lags greater than 5 – 10 discrete time steps between relevant input events and target signals. The vanishing error problem casts doubt on whether standard RNNs can indeed exhibit significant practical advantages over time window-based feedforward networks. A recent model, “Long Short-Term Memory” (LSTM), is not affected by this problem. LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through “constant error carrousels” (CECs) within special units, called cells

— Felix A. Gers, et al., Learning to Forget: Continual Prediction with LSTM, 2000

The two technical problems overcome by LSTMs are vanishing gradients and exploding gradients, both related to how the network is trained.

Unfortunately, the range of contextual information that standard RNNs can access is in practice quite limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network’s recurrent connections. This shortcoming … referred to in the literature as the vanishing gradient problem … Long Short-Term Memory (LSTM) is an RNN architecture specifically designed to address the vanishing gradient problem.

— Alex Graves, et al., A Novel Connectionist System for Unconstrained Handwriting Recognition, 2009

The key to the LSTM solution to the technical problems was the specific internal structure of the units used in the model.

… governed by its ability to deal with vanishing and exploding gradients, the most common challenge in designing and training RNNs. To address this challenge, a particular form of recurrent nets, called LSTM, was introduced and applied with great success to translation and sequence generation.

— Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.

How do LSTMs Work?

Rather than go into the equations that govern how LSTMs are fit, analogy is a useful tool to quickly get a handle on how they work.

We use networks with one input layer, one hidden layer, and one output layer… The (fully) self-connected hidden layer contains memory cells and corresponding gate units… … Each memory cell’s internal architecture guarantees constant error ow within its constant error carrousel CEC… This represents the basis for bridging very long time lags. Two gate units learn to open and close access to error ow within each memory cell’s CEC. The multiplicative input gate affords protection of the CEC from perturbation by irrelevant inputs. Likewise, the multiplicative output gate protects other units from perturbation by currently irrelevant memory contents.

— Sepp Hochreiter and Jurgen Schmidhuber, Long Short-Term Memory, 1997.

Multiple analogies can help to give purchase on what differentiates LSTMs from traditional neural networks comprised of simple neurons.

The Long Short Term Memory architecture was motivated by an analysis of error flow in existing RNNs which found that long time lags were inaccessible to existing architectures, because backpropagated error either blows up or decays exponentially. An LSTM layer consists of a set of recurrently connected blocks, known as memory blocks. These blocks can be thought of as a differentiable version of the memory chips in a digital computer. Each one contains one or more recurrently connected memory cells and three multiplicative units – the input, output and forget gates – that provide continuous analogues of write, read and reset operations for the cells. … The net can only interact with the cells via the gates.

— Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.

It is interesting to note, that even after more than 20 years, the simple (or vanilla) LSTM may still be the best place to start when applying the technique.

The most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets… Learning rate and network size are the most crucial tunable LSTM hyperparameters … … This implies that the hyperparameters can be tuned independently. In particular, the learning rate can be calibrated first using a fairly small network, thus saving a lot of experimentation time.

— Klaus Greff, et al., LSTM: A Search Space Odyssey, 2015

What are LSTM Applications?

It is important to get a handle on exactly what type of sequence learning problems that LSTMs are suitable to address.

Long Short-Term Memory (LSTM) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). … … LSTM holds promise for any sequential processing task in which we suspect that a hierarchical decomposition may exist, but do not know in advance what this decomposition is.

— Felix A. Gers, et al., Learning to Forget: Continual Prediction with LSTM, 2000

The Recurrent Neural Network (RNN) is neural sequence model that achieves state of the art performance on important tasks that include language modeling, speech recognition, and machine translation.

— Wojciech Zaremba, Recurrent Neural Network Regularization, 2014.

Since LSTMs are effective at capturing long-term temporal dependencies without suffering from the optimization hurdles that plague simple recurrent networks (SRNs), they have been used to advance the state of the art for many difficult problems. This includes handwriting recognition and generation, language modeling and translation, acoustic modeling of speech, speech synthesis, protein secondary structure prediction, analysis of audio, and video data among others.

— Klaus Greff, et al., LSTM: A Search Space Odyssey, 2015

What are Bidirectional LSTMs?

A commonly mentioned improvement upon LSTMs are bidirectional LSTMs.

The basic idea of bidirectional recurrent neural nets is to present each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. … This means that for every point in a given sequence, the BRNN has complete, sequential information about all points before and after it. Also, because the net is free to use as much or as little of this context as necessary, there is no need to find a (task-dependent) time-window or target delay size. … for temporal problem";Una introducción suave a las redes de memoria a corto y largo plazo por los expertos;"Tweet Compartir Compartir Última actualización el 20 de febrero de 2020 Las redes de memoria a corto plazo (LSTM) son un tipo de red neuronal recurrente capaz de aprender la dependencia del orden en los problemas de predicción de secuencias. Este es un comportamiento requerido en dominios de problemas complejos como la traducción automática, el reconocimiento de voz y más. Los LSTM son un área compleja de aprendizaje profundo. Puede ser difícil entender qué son los LSTM y cómo los términos como bidireccional y secuencia a secuencia se relacionan con el campo. En esta publicación, obtendrá información sobre los LSTM utilizando las palabras de científicos de investigación que desarrollaron los métodos y los aplicaron a problemas nuevos e importantes. Hay pocos que son mejores para articular de manera clara y precisa tanto la promesa de los LSTM como su funcionamiento que los expertos que los desarrollaron. Exploraremos preguntas clave en el campo de los LSTM utilizando citas de los expertos y, si está interesado, podrá sumergirse en los documentos originales de los que se tomaron las citas. Descubra cómo desarrollar LSTM como apilado, bidireccional, CNN-LSTM, Encoder-Decoder seq2seq y más en mi nuevo libro, con 14 tutoriales paso a paso y código completo. Empecemos. La promesa de las redes neuronales recurrentes Las redes neuronales recurrentes son diferentes de las redes neuronales tradicionales de retroalimentación. Esta diferencia en la adición de complejidad viene con la promesa de nuevos comportamientos que los métodos tradicionales no pueden lograr. Las redes recurrentes ... tienen un estado interno que puede representar información de contexto. … [Ellos] guardan información sobre entradas pasadas durante un período de tiempo que no se fija a priori, sino que depende de sus pesos y de los datos de entrada. ... Una red recurrente cuyas entradas no son fijas, sino que constituyen una secuencia de entrada se puede utilizar para transformar una secuencia de entrada en una secuencia de salida teniendo en cuenta la información contextual de una manera flexible. - Yoshua Bengio, et al., Aprender dependencias a largo plazo con descenso de gradiente es difícil, 1994. El documento define 3 requisitos básicos de una red neuronal recurrente: que el sistema pueda almacenar información durante un período arbitrario. Que el sistema sea resistente al ruido (es decir, fluctuaciones de las entradas que son aleatorias o irrelevantes para predecir una salida correcta). Que los parámetros del sistema sean entrenables (en un tiempo razonable). El documento también describe la &quot;tarea mínima&quot; para demostrar las redes neuronales recurrentes. El contexto es clave. Las redes neuronales recurrentes deben usar el contexto al hacer predicciones, pero en esta medida, también se debe aprender el contexto requerido. ... las redes neuronales recurrentes contienen ciclos que alimentan las activaciones de la red desde un paso de tiempo anterior como entradas a la red para influir en las predicciones en el paso de tiempo actual. Estas activaciones se almacenan en los estados internos de la red que, en principio, pueden contener información contextual temporal a largo plazo. Este mecanismo permite a los RNN explotar una ventana contextual que cambia dinámicamente sobre el historial de la secuencia de entrada: Hassim Sak, et al., Arquitecturas de redes neuronales recurrentes de memoria a corto plazo para modelado acústico a gran escala, 2014 ¿Necesita ayuda con los LSTM para la predicción de secuencia? Tome mi curso gratuito por correo electrónico de 7 días y descubra 6 arquitecturas LSTM diferentes (con código). Haga clic para registrarse y también obtenga una versión gratuita en PDF del libro. ¡Comience su mini curso GRATIS ahora! Los LSTM cumplen la promesa El éxito de los LSTM radica en su pretensión de ser uno de los primeros implementos para superar los problemas técnicos y cumplir la promesa de las redes neuronales recurrentes. Por lo tanto, los RNN estándar no pueden aprender en presencia de retrasos de tiempo mayores de 5 a 10 pasos de tiempo discretos entre los eventos de entrada relevantes y las señales de destino. El problema del error de desaparición arroja dudas sobre si los RNN estándar pueden exhibir ventajas prácticas significativas sobre las redes de retroalimentación basadas en ventanas temporales. Un modelo reciente, &quot;Memoria a corto plazo&quot; (LSTM), no se ve afectado por este problema. LSTM puede aprender a superar retrasos mínimos de tiempo que exceden los 1000 pasos de tiempo discretos al imponer un flujo de error constante a través de &quot;carruseles de error constante&quot; (CEC) dentro de unidades especiales, llamadas celdas - Felix A. Gers, et al., Learning to Forget: Continual Predicción con LSTM, 2000 Los dos problemas técnicos superados por los LSTM son los gradientes que desaparecen y los gradientes explosivos, ambos relacionados con la forma en que se entrena la red. Desafortunadamente, la gama de información contextual a la que pueden acceder los RNN estándar es en la práctica bastante limitada. El problema es que la influencia de una entrada dada en la capa oculta y, por lo tanto, en la salida de la red, decae o explota exponencialmente a medida que circula alrededor de las conexiones recurrentes de la red. Esta deficiencia ... conocida en la literatura como el problema del gradiente de fuga ... La memoria a corto plazo (LSTM) es una arquitectura RNN diseñada específicamente para abordar el problema del gradiente de fuga. - Alex Graves, et al., Un novedoso sistema conexionista para el reconocimiento sin restricciones de escritura a mano, 2009 La clave de la solución LSTM a los problemas técnicos fue la estructura interna específica de las unidades utilizadas en el modelo. ... gobernado por su capacidad para lidiar con gradientes que desaparecen y explotan, el desafío más común en el diseño y capacitación de RNN. Para abordar este desafío, se introdujo una forma particular de redes recurrentes, llamada LSTM, que se aplicó con gran éxito a la traducción y la generación de secuencias. - Alex Graves, et al., Clasificación de fonemas de Framewise con LSTM bidireccional y otras arquitecturas de redes neuronales, 2005. ¿Cómo funcionan los LSTM? En lugar de entrar en las ecuaciones que rigen cómo se ajustan los LSTM, la analogía es una herramienta útil para comprender rápidamente cómo funcionan. Utilizamos redes con una capa de entrada, una capa oculta y una capa de salida ... La capa oculta (totalmente) auto-conectada contiene celdas de memoria y unidades de compuerta correspondientes ... ... La arquitectura interna de cada celda de memoria garantiza un error constante dentro de su CEC de carrusel de error constante ... Esto representa la base para salvar los retrasos de mucho tiempo. Dos unidades de compuerta aprenden a abrir y cerrar el acceso al flujo de error dentro del CEC de cada celda de memoria. La puerta de entrada multiplicativa brinda protección a la CEC contra perturbaciones por entradas irrelevantes. Del mismo modo, la puerta de salida multiplicativa protege a otras unidades de la perturbación por los contenidos de memoria actualmente irrelevantes. - Sepp Hochreiter y Jurgen Schmidhuber, Long Short-Term Memory, 1997. Múltiples analogías pueden ayudar a comprar lo que diferencia a los LSTM de las redes neuronales tradicionales formadas por neuronas simples. La arquitectura de memoria a corto y largo plazo fue motivada por un análisis del flujo de errores en los RNN existentes que descubrió que los retrasos de largo tiempo eran inaccesibles para las arquitecturas existentes, porque el error propagado hacia atrás explota o decae exponencialmente. Una capa LSTM consta de un conjunto de bloques conectados de forma recurrente, conocidos como bloques de memoria. Estos bloques pueden considerarse como una versión diferenciable de los chips de memoria en una computadora digital. Cada uno contiene una o más celdas de memoria conectadas de forma recurrente y tres unidades multiplicativas, las compuertas de entrada, salida y olvido, que proporcionan análogos continuos de operaciones de escritura, lectura y restablecimiento para las celdas. ... La red solo puede interactuar con las células a través de las puertas. - Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005. Es interesante notar que, incluso después de más de 20 años, el LSTM simple (o vainilla) puede ser el mejor lugar para comenzar al aplicar la técnica. La arquitectura LSTM más utilizada (vanilla LSTM) funciona razonablemente bien en varios conjuntos de datos ... La velocidad de aprendizaje y el tamaño de la red son los hiperparámetros LSTM ajustables más importantes ... ... Esto implica que los hiperparámetros se pueden ajustar de forma independiente. En particular, la tasa de aprendizaje se puede calibrar primero usando una red bastante pequeña, ahorrando así mucho tiempo de experimentación. - Klaus Greff, et al., LSTM: A Search Space Odyssey, 2015 ¿Qué son las aplicaciones LSTM? Es importante saber exactamente qué tipo de problemas de aprendizaje de secuencia son adecuados para abordar los LSTM. La memoria larga a corto plazo (LSTM) puede resolver numerosas tareas que los algoritmos de aprendizaje anteriores para redes neuronales recurrentes (RNN) no pueden resolver. ... ... LSTM promete cualquier tarea de procesamiento secuencial en la que sospechemos que puede existir una descomposición jerárquica, pero no sabemos de antemano cuál es esta descomposición. - Felix A. Gers, et al., Learning to Forget: Continual Prediction with LSTM, 2000 The Recurrent Neural Network (RNN) es un modelo de secuencia neural que logra un rendimiento avanzado en tareas importantes que incluyen modelado de lenguaje, reconocimiento de voz y máquina traductora. - Wojciech Zaremba, Regularización de redes neuronales recurrentes, 2014. Dado que los LSTM son efectivos para capturar dependencias temporales a largo plazo sin sufrir los obstáculos de optimización que afectan a las redes recurrentes simples (SRN), se han utilizado para avanzar el estado del arte para muchos Problemas difíciles Esto incluye reconocimiento y generación de escritura a mano, modelado y traducción del lenguaje, modelado acústico del habla, síntesis del habla, predicción de estructura secundaria de proteínas, análisis de datos de audio y video, entre otros. - Klaus Greff, et al., LSTM: A Search Space Odyssey, 2015 ¿Qué son los LSTM bidireccionales? Una mejora comúnmente mencionada sobre los LSTM son los LSTM bidireccionales. La idea básica de las redes neuronales recurrentes bidireccionales es presentar cada secuencia de entrenamiento hacia adelante y hacia atrás a dos redes recurrentes separadas, ambas conectadas a la misma capa de salida. ... Esto significa que para cada punto de una secuencia dada, el BRNN tiene información completa y secuencial sobre todos los puntos anteriores y posteriores. Además, debido a que la red es libre de usar tanto o tan poco de este contexto como sea necesario, no es necesario encontrar una ventana de tiempo (dependiente de la tarea) o un tamaño de retraso objetivo. ... para problemas temporales"
12;machinelearningmastery.com;https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/;2018-09-03;How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course);"# univariate mlp example

from numpy import array

from keras . models import Sequential

from keras . layers import Dense

# define dataset

X = array ( [ [ 10 , 20 , 30 ] , [ 20 , 30 , 40 ] , [ 30 , 40 , 50 ] , [ 40 , 50 , 60 ] ] )

y = array ( [ 40 , 50 , 60 , 70 ] )

# define model

model = Sequential ( )

model . add ( Dense ( 100 , activation = 'relu' , input_dim = 3 ) )

model . add ( Dense ( 1 ) )

model . compile ( optimizer = 'adam' , loss = 'mse' )

# fit model

model . fit ( X , y , epochs = 2000 , verbose = 0 )

# demonstrate prediction

x_input = array ( [ 50 , 60 , 70 ] )

x_input = x_input . reshape ( ( 1 , 3 ) )

yhat = model . predict ( x_input , verbose = 0 )";Cómo comenzar con el aprendizaje profundo para la predicción de series temporales (mini-curso de 7 días);"# Ejemplo de mlp univariante de una matriz de importación numpy de keras. Los modelos importan secuenciales de keras. las capas importan Dense # define dataset X = array ([[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60]]) y = array ([40 , 50, 60, 70]) # define modelo modelo = modelo secuencial (). modelo add (Dense (100, activación = &#39;relu&#39;, input_dim = 3)). Añadir modelo (Denso (1)). compilar (optimizador = &#39;adam&#39;, pérdida = &#39;mse&#39;) # ajustar modelo de modelo. fit (X, y, epochs = 2000, verbose = 0) # demostrar predicción x_input = array ([50, 60, 70]) x_input = x_input. remodelar ((1, 3)) yhat = modelo. predecir (x_input, detallado = 0)"
13;news.mit.edu;http://news.mit.edu/2020/cnt-nanosensor-smartphone-plant-stress-0415;;Nanosensor can alert a smartphone when plants are stressed;"MIT engineers have developed a way to closely track how plants respond to stresses such as injury, infection, and light damage, using sensors made of carbon nanotubes. These sensors can be embedded in plant leaves, where they report on hydrogen peroxide signaling waves.

Plants use hydrogen peroxide to communicate within their leaves, sending out a distress signal that stimulates leaf cells to produce compounds that will help them repair damage or fend off predators such as insects. The new sensors can use these hydrogen peroxide signals to distinguish between different types of stress, as well as between different species of plants.

“Plants have a very sophisticated form of internal communication, which we can now observe for the first time. That means that in real-time, we can see a living plant’s response, communicating the specific type of stress that it’s experiencing,” says Michael Strano, the Carbon P. Dubbs Professor of Chemical Engineering at MIT.

This kind of sensor could be used to study how plants respond to different types of stress, potentially helping agricultural scientists develop new strategies to improve crop yields. The researchers demonstrated their approach in eight different plant species, including spinach, strawberry plants, and arugula, and they believe it could work in many more.

Strano is the senior author of the study, which appears today in Nature Plants. MIT graduate student Tedrick Thomas Salim Lew is the lead author of the paper.

Embedded sensors

Over the past several years, Strano’s lab has been exploring the potential for engineering “nanobionic plants” — plants that incorporate nanomaterials that give the plants new functions, such as emitting light or detecting water shortages. In the new study, he set out to incorporate sensors that would report back on the plants’ health status.

Strano had previously developed carbon nanotube sensors that can detect various molecules, including hydrogen peroxide. About three years ago, Lew began working on trying to incorporate these sensors into plant leaves. Studies in Arabidopsis thaliana, often used for molecular studies of plants, had suggested that plants might use hydrogen peroxide as a signaling molecule, but its exact role was unclear.

Lew used a method called lipid exchange envelope penetration (LEEP) to incorporate the sensors into plant leaves. LEEP, which Strano’s lab developed several years ago, allows for the design of nanoparticles that can penetrate plant cell membranes. As Lew was working on embedding the carbon nanotube sensors, he made a serendipitous discovery.

“I was training myself to get familiarized with the technique, and in the process of the training I accidentally inflicted a wound on the plant. Then I saw this evolution of the hydrogen peroxide signal,” he says.

He saw that after a leaf was injured, hydrogen peroxide was released from the wound site and generated a wave that spread along the leaf, similar to the way that neurons transmit electrical impulses in our brains. As a plant cell releases hydrogen peroxide, it triggers calcium release within adjacent cells, which stimulates those cells to release more hydrogen peroxide.

“Like dominos successively falling, this makes a wave that can propagate much further than a hydrogen peroxide puff alone would,” Strano says. “The wave itself is powered by the cells that receive and propagate it.”

This flood of hydrogen peroxide stimulates plant cells to produce molecules called secondary metabolites, such as flavonoids or carotenoids, which help them to repair the damage. Some plants also produce other secondary metabolites that can be secreted to fend off predators. These metabolites are often the source of the food flavors that we desire in our edible plants, and they are only produced under stress.

A key advantage of the new sensing technique is that it can be used in many different plant species. Traditionally, plant biologists have done much of their molecular biology research in certain plants that are amenable to genetic manipulation, including Arabidopsis thaliana and tobacco plants. However, the new MIT approach is applicable to potentially any plant.

“In this study, we were able to quickly compare eight plant species, and you would not be able to do that with the old tools,” Strano says.

The researchers tested strawberry plants, spinach, arugula, lettuce, watercress, and sorrel, and found that different species appear to produce different waveforms — the distinctive shape produced by mapping the concentration of hydrogen peroxide over time. They hypothesize that each plant’s response is related to its ability to counteract the damage. Each species also appears to respond differently to different types of stress, including mechanical injury, infection, and heat or light damage.

“This waveform holds a lot of information for each species, and even more exciting is that the type of stress on a given plant is encoded in this waveform,” Strano says. “You can look at the real time response that a plant experiences in almost any new environment.”

Stress response

The near-infrared fluorescence produced by the sensors can be imaged using a small infrared camera connected to a Raspberry Pi, a $35 credit-card-sized computer similar to the computer inside a smartphone. “Very inexpensive instrumentation can be used to capture the signal,” Strano says.

Applications for this technology include screening different species of plants for their ability to resist mechanical damage, light, heat, and other forms of stress, Strano says. It could also be used to study how different species respond to pathogens, such as the bacteria that cause citrus greening and the fungus that causes coffee rust.

“One of the things I’m interested in doing is understanding why some types of plants exhibit certain immunity to these pathogens and others don’t,” he says.

Strano and his colleagues in the Disruptive and Sustainable Technology for Agricultural Precision interdisciplinary research group at the Singapore-MIT Alliance for Research and Technology (SMART), MIT’s research enterprise in Singapore, are also interested in studying is how plants respond to different growing conditions in urban farms.

One problem they hope to address is shade avoidance, which is seen in many species of plants when they are grown at high density. Such plants turn on a stress response that diverts their resources into growing taller, instead of putting energy into producing crops. This lowers the overall crop yield, so agricultural researchers are interested in engineering plants so that don’t turn on that response.

“Our sensor allows us to intercept that stress signal and to understand exactly the conditions and the mechanism that are happening upstream and downstream in the plant that gives rise to the shade avoidance,” Strano says.

The research was funded by the National Research Foundation of Singapore, the Singapore Agency for Science, Technology, and Research (A*STAR), and the U.S. Department of Energy Computational Science Graduate Fellowship Program.";El nanosensor puede alertar a un teléfono inteligente cuando las plantas están estresadas;"Los ingenieros del MIT han desarrollado una forma de rastrear de cerca cómo las plantas responden a tensiones como lesiones, infecciones y daños leves, utilizando sensores hechos de nanotubos de carbono. Estos sensores pueden incrustarse en las hojas de las plantas, donde informan sobre las ondas de señalización de peróxido de hidrógeno. Las plantas usan peróxido de hidrógeno para comunicarse dentro de sus hojas, enviando una señal de socorro que estimula las células de la hoja para producir compuestos que les ayudarán a reparar el daño o ahuyentar a los depredadores como los insectos. Los nuevos sensores pueden usar estas señales de peróxido de hidrógeno para distinguir entre diferentes tipos de estrés, así como entre diferentes especies de plantas. “Las plantas tienen una forma muy sofisticada de comunicación interna, que ahora podemos observar por primera vez. Eso significa que en tiempo real, podemos ver la respuesta de una planta viva, comunicando el tipo específico de estrés que está experimentando &quot;, dice Michael Strano, profesor de Ingeniería Química de Carbon P. Dubbs en el MIT. Este tipo de sensor podría usarse para estudiar cómo las plantas responden a diferentes tipos de estrés, lo que podría ayudar a los científicos agrícolas a desarrollar nuevas estrategias para mejorar el rendimiento de los cultivos. Los investigadores demostraron su enfoque en ocho especies de plantas diferentes, incluidas las espinacas, las plantas de fresa y la rúcula, y creen que podría funcionar en muchas más. Strano es el autor principal del estudio, que aparece hoy en Nature Plants. El estudiante graduado del MIT, Tedrick Thomas Salim Lew, es el autor principal del artículo. Sensores integrados Durante los últimos años, el laboratorio de Strano ha estado explorando el potencial para la ingeniería de &quot;plantas nanobiónicas&quot;, plantas que incorporan nanomateriales que dan a las plantas nuevas funciones, como emitir luz o detectar escasez de agua. En el nuevo estudio, se propuso incorporar sensores que informarían sobre el estado de salud de las plantas. Strano había desarrollado previamente sensores de nanotubos de carbono que pueden detectar diversas moléculas, incluido el peróxido de hidrógeno. Hace unos tres años, Lew comenzó a trabajar para tratar de incorporar estos sensores en las hojas de las plantas. Los estudios en Arabidopsis thaliana, a menudo utilizados para estudios moleculares de plantas, habían sugerido que las plantas podrían usar peróxido de hidrógeno como molécula de señalización, pero su papel exacto no estaba claro. Lew utilizó un método llamado penetración de la envoltura de intercambio de lípidos (LEEP) para incorporar los sensores en las hojas de las plantas. LEEP, que el laboratorio de Strano desarrolló hace varios años, permite el diseño de nanopartículas que pueden penetrar las membranas celulares de las plantas. Cuando Lew estaba trabajando en integrar los sensores de nanotubos de carbono, hizo un descubrimiento fortuito. “Me estaba entrenando para familiarizarme con la técnica, y en el proceso del entrenamiento causé una herida accidentalmente en la planta. Entonces vi esta evolución de la señal de peróxido de hidrógeno ”, dice. Vio que después de que una hoja resultó lesionada, el peróxido de hidrógeno se liberó del sitio de la herida y generó una onda que se extendió a lo largo de la hoja, de forma similar a la forma en que las neuronas transmiten impulsos eléctricos en nuestros cerebros. Cuando una célula vegetal libera peróxido de hidrógeno, desencadena la liberación de calcio dentro de las células adyacentes, lo que estimula a esas células a liberar más peróxido de hidrógeno. &quot;Al igual que las fichas de dominó caen sucesivamente, esto genera una ola que puede propagarse mucho más de lo que lo haría una bocanada de peróxido de hidrógeno&quot;, dice Strano. &quot;La onda en sí es alimentada por las células que la reciben y la propagan&quot;. Esta inundación de peróxido de hidrógeno estimula a las células vegetales a producir moléculas llamadas metabolitos secundarios, como flavonoides o carotenoides, que les ayudan a reparar el daño. Algunas plantas también producen otros metabolitos secundarios que pueden secretarse para defenderse de los depredadores. Estos metabolitos son a menudo la fuente de los sabores alimenticios que deseamos en nuestras plantas comestibles, y solo se producen bajo estrés. Una ventaja clave de la nueva técnica de detección es que puede usarse en muchas especies de plantas diferentes. Tradicionalmente, los biólogos de plantas han realizado gran parte de su investigación de biología molecular en ciertas plantas que son susceptibles de manipulación genética, incluidas Arabidopsis thaliana y plantas de tabaco. Sin embargo, el nuevo enfoque MIT es aplicable a potencialmente cualquier planta. &quot;En este estudio, pudimos comparar rápidamente ocho especies de plantas, y usted no podría hacerlo con las herramientas antiguas&quot;, dice Strano. Los investigadores probaron plantas de fresa, espinacas, rúcula, lechuga, berros y acedera, y descubrieron que diferentes especies parecen producir diferentes formas de onda, la forma distintiva producida al mapear la concentración de peróxido de hidrógeno a lo largo del tiempo. Ellos plantean la hipótesis de que la respuesta de cada planta está relacionada con su capacidad para contrarrestar el daño. Cada especie también parece responder de manera diferente a los diferentes tipos de estrés, incluidas las lesiones mecánicas, las infecciones y el daño causado por el calor o la luz. &quot;Esta forma de onda contiene mucha información para cada especie, y aún más emocionante es que el tipo de estrés en una planta determinada está codificado en esta forma de onda&quot;, dice Strano. &quot;Puede ver la respuesta en tiempo real que experimenta una planta en casi cualquier entorno nuevo&quot;. Respuesta al estrés La fluorescencia del infrarrojo cercano producida por los sensores se puede capturar mediante una pequeña cámara infrarroja conectada a una Raspberry Pi, una computadora del tamaño de una tarjeta de crédito de $ 35 similar a la computadora dentro de un teléfono inteligente. &quot;Se puede utilizar instrumentación muy barata para capturar la señal&quot;, dice Strano. Las aplicaciones para esta tecnología incluyen la detección de diferentes especies de plantas por su capacidad de resistir daños mecánicos, luz, calor y otras formas de estrés, dice Strano. También podría usarse para estudiar cómo las diferentes especies responden a los patógenos, como las bacterias que causan el enverdecimiento de los cítricos y el hongo que causa la roya del café. &quot;Una de las cosas que me interesa hacer es entender por qué algunos tipos de plantas exhiben cierta inmunidad a estos patógenos y otras no&quot;, dice. Strano y sus colegas del grupo de investigación interdisciplinaria Tecnología disruptiva y sostenible para la precisión agrícola en la Alianza Singapur-MIT para Investigación y Tecnología (SMART), la empresa de investigación del MIT en Singapur, también están interesados en estudiar cómo las plantas responden a las diferentes condiciones de crecimiento en granjas urbanas. Un problema que esperan abordar es la evitación de la sombra, que se observa en muchas especies de plantas cuando se cultivan a alta densidad. Dichas plantas activan una respuesta al estrés que desvía sus recursos para crecer más alto, en lugar de poner energía en la producción de cultivos. Esto reduce el rendimiento general de los cultivos, por lo que los investigadores agrícolas están interesados en las plantas de ingeniería para que no activen esa respuesta. &quot;Nuestro sensor nos permite interceptar esa señal de estrés y comprender exactamente las condiciones y el mecanismo que están sucediendo aguas arriba y aguas abajo en la planta que da lugar a la evitación de la sombra&quot;, dice Strano. La investigación fue financiada por la Fundación Nacional de Investigación de Singapur, la Agencia de Ciencia, Tecnología e Investigación de Singapur (A * STAR) y el Programa de Becas para Graduados de Ciencias Computacionales del Departamento de Energía de EE. UU."
14;machinelearningmastery.com;http://machinelearningmastery.com/compare-the-performance-of-machine-learning-algorithms-in-r/;2016-02-25;Compare The Performance of Machine Learning Algorithms in R;"# prepare training scheme

control < - trainControl ( method = ""repeatedcv"" , number = 10 , repeats = 3 )

# CART

set . seed ( 7 )

fit . cart < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""rpart"" , trControl = control )

# LDA

set . seed ( 7 )

fit . lda < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""lda"" , trControl = control )

# SVM

set . seed ( 7 )

fit . svm < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""svmRadial"" , trControl = control )

# kNN

set . seed ( 7 )

fit . knn < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""knn"" , trControl = control )

# Random Forest

set . seed ( 7 )

fit . rf < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""rf"" , trControl = control )

# collect resamples";Compare el rendimiento de los algoritmos de aprendizaje automático en R;"# preparar el control del esquema de entrenamiento &lt;- trainControl (método = &quot;repetidocv&quot;, número = 10, repeticiones = 3) # CARRITO establecido. semilla (7) en forma. cart &lt;- train (diabetes ~., data = PimaIndiansDiabetes, method = &quot;rpart&quot;, trControl = control) # conjunto LDA. semilla (7) en forma. lda &lt;- train (diabetes ~., data = PimaIndiansDiabetes, method = &quot;lda&quot;, trControl = control) # conjunto de SVM. semilla (7) en forma. svm &lt;- train (diabetes ~., data = PimaIndiansDiabetes, method = &quot;svmRadial&quot;, trControl = control) # conjunto de kNN. semilla (7) en forma. knn &lt;- train (diabetes ~., data = PimaIndiansDiabetes, method = &quot;knn&quot;, trControl = control) # Conjunto de bosque aleatorio. semilla (7) en forma. rf &lt;- train (diabetes ~., data = PimaIndiansDiabetes, method = &quot;rf&quot;, trControl = control) # recoger muestras"
15;news.mit.edu;http://news.mit.edu/2020/amid-shutdowns-supply-chains-pivot-demand-for-specialized-talent-intensifies-mitx-micromasters-0413;;Amid shutdowns, supply chains pivot and global demand for specialized talent intensifies;"The global landscape of supply chain management has changed drastically in the past several weeks. Businesses, organizations, and people are rapidly innovating to improve supply chains and upskill and reskill the workforce and themselves to accommodate disruptions caused by the global Covid-19 health crisis. Online retailers and logistics providers are announcing vast hiring initiatives, while companies and organizations grapple with the logistics demands of supplying for vital services.

Even in the middle of these disruptions, a cohort of 383 dedicated online learners concluded nine to 18 months of learning to pass their comprehensive final exams, earning their MITx MicroMasters program credentials in supply chain management. These new credential-holders bring the total number of holders to 2,243 from 115 countries. The majority of credential-holders hail from the United States, India, Brazil, Spain, and China, some of the world’s most influential economies. While credential holders’ median age is 31, holders range in age from 21 to 74, practicing diverse business functions. They are currently employed at more than 700 companies worldwide, ranging from the largest multinational corporations to local, family-owned businesses.

Given the volatile nature of logistics during disruptions, the comprehensive theoretical and practical knowledge gained from these courses is already having an impact. “The program significantly changed my mindset to be proactive. This helped me improvise ahead of the current pandemic challenges to provide visibility across my supply chain,” says learner Mohamed El Tayeb, a demand planner in Saudi Arabia. “Technically, everything I learned in the program is coming in handy now.” Similarly, Matthias Stolz, a supply chain management project manager from Germany, claims “The MicroMasters program helped me to be able to make back-of-the-envelope calculations to quantify effects and evaluate risks and opportunities fast. This allowed me to confidently prepare decisions for the top management which have already enabled the company to respond quickly.”

Learners like El Tayeb and Stolz are leading the way on the ground, along with contributors from across the supply chain, to be cited as everyday heroes by MIT’s Center for Transportation and Logistics and the U.S. Department of Agriculture, among many others.

MIT will recognize the contributions of credential holders and program participants in a public online completion celebration on April 15 at 11 a.m. EDT. “Our goal is to pioneer supply chain digital education to shape the leaders of the future,” says program Director Eva Ponce. “We are bringing MIT education to anyone from anywhere to improve the capabilities and prospects of professionals through our massive open online courses. It is my distinct pleasure to thank the committed and passionate team responsible for the development and delivery of this program and to welcome this learner cohort to the credential-holder community, who are the future of the supply chain profession.”

As a new normal becomes apparent in the foreseeable future, experts agree that the global disruptions should serve as a wake-up call for supply chain and logistics managers. They foresee that practitioners will need an array of practical and analytical tools at their disposal to accommodate rapidly changing demands. Teaching supply chain management online is one strategy to meet this dynamic demand. The MITx MicroMasters Program in Supply Chain Management is becoming recognized as a go-to knowledge baseline for individuals and organizations to meet their global demand for talent.";En medio de paradas, el pivote de las cadenas de suministro y la demanda global de talento especializado se intensifica;"El panorama global de la gestión de la cadena de suministro ha cambiado drásticamente en las últimas semanas. Las empresas, las organizaciones y las personas están innovando rápidamente para mejorar las cadenas de suministro y mejorar y volver a capacitar a la fuerza laboral y a ellos mismos para adaptarse a las interrupciones causadas por la crisis de salud mundial de Covid-19. Los minoristas en línea y los proveedores de logística están anunciando grandes iniciativas de contratación, mientras que las empresas y organizaciones se enfrentan a las demandas logísticas de suministrar servicios vitales. Incluso en medio de estas interrupciones, una cohorte de 383 estudiantes en línea dedicados concluyeron de nueve a 18 meses aprendiendo a aprobar sus exámenes finales completos, obteniendo sus credenciales del programa MITx MicroMasters en la gestión de la cadena de suministro. Estos nuevos titulares de credenciales elevan el número total de titulares a 2.243 de 115 países. La mayoría de los titulares de credenciales provienen de los Estados Unidos, India, Brasil, España y China, algunas de las economías más influyentes del mundo. Si bien la edad promedio de los titulares de credenciales es de 31 años, los titulares tienen entre 21 y 74 años, y ejercen diversas funciones comerciales. Actualmente están empleados en más de 700 empresas en todo el mundo, que van desde las corporaciones multinacionales más grandes hasta empresas locales de propiedad familiar. Dada la naturaleza volátil de la logística durante las interrupciones, el conocimiento teórico y práctico integral obtenido de estos cursos ya está teniendo un impacto. “El programa cambió significativamente mi mentalidad para ser proactivo. Esto me ayudó a improvisar antes de los desafíos actuales de la pandemia para proporcionar visibilidad a través de mi cadena de suministro ”, dice el alumno Mohamed El Tayeb, un planificador de demanda en Arabia Saudita. &quot;Técnicamente, todo lo que aprendí en el programa es útil ahora&quot;. Del mismo modo, Matthias Stolz, gerente de proyectos de gestión de la cadena de suministro de Alemania, afirma que “El programa MicroMasters me ayudó a poder hacer cálculos de última generación para cuantificar los efectos y evaluar los riesgos y oportunidades rápidamente. Esto me permitió preparar con confianza decisiones para la alta gerencia que ya han permitido que la compañía responda rápidamente ”. Estudiantes como El Tayeb y Stolz están liderando el camino, junto con colaboradores de toda la cadena de suministro, para ser citados como héroes cotidianos por el Centro de Transporte y Logística del MIT y el Departamento de Agricultura de los EE. UU., Entre muchos otros. El MIT reconocerá las contribuciones de los titulares de credenciales y los participantes del programa en una celebración pública de finalización en línea el 15 de abril a las 11 a.m. EDT. &quot;Nuestro objetivo es ser pioneros en la educación digital de la cadena de suministro para dar forma a los líderes del futuro&quot;, dice la directora del programa Eva Ponce. “Estamos brindando educación MIT a cualquier persona desde cualquier lugar para mejorar las capacidades y las perspectivas de los profesionales a través de nuestros cursos masivos en línea abiertos. Es un placer para mí agradecer al equipo comprometido y apasionado responsable del desarrollo y entrega de este programa y dar la bienvenida a esta cohorte de estudiantes a la comunidad de credenciales, quienes son el futuro de la profesión de la cadena de suministro ”. A medida que se hace evidente una nueva normalidad en el futuro previsible, los expertos coinciden en que las interrupciones globales deberían servir como una llamada de atención para los gerentes de logística y la cadena de suministro. Previenen que los profesionales necesitarán una variedad de herramientas prácticas y analíticas a su disposición para satisfacer las demandas que cambian rápidamente. La enseñanza de la gestión de la cadena de suministro en línea es una estrategia para satisfacer esta demanda dinámica. El Programa MITx MicroMasters en Gestión de la Cadena de Suministro se está convirtiendo en una referencia básica de conocimiento para individuos y organizaciones para satisfacer su demanda global de talento."
16;machinelearningmastery.com;https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/;2020-01-14;Random Oversampling and Undersampling for Imbalanced Classification;"# example of random oversampling to balance the class distribution

from collections import Counter

from sklearn . datasets import make_classification

from imblearn . over_sampling import RandomOverSampler

# define dataset

X , y = make_classification ( n_samples = 10000 , weights = [ 0.99 ] , flip_y = 0 )

# summarize class distribution

print ( Counter ( y ) )

# define oversampling strategy

oversample = RandomOverSampler ( sampling_strategy = 'minority' )

# fit and apply the transform

X_over , y_over = oversample . fit_resample ( X , y )

# summarize class distribution";Sobremuestreo aleatorio y submuestreo para clasificación desequilibrada;"# ejemplo de sobremuestreo aleatorio para equilibrar la distribución de clases desde el contador de importación de colecciones desde sklearn. los conjuntos de datos importan make_classification de imblearn. over_sampling import RandomOverSampler # define dataset X, y = make_classification (n_samples = 10000, weights = [0.99], flip_y = 0) # resume la distribución de la clase print (Counter (y)) # define la estrategia de sobremuestreo oversample = RandomOverSampler (samples_strategy = &#39;minority&#39; ) # ajusta y aplica la transformación X_over, y_over = oversample. fit_resample (X, y) # resume la distribución de clases"
17;machinelearningmastery.com;https://machinelearningmastery.com/scale-machine-learning-data-scratch-python/;2016-10-13;How to Scale Machine Learning Data From Scratch With Python;"from csv import reader

from math import sqrt

# Load a CSV file

def load_csv ( filename ) :

file = open ( filename , ""rb"" )

lines = reader ( file )

dataset = list ( lines )

return dataset

# Convert string column to float

def str_column_to_float ( dataset , column ) :

for row in dataset :

row [ column ] = float ( row [ column ] . strip ( ) )

# calculate column means

def column_means ( dataset ) :

means = [ 0 for i in range ( len ( dataset [ 0 ] ) ) ]

for i in range ( len ( dataset [ 0 ] ) ) :

col_values = [ row [ i ] for row in dataset ]

means [ i ] = sum ( col_values ) / float ( len ( dataset ) )

return means

# calculate column standard deviations

def column_stdevs ( dataset , means ) :

stdevs = [ 0 for i in range ( len ( dataset [ 0 ] ) ) ]

for i in range ( len ( dataset [ 0 ] ) ) :

variance = [ pow ( row [ i ] - means [ i ] , 2 ) for row in dataset ]

stdevs [ i ] = sum ( variance )

stdevs = [ sqrt ( x / ( float ( len ( dataset ) - 1 ) ) ) for x in stdevs ]

return stdevs

# standardize dataset

def standardize_dataset ( dataset , means , stdevs ) :

for row in dataset :

for i in range ( len ( row ) ) :

row [ i ] = ( row [ i ] - means [ i ] ) / stdevs [ i ]

# Load pima-indians-diabetes dataset

filename = 'pima-indians-diabetes.csv'

dataset = load_csv ( filename )

print ( 'Loaded data file {0} with {1} rows and {2} columns' ) . format ( filename , len ( dataset ) , len ( dataset [ 0 ] ) )

# convert string columns to float

for i in range ( len ( dataset [ 0 ] ) ) :

str_column_to_float ( dataset , i )

print ( dataset [ 0 ] )

# Estimate mean and standard deviation

means = column_means ( dataset )

stdevs = column_stdevs ( dataset , means )

# standardize dataset

standardize_dataset ( dataset , means , stdevs )";Cómo escalar datos de aprendizaje automático desde cero con Python;"desde csv import reader desde math import sqrt # Cargue un archivo CSV def load_csv (nombre de archivo): file = open (filename, &quot;rb&quot;) lines = reader (file) dataset = list (lines) return dataset # Convertir columna de cadena a flotante def str_column_to_float (conjunto de datos, columna): para fila en conjunto de datos: fila [columna] = flotante (fila [columna]. tira ()) # calcular columna significa def column_means (conjunto de datos): significa = [0 para i en rango (len (conjunto de datos [0]))] para i en rango (len (conjunto de datos [0])): valores_col = [fila [i] para fila en conjunto de datos] significa [i] = suma (valores_col) / float (len (conjunto de datos)) retorno significa # calcular las desviaciones estándar de la columna def column_stdevs (conjunto de datos, medias): stdevs = [0 para i en rango (len (conjunto de datos [0]))]] para i en rango (len (conjunto de datos [0])): varianza = [pow (fila [i] - significa [i], 2) para la fila en el conjunto de datos] stdevs [i] = sum (varianza) stdevs = [sqrt (x / (float (len (dataset) - 1)))) para x en stdevs ] devolver stdevs # estandarizar conjunto de datos def standardize_dataset (conjunto de datos, medias, stdevs): para fila en conjunto de datos: para i en rango (len (fila)): fila [i] = (fila [i] - significa [i]) / stdevs [i] # Carga conjunto de datos pima-indians-diabetes filename = &#39;pima-indians-diabetes.csv&#39; dataset = load_csv (filename) print (&#39;Archivo de datos cargados {0} con {1} filas y {2} columnas&#39;). formato (nombre de archivo, len (conjunto de datos), len (conjunto de datos [0])) # convertir columnas de cadena para flotar para i en rango (len (conjunto de datos [0])): str_column_to_float (conjunto de datos, i) print (conjunto de datos [0]) # Estimación de la media y la desviación estándar medias = column_means (conjunto de datos) stdevs = column_stdevs (conjunto de datos, medias) # estandarizar conjunto de datos standardize_dataset (conjunto de datos, medias, stdevs)"
18;machinelearningmastery.com;https://machinelearningmastery.com/gentle-introduction-generative-long-short-term-memory-networks/;2017-08-24;Gentle Introduction to Generative Long Short-Term Memory Networks;"Tweet Share Share

Last Updated on August 14, 2019

The Long Short-Term Memory recurrent neural network was developed for sequence prediction.

In addition to sequence prediction problems. LSTMs can also be used as a generative model

In this post, you will discover how LSTMs can be used as generative models.

After completing this post, you will know:

About generative models, with a focus on generative models for text called language modeling.

Examples of applications where LSTM Generative models have been used.

Examples of how to model text for generative models with LSTMs.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Generative Models

LSTMs can be used as a generative model.

Given a large corpus of sequence data, such as text documents, LSTM models can be designed to learn the general structural properties of the corpus, and when given a seed input, can generate new sequences that are representative of the original corpus.

The problem of developing a model to generalize a corpus of text is called language modeling in the field of natural language processing. A language model may work at the word level and learn the probabilistic relationships between words in a document in order to accurately complete a sentence and generate entirely new sentences. At its most challenging, language models work at the character level, learning from sequences of characters, and generating new sequences one character at a time.

The goal of character-level language modeling is to predict the next character in a sequence.

— Generating Text with Recurrent Neural Networks, 2011.

Although more challenging, the added flexibility of a character-level model allows new words to be generated, punctuation added, and the generation of any other structures that may exist in the text data.

… predicting one character at a time is more interesting from the perspective of sequence generation, because it allows the network to invent novel words and strings.

— Generating Sequences With Recurrent Neural Networks, 2013.

Language modeling is by far the most studied application of Generative LSTMs, perhaps because of the use of standard datasets where model performance can be quantified and compared. This approach has been used to generate text on a suite of interesting language modeling problems, such as:

Generating Wikipedia articles (including markup).

Generating snippets from great authors like Shakespeare.

Generating technical manuscripts (including markup).

Generating computer source code.

Generating article headlines.

The quality of the results vary; for example, the markup or source code may require manual intervention to render or compile. Nevertheless, the results are impressive.

The approach has also been applied to different domains where a large corpus of existing sequence information is available and new sequences can be generated one step at a time, such as:

Handwriting generation.

Music generation.

Speech generation.

Generative LSTMs

A Generative LSTM is not really architecture, it is more a change in perspective about what an LSTM predictive model learns and how the model is used.

We could conceivably use any LSTM architecture as a generative model. In this case, we will use a simple Vanilla LSTM.

In the case of a character-level language model, the alphabet of all possible characters is fixed. A one hot encoding is used both for learning input sequences and predicting output sequences.

A one-to-one model is used where one step is predicted for each input time step. This means that input sequences may require specialized handling in order to be vectorized or formatted for efficiently training a supervised model.

For example, given the sequence:

""hello world"" 1 ""hello world""

A dataset would need to be constructed such as:

'h' => 'e' 'e' => 'l' 'l' => 'l' ... 1 2 3 4 'h' => 'e' 'e' => 'l' 'l' => 'l' ...

This could be presented as-is as a dataset of one time step samples, which could be quite limiting to the network (e.g. no BPTT).

Alternately, it could be vectorized to a fixed-length input sequence for a many-to-one time step model, such as:

['h', 'e', 'l'] => 'l' ['e', 'l', 'l'] => 'o' ['l', 'l', 'o'] => ' ' ... 1 2 3 4 ['h', 'e', 'l'] => 'l' ['e', 'l', 'l'] => 'o' ['l', 'l', 'o'] => ' ' ...

Or, a fixed-length output sequence for a one-to-many time step model:

'h' => ['e', 'l', 'l'] 'e' => ['l', 'l', 'o'] 'l' => ['l', 'o', ' '] ... 1 2 3 4 'h' => ['e', 'l', 'l'] 'e' => ['l', 'l', 'o'] 'l' => ['l', 'o', ' '] ...

Or some variation on these approaches.

Note that the same vectorized representation would be required when making predictions, meaning that predicted characters would need to be presented as input for subsequent samples. This could be quite clumsy in implementation.

The internal state of the network may need careful management, perhaps reset at choice locations in the input sequence (e.g. end of paragraph, page, or chapter) rather than at the end of each input sequence.

Further Reading

This section provides more resources on the topic if you are looking go deeper.

Papers

Posts

Summary

In this post, you discovered the use of LSTMs as generative models.

Specifically, you learned:

About generative models, with a focus on generative models for text called language modeling.

Examples of applications where LSTM Generative models have been used.

Examples of how to model text for generative models with LSTMs.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop LSTMs for Sequence Prediction Today! Develop Your Own LSTM models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Long Short-Term Memory Networks with Python It provides self-study tutorials on topics like:

CNN LSTMs, Encoder-Decoder LSTMs, generative models, data preparation, making predictions and much more... Finally Bring LSTM Recurrent Neural Networks to

Your Sequence Predictions Projects Skip the Academics. Just Results. See What's Inside";Introducción suave a las redes de memoria generativas de corto y largo plazo;"Tweet Compartir Compartir Última actualización el 14 de agosto de 2019 La red neuronal recurrente de memoria a corto plazo se desarrolló para la predicción de secuencias. Además de los problemas de predicción de secuencia. Los LSTM también se pueden usar como modelo generativo En esta publicación, descubrirá cómo los LSTM se pueden usar como modelos generativos. Después de completar esta publicación, sabrá: Acerca de los modelos generativos, con un enfoque en los modelos generativos para texto llamado modelado de lenguaje. Ejemplos de aplicaciones donde se han utilizado modelos LSTM Generative. Ejemplos de cómo modelar texto para modelos generativos con LSTM. Descubra cómo desarrollar LSTM como apilado, bidireccional, CNN-LSTM, Encoder-Decoder seq2seq y más en mi nuevo libro, con 14 tutoriales paso a paso y código completo. Empecemos. ¿Necesita ayuda con LSTM para la predicción de secuencia? Tome mi curso gratuito por correo electrónico de 7 días y descubra 6 arquitecturas LSTM diferentes (con código). Haga clic para registrarse y también obtenga una versión gratuita en PDF del libro. ¡Comience su mini curso GRATIS ahora! Modelos generativos Los LSTM pueden usarse como modelo generativo. Dado un gran corpus de datos de secuencia, como documentos de texto, los modelos LSTM pueden diseñarse para aprender las propiedades estructurales generales del corpus, y cuando se les da una entrada de semilla, pueden generar nuevas secuencias que son representativas del corpus original. El problema de desarrollar un modelo para generalizar un corpus de texto se llama modelado del lenguaje en el campo del procesamiento del lenguaje natural. Un modelo de lenguaje puede funcionar a nivel de palabra y aprender las relaciones probabilísticas entre palabras en un documento para completar con precisión una oración y generar oraciones completamente nuevas. En su forma más desafiante, los modelos de lenguaje funcionan a nivel de personaje, aprenden de secuencias de caracteres y generan nuevas secuencias de un carácter a la vez. El objetivo del modelado de lenguaje a nivel de caracteres es predecir el siguiente carácter en una secuencia. - Generación de texto con redes neuronales recurrentes, 2011. Aunque es más desafiante, la flexibilidad adicional de un modelo a nivel de caracteres permite generar nuevas palabras, agregar signos de puntuación y generar cualquier otra estructura que pueda existir en los datos de texto. … Predecir un personaje a la vez es más interesante desde la perspectiva de la generación de secuencias, porque permite que la red invente nuevas palabras y cadenas. - Generación de secuencias con redes neuronales recurrentes, 2013. El modelado de lenguaje es, con mucho, la aplicación más estudiada de LSTM generativos, quizás debido al uso de conjuntos de datos estándar donde el rendimiento del modelo puede cuantificarse y compararse. Este enfoque se ha utilizado para generar texto en un conjunto de problemas de modelado de lenguaje interesantes, como: Generar artículos de Wikipedia (incluido el marcado). Generando fragmentos de grandes autores como Shakespeare. Generación de manuscritos técnicos (incluido el marcado). Generando código fuente de computadora. Generando titulares de artículos. La calidad de los resultados varía; por ejemplo, el marcado o el código fuente pueden requerir intervención manual para renderizar o compilar. Sin embargo, los resultados son impresionantes. El enfoque también se ha aplicado a diferentes dominios donde está disponible un gran corpus de información de secuencia existente y se pueden generar nuevas secuencias paso a paso, tales como: Generación de escritura a mano. Generación de música. Generación de discurso. Los LSTM generativos Un LSTM generativo no es realmente arquitectura, es más un cambio de perspectiva sobre lo que aprende un modelo predictivo de LSTM y cómo se usa el modelo. Podríamos utilizar cualquier arquitectura LSTM como modelo generativo. En este caso, utilizaremos un simple Vanilla LSTM. En el caso de un modelo de lenguaje a nivel de caracteres, el alfabeto de todos los caracteres posibles es fijo. Se utiliza una codificación única para aprender secuencias de entrada y predecir secuencias de salida. Se utiliza un modelo uno a uno donde se predice un paso para cada paso de tiempo de entrada. Esto significa que las secuencias de entrada pueden requerir un manejo especializado para ser vectorizadas o formateadas para entrenar eficientemente un modelo supervisado. Por ejemplo, dada la secuencia: &quot;hello world&quot; 1 &quot;hello world&quot; Sería necesario construir un conjunto de datos como: &#39;h&#39; =&gt; &#39;e&#39; &#39;e&#39; =&gt; &#39;l&#39; &#39;l&#39; =&gt; &#39;l&#39; ... 1 2 3 4 &#39;h&#39; =&gt; &#39;e&#39; &#39;e&#39; =&gt; &#39;l&#39; &#39;l&#39; =&gt; &#39;l&#39; ... Esto podría presentarse tal cual como un conjunto de datos de muestras de un solo paso, que podría ser bastante limitante para la red (por ejemplo, no BPTT). Alternativamente, se podría vectorizar a una secuencia de entrada de longitud fija para un modelo de paso de tiempo de muchos a uno, como: [&#39;h&#39;, &#39;e&#39;, &#39;l&#39;] =&gt; &#39;l&#39; [&#39;e&#39;, &#39;l&#39;, &#39;l&#39;] =&gt; &#39;o&#39; [&#39;l&#39;, &#39;l&#39;, &#39;o&#39;] =&gt; &#39;&#39; ... 1 2 3 4 [&#39;h&#39;, &#39;e&#39;, &#39;l&#39;] =&gt; &#39;l&#39; [&#39;e&#39;, &#39;l&#39;, &#39;l&#39;] =&gt; &#39;o&#39; [&#39;l&#39;, &#39;l&#39;, &#39;o&#39;] =&gt; &#39;&#39; ... O, una salida de longitud fija secuencia para un modelo de pasos de uno a muchos: &#39;h&#39; =&gt; [&#39;e&#39;, &#39;l&#39;, &#39;l&#39;] &#39;e&#39; =&gt; [&#39;l&#39;, &#39;l&#39;, &#39;o&#39;] &#39;l &#39;=&gt; [&#39; l &#39;,&#39; o &#39;,&#39; &#39;] ... 1 2 3 4&#39; h &#39;=&gt; [&#39; e &#39;,&#39; l &#39;,&#39; l &#39;]&#39; e &#39;=&gt; [&#39; l &#39;, &#39;l&#39;, &#39;o&#39;] &#39;l&#39; =&gt; [&#39;l&#39;, &#39;o&#39;, &#39;&#39;] ... O alguna variación en estos enfoques. Tenga en cuenta que se requeriría la misma representación vectorizada al hacer predicciones, lo que significa que los caracteres pronosticados deberían presentarse como entrada para muestras posteriores. Esto podría ser bastante torpe en la implementación. El estado interno de la red puede necesitar una administración cuidadosa, tal vez restablecer en ubicaciones elegidas en la secuencia de entrada (por ejemplo, al final del párrafo, página o capítulo) en lugar de al final de cada secuencia de entrada. Lecturas adicionales Esta sección proporciona más recursos sobre el tema si está buscando profundizar. Resumen de publicaciones de artículos En esta publicación, descubrió el uso de LSTM como modelos generativos. Específicamente, aprendiste: sobre modelos generativos, con un enfoque en modelos generativos para texto llamado modelado de lenguaje. Ejemplos de aplicaciones donde se han utilizado modelos LSTM Generative. Ejemplos de cómo modelar texto para modelos generativos con LSTM. ¿Tiene usted alguna pregunta? Haga sus preguntas en los comentarios a continuación y haré todo lo posible para responder. ¡Desarrolle LSTM para la predicción de secuencia hoy! Desarrolle sus propios modelos LSTM en minutos ... con solo unas pocas líneas de código de Python Descubra cómo en mi nuevo Ebook: Redes de memoria a corto plazo con Python Proporciona tutoriales de autoaprendizaje sobre temas como: CNN LSTM, Encoder-Decoder LSTM , modelos generativos, preparación de datos, hacer predicciones y mucho más ... Finalmente, lleve las redes neuronales recurrentes LSTM a sus proyectos de predicciones de secuencia Skip the Academics. Solo resultados. Mira lo que hay dentro"
19;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/;2019-05-12;How to Develop a CNN From Scratch for CIFAR-10 Photo Classification;"# baseline model with dropout and data augmentation on the cifar10 dataset

import sys

from matplotlib import pyplot

from keras . datasets import cifar10

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Conv2D

from keras . layers import MaxPooling2D

from keras . layers import Dense

from keras . layers import Flatten

from keras . optimizers import SGD

from keras . preprocessing . image import ImageDataGenerator

from keras . layers import Dropout

from keras . layers import BatchNormalization

# load train and test dataset

def load_dataset ( ) :

# load dataset

( trainX , trainY ) , ( testX , testY ) = cifar10 . load_data ( )

# one hot encode target values

trainY = to_categorical ( trainY )

testY = to_categorical ( testY )

return trainX , trainY , testX , testY

# scale pixels

def prep_pixels ( train , test ) :

# convert from integers to floats

train_norm = train . astype ( 'float32' )

test_norm = test . astype ( 'float32' )

# normalize to range 0-1

train_norm = train_norm / 255.0

test_norm = test_norm / 255.0

# return normalized images

return train_norm , test_norm

# define cnn model

def define_model ( ) :

model = Sequential ( )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 32 , 32 , 3 ) ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.2 ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.3 ) )

model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( BatchNormalization ( ) )

model . add ( Dropout ( 0.5 ) )

model . add ( Dense ( 10 , activation = 'softmax' ) )

# compile model

opt = SGD ( lr = 0.001 , momentum = 0.9 )

model . compile ( optimizer = opt , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )

return model

# plot diagnostic learning curves

def summarize_diagnostics ( history ) :

# plot loss

pyplot . subplot ( 211 )

pyplot . title ( 'Cross Entropy Loss' )

pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' )

# plot accuracy

pyplot . subplot ( 212 )

pyplot . title ( 'Classification Accuracy' )

pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' )

pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' )

# save plot to file

filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ]

pyplot . savefig ( filename + '_plot.png' )

pyplot . close ( )

# run the test harness for evaluating a model

def run_test_harness ( ) :

# load dataset

trainX , trainY , testX , testY = load_dataset ( )

# prepare pixel data

trainX , testX = prep_pixels ( trainX , testX )

# define model

model = define_model ( )

# create data generator

datagen = ImageDataGenerator ( width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True )

# prepare iterator

it_train = datagen . flow ( trainX , trainY , batch_size = 64 )

# fit model

steps = int ( trainX . shape [ 0 ] / 64 )

history = model . fit_generator ( it_train , steps_per_epoch = steps , epochs = 400 , validation_data = ( testX , testY ) , verbose = 0 )

# evaluate model

_ , acc = model . evaluate ( testX , testY , verbose = 0 )

print ( '> %.3f' % ( acc * 100.0 ) )

# learning curves

summarize_diagnostics ( history )

# entry point, run the test harness";Cómo desarrollar una CNN desde cero para la clasificación de fotos CIFAR-10;"# modelo de línea de base con abandono y aumento de datos en el conjunto de datos cifar10 import sys from matplotlib import pyplot from keras. los conjuntos de datos importan cifar10 desde keras. utils import to_categorical de keras. Los modelos importan secuenciales de keras. las capas importan Conv2D desde keras. las capas importan MaxPooling2D de keras. Las capas importan densas de keras. las capas importan Flatten de keras. Los optimizadores importan SGD de keras. preprocesamiento importación de imágenes ImageDataGenerator desde keras. Las capas importan el abandono de keras. las capas importan BatchNormalization # load train y test dataset def load_dataset (): # load dataset (trainX, trainY), (testX, testY) = cifar10. load_data () # uno valores de codificación en caliente de destino trainY = to_categorical (trainY) testY = to_categorical (testY) return trainX, trainY, testX, testY # escala píxeles def prep_pixels (train, test): # convertir de enteros a flotadores train_norm = train. astype (&#39;float32&#39;) test_norm = prueba. astype (&#39;float32&#39;) # normalize to range 0-1 train_norm = train_norm / 255.0 test_norm = test_norm / 255.0 # return normalized images return train_norm, test_norm # define cnn model def define_model (): model = Sequential () model. modelo add (Conv2D (32, (3, 3), activación = &#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;, padding = &#39;same&#39;, input_shape = (32, 32, 3)))). modelo add (BatchNormalization ()). modelo add (Conv2D (32, (3, 3), activación = &#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;, padding = &#39;same&#39;)). modelo add (BatchNormalization ()). Añadir modelo (MaxPooling2D ((2, 2))). agregar modelo (Dropout (0.2)). modelo add (Conv2D (64, (3, 3), activación = &#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;, padding = &#39;same&#39;)). modelo add (BatchNormalization ()). modelo add (Conv2D (64, (3, 3), activación = &#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;, padding = &#39;same&#39;)). modelo add (BatchNormalization ()). Añadir modelo (MaxPooling2D ((2, 2))). Añadir modelo (Dropout (0.3)). modelo add (Conv2D (128, (3, 3), activación = &#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;, padding = &#39;same&#39;)). modelo add (BatchNormalization ()). modelo add (Conv2D (128, (3, 3), activación = &#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;, padding = &#39;same&#39;)). modelo add (BatchNormalization ()). Añadir modelo (MaxPooling2D ((2, 2))). agregar modelo (Dropout (0.4)). agregar modelo (Flatten ()). modelo add (Dense (128, activación = &#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;)). modelo add (BatchNormalization ()). Añadir modelo (Dropout (0.5)). add (Dense (10, activación = &#39;softmax&#39;)) # compilar modelo opt = SGD (lr = 0.001, momentum = 0.9) modelo. compilar (optimizador = opt, loss = &#39;categorical_crossentropy&#39;, metrics = [&#39;precision&#39;]) devuelve el modelo # plot diagnóstico curvas de aprendizaje def summaryize_diagnostics (history): # plot loss pyplot. subplot (211) pyplot. Título (&#39;Pérdida de entropía cruzada&#39;). plot (history. history [&#39;loss&#39;], color = &#39;blue&#39;, label = &#39;train&#39;) pyplot. plot (history. history [&#39;val_loss&#39;], color = &#39;orange&#39;, label = &#39;test&#39;) # plot precision pyplot. subplot (212) pyplot. título (&#39;Precisión de clasificación&#39;) pyplot. plot (history. history [&#39;precision&#39;], color = &#39;blue&#39;, label = &#39;train&#39;) pyplot. plot (history. history [&#39;val_accuracy&#39;], color = &#39;orange&#39;, label = &#39;test&#39;) # guardar plot en el archivo filename = sys. argv [0]. split (&#39;/&#39;) [- 1] pyplot. savefig (nombre de archivo + &#39;_plot.png&#39;) pyplot. close () # ejecuta el arnés de prueba para evaluar un modelo def run_test_harness (): # carga el conjunto de datos trainX, trainY, testX, testY = load_dataset () # prepara los datos de píxeles trainX, testX = prep_pixels (trainX, testX) # define el modelo del modelo = define_model () # crear generador de datos datagen = ImageDataGenerator (width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True) # prepare iterator it_train = datagen. flow (trainX, trainY, batch_size = 64) # fit model steps = int (trainX. shape [0] / 64) history = model. fit_generator (it_train, steps_per_epoch = steps, epochs = 400, validation_data = (testX, testY), verbose = 0) # evaluar modelo _, acc = modelo. evaluar (testX, testY, detallado = 0) imprimir (&#39;&gt;% .3f&#39;% (acc * 100.0)) # curvas de aprendizaje summaryize_diagnostics (historial) # punto de entrada, ejecutar el arnés de prueba"
20;news.mit.edu;http://news.mit.edu/2020/engineers-3d-print-brain-implants-0330;;Engineers 3D print soft, rubbery brain implants;"The brain is one of our most vulnerable organs, as soft as the softest tofu. Brain implants, on the other hand, are typically made from metal and other rigid materials that over time can cause inflammation and the buildup of scar tissue.

MIT engineers are working on developing soft, flexible neural implants that can gently conform to the brain’s contours and monitor activity over longer periods, without aggravating surrounding tissue. Such flexible electronics could be softer alternatives to existing metal-based electrodes designed to monitor brain activity, and may also be useful in brain implants that stimulate neural regions to ease symptoms of epilepsy, Parkinson’s disease, and severe depression.

Led by Xuanhe Zhao, a professor of mechanical engineering and of civil and environmental engineering, the research team has now developed a way to 3D print neural probes and other electronic devices that are as soft and flexible as rubber.

The devices are made from a type of polymer, or soft plastic, that is electrically conductive. The team transformed this normally liquid-like conducting polymer solution into a substance more like viscous toothpaste — which they could then feed through a conventional 3D printer to make stable, electrically conductive patterns.

The team printed several soft electronic devices, including a small, rubbery electrode, which they implanted in the brain of a mouse. As the mouse moved freely in a controlled environment, the neural probe was able to pick up on the activity from a single neuron. Monitoring this activity can give scientists a higher-resolution picture of the brain’s activity, and can help in tailoring therapies and long-term brain implants for a variety of neurological disorders.

“We hope by demonstrating this proof of concept, people can use this technology to make different devices, quickly,” says Hyunwoo Yuk, a graduate student in Zhao’s group at MIT. “They can change the design, run the printing code, and generate a new design in 30 minutes. Hopefully this will streamline the development of neural interfaces, fully made of soft materials.”

Yuk and Zhao have published their results today in the journal Nature Communications. Their co-authors include Baoyang Lu and Jingkun Xu of the Jiangxi Science and Technology Normal University, along with Shen Lin and Jianhong Luo of Zheijiang University’s School of Medicine.

The team printed several soft electronic devices, including a small, rubbery electrode.

From soap water to toothpaste

Conducting polymers are a class of materials that scientists have eagerly explored in recent years for their unique combination of plastic-like flexibility and metal-like electrical conductivity. Conducting polymers are used commercially as antistatic coatings, as they can effectively carry away any electrostatic charges that build up on electronics and other static-prone surfaces.

“These polymer solutions are easy to spray on electrical devices like touchscreens,” Yuk says. “But the liquid form is mostly for homogenous coatings, and it’s difficult to use this for any two-dimensional, high-resolution patterning. In 3D, it’s impossible.”

Yuk and his colleagues reasoned that if they could develop a printable conducting polymer, they could then use the material to print a host of soft, intricately patterned electronic devices, such as flexible circuits, and single-neuron electrodes.

In their new study, the team report modifying poly (3,4-ethylenedioxythiophene) polystyrene sulfonate, or PEDOT:PSS, a conducting polymer typically supplied in the form of an inky, dark-blue liquid. The liquid is a mixture of water and nanofibers of PEDOT:PSS. The liquid gets its conductivity from these nanofibers, which, when they come in contact, act as a sort of tunnel through which any electrical charge can flow.

If the researchers were to feed this polymer into a 3D printer in its liquid form, it would simply bleed across the underlying surface. So the team looked for a way to thicken the polymer while retaining the material’s inherent electrical conductivity.

They first freeze-dried the material, removing the liquid and leaving behind a dry matrix, or sponge, of nanofibers. Left alone, these nanofibers would become brittle and crack. So the researchers then remixed the nanofibers with a solution of water and an organic solvent, which they had previously developed, to form a hydrogel — a water-based, rubbery material embedded with nanofibers.

They made hydrogels with various concentrations of nanofibers, and found that a range between 5 to 8 percent by weight of nanofibers produced a toothpaste-like material that was both electrically conductive and suitable for feeding into a 3D printer.

“Initially, it’s like soap water,” Zhao says. “We condense the nanofibers and make it viscous like toothpaste, so we can squeeze it out as a thick, printable liquid.”

Implants on demand

The researchers fed the new conducting polymer into a conventional 3D printer and found they could produce intricate patterns that remained stable and electrically conductive.

As a proof of concept, they printed a small, rubbery electrode, about the size of a piece of confetti. The electrode consists of a layer of flexible, transparent polymer, over which they then printed the conducting polymer, in thin, parallel lines that converged at a tip, measuring about 10 microns wide — small enough to pick up electrical signals from a single neuron.

MIT researchers print flexible circuits (shown here) and other soft electrical devices using new 3-D-printing technique and conducting polymer ink.

The team implanted the electrode in the brain of a mouse and found it could pick up electrical signals from a single neuron.

“Traditionally, electrodes are rigid metal wires, and once there are vibrations, these metal electrodes could damage tissue,” Zhao says. “We’ve shown now that you could insert a gel probe instead of a needle.”

In principle, such soft, hydrogel-based electrodes might even be more sensitive than conventional metal electrodes. That’s because most metal electrodes conduct electricity in the form of electrons, whereas neurons in the brain produce electrical signals in the form of ions. Any ionic current produced by the brain needs to be converted into an electrical signal that a metal electrode can register — a conversion that can result in some part of the signal getting lost in translation. What’s more, ions can only interact with a metal electrode at its surface, which can limit the concentration of ions that the electrode can detect at any given time.

In contrast, the team’s soft electrode is made from electron-conducting nanofibers, embedded in a hydrogel — a water-based material that ions can freely pass through.

“The beauty of a conducting polymer hydrogel is, on top of its soft mechanical properties, it is made of hydrogel, which is ionically conductive, and also a porous sponge of nanofibers, which the ions can flow in and out of,” Lu says. “Because the electrode’s whole volume is active, its sensitivity is enhanced.”

In addition to the neural probe, the team also fabricated a multielectrode array — a small, Post-it-sized square of plastic, printed with very thin electrodes, over which the researchers also printed a round plastic well. Neuroscientists typically fill the wells of such arrays with cultured neurons, and can study their activity through the signals that are detected by the device’s underlying electrodes.

For this demonstration, the group showed they could replicate the complex designs of such arrays using 3D printing, versus traditional lithography techniques, which

involve carefully etching metals, such as gold, into prescribed patterns, or masks — a process that can take days to complete a single device.

“We make the same geometry and resolution of this device using 3D printing, in less than an hour,” Yuk says. “This process may replace or supplement lithography techniques, as a simpler and cheaper way to make a variety of neurological devices, on demand.”";Los ingenieros imprimen en 3D implantes cerebrales suaves y gomosos;"El cerebro es uno de nuestros órganos más vulnerables, tan suave como el tofu más suave. Los implantes cerebrales, por otro lado, generalmente están hechos de metal y otros materiales rígidos que con el tiempo pueden causar inflamación y la acumulación de tejido cicatricial. Los ingenieros del MIT están trabajando en el desarrollo de implantes neuronales suaves y flexibles que pueden ajustarse suavemente a los contornos del cerebro y controlar la actividad durante períodos más largos, sin agravar el tejido circundante. Tales dispositivos electrónicos flexibles podrían ser alternativas más suaves a los electrodos existentes basados en metales diseñados para monitorear la actividad cerebral, y también pueden ser útiles en implantes cerebrales que estimulan las regiones neurales para aliviar los síntomas de epilepsia, enfermedad de Parkinson y depresión severa. Dirigido por Xuanhe Zhao, profesor de ingeniería mecánica y de ingeniería civil y ambiental, el equipo de investigación ha desarrollado una forma de imprimir en 3D sondas neuronales y otros dispositivos electrónicos que son tan suaves y flexibles como el caucho. Los dispositivos están hechos de un tipo de polímero, o plástico blando, que es eléctricamente conductor. El equipo transformó esta solución de polímero conductor normalmente líquido en una sustancia más parecida a una pasta de dientes viscosa, que luego podrían alimentar a través de una impresora 3D convencional para crear patrones estables y eléctricamente conductores. El equipo imprimió varios dispositivos electrónicos blandos, incluido un pequeño electrodo de goma, que implantaron en el cerebro de un ratón. A medida que el ratón se movía libremente en un entorno controlado, la sonda neural pudo detectar la actividad de una sola neurona. El monitoreo de esta actividad puede brindar a los científicos una imagen de mayor resolución de la actividad del cerebro, y puede ayudar a adaptar las terapias y los implantes cerebrales a largo plazo para una variedad de trastornos neurológicos. &quot;Esperamos que al demostrar esta prueba de concepto, las personas puedan usar esta tecnología para hacer diferentes dispositivos, rápidamente&quot;, dice Hyunwoo Yuk, un estudiante graduado en el grupo de Zhao en el MIT. “Pueden cambiar el diseño, ejecutar el código de impresión y generar un nuevo diseño en 30 minutos. Esperemos que esto agilice el desarrollo de interfaces neuronales, totalmente hechas de materiales blandos &quot;. Yuk y Zhao han publicado sus resultados hoy en la revista Nature Communications. Sus coautores incluyen a Baoyang Lu y Jingkun Xu de la Universidad Normal de Ciencia y Tecnología de Jiangxi, junto con Shen Lin y Jianhong Luo de la Facultad de Medicina de la Universidad de Zheijiang. El equipo imprimió varios dispositivos electrónicos blandos, incluido un pequeño electrodo de goma. Desde agua jabonosa hasta pasta de dientes Los polímeros conductores son una clase de materiales que los científicos han explorado con entusiasmo en los últimos años por su combinación única de flexibilidad plástica y conductividad eléctrica metálica. Los polímeros conductores se utilizan comercialmente como recubrimientos antiestáticos, ya que pueden eliminar eficazmente cualquier carga electrostática que se acumule en la electrónica y otras superficies propensas a la electricidad estática. &quot;Estas soluciones de polímeros son fáciles de rociar en dispositivos eléctricos como pantallas táctiles&quot;, dice Yuk. “Pero la forma líquida es principalmente para recubrimientos homogéneos, y es difícil usar esto para cualquier diseño bidimensional de alta resolución. En 3D, es imposible &quot;. Yuk y sus colegas razonaron que si pudieran desarrollar un polímero conductor imprimible, podrían usar el material para imprimir una gran cantidad de dispositivos electrónicos blandos e intrincadamente diseñados, como circuitos flexibles y electrodos de neurona única. En su nuevo estudio, el equipo informa que modifica el poli (3,4-etilendioxitiofeno) sulfonato de poliestireno o PEDOT: PSS, un polímero conductor que se suministra típicamente en forma de un líquido azul oscuro. El líquido es una mezcla de agua y nanofibras de PEDOT: PSS. El líquido obtiene su conductividad de estas nanofibras que, cuando entran en contacto, actúan como una especie de túnel a través del cual puede fluir cualquier carga eléctrica. Si los investigadores alimentaran este polímero en una impresora 3D en su forma líquida, simplemente sangraría por la superficie subyacente. Entonces, el equipo buscó una forma de espesar el polímero mientras se conserva la conductividad eléctrica inherente del material. Primero liofilizaron el material, eliminaron el líquido y dejaron una matriz seca o esponja de nanofibras. Dejados solos, estas nanofibras se volverían quebradizas y se agrietarían. Entonces, los investigadores remezclaron las nanofibras con una solución de agua y un solvente orgánico, que habían desarrollado previamente, para formar un hidrogel, un material gomoso a base de agua incrustado con nanofibras. Hicieron hidrogeles con varias concentraciones de nanofibras, y descubrieron que un rango entre 5 y 8 por ciento en peso de nanofibras producía un material similar a la pasta de dientes que era eléctricamente conductor y adecuado para alimentar a una impresora 3D. &quot;Inicialmente, es como agua jabonosa&quot;, dice Zhao. &quot;Condensamos las nanofibras y las hacemos viscosas como pasta de dientes, para que podamos exprimirlas como un líquido espeso e imprimible&quot;. Implantes bajo demanda Los investigadores introdujeron el nuevo polímero conductor en una impresora 3D convencional y descubrieron que podían producir patrones intrincados que permanecían estables y eléctricamente conductores. Como prueba de concepto, imprimieron un pequeño electrodo de goma, del tamaño de una pieza de confeti. El electrodo consiste en una capa de polímero flexible y transparente, sobre el cual imprimen el polímero conductor, en líneas finas y paralelas que convergen en una punta, que mide aproximadamente 10 micras de ancho, lo suficientemente pequeño como para captar señales eléctricas de una sola neurona. Los investigadores del MIT imprimen circuitos flexibles (que se muestran aquí) y otros dispositivos eléctricos blandos utilizando la nueva técnica de impresión en 3-D y conduciendo tinta polimérica. El equipo implantó el electrodo en el cerebro de un ratón y descubrió que podía captar señales eléctricas de una sola neurona. &quot;Tradicionalmente, los electrodos son cables de metal rígidos, y una vez que hay vibraciones, estos electrodos de metal podrían dañar el tejido&quot;, dice Zhao. &quot;Hemos demostrado ahora que puede insertar una sonda de gel en lugar de una aguja&quot;. En principio, tales electrodos suaves basados en hidrogel podrían incluso ser más sensibles que los electrodos metálicos convencionales. Esto se debe a que la mayoría de los electrodos metálicos conducen electricidad en forma de electrones, mientras que las neuronas en el cerebro producen señales eléctricas en forma de iones. Cualquier corriente iónica producida por el cerebro debe convertirse en una señal eléctrica que un electrodo de metal pueda registrar, una conversión que puede provocar que parte de la señal se pierda en la traducción. Además, los iones solo pueden interactuar con un electrodo metálico en su superficie, lo que puede limitar la concentración de iones que el electrodo puede detectar en un momento dado. En contraste, el electrodo blando del equipo está hecho de nanofibras conductoras de electrones, incrustadas en un hidrogel, un material a base de agua que los iones pueden atravesar libremente. &quot;La belleza de un hidrogel de polímero conductor es que, además de sus propiedades mecánicas blandas, está hecho de hidrogel, que es iónicamente conductor, y también una esponja porosa de nanofibras, de la cual los iones pueden fluir dentro y fuera&quot;, dice Lu . &quot;Debido a que todo el volumen del electrodo está activo, se mejora su sensibilidad&quot;. Además de la sonda neural, el equipo también fabricó una matriz multielectrodo: un pequeño cuadrado de plástico de tamaño Post-it, impreso con electrodos muy finos, sobre el cual los investigadores también imprimieron un pozo redondo de plástico. Los neurocientíficos típicamente llenan los pozos de tales matrices con neuronas cultivadas, y pueden estudiar su actividad a través de las señales que son detectadas por los electrodos subyacentes del dispositivo. Para esta demostración, el grupo demostró que podían replicar los diseños complejos de tales matrices utilizando la impresión 3D, en comparación con las técnicas de litografía tradicionales, que implican grabar metales cuidadosamente, como el oro, en patrones prescritos o máscaras, un proceso que puede llevar días completar. Un solo dispositivo. &quot;Hacemos la misma geometría y resolución de este dispositivo mediante la impresión 3D, en menos de una hora&quot;, dice Yuk. &quot;Este proceso puede reemplazar o complementar las técnicas de litografía, como una forma más simple y económica de fabricar una variedad de dispositivos neurológicos, bajo demanda&quot;."
21;news.mit.edu;http://news.mit.edu/2020/warning-labels-fake-news-trustworthy-0303;;The catch to putting warning labels on fake news;"After the 2016 U.S. presidential election, Facebook began putting warning tags on news stories fact-checkers judged to be false. But there’s a catch: Tagging some stories as false makes readers more willing to believe other stories and share them with friends, even if those additional, untagged stories also turn out to be false.

That is the main finding of a new study co-authored by an MIT professor, based on multiple experiments with news consumers. The researchers call this unintended consequence — in which the selective labeling of false news makes other news stories seem more legitimate — the “implied-truth effect” in news consumption.

“Putting a warning on some content is going to make you think, to some extent, that all of the other content without the warning might have been checked and verified,” says David Rand, the Erwin H. Schell Professor at the MIT Sloan School of Management and co-author of a newly published paper detailing the study.

“There’s no way the fact-checkers can keep up with the stream of misinformation, so even if the warnings do really reduce belief in the tagged stories, you still have a problem, because of the implied truth effect,” Rand adds.

Moreover, Rand observes, the implied truth effect “is actually perfectly rational” on the part of readers, since there is ambiguity about whether untagged stories were verified or just not yet checked. “That makes these warnings potentially problematic,” he says. “Because people will reasonably make this inference.”

Even so, the findings also suggest a solution: Placing “Verified” tags on stories found to be true eliminates the problem.

The paper, “The Implied Truth Effect,” has just appeared in online form in the journal Management Science. In addition to Rand, the authors are Gordon Pennycook, an assistant professor of psychology at the University of Regina; Adam Bear, a postdoc in the Cushman Lab at Harvard University; and Evan T. Collins, an undergraduate researcher on the project from Yale University.

BREAKING: More labels are better

To conduct the study, the researchers conducted a pair of online experiments with a total of 6,739 U.S. residents, recruited via Amazon’s Mechanical Turk platform. Participants were given a variety of true and false news headlines in a Facebook-style format. The false stories were chosen from the website Snopes.com and included headlines such as “BREAKING NEWS: Hillary Clinton Filed for Divorce in New York Courts” and “Republican Senator Unveils Plan To Send All Of America’s Teachers Through A Marine Bootcamp.”

The participants viewed an equal mix of true stories and false stories, and were asked whether they would consider sharing each story on social media. Some participants were assigned to a control group in which no stories were labeled; others saw a set of stories where some of the false ones displayed a “FALSE” label; and some participants saw a set of stories with warning labels on some false stories and “TRUE” verification labels for some true stories.

In the first place, stamping warnings on false stories does make people less likely to consider sharing them. For instance, with no labels being used at all, participants considered sharing 29.8 percent of false stories in the sample. That figure dropped to 16.1 percent of false stories that had a warning label attached.

However, the researchers also saw the implied truth effect take effect. Readers were willing to share 36.2 percent of the remaining false stories that did not have warning labels, up from 29.8 percent.

“We robustly observe this implied-truth effect, where if false content doesn’t have a warning, people believe it more and say they would be more likely to share it,” Rand notes.

But when the warning labels on some false stories were complemented with verification labels on some of the true stories, participants were less likely to consider sharing false stories, across the board. In those circumstances, they shared only 13.7 percent of the headlines labeled as false, and just 26.9 percent of the nonlabeled false stories.

“If, in addition to putting warnings on things fact-checkers find to be false, you also put verification panels on things fact-checkers find to be true, then that solves the problem, because there’s no longer any ambiguity,” Rand says. “If you see a story without a label, you know it simply hasn’t been checked.”

Policy implications

The findings come with one additional twist that Rand emphasizes, namely, that participants in the survey did not seem to reject warnings on the basis of ideology. They were still likely to change their perceptions of stories with warning or verifications labels, even if discredited news items were “concordant” with their stated political views.

“These results are not consistent with the idea that our reasoning powers are hijacked by our partisanship,” Rand says.

Rand notes that, while continued research on the subject is important, the current study suggests a straightforward way that social media platforms can take action to further improve their systems of labeling online news content.

“I think this has clear policy implications when platforms are thinking about attaching warnings,” he says. “They should be very careful to check not just the effect of the warnings on the content with the tag, but also check the effects on all the other content.”

Support for the research was provided, in part, by the Ethics and Governance of Artificial Intelligence Initiative of the Miami Foundation, and the Social Sciences and Humanities Research Council of Canada.";La trampa de poner etiquetas de advertencia en noticias falsas;"Después de las elecciones presidenciales estadounidenses de 2016, Facebook comenzó a poner etiquetas de advertencia en las noticias que los verificadores de hechos juzgaban falsos. Pero hay un inconveniente: etiquetar algunas historias como falsas hace que los lectores estén más dispuestos a creer otras historias y compartirlas con amigos, incluso si esas historias adicionales sin etiquetar también resultan ser falsas. Ese es el hallazgo principal de un nuevo estudio en coautoría de un profesor del MIT, basado en múltiples experimentos con consumidores de noticias. Los investigadores llaman a esta consecuencia involuntaria, en la que el etiquetado selectivo de noticias falsas hace que otras noticias parezcan más legítimas, el &quot;efecto de verdad implícita&quot; en el consumo de noticias. &quot;Poner una advertencia sobre algún contenido te hará pensar, hasta cierto punto, que todo el otro contenido sin la advertencia podría haber sido verificado y verificado&quot;, dice David Rand, el profesor Erwin H. Schell de la Escuela MIT Sloan. de Gestión y coautor de un artículo recientemente publicado que detalla el estudio. &quot;No hay forma de que los verificadores de datos puedan mantenerse al día con la corriente de información errónea, por lo que incluso si las advertencias realmente reducen la creencia en las historias etiquetadas, todavía tiene un problema, debido al efecto de verdad implícito&quot;, agrega Rand. Además, observa Rand, el efecto de verdad implícito &quot;es en realidad perfectamente racional&quot; por parte de los lectores, ya que existe una ambigüedad sobre si las historias sin etiquetar se verificaron o no todavía. &quot;Eso hace que estas advertencias sean potencialmente problemáticas&quot;, dice. &quot;Porque la gente razonablemente hará esta inferencia&quot;. Aun así, los hallazgos también sugieren una solución: colocar etiquetas &quot;verificadas&quot; en las historias que se consideran verdaderas elimina el problema. El artículo, &quot;El efecto de la verdad implícita&quot;, acaba de aparecer en línea en la revista Management Science. Además de Rand, los autores son Gordon Pennycook, profesor asistente de psicología en la Universidad de Regina; Adam Bear, un postdoc en el Cushman Lab de la Universidad de Harvard; y Evan T. Collins, investigador universitario sobre el proyecto de la Universidad de Yale. ROMPIENDO: Más etiquetas son mejores Para realizar el estudio, los investigadores realizaron un par de experimentos en línea con un total de 6,739 residentes de EE. UU., Reclutados a través de la plataforma Mechanical Turk de Amazon. Los participantes recibieron una variedad de titulares de noticias verdaderas y falsas en un formato estilo Facebook. Las historias falsas fueron elegidas del sitio web Snopes.com e incluyeron titulares como &quot;NOTICIAS: Hillary Clinton solicitó el divorcio en los tribunales de Nueva York&quot; y &quot;El senador republicano revela el plan para enviar a todos los docentes de Estados Unidos a través de un campamento militar&quot;. Los participantes vieron una mezcla igual de historias verdaderas e historias falsas, y se les preguntó si considerarían compartir cada historia en las redes sociales. Algunos participantes fueron asignados a un grupo de control en el que no se etiquetaron historias; otros vieron un conjunto de historias donde algunos de los falsos mostraban una etiqueta de &quot;FALSO&quot;; y algunos participantes vieron un conjunto de historias con etiquetas de advertencia en algunas historias falsas y etiquetas de verificación &quot;VERDADERAS&quot; para algunas historias verdaderas. En primer lugar, estampar advertencias en historias falsas hace que las personas sean menos propensas a considerar compartirlas. Por ejemplo, sin usar etiquetas, los participantes consideraron compartir el 29.8 por ciento de las historias falsas en la muestra. Esa cifra cayó al 16.1 por ciento de las historias falsas que tenían una etiqueta de advertencia adjunta. Sin embargo, los investigadores también vieron el efecto de verdad implícito en efecto. Los lectores estaban dispuestos a compartir el 36.2 por ciento de las historias falsas restantes que no tenían etiquetas de advertencia, en comparación con el 29.8 por ciento. &quot;Observamos con firmeza este efecto de verdad implícita, donde si el contenido falso no tiene una advertencia, la gente lo cree más y dice que sería más probable que lo compartiera&quot;, señala Rand. Pero cuando las etiquetas de advertencia en algunas historias falsas se complementaron con etiquetas de verificación en algunas de las historias verdaderas, era menos probable que los participantes consideraran compartir historias falsas en todos los ámbitos. En esas circunstancias, compartieron solo el 13.7 por ciento de los titulares etiquetados como falsos, y solo el 26.9 por ciento de las historias falsas no etiquetadas. &quot;Si, además de poner advertencias sobre las cosas que los verificadores de datos encuentran falsas, también se colocan paneles de verificación en las cosas que los verificadores de hechos son verdaderas, entonces eso resuelve el problema, porque ya no hay ambigüedad&quot;, dice Rand. &quot;Si ves una historia sin una etiqueta, sabes que simplemente no ha sido revisada&quot;. Implicaciones políticas Los hallazgos vienen con un giro adicional que Rand enfatiza, a saber, que los participantes en la encuesta no parecían rechazar las advertencias sobre la base de la ideología. Aún era probable que cambiaran sus percepciones de las historias con etiquetas de advertencia o verificación, incluso si las noticias desacreditadas eran &quot;concordantes&quot; con sus puntos de vista políticos declarados. &quot;Estos resultados no son consistentes con la idea de que nuestros poderes de razonamiento son secuestrados por nuestro partidismo&quot;, dice Rand. Rand señala que, si bien la investigación continua sobre el tema es importante, el estudio actual sugiere una forma directa en que las plataformas de redes sociales pueden tomar medidas para mejorar aún más sus sistemas de etiquetado de contenido de noticias en línea. &quot;Creo que esto tiene implicaciones políticas claras cuando las plataformas están pensando en adjuntar advertencias&quot;, dice. &quot;Deben tener mucho cuidado de verificar no solo el efecto de las advertencias en el contenido con la etiqueta, sino también verificar los efectos en el resto del contenido&quot;. El apoyo a la investigación fue proporcionado, en parte, por la Iniciativa de Ética y Gobernanza de la Inteligencia Artificial de la Fundación Miami, y el Consejo de Investigación de Ciencias Sociales y Humanidades de Canadá."
22;machinelearningmastery.com;https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/;2018-12-09;Use Early Stopping to Halt the Training of Neural Networks At the Right Time;"# generate two moons dataset

from sklearn . datasets import make_moons

from matplotlib import pyplot

from pandas import DataFrame

# generate 2d classification dataset

X , y = make_moons ( n_samples = 100 , noise = 0.2 , random_state = 1 )

# scatter plot, dots colored by class value

df = DataFrame ( dict ( x = X [ : , 0 ] , y = X [ : , 1 ] , label = y ) )

colors = { 0 : 'red' , 1 : 'blue' }

fig , ax = pyplot . subplots ( )

grouped = df . groupby ( 'label' )

for key , group in grouped :

group . plot ( ax = ax , kind = 'scatter' , x = 'x' , y = 'y' , label = key , color = colors [ key ] )";Use la detención temprana para detener el entrenamiento de las redes neuronales en el momento adecuado;"# genera dos lunas dataset de sklearn. conjuntos de datos importan make_moons de matplotlib import pyplot de pandas import DataFrame # Genera dataset de clasificación 2D X, y = make_moons (n_samples = 100, noise = 0.2, random_state = 1) # diagrama de dispersión, puntos coloreados por valor de clase df = DataFrame (dict (x = X [:, 0], y = X [:, 1], etiqueta = y)) colores = {0: &#39;rojo&#39;, 1: &#39;azul&#39;} fig, ax = pyplot. subplots () agrupados = df. groupby (&#39;etiqueta&#39;) para clave, grupo en agrupado: grupo. plot (ax = ax, kind = &#39;dispersión&#39;, x = &#39;x&#39;, y = &#39;y&#39;, etiqueta = clave, color = colores [clave])"
23;machinelearningmastery.com;http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/;2016-03-20;Overfitting and Underfitting With Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

The cause of poor performance in machine learning is either overfitting or underfitting the data.

In this post, you will discover the concept of generalization in machine learning and the problems of overfitting and underfitting that go along with it.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Approximate a Target Function in Machine Learning

Supervised machine learning is best understood as approximating a target function (f) that maps input variables (X) to an output variable (Y).

Y = f(X)

This characterization describes the range of classification and prediction problems and the machine algorithms that can be used to address them.

An important consideration in learning the target function from the training data is how well the model generalizes to new data. Generalization is important because the data we collect is only a sample, it is incomplete and noisy.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Generalization in Machine Learning

In machine learning we describe the learning of the target function from training data as inductive learning.

Induction refers to learning general concepts from specific examples which is exactly the problem that supervised machine learning problems aim to solve. This is different from deduction that is the other way around and seeks to learn specific concepts from general rules.

Generalization refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning.

The goal of a good machine learning model is to generalize well from the training data to any data from the problem domain. This allows us to make predictions in the future on data the model has never seen.

There is a terminology used in machine learning when we talk about how well a machine learning model learns and generalizes to new data, namely overfitting and underfitting.

Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms.

Statistical Fit

In statistics, a fit refers to how well you approximate a target function.

This is good terminology to use in machine learning, because supervised machine learning algorithms seek to approximate the unknown underlying mapping function for the output variables given the input variables.

Statistics often describe the goodness of fit which refers to measures used to estimate how well the approximation of the function matches the target function.

Some of these methods are useful in machine learning (e.g. calculating the residual errors), but some of these techniques assume we know the form of the target function we are approximating, which is not the case in machine learning.

If we knew the form of the target function, we would use it directly to make predictions, rather than trying to learn an approximation from samples of noisy training data.

Overfitting in Machine Learning

Overfitting refers to a model that models the training data too well.

Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.

Overfitting is more likely with nonparametric and nonlinear models that have more flexibility when learning a target function. As such, many nonparametric machine learning algorithms also include parameters or techniques to limit and constrain how much detail the model learns.

For example, decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up.

Underfitting in Machine Learning

Underfitting refers to a model that can neither model the training data nor generalize to new data.

An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.

Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.

A Good Fit in Machine Learning

Ideally, you want to select a model at the sweet spot between underfitting and overfitting.

This is the goal, but is very difficult to do in practice.

To understand this goal, we can look at the performance of a machine learning algorithm over time as it is learning a training data. We can plot both the skill on the training data and the skill on a test dataset we have held back from the training process.

Over time, as the algorithm learns, the error for the model on the training data goes down and so does the error on the test dataset. If we train for too long, the performance on the training dataset may continue to decrease because the model is overfitting and learning the irrelevant detail and noise in the training dataset. At the same time the error for the test set starts to rise again as the model’s ability to generalize decreases.

The sweet spot is the point just before the error on the test dataset starts to increase where the model has good skill on both the training dataset and the unseen test dataset.

You can perform this experiment with your favorite machine learning algorithms. This is often not useful technique in practice, because by choosing the stopping point for training using the skill on the test dataset it means that the testset is no longer “unseen” or a standalone objective measure. Some knowledge (a lot of useful knowledge) about that data has leaked into the training procedure.

There are two additional techniques you can use to help find the sweet spot in practice: resampling methods and a validation dataset.

How To Limit Overfitting

Both overfitting and underfitting can lead to poor model performance. But by far the most common problem in applied machine learning is overfitting.

Overfitting is such a problem because the evaluation of machine learning algorithms on training data is different from the evaluation we actually care the most about, namely how well the algorithm performs on unseen data.

There are two important techniques that you can use when evaluating machine learning algorithms to limit overfitting:

Use a resampling technique to estimate model accuracy. Hold back a validation dataset.

The most popular resampling technique is k-fold cross validation. It allows you to train and test your model k-times on different subsets of training data and build up an estimate of the performance of a machine learning model on unseen data.

A validation dataset is simply a subset of your training data that you hold back from your machine learning algorithms until the very end of your project. After you have selected and tuned your machine learning algorithms on your training dataset you can evaluate the learned models on the validation dataset to get a final objective idea of how the models might perform on unseen data.

Using cross validation is a gold standard in applied machine learning for estimating model accuracy on unseen data. If you have the data, using a validation dataset is also an excellent practice.

Further Reading

This section lists some recommended resources if you are looking to learn more about generalization, overfitting and underfitting in machine learning.

Summary

In this post, you discovered that machine learning is solving problems by the method of induction.

You learned that generalization is a description of how well the concepts learned by a model apply to new data. Finally, you learned about the terminology of generalization in machine learning of overfitting and underfitting:

Overfitting : Good performance on the training data, poor generliazation to other data.

: Good performance on the training data, poor generliazation to other data. Underfitting: Poor performance on the training data and poor generalization to other data

Do you have any questions about overfitting, underfitting or this post? Leave a comment and ask your question and I will do my best to answer it.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside";Overfitting y Underfitting With Machine Learning Algorithms;"Tweet Compartir Compartir Última actualización el 12 de agosto de 2019 La causa del bajo rendimiento en el aprendizaje automático es sobreajustar o subadaptar los datos. En esta publicación, descubrirá el concepto de generalización en el aprendizaje automático y los problemas de sobreajuste y subadaptación que lo acompañan. Descubra cómo funcionan los algoritmos de aprendizaje automático, incluidos kNN, árboles de decisión, ingenuos bayes, SVM, conjuntos y mucho más en mi nuevo libro, con 22 tutoriales y ejemplos en Excel. Empecemos. Aproximación de una función objetivo en el aprendizaje automático El aprendizaje automático supervisado se entiende mejor como una función objetivo (f) que asigna las variables de entrada (X) a una variable de salida (Y). Y = f (X) Esta caracterización describe el rango de problemas de clasificación y predicción y los algoritmos de la máquina que pueden usarse para abordarlos. Una consideración importante al aprender la función objetivo de los datos de entrenamiento es qué tan bien el modelo se generaliza a los nuevos datos. La generalización es importante porque los datos que recopilamos son solo una muestra, son incompletos y ruidosos. Obtenga su mapa mental de algoritmos GRATUITOS He creado un práctico mapa mental de más de 60 algoritmos organizados por tipo. Descárguelo, imprímalo y úselo. Descargue gratis También obtenga acceso exclusivo al algoritmo de aprendizaje automático del mini curso por correo electrónico. Generalización en el aprendizaje automático En el aprendizaje automático describimos el aprendizaje de la función objetivo a partir de los datos de entrenamiento como aprendizaje inductivo. La inducción se refiere al aprendizaje de conceptos generales a partir de ejemplos específicos, que es exactamente el problema que los problemas supervisados de aprendizaje automático tienen como objetivo resolver. Esto es diferente de la deducción que es al revés y busca aprender conceptos específicos de las reglas generales. La generalización se refiere a qué tan bien los conceptos aprendidos por un modelo de aprendizaje automático se aplican a ejemplos específicos no vistos por el modelo cuando estaba aprendiendo. El objetivo de un buen modelo de aprendizaje automático es generalizar bien desde los datos de entrenamiento hasta cualquier dato del dominio del problema. Esto nos permite hacer predicciones en el futuro sobre datos que el modelo nunca ha visto. Hay una terminología utilizada en el aprendizaje automático cuando hablamos de qué tan bien un modelo de aprendizaje automático aprende y generaliza a los nuevos datos, es decir, sobreajuste y falta de ajuste. El sobreajuste y el subajuste son las dos causas principales del bajo rendimiento de los algoritmos de aprendizaje automático. Ajuste estadístico En las estadísticas, un ajuste se refiere a qué tan bien aproxima una función objetivo. Esta es una buena terminología para usar en el aprendizaje automático, porque los algoritmos supervisados de aprendizaje automático buscan aproximar la función de mapeo subyacente desconocida para las variables de salida dadas las variables de entrada. Las estadísticas a menudo describen la bondad de ajuste que se refiere a las medidas utilizadas para estimar qué tan bien la aproximación de la función coincide con la función objetivo. Algunos de estos métodos son útiles en el aprendizaje automático (por ejemplo, calcular los errores residuales), pero algunas de estas técnicas suponen que conocemos la forma de la función objetivo que estamos aproximando, que no es el caso en el aprendizaje automático. Si supiéramos la forma de la función objetivo, la usaríamos directamente para hacer predicciones, en lugar de tratar de aprender una aproximación a partir de muestras de datos de entrenamiento ruidosos. Overfitting en Machine Learning Overfitting se refiere a un modelo que modela muy bien los datos de entrenamiento. El sobreajuste ocurre cuando un modelo aprende los detalles y el ruido en los datos de entrenamiento en la medida en que impacta negativamente el rendimiento del modelo en los nuevos datos. Esto significa que el ruido o las fluctuaciones aleatorias en los datos de entrenamiento son recogidas y aprendidas como conceptos por el modelo. El problema es que estos conceptos no se aplican a los datos nuevos y afectan negativamente la capacidad de los modelos para generalizar. El sobreajuste es más probable con modelos no paramétricos y no lineales que tienen más flexibilidad al aprender una función objetivo. Como tal, muchos algoritmos de aprendizaje automático no paramétricos también incluyen parámetros o técnicas para limitar y restringir cuántos detalles aprende el modelo. Por ejemplo, los árboles de decisión son un algoritmo de aprendizaje automático no paramétrico que es muy flexible y está sujeto a datos de capacitación sobreajustados. Este problema se puede solucionar podando un árbol después de haberlo aprendido para eliminar algunos de los detalles que ha recogido. Ajuste insuficiente en el aprendizaje automático El ajuste insuficiente se refiere a un modelo que no puede modelar los datos de entrenamiento ni generalizar a nuevos datos. Un modelo de aprendizaje automático no adaptado no es un modelo adecuado y será obvio, ya que tendrá un bajo rendimiento en los datos de entrenamiento. La falta de ajuste a menudo no se analiza, ya que es fácil de detectar dada una buena métrica de rendimiento. El remedio es seguir adelante y probar algoritmos alternativos de aprendizaje automático. Sin embargo, proporciona un buen contraste con el problema del sobreajuste. Un buen ajuste en el aprendizaje automático Idealmente, desea seleccionar un modelo en el punto óptimo entre el ajuste y el sobreajuste. Este es el objetivo, pero es muy difícil de hacer en la práctica. Para comprender este objetivo, podemos observar el rendimiento de un algoritmo de aprendizaje automático a lo largo del tiempo, ya que está aprendiendo datos de capacitación. Podemos trazar tanto la habilidad en los datos de entrenamiento como la habilidad en un conjunto de datos de prueba que hemos retenido del proceso de entrenamiento. Con el tiempo, a medida que el algoritmo aprende, el error del modelo en los datos de entrenamiento disminuye y también el error en el conjunto de datos de prueba. Si entrenamos durante demasiado tiempo, el rendimiento en el conjunto de datos de entrenamiento puede continuar disminuyendo porque el modelo se sobreajusta y aprende los detalles irrelevantes y el ruido en el conjunto de datos de entrenamiento. Al mismo tiempo, el error para el conjunto de prueba comienza a aumentar nuevamente a medida que disminuye la capacidad del modelo para generalizar. El punto óptimo es el punto justo antes de que el error en el conjunto de datos de prueba comience a aumentar cuando el modelo tiene una buena habilidad tanto en el conjunto de datos de entrenamiento como en el conjunto de datos de prueba invisible. Puede realizar este experimento con sus algoritmos de aprendizaje automático favoritos. Esto no suele ser una técnica útil en la práctica, porque al elegir el punto de parada para el entrenamiento utilizando la habilidad en el conjunto de datos de prueba significa que el conjunto de prueba ya no es &quot;invisible&quot; o una medida objetiva independiente. Algunos conocimientos (muchos conocimientos útiles) sobre esos datos se han filtrado en el procedimiento de capacitación. Hay dos técnicas adicionales que puede usar para ayudar a encontrar el punto óptimo en la práctica: métodos de remuestreo y un conjunto de datos de validación. Cómo limitar el sobreajuste Tanto el sobreajuste como la falta de adaptación pueden conducir a un bajo rendimiento del modelo. Pero, con mucho, el problema más común en el aprendizaje automático aplicado es el sobreajuste. El sobreajuste es un problema porque la evaluación de los algoritmos de aprendizaje automático en los datos de entrenamiento es diferente de la evaluación que realmente nos interesa, es decir, qué tan bien se desempeña el algoritmo en datos no vistos. Existen dos técnicas importantes que puede usar al evaluar algoritmos de aprendizaje automático para limitar el sobreajuste: Use una técnica de remuestreo para estimar la precisión del modelo. Retener un conjunto de datos de validación. La técnica de remuestreo más popular es la validación cruzada k-fold. Le permite entrenar y probar su modelo k veces en diferentes subconjuntos de datos de entrenamiento y generar una estimación del rendimiento de un modelo de aprendizaje automático en datos no vistos. Un conjunto de datos de validación es simplemente un subconjunto de sus datos de entrenamiento que retiene de sus algoritmos de aprendizaje automático hasta el final de su proyecto. Después de haber seleccionado y ajustado sus algoritmos de aprendizaje automático en su conjunto de datos de entrenamiento, puede evaluar los modelos aprendidos en el conjunto de datos de validación para obtener una idea objetiva final de cómo podrían funcionar los modelos en datos no vistos. El uso de la validación cruzada es un estándar de oro en el aprendizaje automático aplicado para estimar la precisión del modelo en datos no vistos. Si tiene los datos, utilizar un conjunto de datos de validación también es una práctica excelente. Lecturas adicionales Esta sección enumera algunos recursos recomendados si desea obtener más información sobre generalización, sobreajuste y falta de ajuste en el aprendizaje automático. Resumen En esta publicación, descubrió que el aprendizaje automático está resolviendo problemas por el método de inducción. Aprendiste que la generalización es una descripción de qué tan bien se aplican los conceptos aprendidos por un modelo a los datos nuevos. Finalmente, aprendió acerca de la terminología de la generalización en el aprendizaje automático de sobreajuste y subadaptación: Sobreajuste: buen rendimiento en los datos de capacitación, mala generalización de otros datos. : Buen desempeño en los datos de capacitación, poca generación de otros datos. Ajuste insuficiente: bajo rendimiento en los datos de entrenamiento y poca generalización a otros datos ¿Tiene alguna pregunta sobre el ajuste excesivo, el ajuste insuficiente o esta publicación? Deje un comentario y haga su pregunta y haré todo lo posible para responderla. ¡Descubra cómo funcionan los algoritmos de aprendizaje automático! Vea cómo funcionan los algoritmos en minutos ... con ejemplos simples y aritméticos. Descubra cómo en mi nuevo libro electrónico: Algoritmos de aprendizaje automático maestro. Cubre explicaciones y ejemplos de 10 algoritmos principales, como: Regresión lineal, vecinos más cercanos k, máquinas de vectores de soporte y mucho más ... Finalmente, retire el telón de los algoritmos de aprendizaje automático Omita los aspectos académicos. Solo resultados. Mira lo que hay dentro"
24;machinelearningmastery.com;http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/;2016-04-14;K-Nearest Neighbors for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

In this post you will discover the k-Nearest Neighbors (KNN) algorithm for classification and regression. After reading this post you will know.

The model representation used by KNN.

How a model is learned using KNN (hint, it’s not).

How to make predictions using KNN

The many names for KNN including how different fields refer to it.

How to prepare your data to get the most from KNN.

Where to look to learn more about the KNN algorithm.

This post was written for developers and assumes no background in statistics or mathematics. The focus is on how the algorithm works and how to use it for predictive modeling problems. If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

KNN Model Representation

The model representation for KNN is the entire training dataset.

It is as simple as that.

KNN has no model other than storing the entire dataset, so there is no learning required.

Efficient implementations can store the data using complex data structures like k-d trees to make look-up and matching of new patterns during prediction efficient.

Because the entire training dataset is stored, you may want to think carefully about the consistency of your training data. It might be a good idea to curate it, update it often as new data becomes available and remove erroneous and outlier data.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Making Predictions with KNN

KNN makes predictions using the training dataset directly.

Predictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.

To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance.

Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j.

EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )

Other popular distance measures include:

Hamming Distance : Calculate the distance between binary vectors (more).

: Calculate the distance between binary vectors (more). Manhattan Distance : Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more).

: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more). Minkowski Distance: Generalization of Euclidean and Manhattan distance (more).

There are many other distance measures that can be used, such as Tanimoto, Jaccard, Mahalanobis and cosine distance. You can choose the best distance metric based on the properties of your data. If you are unsure, you can experiment with different distance metrics and different values of K together and see which mix results in the most accurate models.

Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.).

The value for K can be found by algorithm tuning. It is a good idea to try many different values for K (e.g. values from 1 to 21) and see what works best for your problem.

The computational complexity of KNN increases with the size of the training dataset. For very large training sets, KNN can be made stochastic by taking a sample from the training dataset from which to calculate the K-most similar instances.

KNN has been around for a long time and has been very well studied. As such, different disciplines have different names for it, for example:

Instance-Based Learning : The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain).

: The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain). Lazy Learning : No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm.

: No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm. Non-Parametric: KNN makes no assumptions about the functional form of the problem being solved. As such KNN is referred to as a non-parametric machine learning algorithm.

KNN can be used for regression and classification problems.

KNN for Regression

When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.

KNN for Classification

When KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction.

Class probabilities can be calculated as the normalized frequency of samples that belong to each class in the set of K most similar instances for a new data instance. For example, in a binary classification problem (class is 0 or 1):

p(class=0) = count(class=0) / (count(class=0)+count(class=1))

If you are using K and you have an even number of classes (e.g. 2) it is a good idea to choose a K value with an odd number to avoid a tie. And the inverse, use an even number for K when you have an odd number of classes.

Ties can be broken consistently by expanding K by 1 and looking at the class of the next most similar instance in the training dataset.

Curse of Dimensionality

KNN works well with a small number of input variables (p), but struggles when the number of inputs is very large.

Each input variable can be considered a dimension of a p-dimensional input space. For example, if you had two input variables x1 and x2, the input space would be 2-dimensional.

As the number of dimensions increases the volume of the input space increases at an exponential rate.

In high dimensions, points that may be similar may have very large distances. All points will be far away from each other and our intuition for distances in simple 2 and 3-dimensional spaces breaks down. This might feel unintuitive at first, but this general problem is called the “Curse of Dimensionality“.

Best Prepare Data for KNN

Rescale Data : KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution.

: KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution. Address Missing Data : Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.

: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed. Lower Dimensionality: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space.

Further Reading

If you are interested in implementing KNN from scratch in Python, checkout the post:

Below are some good machine learning texts that cover the KNN algorithm from a predictive modeling perspective.

Also checkout K-Nearest Neighbors on Wikipedia.

Summary

In this post you discovered the KNN machine learning algorithm. You learned that:

KNN stores the entire training dataset which it uses as its representation.

KNN does not learn any model.

KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance.

There are many distance measures to choose from to match the structure of your input data.

That it is a good idea to rescale your data, such as using normalization, when using KNN.

If you have any questions about this post or the KNN algorithm ask in the comments and I will do my best to answer.

Discover How Machine Learning Algorithms Work! See How Algorithms Work in Minutes ...with just arithmetic and simple examples Discover how in my new Ebook:

Master Machine Learning Algorithms It covers explanations and examples of 10 top algorithms, like:

Linear Regression, k-Nearest Neighbors, Support Vector Machines and much more... Finally, Pull Back the Curtain on

Machine Learning Algorithms Skip the Academics. Just Results. See What's Inside";Vecinos K-Nearest para aprendizaje automático;"Tweet Compartir Compartir Última actualización el 12 de agosto de 2019 En esta publicación, descubrirá el algoritmo k-Nearest Neighbours (KNN) para clasificación y regresión. Después de leer esta publicación lo sabrás. La representación del modelo utilizada por KNN. Cómo se aprende un modelo usando KNN (pista, no lo es). Cómo hacer predicciones usando KNN Los muchos nombres para KNN, incluyendo cómo se refieren a él los diferentes campos. Cómo preparar sus datos para aprovechar al máximo KNN. Dónde buscar para obtener más información sobre el algoritmo KNN. Esta publicación fue escrita para desarrolladores y no asume antecedentes en estadística o matemáticas. El foco está en cómo funciona el algoritmo y cómo usarlo para problemas de modelado predictivo. Si tiene alguna pregunta, deje un comentario y haré todo lo posible para responder. Descubra cómo funcionan los algoritmos de aprendizaje automático, incluidos kNN, árboles de decisión, ingenuos bayes, SVM, conjuntos y mucho más en mi nuevo libro, con 22 tutoriales y ejemplos en Excel. Empecemos. Representación del modelo KNN La representación del modelo para KNN es el conjunto de datos de entrenamiento completo. Es tan simple como eso. KNN no tiene otro modelo que no sea almacenar todo el conjunto de datos, por lo que no se requiere aprendizaje. Las implementaciones eficientes pueden almacenar los datos utilizando estructuras de datos complejas como árboles kd para que la búsqueda y la coincidencia de nuevos patrones durante la predicción sean eficientes. Debido a que se almacena todo el conjunto de datos de entrenamiento, es posible que desee pensar cuidadosamente sobre la consistencia de sus datos de entrenamiento. Puede ser una buena idea seleccionarlo, actualizarlo a menudo a medida que haya nuevos datos disponibles y eliminar datos erróneos y atípicos. Obtenga su mapa mental de algoritmos GRATUITOS He creado un práctico mapa mental de más de 60 algoritmos organizados por tipo. Descárguelo, imprímalo y úselo. Descargue gratis También obtenga acceso exclusivo al algoritmo de aprendizaje automático del mini curso por correo electrónico. Hacer predicciones con KNN KNN hace predicciones usando el conjunto de datos de entrenamiento directamente. Se hacen predicciones para una nueva instancia (x) buscando en todo el conjunto de entrenamiento las K instancias más similares (las vecinas) y resumiendo la variable de salida para esas K instancias. Para la regresión, esta podría ser la variable de salida media, en la clasificación podría ser el valor de clase de modo (o el más común). Para determinar cuáles de las K instancias en el conjunto de datos de entrenamiento son más similares a una nueva entrada, se utiliza una medida de distancia. Para las variables de entrada de valor real, la medida de distancia más popular es la distancia euclidiana. La distancia euclidiana se calcula como la raíz cuadrada de la suma de las diferencias al cuadrado entre un nuevo punto (x) y un punto existente (xi) en todos los atributos de entrada j. EuclideanDistance (x, xi) = sqrt (sum ((xj - xij) ^ 2)) Otras medidas de distancia populares incluyen: Distancia de Hamming: Calcular la distancia entre vectores binarios (más). : Calcular la distancia entre vectores binarios (más). Distancia de Manhattan: calcule la distancia entre vectores reales utilizando la suma de su diferencia absoluta. También se llama City Block Distance (más). : Calcular la distancia entre vectores reales usando la suma de su diferencia absoluta. También se llama City Block Distance (más). Distancia de Minkowski: generalización de la distancia euclidiana y de Manhattan (más). Hay muchas otras medidas de distancia que se pueden utilizar, como Tanimoto, Jaccard, Mahalanobis y la distancia cosenoidal. Puede elegir la mejor métrica de distancia en función de las propiedades de sus datos. Si no está seguro, puede experimentar con diferentes métricas de distancia y diferentes valores de K juntos y ver qué resultados de la mezcla en los modelos más precisos. Euclidiana es una buena medida de distancia para usar si las variables de entrada son de tipo similar (por ejemplo, todos los anchos y alturas medidas). La distancia de Manhattan es una buena medida para usar si las variables de entrada no son similares en tipo (como edad, género, altura, etc.). El valor de K se puede encontrar mediante el ajuste del algoritmo. Es una buena idea probar muchos valores diferentes para K (por ejemplo, valores del 1 al 21) y ver qué funciona mejor para su problema. La complejidad computacional de KNN aumenta con el tamaño del conjunto de datos de entrenamiento. Para conjuntos de entrenamiento muy grandes, KNN puede volverse estocástico al tomar una muestra del conjunto de datos de entrenamiento a partir del cual se calculan las instancias K más similares. KNN ha existido por mucho tiempo y ha sido muy bien estudiado. Como tal, diferentes disciplinas tienen diferentes nombres, por ejemplo: Aprendizaje basado en instancias: las instancias de entrenamiento sin procesar se utilizan para hacer predicciones. Como tal, KNN a menudo se conoce como aprendizaje basado en instancias o aprendizaje basado en casos (donde cada instancia de capacitación es un caso del dominio del problema). : Las instancias de entrenamiento sin procesar se utilizan para hacer predicciones. Como tal, KNN a menudo se conoce como aprendizaje basado en instancias o aprendizaje basado en casos (donde cada instancia de capacitación es un caso del dominio del problema). Aprendizaje diferido: no se requiere el aprendizaje del modelo y todo el trabajo ocurre en el momento en que se solicita una predicción. Como tal, KNN a menudo se conoce como un algoritmo de aprendizaje lento. : No se requiere el aprendizaje del modelo y todo el trabajo ocurre en el momento en que se solicita una predicción. Como tal, KNN a menudo se conoce como un algoritmo de aprendizaje lento. No paramétrico: KNN no hace suposiciones sobre la forma funcional del problema que se está resolviendo. Como tal, KNN se conoce como un algoritmo de aprendizaje automático no paramétrico. KNN puede usarse para problemas de regresión y clasificación. KNN para regresión Cuando KNN se usa para problemas de regresión, la predicción se basa en la media o la mediana de las K instancias más similares. KNN para clasificación Cuando KNN se usa para clasificación, la salida se puede calcular como la clase con la frecuencia más alta de las instancias K-más similares. Cada instancia esencialmente vota por su clase y la clase con más votos se toma como la predicción. Las probabilidades de clase se pueden calcular como la frecuencia normalizada de muestras que pertenecen a cada clase en el conjunto de K instancias más similares para una nueva instancia de datos. Por ejemplo, en un problema de clasificación binaria (la clase es 0 o 1): p (clase = 0) = cuenta (clase = 0) / (cuenta (clase = 0) + cuenta (clase = 1)) Si está utilizando K y tiene un número par de clases (por ejemplo, 2) es una buena idea elegir un valor K con un número impar para evitar un empate. Y a la inversa, use un número par para K cuando tenga un número impar de clases. Los lazos se pueden romper de manera consistente expandiendo K por 1 y mirando la clase de la siguiente instancia más similar en el conjunto de datos de entrenamiento. La maldición de la dimensionalidad KNN funciona bien con una pequeña cantidad de variables de entrada (p), pero tiene dificultades cuando la cantidad de entradas es muy grande. Cada variable de entrada puede considerarse una dimensión de un espacio de entrada p-dimensional. Por ejemplo, si tuviera dos variables de entrada x1 y x2, el espacio de entrada sería bidimensional. A medida que aumenta el número de dimensiones, el volumen del espacio de entrada aumenta a una velocidad exponencial. En grandes dimensiones, los puntos que pueden ser similares pueden tener distancias muy grandes. Todos los puntos estarán muy lejos unos de otros y nuestra intuición para distancias en espacios simples de 2 y 3 dimensiones se desmorona. Esto puede parecer poco intuitivo al principio, pero este problema general se llama la &quot;Maldición de la Dimensionalidad&quot;. Mejor preparación de datos para KNN Reescalar datos: KNN funciona mucho mejor si todos los datos tienen la misma escala. Normalizar sus datos al rango [0, 1] es una buena idea. También puede ser una buena idea estandarizar sus datos si tiene una distribución gaussiana. : KNN funciona mucho mejor si todos los datos tienen la misma escala. Normalizar sus datos al rango [0, 1] es una buena idea. También puede ser una buena idea estandarizar sus datos si tiene una distribución gaussiana. Dirección de datos faltantes: los datos faltantes significarán que la distancia entre muestras no se puede calcular. Estas muestras podrían excluirse o los valores faltantes podrían imputarse. : La falta de datos significará que la distancia entre muestras no se puede calcular. Estas muestras podrían excluirse o los valores faltantes podrían imputarse. Dimensionalidad inferior: KNN es adecuado para datos de dimensiones inferiores. Puede probarlo con datos de alta dimensión (cientos o miles de variables de entrada) pero tenga en cuenta que puede no funcionar tan bien como otras técnicas. KNN puede beneficiarse de la selección de características que reduce la dimensionalidad del espacio de características de entrada. Lecturas adicionales Si está interesado en implementar KNN desde cero en Python, consulte la publicación: A continuación se presentan algunos buenos textos de aprendizaje automático que cubren el algoritmo KNN desde una perspectiva de modelado predictivo. También verifique K-Nearest Neighbours en Wikipedia. Resumen En esta publicación descubrió el algoritmo de aprendizaje automático KNN. Aprendiste que: KNN almacena todo el conjunto de datos de entrenamiento que utiliza como su representación. KNN no aprende ningún modelo. KNN hace predicciones justo a tiempo calculando la similitud entre una muestra de entrada y cada instancia de entrenamiento. Hay muchas medidas de distancia para elegir para que coincida con la estructura de sus datos de entrada. Que es una buena idea reescalar sus datos, como usar la normalización, cuando usa KNN. Si tiene alguna pregunta sobre esta publicación o el algoritmo KNN, pregunte en los comentarios y haré todo lo posible para responder. ¡Descubra cómo funcionan los algoritmos de aprendizaje automático! Vea cómo funcionan los algoritmos en minutos ... con ejemplos simples y aritméticos. Descubra cómo en mi nuevo libro electrónico: Algoritmos de aprendizaje automático maestro. Cubre explicaciones y ejemplos de 10 algoritmos principales, como: Regresión lineal, vecinos más cercanos k, máquinas de vectores de soporte y mucho más ... Finalmente, retire el telón de los algoritmos de aprendizaje automático Omita los aspectos académicos. Solo resultados. Mira lo que hay dentro"
25;machinelearningmastery.com;https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/;2020-01-02;How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification;"# calculates precision for 1:100 dataset with 90 tp and 30 fp

from sklearn . metrics import precision_score

# define actual

act_pos = [ 1 for _ in range ( 100 ) ]

act_neg = [ 0 for _ in range ( 10000 ) ]

y_true = act_pos + act_neg

# define predictions

pred_pos = [ 0 for _ in range ( 10 ) ] + [ 1 for _ in range ( 90 ) ]

pred_neg = [ 1 for _ in range ( 30 ) ] + [ 0 for _ in range ( 9970 ) ]

y_pred = pred_pos + pred_neg

# calculate prediction

precision = precision_score ( y_true , y_pred , average = 'binary' )";Cómo calcular la precisión, la recuperación y la medida F para la clasificación desequilibrada;"# calcula la precisión para el conjunto de datos 1: 100 con 90 tp y 30 fp de sklearn. importar métrica precision_score # define act_pos = [1 para _ en rango (100)] act_neg = [0 para _ en rango (10000)] y_true = act_pos + act_neg # define predicciones pred_pos = [0 para _ en rango (10)] + [1 para _ en rango (90)] pred_neg = [1 para _ en rango (30)] + [0 para _ en rango (9970)] y_pred = pred_pos + pred_neg # calcular predicción precisión = precisión_punta (y_true, y_pred, promedio = &#39;binario&#39;)"
26;machinelearningmastery.com;https://machinelearningmastery.com/statistical-tolerance-intervals-in-machine-learning/;2018-05-31;A Gentle Introduction to Statistical Tolerance Intervals in Machine Learning;"# parametric tolerance interval

from numpy . random import seed

from numpy . random import randn

from numpy import mean

from numpy import sqrt

from scipy . stats import chi2

from scipy . stats import norm

# seed the random number generator

seed ( 1 )

# generate dataset

data = 5 * randn ( 100 ) + 50

# specify degrees of freedom

n = len ( data )

dof = n - 1

# specify data coverage

prop = 0.95

prop_inv = ( 1.0 - prop ) / 2.0

gauss_critical = norm . isf ( prop_inv )

print ( 'Gaussian critical value: %.3f (coverage=%d%%)' % ( gauss_critical , prop* 100 ) )

# specify confidence

prob = 0.99

chi_critical = chi2 . isf ( q = prob , df = dof )

print ( 'Chi-Squared critical value: %.3f (prob=%d%%, dof=%d)' % ( chi_critical , prob* 100 , dof ) )

# tolerance

interval = sqrt ( ( dof * ( 1 + ( 1 / n ) ) * gauss_critical* * 2 ) / chi_critical )

print ( 'Tolerance Interval: %.3f' % interval )

# summarize

data_mean = mean ( data )

lower , upper = data_mean - interval , data_mean + interval";Una suave introducción a los intervalos de tolerancia estadística en el aprendizaje automático;"# intervalo de tolerancia paramétrico de numpy. importación aleatoria de semillas de numpy. importación aleatoria randn de numpy importación media de numpy import sqrt de scipy. Las estadísticas importan chi2 de scipy. estadística de importación norma # semilla el generador de números aleatorios semilla (1) # genera datos del conjunto de datos = 5 * randn (100) + 50 # especifica grados de libertad n = len (datos) dof = n - 1 # especifica cobertura de datos prop = 0.95 prop_inv = (1.0 - prop) / 2.0 gauss_critical = norma. isf (prop_inv) print (&#39;Valor crítico gaussiano:% .3f (cobertura =% d %%)&#39;% (gauss_critical, prop * 100)) # especificar confianza prob = 0.99 chi_crítico = chi2. isf (q = prob, df = dof) print (&#39;Valor crítico cuadrado ji:% .3f (prob =% d %%, dof =% d)&#39;% (chi_crítico, prob * 100, dof)) # intervalo de tolerancia = sqrt ((dof * (1 + (1 / n)) * gauss_critical * * 2) / chi_critical) print (&#39;Intervalo de tolerancia:% .3f&#39;% intervalo) # resume data_mean = mean (data) inferior, superior = data_mean - Intervalo, Data_mean + Intervalo"
27;news.mit.edu;http://news.mit.edu/2020/searching-covid-19-protein-test-hadley-sikes-0417;;3 Questions: Hadley Sikes on searching for a Covid-19 protein test;"Before the world was alerted to the threat of a novel coronavirus spreading out from China, Hadley Sikes was already well acquainted with developing molecular technology to improve diagnosis and treatment of diseases. Now working on a crucial diagnostic test to find people with Covid-19, the Esther and Harold E. Edgerton Associate Professor of chemical engineering at MIT and principal investigator of the Antimicrobial Resistance Interdisciplinary Research Group at Singapore-MIT Alliance for Research and Technology (SMART), and her collaborators have managed to condense months of work into a matter of a few weeks.

Q: Where does your work fit in with the global Covid-19 research effort?

A: The Covid-19 pandemic has presented a huge challenge for the world’s capacity for diagnostic testing. It has given us a clearer picture of what our actual capabilities are because all the countries in this effort are as motivated as they could ever be to deploy technologies that can test populations as quickly and accurately as possible.

Scientists have only been able to deploy one kind of test so far to identify people who have Covid-19. The coronavirus that causes this illness is made out of proteins and RNA, and so far, we only detect its RNA. RNA tests are complicated and can take hours, or even days, for doctors to receive the results. A faster version of an RNA test was just announced, but it also requires laboratory equipment and it is difficult to produce as many tests are needed.

What I’ve been working on at MIT and SMART, MIT’s research enterprise in Singapore, is developing protein tests that are quick to run and don’t require laboratories. These tests can find out if viral proteins are present in bodily fluids and also if a patient’s immune system has responded to the SARS-CoV-2 virus.

That is information that is critical, especially in this situation whereby countries are shutting down. If you know who has had the infection and recovered, and thus now has immunity, you have the potential to keep things running without putting more people at risk.

Before this outbreak, we had been working with support from the Deshpande Center on protein tests to diagnose malaria, dengue, and tuberculosis. Our goal was to find a way to capture more of the proteins made by these pathogens by developing binding reagents that concentrate the proteins within a testing device. We also wanted our tests to be affordable and easy to produce in large quantities.

Developing the reagents is a slow but crucial part of the process of developing a clinical protein diagnostic, and typically takes longer than for nucleic acid tests.

Dr. Eric Miller, who is in my lab at MIT, and Dr. Patthara Kongsuphol at SMART, have been working on engineering reagents that capture more of the scarce viral proteins in a patient’s bodily fluids. If more of these viral proteins can be captured, the test can be more sensitive.

With the Covid-19 pandemic as his motivation, Dr. Miller figured out how to engineer these binding agents in just two weeks — much sooner than the several months it might typically take. On the SMART side, Dr. Kongsuphol has been leading our efforts in Singapore to integrate these agents into diverse test formats that can be challenged with clinical samples.

We are aiming to create a test that can work in 10 minutes and doesn’t require specialized instruments or laboratory infrastructure. In this way, it can be carried out at an airport or a clinic to accurately show if a person either has or is immune to Covid-19. It’s challenging to make a test that is sensitive and accurate enough, and also a huge challenge to scale up production of such a test fast enough to have an impact when a new pathogen emerges.

Q: What influence has Singapore had on your work?

A: I have been at MIT for 10 years and started working with SMART two years ago. Joining an interdisciplinary research team in Singapore has given me a really great chance to work on a pressing medical problem of our time, which is antimicrobial resistance. It allows me to work with world-class clinicians and government agencies that are international leaders in public health, and with top researchers at Nanyang Technological University, the National University of Singapore, and the Agency for Science, Technology and Research.

I spent January in Singapore and went back again at the beginning of March, just as the outbreak was emerging in the United States. I really wanted to learn more about how Singapore’s experience during the SARS outbreak in the early 2000s allowed them to respond so effectively to this outbreak, particularly with diagnostic testing.

It was nerve-wracking being separated from my family at this time. I have three young children and a husband who has a full-time job. Because of the 12-hour time difference with Boston, we had a lot of late-night and early-morning FaceTime chats.

They have been really supportive of the work my team and I are trying to do. I think they are glad I went to Singapore because they see that I am doing what I can to play a role in figuring out effective responses to this, and future public health crises. The mission provides a powerful sense of purpose.

The United States is fortunate that it had not experienced the SARS or MERS outbreaks Singapore and other Asian countries had been through, but this means we are lacking in the knowledge and experience these countries have gained. The United States and other countries can learn a lot from Singapore.

After that event, Singaporean officials analyzed everything that happened and put in place new public health measures designed to effectively manage and contain any future outbreaks. By doing this, they have developed a world-class response.

Q: What did you learn in Singapore?

A: I valued getting to speak to the doctors on the ground who were fighting the Covid-19 outbreak in Singapore and learning what they had seen during their case management amid the crisis.

When I arrived in Singapore, I was honored to get to speak with Dr. Sidney Yee, CEO of the Diagnostics Development Hub, who had worked to rapidly produce a high-performing RNA test and ensure it was quickly deployed as part of Singapore’s effective response to Covid-19.

I was also able to talk to my colleague at SMART, Dr. Tsin Wen Yeo, while he was doing shifts in the Intensive Care Unit, caring for Covid-19 patients. He gave me his views about what was required from a diagnostic protein test. I think it was an incredible opportunity to understand the needs of doctors in different settings and it focused the efforts of my team. Understanding how diagnostic tests will be used allows us to prioritize the things that doctors find most important.

You could make all sorts of diagnostic tests, but the ones we focus our effort on are the ones that are going to provide doctors with actionable information that will help them treat their patients.

This is a really interesting time now that there is a sudden emphasis on needing better, faster diagnostics for the world’s health-care systems. Engineers have a big role in providing these, for the benefit of patients and health workers, and also to help economies get back on their feet. I hope that this desire for more practical diagnostic research continues after we recover from this outbreak.";3 preguntas: Hadley Sikes sobre la búsqueda de una prueba de proteína Covid-19;Antes de que el mundo fuera alertado sobre la amenaza de un nuevo coronavirus que se extendía desde China, Hadley Sikes ya estaba familiarizado con el desarrollo de tecnología molecular para mejorar el diagnóstico y el tratamiento de enfermedades. Ahora trabajando en una prueba de diagnóstico crucial para encontrar personas con Covid-19, el profesor asociado de ingeniería química Esther y Harold E. Edgerton en el MIT e investigador principal del Grupo de Investigación Interdisciplinaria de Resistencia Antimicrobiana en la Alianza de Investigación y Tecnología Singapur-MIT (SMART ), y sus colaboradores han logrado condensar meses de trabajo en unas pocas semanas. P: ¿Dónde encaja su trabajo con el esfuerzo de investigación global de Covid-19? R: La pandemia de Covid-19 ha presentado un gran desafío para la capacidad mundial de pruebas de diagnóstico. Nos ha dado una idea más clara de cuáles son nuestras capacidades reales porque todos los países en este esfuerzo están tan motivados como podrían estar para implementar tecnologías que puedan evaluar a las poblaciones de la manera más rápida y precisa posible. Los científicos solo han podido implementar un tipo de prueba hasta el momento para identificar a las personas que tienen Covid-19. El coronavirus que causa esta enfermedad está hecho de proteínas y ARN, y hasta ahora, solo detectamos su ARN. Las pruebas de ARN son complicadas y pueden tomar horas, o incluso días, para que los médicos reciban los resultados. Se acaba de anunciar una versión más rápida de una prueba de ARN, pero también requiere equipo de laboratorio y es difícil de producir ya que se necesitan muchas pruebas. En lo que he estado trabajando en MIT y SMART, la empresa de investigación de MIT en Singapur, es desarrollar pruebas de proteínas que se ejecutan rápidamente y no requieren laboratorios. Estas pruebas pueden determinar si las proteínas virales están presentes en los fluidos corporales y también si el sistema inmunitario de un paciente ha respondido al virus SARS-CoV-2. Esa es información que es crítica, especialmente en esta situación en la que los países están cerrando. Si sabe quién tuvo la infección y se recuperó, y ahora tiene inmunidad, tiene el potencial de mantener las cosas funcionando sin poner en riesgo a más personas. Antes de este brote, habíamos estado trabajando con el apoyo del Centro Deshpande en pruebas de proteínas para diagnosticar la malaria, el dengue y la tuberculosis. Nuestro objetivo era encontrar una manera de capturar más proteínas producidas por estos patógenos mediante el desarrollo de reactivos de unión que concentran las proteínas dentro de un dispositivo de prueba. También queríamos que nuestras pruebas fueran asequibles y fáciles de producir en grandes cantidades. El desarrollo de los reactivos es una parte lenta pero crucial del proceso de desarrollo de un diagnóstico clínico de proteínas, y generalmente lleva más tiempo que las pruebas de ácido nucleico. El Dr. Eric Miller, que está en mi laboratorio en el MIT, y el Dr. Patthara Kongsuphol en SMART, han estado trabajando en reactivos de ingeniería que capturan más de las escasas proteínas virales en los fluidos corporales de un paciente. Si se pueden capturar más de estas proteínas virales, la prueba puede ser más sensible. Con la pandemia de Covid-19 como su motivación, el Dr. Miller descubrió cómo diseñar estos agentes aglutinantes en solo dos semanas, mucho antes de los varios meses que normalmente podría llevar. Del lado SMART, el Dr. Kongsuphol ha liderado nuestros esfuerzos en Singapur para integrar estos agentes en diversos formatos de prueba que pueden ser desafiados con muestras clínicas. Nuestro objetivo es crear una prueba que pueda funcionar en 10 minutos y no requiera instrumentos especializados o infraestructura de laboratorio. De esta manera, se puede llevar a cabo en un aeropuerto o en una clínica para mostrar con precisión si una persona tiene o es inmune a Covid-19. Es un desafío realizar una prueba que sea lo suficientemente sensible y precisa, y también un gran desafío para aumentar la producción de dicha prueba lo suficientemente rápido como para tener un impacto cuando surge un nuevo patógeno. P: ¿Qué influencia ha tenido Singapur en su trabajo? R: He estado en MIT durante 10 años y comencé a trabajar con SMART hace dos años. Unirme a un equipo de investigación interdisciplinario en Singapur me ha dado una gran oportunidad de trabajar en un problema médico urgente de nuestro tiempo, que es la resistencia a los antimicrobianos. Me permite trabajar con médicos de clase mundial y agencias gubernamentales que son líderes internacionales en salud pública, y con los mejores investigadores de la Universidad Tecnológica de Nanyang, la Universidad Nacional de Singapur y la Agencia de Ciencia, Tecnología e Investigación. Pasé enero en Singapur y volví a principios de marzo, justo cuando el brote estaba surgiendo en los Estados Unidos. Realmente quería aprender más sobre cómo la experiencia de Singapur durante el brote de SARS a principios de la década de 2000 les permitió responder de manera tan efectiva a este brote, particularmente con pruebas de diagnóstico. Fue estresante estar separado de mi familia en este momento. Tengo tres hijos pequeños y un esposo que tiene un trabajo de tiempo completo. Debido a la diferencia horaria de 12 horas con Boston, tuvimos muchos chats FaceTime a altas horas de la noche y temprano en la mañana. Han apoyado mucho el trabajo que mi equipo y yo estamos tratando de hacer. Creo que están contentos de haber ido a Singapur porque ven que estoy haciendo todo lo posible para desempeñar un papel en encontrar respuestas efectivas a esto y futuras crisis de salud pública. La misión proporciona un poderoso sentido de propósito. Estados Unidos es afortunado de no haber experimentado los brotes de SARS o MERS que Singapur y otros países asiáticos han pasado, pero esto significa que nos falta el conocimiento y la experiencia que estos países han adquirido. Estados Unidos y otros países pueden aprender mucho de Singapur. Después de ese evento, los funcionarios de Singapur analizaron todo lo que sucedió y establecieron nuevas medidas de salud pública diseñadas para manejar y contener de manera efectiva cualquier brote futuro. Al hacer esto, han desarrollado una respuesta de clase mundial. P: ¿Qué aprendiste en Singapur? R: Valoré hablar con los médicos en el terreno que estaban luchando contra el brote de Covid-19 en Singapur y conocer lo que habían visto durante el manejo de su caso en medio de la crisis. Cuando llegué a Singapur, tuve el honor de hablar con el Dr. Sidney Yee, CEO del Centro de Desarrollo de Diagnósticos, que había trabajado para producir rápidamente una prueba de ARN de alto rendimiento y garantizar que se implementara rápidamente como parte de la respuesta efectiva de Singapur a Covid-19. También pude hablar con mi colega en SMART, el Dr. Tsin Wen Yeo, mientras hacía turnos en la Unidad de Cuidados Intensivos, cuidando a pacientes con Covid-19. Me dio sus puntos de vista sobre lo que se requería de una prueba de diagnóstico de proteínas. Creo que fue una oportunidad increíble para comprender las necesidades de los médicos en diferentes entornos y centró los esfuerzos de mi equipo. Comprender cómo se utilizarán las pruebas de diagnóstico nos permite priorizar las cosas que los médicos consideran más importantes. Podrías hacer todo tipo de pruebas de diagnóstico, pero las que enfocamos nuestro esfuerzo son las que proporcionarán a los médicos información procesable que los ayudará a tratar a sus pacientes. Este es un momento realmente interesante ahora que hay un énfasis repentino en la necesidad de diagnósticos mejores y más rápidos para los sistemas de atención médica del mundo. Los ingenieros tienen un papel importante al proporcionarlos, en beneficio de los pacientes y los trabajadores de la salud, y también para ayudar a las economías a recuperarse. Espero que este deseo de una investigación diagnóstica más práctica continúe después de que nos recuperemos de este brote.
28;machinelearningmastery.com;https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/#comments;2020-03-31;Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost;"# gradient boosting for classification in scikit-learn

from numpy import mean

from numpy import std

from sklearn . datasets import make_classification

from sklearn . ensemble import GradientBoostingClassifier

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from matplotlib import pyplot

# define dataset

X , y = make_classification ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_redundant = 5 , random_state = 1 )

# evaluate the model

model = GradientBoostingClassifier ( )

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

n_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' )

print ( 'Accuracy: %.3f (%.3f)' % ( mean ( n_scores ) , std ( n_scores ) ) )

# fit the model on the whole dataset

model = GradientBoostingClassifier ( )

model . fit ( X , y )

# make a single prediction

row = [ [ 2.56999479 , - 0.13019997 , 3.16075093 , - 4.35936352 , - 1.61271951 , - 1.39352057 , - 2.48924933 , - 1.93094078 , 3.26130366 , 2.05692145 ] ]

yhat = model . predict ( row )";Aumento de gradiente con Scikit-Learn, XGBoost, LightGBM y CatBoost;"# aumento de gradiente para la clasificación en scikit-learn de numpy import media de numpy import std de sklearn. los conjuntos de datos importan make_classification de sklearn. Importación de conjunto GradientBoostingClassifier desde sklearn. model_selection importa cross_val_score desde sklearn. model_selection import RepeatedStratifiedKFold desde matplotlib import pyplot # define dataset X, y = make_classification (n_samples = 1000, n_features = 10, n_informative = 5, n_redundant = 5, random_state = 1) # evalúa el modelo del modelo = GradientBoostingClassifier (S) ct = RepetitBoostingClassifier (s c_ = = 10, n_repeats = 3, random_state = 1) n_scores = cross_val_score (modelo, X, y, puntuación = &#39;precisión&#39;, cv = cv, n_jobs = - 1, error_score = &#39;subir&#39;) print (&#39;Precisión:% .3f (% .3f) &#39;% (mean (n_scores), std (n_scores))) # se ajusta al modelo en todo el modelo de conjunto de datos = modelo GradientBoostingClassifier (). fit (X, y) # hacer una sola fila de predicción = [[2.56999479, - 0.13019997, 3.16075093, - 4.35936352, - 1.61271951, - 1.39352057, - 2.48924933, - 1.93094078, 3.26130366, 2.05692145]] yhat = modelo. predecir (fila)"
29;machinelearningmastery.com;https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/;2020-02-11;Bagging and Random Forest for Imbalanced Classification;"# bagged decision trees on an imbalanced classification problem

from numpy import mean

from sklearn . datasets import make_classification

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from sklearn . ensemble import BaggingClassifier

# generate dataset

X , y = make_classification ( n_samples = 10000 , n_features = 2 , n_redundant = 0 ,

n_clusters_per_class = 1 , weights = [ 0.99 ] , flip_y = 0 , random_state = 4 )

# define model

model = BaggingClassifier ( )

# define evaluation procedure

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

# evaluate model

scores = cross_val_score ( model , X , y , scoring = 'roc_auc' , cv = cv , n_jobs = - 1 )

# summarize performance";Ensacado y bosque aleatorio para clasificación desequilibrada;"# árboles de decisión embolsados en un problema de clasificación desequilibrada de la media de importación numpy de sklearn. los conjuntos de datos importan make_classification de sklearn. model_selection importa cross_val_score desde sklearn. model_selection import RepeatedStratifiedKFold desde sklearn. Importar conjunto BaggingClassifier # generar conjunto de datos X, y = make_classification (n_samples = 10000, n_features = 2, n_redundant = 0, n_clusters_per_class = 1, weights = [0.99], flip_y = 0, random_state = 4) # definir modelo modelo = BaggingClassifier () # define el procedimiento de evaluación cv = RepeatedStratifiedKFold (n_splits = 10, n_repeats = 3, random_state = 1) # evalúa las puntuaciones del modelo = cross_val_score (model, X, y, scoring = &#39;roc_auc&#39;, cv = cv, n_jobs = - 1) # resume actuación"
30;machinelearningmastery.com;http://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/;2014-02-06;Why you should be Spot-Checking Algorithms on your Machine Learning Problems;"Tweet Share Share

Last Updated on April 7, 2018

Spot-checking algorithms is about getting a quick assessment of a bunch of different algorithms on your machine learning problem so that you know what algorithms to focus on and what to discard.

In this post you will discover the 3 benefits of spot-checking algorithms, 5 tips for spot-checking on your next problem and the top 10 most popular data mining algorithms that you could use in your suite of algorithms to spot-check.

Spot-Checking Algorithms

Spot-checking algorithms is a part of the process of applied machine learning. On a new problem, you need to quickly determine which type or class of algorithms is good at picking out the structure in your problem and which are not.

The alternative to spot checking is that you feel overwhelmed by the vast number of algorithms and algorithm types that you could try that you end up trying very few or going with what has worked for you in the past. This results in wasted time and sub-par results.

Benefits of Spot-Checking Algorithms

There are 3 key benefits of spot-checking algorithms on your machine learning problems:

Speed : You could spend a lot of time playing around with different algorithms, tuning parameters and thinking about what algorithms will do well on your problem. I have been there and end up testing the same algorithms over and over because I have not been systematic. A single spot-check experiment can save hours, days and even weeks of noodling around.

: You could spend a lot of time playing around with different algorithms, tuning parameters and thinking about what algorithms will do well on your problem. I have been there and end up testing the same algorithms over and over because I have not been systematic. A single spot-check experiment can save hours, days and even weeks of noodling around. Objective : There is a tendency to go with what has worked for you before. We pick our favorite algorithm (or algorithms) and apply them to every problem we see. The power of machine learning is that there are so many different ways to approach a given problem. A spot-check experiment allows you to automatically and objectively discover those algorithms that are the best at picking out the structure in the problem so you can focus your attention.

: There is a tendency to go with what has worked for you before. We pick our favorite algorithm (or algorithms) and apply them to every problem we see. The power of machine learning is that there are so many different ways to approach a given problem. A spot-check experiment allows you to automatically and objectively discover those algorithms that are the best at picking out the structure in the problem so you can focus your attention. Results: Spot-checking algorithms gets you usable results, fast. You may discover a good enough solution in the first spot experiment. Alternatively, you may quickly learn that your dataset does not expose enough structure for any mainstream algorithm to do well. Spot-checking gives you the results you need to decide whether to move forward and optimize a given model or backward and revisit the presentation of the problem.

I think spot checking mainstream algorithms on your problem is a no-brainer first step.

Tips for Spot-Checking Algorithms

There are some things you can do when you are spot-checking algorithms to ensure you are getting useful and actionable results.

Below are 5 tips to ensure you are getting the most from spot-checking machine learning algorithms on your problem.

Algorithm Diversity : You want a good mix of algorithm types. I like to include instance based methods (live LVQ and knn), functions and kernels (like neural nets, regression and SVM), rule systems (like Decision Table and RIPPER) and decision trees (like CART, ID3 and C4.5).

: You want a good mix of algorithm types. I like to include instance based methods (live LVQ and knn), functions and kernels (like neural nets, regression and SVM), rule systems (like Decision Table and RIPPER) and decision trees (like CART, ID3 and C4.5). Best Foot Forward : Each algorithm needs to be given a chance to put it’s best foot forward. This does not mean performing a sensitivity analysis on the parameters of each algorithm, but using experiments and heuristics to give each algorithm a fair chance. For example if kNN is in the mix, give it 3 chances with k values of 1, 5 and 7.

: Each algorithm needs to be given a chance to put it’s best foot forward. This does not mean performing a sensitivity analysis on the parameters of each algorithm, but using experiments and heuristics to give each algorithm a fair chance. For example if kNN is in the mix, give it 3 chances with k values of 1, 5 and 7. Formal Experiment : Don’t play. There is a huge temptation to try lots of different things in an informal manner, to play around with algorithms on your problem. The idea of spot-checking is to get to the methods that do well on the problem, fast. Design the experiment, run it, then analyze the results. Be methodical. I like to rank algorithms by their statistical significant wins (in pairwise comparisons) and take the top 3-5 as a basis for tuning.

: Don’t play. There is a huge temptation to try lots of different things in an informal manner, to play around with algorithms on your problem. The idea of spot-checking is to get to the methods that do well on the problem, fast. Design the experiment, run it, then analyze the results. Be methodical. I like to rank algorithms by their statistical significant wins (in pairwise comparisons) and take the top 3-5 as a basis for tuning. Jumping-off Point : The best performing algorithms are a starting point not the solution to the problem. The algorithms that are shown to be effective may not be the best algorithms for the job. They are most likely to be useful pointers to types of algorithms that perform well on the problem. For example, if kNN does well, consider follow-up experiments on all the instance based methods and variations of kNN you can think of.

: The best performing algorithms are a starting point not the solution to the problem. The algorithms that are shown to be effective may not be the best algorithms for the job. They are most likely to be useful pointers to types of algorithms that perform well on the problem. For example, if kNN does well, consider follow-up experiments on all the instance based methods and variations of kNN you can think of. Build Your Short-list: As you learn and try many different algorithms you can add new algorithms to the suite of algorithms that you use in a spot-check experiment. When I discover a particularly powerful configuration of an algorithm, I like to generalize it and include it in my suite, making my suite more robust for the next problem.

Start building up your suite of algorithms for spot check experiments.

Top 10 Algorithms

There was a paper published in 2008 titled “Top 10 algorithms in data mining“. Who could go past a title like that? It was also turned into a book “The Top Ten Algorithms in Data Mining” and inspired the structure of another “Machine Learning in Action”.

This might be a good paper for you to jump start your short-list of algorithms to spot-check on your next machine learning problem. The top 10 algorithms for data mining listed in the paper were.

C4.5 This is a decision tree algorithm and includes descendent methods like the famous C5.0 and ID3 algorithms.

k-means. The go-to clustering algorithm.

Support Vector Machines. This is really a huge field of study.

Apriori. This is the go-to algorithm for rule extraction.

EM. Along with k-means, go-to clustering algorithm.

PageRank. I rarely touch graph-based problems.

AdaBoost. This is really the family of boosting ensemble methods.

knn (k-nearest neighbor). Simple and effective instance-based method.

Naive Bayes. Simple and robust use of Bayes theorem on data.

CART (classification and regression trees) another tree-based method.

There is also a great Quora question on this topic that you could mine for ideas of algorithms to try on your problem.

Resources

Which algorithms do you like to spot-check on problems? Do you have a favorite?";¿Por qué debería ser Algoritmos de comprobación puntual en sus problemas de aprendizaje automático?;"Tweet Compartir Compartir Última actualización el 7 de abril de 2018 Los algoritmos de verificación puntual se trata de obtener una evaluación rápida de un montón de algoritmos diferentes en su problema de aprendizaje automático para que sepa en qué algoritmos enfocarse y qué descartar. En esta publicación, descubrirá los 3 beneficios de los algoritmos de comprobación puntual, 5 consejos para la comprobación puntual de su próximo problema y los 10 algoritmos de minería de datos más populares que podría utilizar en su conjunto de algoritmos para la comprobación puntual. Algoritmos de comprobación puntual Los algoritmos de comprobación puntual son parte del proceso de aprendizaje automático aplicado. En un nuevo problema, debe determinar rápidamente qué tipo o clase de algoritmos es bueno para elegir la estructura de su problema y cuáles no. La alternativa a la comprobación puntual es que te sientes abrumado por la gran cantidad de algoritmos y tipos de algoritmos que podrías probar y que terminas probando muy pocos o siguiendo lo que te ha funcionado en el pasado. Esto resulta en tiempo perdido y resultados por debajo del par. Beneficios de los algoritmos de comprobación puntual Hay tres ventajas clave de los algoritmos de comprobación puntual en sus problemas de aprendizaje automático: Velocidad: podría pasar mucho tiempo jugando con diferentes algoritmos, ajustando parámetros y pensando qué algoritmos funcionarán bien en su problema . He estado allí y termino probando los mismos algoritmos una y otra vez porque no he sido sistemático. Un solo experimento de verificación puntual puede ahorrar horas, días e incluso semanas de ganas. : Podría pasar mucho tiempo jugando con diferentes algoritmos, ajustando parámetros y pensando qué algoritmos funcionarán bien en su problema. He estado allí y termino probando los mismos algoritmos una y otra vez porque no he sido sistemático. Un solo experimento de verificación puntual puede ahorrar horas, días e incluso semanas de ganas. Objetivo: hay una tendencia a ir con lo que te ha funcionado antes. Elegimos nuestro algoritmo (o algoritmos) favorito y los aplicamos a cada problema que vemos. El poder del aprendizaje automático es que hay muchas formas diferentes de abordar un problema determinado. Un experimento de verificación puntual le permite descubrir de forma automática y objetiva los algoritmos que son mejores para seleccionar la estructura del problema para que pueda centrar su atención. : Hay una tendencia a ir con lo que te ha funcionado antes. Elegimos nuestro algoritmo (o algoritmos) favorito y los aplicamos a cada problema que vemos. El poder del aprendizaje automático es que hay muchas formas diferentes de abordar un problema determinado. Un experimento de verificación puntual le permite descubrir de forma automática y objetiva los algoritmos que son mejores para seleccionar la estructura del problema para que pueda centrar su atención. Resultados: los algoritmos de comprobación puntual le permiten obtener resultados útiles rápidamente. Puede descubrir una solución lo suficientemente buena en el primer experimento puntual. Alternativamente, puede aprender rápidamente que su conjunto de datos no expone suficiente estructura para que cualquier algoritmo convencional funcione bien. La verificación puntual le brinda los resultados que necesita para decidir si avanzar y optimizar un modelo dado o hacia atrás y volver a la presentación del problema. Creo que verificar los algoritmos principales de su problema es un primer paso obvio. Consejos para los algoritmos de comprobación puntual Hay algunas cosas que puede hacer cuando está utilizando algoritmos de comprobación puntual para asegurarse de obtener resultados útiles y procesables. A continuación se presentan 5 consejos para garantizar que obtenga el máximo provecho de los algoritmos de aprendizaje automático de comprobación puntual de su problema. Diversidad de algoritmos: desea una buena combinación de tipos de algoritmos. Me gusta incluir métodos basados en instancias (live LVQ y knn), funciones y núcleos (como redes neuronales, regresión y SVM), sistemas de reglas (como Tabla de decisiones y RIPPER) y árboles de decisión (como CART, ID3 y C4.5). : Desea una buena combinación de tipos de algoritmos. Me gusta incluir métodos basados en instancias (live LVQ y knn), funciones y núcleos (como redes neuronales, regresión y SVM), sistemas de reglas (como Tabla de decisiones y RIPPER) y árboles de decisión (como CART, ID3 y C4.5). Mejor avance: cada algoritmo debe tener la oportunidad de poner su mejor avance. Esto no significa realizar un análisis de sensibilidad sobre los parámetros de cada algoritmo, sino usar experimentos y heurísticas para dar a cada algoritmo una oportunidad justa. Por ejemplo, si kNN está en la mezcla, dale 3 oportunidades con k valores de 1, 5 y 7.: Cada algoritmo debe tener la oportunidad de poner su mejor pie adelante. Esto no significa realizar un análisis de sensibilidad sobre los parámetros de cada algoritmo, sino usar experimentos y heurísticas para dar a cada algoritmo una oportunidad justa. Por ejemplo, si kNN está en la mezcla, dale 3 oportunidades con k valores de 1, 5 y 7. Experimento formal: no juegues. Hay una gran tentación de probar muchas cosas diferentes de manera informal, para jugar con algoritmos sobre su problema. La idea de la verificación puntual es llegar rápidamente a los métodos que funcionan bien en el problema. Diseñe el experimento, ejecútelo y luego analice los resultados. Sé metódico Me gusta clasificar los algoritmos por sus ganancias estadísticamente significativas (en comparaciones por pares) y tomar los primeros 3-5 como base para el ajuste. : No juegues. Hay una gran tentación de probar muchas cosas diferentes de manera informal, para jugar con algoritmos sobre su problema. La idea de la verificación puntual es llegar rápidamente a los métodos que funcionan bien en el problema. Diseñe el experimento, ejecútelo y luego analice los resultados. Sé metódico Me gusta clasificar los algoritmos por sus ganancias estadísticamente significativas (en comparaciones por pares) y tomar los primeros 3-5 como base para el ajuste. Punto de salto: Los mejores algoritmos de rendimiento son un punto de partida, no la solución al problema. Los algoritmos que se muestran efectivos pueden no ser los mejores algoritmos para el trabajo. Es más probable que sean punteros útiles para tipos de algoritmos que funcionan bien en el problema. Por ejemplo, si kNN funciona bien, considere realizar experimentos de seguimiento en todos los métodos basados en instancias y variaciones de kNN que se le ocurran. : Los algoritmos de mejor rendimiento son un punto de partida, no la solución al problema. Los algoritmos que se muestran efectivos pueden no ser los mejores algoritmos para el trabajo. Es más probable que sean punteros útiles para tipos de algoritmos que funcionan bien en el problema. Por ejemplo, si kNN funciona bien, considere realizar experimentos de seguimiento en todos los métodos basados en instancias y variaciones de kNN que se le ocurran. Cree su lista corta: a medida que aprende y prueba muchos algoritmos diferentes, puede agregar nuevos algoritmos al conjunto de algoritmos que utiliza en un experimento de verificación puntual. Cuando descubro una configuración particularmente poderosa de un algoritmo, me gusta generalizarlo e incluirlo en mi suite, haciendo que mi suite sea más robusta para el siguiente problema. Comience a construir su conjunto de algoritmos para experimentos de verificación puntual. Los 10 algoritmos principales Hubo un artículo publicado en 2008 titulado &quot;Los 10 algoritmos principales en la minería de datos&quot;. ¿Quién podría pasar un título como ese? También se convirtió en un libro &quot;Los diez mejores algoritmos en minería de datos&quot; e inspiró la estructura de otro &quot;Aprendizaje automático en acción&quot;. Este podría ser un buen documento para que comience rápidamente su corta lista de algoritmos para verificar su próximo problema de aprendizaje automático. Los 10 algoritmos principales para la minería de datos enumerados en el documento fueron. C4.5 Este es un algoritmo de árbol de decisión e incluye métodos descendientes como los famosos algoritmos C5.0 e ID3. k-significa. El algoritmo de agrupación de inicio. Soporte de máquinas de vectores. Este es realmente un gran campo de estudio. A priori. Este es el algoritmo de referencia para la extracción de reglas. EM. Junto con k-means, vaya al algoritmo de agrupación. Rango de página. Raramente toco problemas basados en gráficos. AdaBoost. Esta es realmente la familia de los métodos de conjunto de refuerzo. knn (k-vecino más cercano). Método simple y efectivo basado en instancias. Ingenuo Bayes. Uso simple y robusto del teorema de Bayes en los datos. CART (árboles de clasificación y regresión) otro método basado en árboles. También hay una gran pregunta de Quora sobre este tema que podría extraer para obtener ideas de algoritmos para probar su problema. Recursos ¿Qué algoritmos le gusta verificar en los problemas? ¿Tienes un favorito?"
31;machinelearningmastery.com;https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/;2017-09-19;7 Applications of Deep Learning for Natural Language Processing;"Tweet Share Share

Last Updated on August 7, 2019

The field of natural language processing is shifting from statistical methods to neural network methods.

There are still many challenging problems to solve in natural language. Nevertheless, deep learning methods are achieving state-of-the-art results on some specific language problems.

It is not just the performance of deep learning models on benchmark problems that is most interesting; it is the fact that a single model can learn word meaning and perform language tasks, obviating the need for a pipeline of specialized and hand-crafted methods.

In this post, you will discover 7 interesting natural language processing tasks where deep learning methods are achieving some headway.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we will look at the following 7 natural language processing problems.

Text Classification Language Modeling Speech Recognition Caption Generation Machine Translation Document Summarization Question Answering

I have tried to focus on the types of end-user problems that you may be interested in, as opposed to more academic or linguistic sub-problems where deep learning does well such as part-of-speech tagging, chunking, named entity recognition, and so on.

Each example provides a description of the problem, an example, and references to papers that demonstrate the methods and results. Most references are drawn from Goldberg’s excellent 2015 primer on deep learning for NLP researchers.

Do you have a favorite NLP application for deep learning that is not listed?

Let me know in the comments below.

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Text Classification

Given an example of text, predict a predefined class label.

The goal of text categorization is to classify the topic or theme of a document.

— Page 575, Foundations of Statistical Natural Language Processing, 1999.

A popular classification example is sentiment analysis where class labels represent the emotional tone of the source text such as “positive” or “negative“.

Below are a 3 more examples:

Spam filtering, classifying email text as spam or not.

Language identification, classifying the language of the source text.

Genre classification, classifying the genre of a fictional story.

Further, the problem may be framed in a way that requires multiple classes assigned to a text, so-called multi-label classification. Such as predicting multiple hashtags for a source tweet.

For more on the general topic, see:

Below are 3 examples of deep learning papers for text classification:

2. Language Modeling

Language modeling is really a subtask of more interesting natural language problems, specifically those that condition the language model on some other input.

… the problem is to predict the next word given the previous words. The task is fundamental to speech or optical character recognition, and is also used for spelling correction, handwriting recognition, and statistical machine translation.

— Page 191, Foundations of Statistical Natural Language Processing, 1999.

In addition to the academic interest in language modeling, it is a key component of many deep learning natural language processing architectures.

A language model learns the probabilistic relationship between words such that new sequences of words can be generated that are statistically consistent with the source text.

Alone, language models can be used for text or speech generation; for example:

Generating new article headlines.

Generating new sentences, paragraphs, or documents.

Generating suggested continuation of a sentence.

For more in language modeling, see:

Below is an example of deep learning for language modeling (only):

Language model of English texts, books and news articles. A Neural Probabilistic Language Model, 2003



3. Speech Recognition

Speech recognition is the problem of understanding what was said.

The task of speech recognition is to map an acoustic signal containing a spoken natural language utterance into the corresponding sequence of words intended by the speaker.

— Page 458, Deep Learning, 2016.

Given an utterance of text as audio data, the model must produce human readable text.

Given the automatic nature of the process, the problem may also be called Automatic Speech Recognition (ASR).

A language model is used to create the text output that is conditioned on the audio data.

Some examples include:

Transcribing a speech.

Creating text captions for a movie or TV show.

Issuing commands to the radio while driving.

For more on speech recognition, see:

Below are 3 examples of deep learning for speech recognition.

4. Caption Generation

Caption generation is the problem of describing the contents of an image.

Given a digital image, such as a photo, generate a textual description of the contents of the image.

A language model is used to create the caption that is conditioned on the image.

Some examples include:

Describing the contents of a scene.

Creating a caption for a photograph.

Describing a video.

This is not just an application for the hearing impaired, but also in generating human readable text for image and video data that can be searched, such as on the web.

Below are 3 examples of deep learning for caption generation:

5. Machine Translation

Machine translation is the problem of converting a source text in one language to another language.

Machine translation, the automatic translation of text or speech from one language to another, is one [of] the most important applications of NLP.

— Page 463, Foundations of Statistical Natural Language Processing, 1999.

Given that deep neural networks are used, the field is referred to as neural machine translation.

In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this int a sequence of symbols in another language. This is commonly applied to natural languages, such as translating from English to French. Deep learning has recently begun to have an important impact on this kind of task.

— Page 98, Deep Learning, 2016.

A language model is used to output the destination text in the second language, conditioned on the source text.

Some examples include:

Translating a text document from French to English.

Translating Spanish audio to German text.

Translating English text to Italian audio.

For more on neural machine translation, see:

Below are 3 examples of deep learning for machine translation:

6. Document Summarization

Document summarization is the task where a short description of a text document is created.

As above, a language model is used to output the summary conditioned on the full document.

Some examples of document summarization include:

Creating a heading for a document.

Creating an abstract of a document.

For more on the topic, see:

Below are 3 examples of deep learning for document summarization:

7. Question Answering

Question answering is the problem where given a subject, such as a document of text, answer a specific question about the subject.

… question answering systems which try to answer a user query that is formulated in the form of a question by return the appropriate none phrase such as a location, a person, or a date. For example, the question Why killed President Kennedy? might be answered with the noun phrase Oswald

— Page 377, Foundations of Statistical Natural Language Processing, 1999.

Some examples include:

For more information on question answering, see:

Answering questions about Wikipedia articles.

Answering questions about news articles.

Answering questions about medical records.

Below are 3 examples of deep learning for question answering:

Further Reading

This section provides more resources on deep learning applications for NLP if you are looking go deeper.

Summary

In this post, you discovered 7 applications of deep learning to natural language processing tasks.

Was your favorite example of deep learning for NLP missed?

Let me know in the comments.

Do you have any questions?

Ask your questions in the comments below and I will do my best to answer.

Develop Deep Learning models for Text Data Today! Develop Your Own Text models in Minutes ...with just a few lines of python code Discover how in my new Ebook:

Deep Learning for Natural Language Processing It provides self-study tutorials on topics like:

Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more... Finally Bring Deep Learning to your Natural Language Processing Projects Skip the Academics. Just Results. See What's Inside";7 aplicaciones de aprendizaje profundo para el procesamiento del lenguaje natural;"Tweet Compartir Compartir Última actualización el 7 de agosto de 2019 El campo del procesamiento del lenguaje natural está cambiando de métodos estadísticos a métodos de redes neuronales. Todavía hay muchos problemas desafiantes para resolver en lenguaje natural. Sin embargo, los métodos de aprendizaje profundo están logrando resultados de vanguardia en algunos problemas de lenguaje específicos. Lo más interesante no es solo el rendimiento de los modelos de aprendizaje profundo sobre problemas de referencia; Es el hecho de que un solo modelo puede aprender el significado de las palabras y realizar tareas de lenguaje, obviando la necesidad de una tubería de métodos especializados y artesanales. En esta publicación, descubrirá 7 interesantes tareas de procesamiento del lenguaje natural donde los métodos de aprendizaje profundo están logrando cierto avance. Descubra cómo desarrollar modelos de aprendizaje profundo para la clasificación de textos, traducción, subtítulos de fotos y más en mi nuevo libro, con 30 tutoriales paso a paso y código fuente completo. Empecemos. Descripción general En esta publicación, veremos los siguientes 7 problemas de procesamiento del lenguaje natural. Clasificación de texto Modelado de idiomas Reconocimiento de voz Generación de subtítulos Traducción automática Resumen de documentos Respuestas a preguntas He tratado de centrarme en los tipos de problemas del usuario final que le pueden interesar, en lugar de los subproblemas más académicos o lingüísticos donde el aprendizaje profundo funciona bien. como etiquetado, fragmentación, reconocimiento de entidades con nombre, etc. Cada ejemplo proporciona una descripción del problema, un ejemplo y referencias a documentos que demuestran los métodos y resultados. La mayoría de las referencias provienen del excelente manual de 2015 de Goldberg sobre aprendizaje profundo para investigadores de PNL. ¿Tiene una aplicación de PNL favorita para el aprendizaje profundo que no figura en la lista? Déjame saber abajo en los comentarios. ¿Necesita ayuda con Deep Learning para datos de texto? Tome mi curso gratuito de 7 días por correo electrónico ahora (con código). Haga clic para registrarse y también obtenga una versión gratuita en PDF del libro. Comience su curso intensivo GRATUITO ahora 1. Clasificación de texto Dado un ejemplo de texto, prediga una etiqueta de clase predefinida. El objetivo de la categorización de texto es clasificar el tema o el tema de un documento. - Página 575, Fundamentos del procesamiento estadístico del lenguaje natural, 1999. Un ejemplo de clasificación popular es el análisis de sentimientos donde las etiquetas de clase representan el tono emocional del texto fuente, como &quot;positivo&quot; o &quot;negativo&quot;. A continuación hay 3 ejemplos más: Filtrado de spam, clasificando el texto del correo electrónico como spam o no. Identificación del idioma, clasificando el idioma del texto fuente. Clasificación de género, clasificando el género de una historia de ficción. Además, el problema puede enmarcarse de una manera que requiere múltiples clases asignadas a un texto, llamada clasificación de etiquetas múltiples. Como predecir múltiples hashtags para un tweet fuente. Para obtener más información sobre el tema general, consulte: A continuación se presentan 3 ejemplos de documentos de aprendizaje profundo para la clasificación de textos: 2. Modelado de idiomas El modelado de idiomas es realmente una subtarea de problemas de lenguaje natural más interesantes, específicamente aquellos que condicionan el modelo de lenguaje en alguna otra entrada. ... el problema es predecir la siguiente palabra dadas las palabras anteriores. La tarea es fundamental para el reconocimiento de voz o de caracteres ópticos, y también se utiliza para la corrección ortográfica, el reconocimiento de escritura a mano y la traducción automática estadística. - Página 191, Fundamentos del procesamiento estadístico del lenguaje natural, 1999. Además del interés académico en el modelado del lenguaje, es un componente clave de muchas arquitecturas de procesamiento del lenguaje natural de aprendizaje profundo. Un modelo de lenguaje aprende la relación probabilística entre palabras de tal manera que se pueden generar nuevas secuencias de palabras que sean estadísticamente consistentes con el texto fuente. Solo, los modelos de lenguaje se pueden usar para generar texto o voz; por ejemplo: generar nuevos titulares de artículos. Generando nuevas oraciones, párrafos o documentos. Generando la continuación sugerida de una oración. Para obtener más información sobre el modelado de idiomas, consulte: A continuación se muestra un ejemplo de aprendizaje profundo para el modelado de idiomas (solo): Modelo de lenguaje de textos en inglés, libros y artículos de noticias. A Neural Probabilistic Language Model, 2003 3. Reconocimiento del habla El reconocimiento del habla es el problema de entender lo que se dijo. La tarea del reconocimiento de voz es mapear una señal acústica que contenga un enunciado de lenguaje natural hablado en la secuencia correspondiente de palabras previstas por el hablante. - Página 458, Deep Learning, 2016. Dada una emisión de texto como datos de audio, el modelo debe producir texto legible por humanos. Dada la naturaleza automática del proceso, el problema también se puede llamar Reconocimiento automático de voz (ASR). Se utiliza un modelo de idioma para crear la salida de texto que está condicionada a los datos de audio. Algunos ejemplos incluyen: Transcripción de un discurso. Crear subtítulos de texto para una película o programa de televisión. Emitir comandos a la radio mientras conduce. Para obtener más información sobre el reconocimiento de voz, consulte: A continuación se presentan 3 ejemplos de aprendizaje profundo para el reconocimiento de voz. 4. Generación de subtítulos La generación de subtítulos es el problema de describir el contenido de una imagen. Dada una imagen digital, como una foto, genera una descripción textual de los contenidos de la imagen. Se utiliza un modelo de idioma para crear el título que está condicionado a la imagen. Algunos ejemplos incluyen: Describir el contenido de una escena. Crear un subtítulo para una fotografía. Describiendo un video. Esta no es solo una aplicación para personas con discapacidad auditiva, sino también para generar texto legible por humanos para datos de imagen y video que se pueden buscar, como en la web. A continuación se presentan 3 ejemplos de aprendizaje profundo para la generación de subtítulos: 5. Traducción automática La traducción automática es el problema de convertir un texto fuente en un idioma a otro idioma. La traducción automática, la traducción automática de texto o voz de un idioma a otro, es una [de] las aplicaciones más importantes de PNL. - Página 463, Fundamentos del procesamiento estadístico del lenguaje natural, 1999. Dado que se utilizan redes neuronales profundas, el campo se denomina traducción automática neuronal. En una tarea de traducción automática, la entrada ya consiste en una secuencia de símbolos en algún idioma, y el programa de computadora debe convertir esto en una secuencia de símbolos en otro idioma. Esto se aplica comúnmente a los idiomas naturales, como la traducción del inglés al francés. El aprendizaje profundo ha comenzado recientemente a tener un impacto importante en este tipo de tarea. - Página 98, Deep Learning, 2016. Se utiliza un modelo de idioma para generar el texto de destino en el segundo idioma, condicionado al texto fuente. Algunos ejemplos incluyen: Traducir un documento de texto del francés al inglés. Traducción de audio en español a texto en alemán. Traducción de texto en inglés al audio italiano. Para obtener más información sobre la traducción automática neuronal, consulte: A continuación se presentan 3 ejemplos de aprendizaje profundo para la traducción automática: 6. Resumen de documentos El resumen de documentos es la tarea en la que se crea una breve descripción de un documento de texto. Como anteriormente, se utiliza un modelo de lenguaje para generar el resumen condicionado en el documento completo. Algunos ejemplos de resumen de documentos incluyen: Crear un encabezado para un documento. Crear un resumen de un documento. Para obtener más información sobre el tema, consulte: A continuación se presentan 3 ejemplos de aprendizaje profundo para el resumen de documentos: 7. Respuesta a preguntas La respuesta a preguntas es el problema en el que, dado un tema, como un documento de texto, responde una pregunta específica sobre el tema. … Sistemas de contestación de preguntas que intentan responder una consulta del usuario que se formula en forma de una pregunta devolviendo la frase que no corresponde, como una ubicación, una persona o una fecha. Por ejemplo, la pregunta ¿Por qué mató al presidente Kennedy? podría responderse con la frase nominal Oswald - Página 377, Fundamentos del procesamiento estadístico del lenguaje natural, 1999. Algunos ejemplos incluyen: Para obtener más información sobre la respuesta a preguntas, consulte: Responder preguntas sobre artículos de Wikipedia. Responder preguntas sobre artículos de noticias. Responder preguntas sobre registros médicos. A continuación se presentan 3 ejemplos de aprendizaje profundo para responder preguntas: Lecturas adicionales Esta sección proporciona más recursos sobre aplicaciones de aprendizaje profundo para PNL si está buscando profundizar. Resumen En esta publicación, descubrió 7 aplicaciones de aprendizaje profundo para tareas de procesamiento de lenguaje natural. ¿Se perdió su ejemplo favorito de aprendizaje profundo para PNL? Házmelo saber en los comentarios. ¿Tiene usted alguna pregunta? Haga sus preguntas en los comentarios a continuación y haré todo lo posible para responder. ¡Desarrolle modelos de aprendizaje profundo para datos de texto hoy! Desarrolle sus propios modelos de texto en minutos ... con solo unas pocas líneas de código de Python Descubra cómo en mi nuevo Ebook: Aprendizaje profundo para el procesamiento del lenguaje natural Proporciona tutoriales de autoaprendizaje sobre temas como: Bolsa de palabras, Incrustación de palabras, Modelos de lenguaje, generación de subtítulos, traducción de texto y mucho más ... Finalmente, lleve el aprendizaje profundo a sus proyectos de procesamiento de lenguaje natural Omita lo académico. Solo resultados. Mira lo que hay dentro"
32;machinelearningmastery.com;https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/;2018-01-09;How to Develop a Neural Machine Translation System from Scratch;"from pickle import load

from numpy import array

from numpy import argmax

from keras . preprocessing . text import Tokenizer

from keras . preprocessing . sequence import pad_sequences

from keras . models import load_model

from nltk . translate . bleu_score import corpus_bleu

# load a clean dataset

def load_clean_sentences ( filename ) :

return load ( open ( filename , 'rb' ) )

# fit a tokenizer

def create_tokenizer ( lines ) :

tokenizer = Tokenizer ( )

tokenizer . fit_on_texts ( lines )

return tokenizer

# max sentence length

def max_length ( lines ) :

return max ( len ( line . split ( ) ) for line in lines )

# encode and pad sequences

def encode_sequences ( tokenizer , length , lines ) :

# integer encode sequences

X = tokenizer . texts_to_sequences ( lines )

# pad sequences with 0 values

X = pad_sequences ( X , maxlen = length , padding = 'post' )

return X

# map an integer to a word

def word_for_id ( integer , tokenizer ) :

for word , index in tokenizer . word_index . items ( ) :

if index == integer :

return word

return None

# generate target given source sequence

def predict_sequence ( model , tokenizer , source ) :

prediction = model . predict ( source , verbose = 0 ) [ 0 ]

integers = [ argmax ( vector ) for vector in prediction ]

target = list ( )

for i in integers :

word = word_for_id ( i , tokenizer )

if word is None :

break

target . append ( word )

return ' ' . join ( target )

# evaluate the skill of the model

def evaluate_model ( model , tokenizer , sources , raw_dataset ) :

actual , predicted = list ( ) , list ( )

for i , source in enumerate ( sources ) :

# translate encoded source text

source = source . reshape ( ( 1 , source . shape [ 0 ] ) )

translation = predict_sequence ( model , eng_tokenizer , source )

raw_target , raw_src = raw_dataset [ i ]

if i < 10 :

print ( 'src=[%s], target=[%s], predicted=[%s]' % ( raw_src , raw_target , translation ) )

actual . append ( [ raw_target . split ( ) ] )

predicted . append ( translation . split ( ) )

# calculate BLEU score

print ( 'BLEU-1: %f' % corpus_bleu ( actual , predicted , weights = ( 1.0 , 0 , 0 , 0 ) ) )

print ( 'BLEU-2: %f' % corpus_bleu ( actual , predicted , weights = ( 0.5 , 0.5 , 0 , 0 ) ) )

print ( 'BLEU-3: %f' % corpus_bleu ( actual , predicted , weights = ( 0.3 , 0.3 , 0.3 , 0 ) ) )

print ( 'BLEU-4: %f' % corpus_bleu ( actual , predicted , weights = ( 0.25 , 0.25 , 0.25 , 0.25 ) ) )

# load datasets

dataset = load_clean_sentences ( 'english-german-both.pkl' )

train = load_clean_sentences ( 'english-german-train.pkl' )

test = load_clean_sentences ( 'english-german-test.pkl' )

# prepare english tokenizer

eng_tokenizer = create_tokenizer ( dataset [ : , 0 ] )

eng_vocab_size = len ( eng_tokenizer . word_index ) + 1

eng_length = max_length ( dataset [ : , 0 ] )

# prepare german tokenizer

ger_tokenizer = create_tokenizer ( dataset [ : , 1 ] )

ger_vocab_size = len ( ger_tokenizer . word_index ) + 1

ger_length = max_length ( dataset [ : , 1 ] )

# prepare data

trainX = encode_sequences ( ger_tokenizer , ger_length , train [ : , 1 ] )

testX = encode_sequences ( ger_tokenizer , ger_length , test [ : , 1 ] )

# load model

model = load_model ( 'model.h5' )

# test on some training sequences

print ( 'train' )

evaluate_model ( model , eng_tokenizer , trainX , train )

# test on some test sequences

print ( 'test' )";Cómo desarrollar un sistema de traducción automática neuronal desde cero;"desde pickle import load desde numpy import array desde numpy import argmax desde keras. preprocesamiento Tokenizer de importación de texto desde keras. preprocesamiento secuencia importar pad_sequences de keras. los modelos importan load_model de nltk. traducir bleu_score import corpus_bleu # cargar un conjunto de datos limpio def load_clean_sentences (nombre de archivo): return load (abrir (nombre de archivo, &#39;rb&#39;)) # ajustar un tokenizador def create_tokenizer (líneas): tokenizer = Tokenizer () tokenizer. fit_on_texts (líneas) return tokenizer # longitud máxima de la oración def max_length (líneas): return max (len (línea. split ()) para línea en líneas) # codificar y secuencias de pad def encode_sequences (tokenizer, length, lines): # codificación entera secuencias X = tokenizer. text_to_sequences (líneas) # secuencias de pad con 0 valores X = pad_sequences (X, maxlen = length, padding = &#39;post&#39;) devuelve X # asigna un entero a una palabra def word_for_id (entero, tokenizer): por palabra, indexa en el tokenizer. word_index. items (): if index == integer: return word return None # generar destino fuente dada secuencia def predict_sequence (model, tokenizer, source): prediction = model. predecir (fuente, detallado = 0) [0] enteros = [argmax (vector) para vector en predicción] target = list () para i en enteros: word = word_for_id (i, tokenizer) si la palabra es None: break target. agregar (palabra) regresar &#39;&#39;. join (target) # evalúa la habilidad del modelo def eval_model (model, tokenizer, sources, raw_dataset): actual, predicho = lista (), list () para i, fuente en enumerate (fuentes): # traduce fuente de texto fuente codificada = fuente. remodelar ((1, fuente. forma [0])) traducción = predecir_secuencia (modelo, eng_tokenizer, fuente) raw_target, raw_src = raw_dataset [i] if i &lt;10: print (&#39;src = [% s], target = [% s], predicho = [% s] &#39;% (raw_src, raw_target, translation)) actual. append ([raw_target. split ()]) predicho. agregar (traducción. división ()) # calcular puntaje BLEU print (&#39;BLEU-1:% f&#39;% corpus_bleu (real, predicho, pesos = (1.0, 0, 0, 0))) print (&#39;BLEU-2:% f &#39;% corpus_bleu (real, pronosticado, pesos = (0.5, 0.5, 0, 0))) print (&#39; BLEU-3:% f &#39;% corpus_bleu (real, predicho, pesos = (0.3, 0.3, 0.3, 0) )) print (&#39;BLEU-4:% f&#39;% corpus_bleu (real, predicho, pesos = (0.25, 0.25, 0.25, 0.25))) # cargar conjuntos de datos dataset = load_clean_sentences (&#39;english-german-both.pkl&#39;) train = load_clean_sentences (&#39;english-german-train.pkl&#39;) test = load_clean_sentences (&#39;english-german-test.pkl&#39;) # preparar el tokenizador inglés eng_tokenizer = create_tokenizer (conjunto de datos [:, 0]) eng_vocab_size = len (eng_tokenizer. word_index) + 1 eng_length = max_length (conjunto de datos [:, 0]) # preparar el tokenizador alemán ger_tokenizer = create_tokenizer (conjunto de datos [:, 1]) ger_vocab_size = len (ger_tokenizer. Word_index) + 1 ger_length = max_length (conjunto de datos [:, 1]) # preparar datos trainX = encode_sequences (ger_tokenizer, ger_length, train [:, 1]) testX = encode_sequences (ger_tokenizer, ger_length, test [:, 1]) # load model model = load_model (&#39;model.h5&#39;) # prueba en algunas secuencias de entrenamiento print (&#39;train&#39;) Evaluation_model (model, eng_tokenizer, trainX, train) # prueba en algunas secuencias de prueba print (&#39;test&#39;)"
33;machinelearningmastery.com;http://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/;2014-02-18;How To Choose The Right Test Options When Evaluating Machine Learning Algorithms;"Tweet Share Share

Last Updated on June 21, 2016

The test options you use when evaluating machine learning algorithms can mean the difference between over-learning, a mediocre result and a usable state-of-the-art result that you can confidently shout from the roof tops (you really do feel like doing that sometimes).

In this post you will discover the standard test options you can use in your algorithm evaluation test harness and how to choose the right options next time.

Randomness

The root of the difficulty in choosing the right test options is randomness. Most (almost all) machine learning algorithms use randomness in some way. The randomness may be explicit in the algorithm or may be in the sample of the data selected to train the algorithm.

This does not mean that the algorithms produce random results, it means that they produce results with some noise or variance. We call this type of limited variance, stochastic and the algorithms that exploit it, stochastic algorithms.

Train and Test on Same Data

If you have a dataset, you may want to train the model on the dataset and then report the results of the model on that dataset. That’s how good the model is, right?

The problem with this approach of evaluating algorithms is that you indeed will know the performance of the algorithm on the dataset, but do not have any indication of how the algorithm will perform on data that the model was not trained on (so-called unseen data).

This matters, only if you want to use the model to make predictions on unseen data.

Split Test

A simple way to use one dataset to both train and estimate the performance of the algorithm on unseen data is to split the dataset. You take the dataset, and split it into a training dataset and a test dataset. For example, you randomly select 66% of the instances for training and use the remaining 34% as a test dataset.

The algorithm is run on the training dataset and a model is created and assessed on the test dataset and you get a performance accuracy, lets say 87% classification accuracy.

Spit tests are fast and great when you have a lot of data or when training a model is expensive (it resources or time). A split test on a very very large dataset can produce an accurate estimate of the actual performance of the algorithm.

How good is the algorithm on the data? Can we confidently say it can achieve an accuracy of 87%?

A problem is that if we spit the training dataset again into a different 66%/34% split, we would get a different result from our algorithm. This is called model variance.

Multiple Split Tests

A solution to our problem with the split test getting different results on different splits of the dataset is to reduce the variance of the random process and do it many times. We can collect the results from a fair number of runs (say 10) and take the average.

For example, let’s say we split our dataset 66%/34%, ran our algorithm and got an accuracy and we did this 10 times with 10 different splits. We might have 10 accuracy scores as follows: 87, 87, 88, 89, 88, 86, 88, 87, 88, 87.

The average performance of our model is 87.5, with a standard deviation of about 0.85.

A problem with multiple split tests is that it is possible that some data instance are never included for training or testing, where as others may be selected multiple times. The effect is that this may skew results and may not give an meaningful idea of the accuracy of the algorithm.

Cross Validation

A solution to the problem of ensuring each instance is used for training and testing an equal number of times while reducing the variance of an accuracy score is to use cross validation. Specifically k-fold cross validation, where k is the number of splits to make in the dataset.

For example, let’s choose a value of k=10 (very common). This will split the dataset into 10 parts (10 folds) and the algorithm will be run 10 times. Each time the algorithm is run, it will be trained on 90% of the data and tested on 10%, and each run of the algorithm will change which 10% of the data the algorithm is tested on.

In this example, each data instance will be used as a training instance exactly 9 times and as a test instance 1 time. The accuracy will not be a mean and a standard deviation, but instead will be an exact accuracy score of how many correct predictions were made.

The k-fold cross validation method is the go-to method for evaluating the performance of an algorithm on a dataset. You want to choose k-values that give you a good sized training and test dataset for your algorithm. Not too disproportionate (too large or small for training or test). If you have a lot of data, you may may have to resort to either sampling the data or reverting to a split test.

Cross validation does give an unbiased estimation of the algorithms performance on unseen data, but what if the algorithm itself uses randomness. The algorithm would produce different results for the same training data each time it was trained with a different random number seed (start of the sequence of pseudo-randomness). Cross validation does not account for variance in the algorithm’s predictions.

Another point of concern is that cross validation itself uses randomness to decide how to split the dataset into k folds. Cross validation does not estimate how the algorithm perform with different sets of folds.

This only matters if you want to understand how robust the algorithm is on the dataset.

Multiple Cross Validation

A way to account for the variance in the algorithm itself is to run cross validation multiple times and take the mean and the standard deviation of the algorithm accuracy from each run.

This will will give you an an estimate of the performance of the algorithm on the dataset and an estimation of how robust (the size of the standard deviation) the performance is.

If you have one mean and standard deviation for algorithm A and another mean and standard deviation for algorithm B and they differ (for example, algorithm A has a higher accuracy), how do you know if the difference is meaningful?

This only matters if you want to compare the results between algorithms.

Statistical Significance

A solution to comparing algorithm performance measures when using multiple runs of k-fold cross validation is to use statistical significance tests (like the Student’s t-test).

The results from multiple runs of k-fold cross validation is a list of numbers. We like to summarize these numbers using the mean and standard deviation. You can think of these numbers as a sample from an underlying population. A statistical significance test answers the question: are two samples drawn from the same population? (no difference). If the answer is “yes”, then, even if the mean and standard deviations differ, the difference can be said to be not statistically significant.

We can use statistical significance tests to give meaning to the differences (or lack there of) between algorithm results when using multiple runs (like multiple runs of k-fold cross validation with different random number seeds). This can when we want to make accurate claims about results (algorithm A was better than algorithm B and the difference was statistically significant)

This is not the end of the story, because there are different statistical significance tests (parametric and nonparametric) and parameters to those tests (p-value). I’m going to draw the line here because if you have followed me this far, you now know enough about selecting test options to produce rigorous (publishable!) results.

Summary

In this post you have discovered the difference between the main test options available to you when designing a test harness to evaluate machine learning algorithms.

Specifically, you learned the utility and problems with:

Training and testing on the same dataset

Split tests

Multiple split tests

Cross validation

Multiple cross validation

Statistical significance

When in doubt, use k-fold cross validation (k=10) and use multiple runs of k-fold cross validation with statistical significance tests when you want to meaningfully compare algorithms on your dataset.";Cómo elegir las opciones de prueba correctas al evaluar algoritmos de aprendizaje automático;"Tweet Compartir Compartir Última actualización el 21 de junio de 2016 Las opciones de prueba que utiliza al evaluar algoritmos de aprendizaje automático pueden significar la diferencia entre el sobreaprendizaje, un resultado mediocre y un resultado de vanguardia utilizable que puede gritar con confianza desde el tejados (realmente tienes ganas de hacerlo a veces). En esta publicación descubrirá las opciones de prueba estándar que puede usar en su arnés de prueba de evaluación de algoritmos y cómo elegir las opciones correctas la próxima vez. Aleatoriedad La raíz de la dificultad para elegir las opciones de prueba correctas es la aleatoriedad. La mayoría (casi todos) los algoritmos de aprendizaje automático utilizan la aleatoriedad de alguna manera. La aleatoriedad puede ser explícita en el algoritmo o puede estar en la muestra de los datos seleccionados para entrenar el algoritmo. Esto no significa que los algoritmos produzcan resultados aleatorios, significa que producen resultados con algo de ruido o variación. Llamamos a este tipo de varianza limitada, estocástico y los algoritmos que lo explotan, algoritmos estocásticos. Entrene y pruebe con los mismos datos Si tiene un conjunto de datos, puede entrenar el modelo en el conjunto de datos y luego informar los resultados del modelo en ese conjunto de datos. Así de bueno es el modelo, ¿verdad? El problema con este enfoque de evaluación de algoritmos es que realmente conocerá el rendimiento del algoritmo en el conjunto de datos, pero no tiene ninguna indicación de cómo funcionará el algoritmo en los datos sobre los que el modelo no recibió capacitación (los llamados datos no vistos ) Esto es importante, solo si desea utilizar el modelo para hacer predicciones sobre datos no vistos. Prueba dividida Una manera simple de usar un conjunto de datos para entrenar y estimar el rendimiento del algoritmo en datos no vistos es dividir el conjunto de datos. Usted toma el conjunto de datos y lo divide en un conjunto de datos de entrenamiento y un conjunto de datos de prueba. Por ejemplo, selecciona aleatoriamente el 66% de las instancias para el entrenamiento y usa el 34% restante como un conjunto de datos de prueba. El algoritmo se ejecuta en el conjunto de datos de entrenamiento y se crea y evalúa un modelo en el conjunto de datos de prueba y se obtiene una precisión de rendimiento, digamos un 87% de precisión de clasificación. Las pruebas de saliva son rápidas y excelentes cuando tienes muchos datos o cuando entrenar un modelo es costoso (recursos o tiempo). Una prueba dividida en un conjunto de datos muy muy grande puede producir una estimación precisa del rendimiento real del algoritmo. ¿Qué tan bueno es el algoritmo en los datos? ¿Podemos decir con confianza que puede lograr una precisión del 87%? Un problema es que si dividimos el conjunto de datos de entrenamiento nuevamente en una división diferente del 66% / 34%, obtendríamos un resultado diferente de nuestro algoritmo. Esto se llama varianza del modelo. Pruebas divididas múltiples Una solución a nuestro problema con la prueba dividida obteniendo diferentes resultados en diferentes divisiones del conjunto de datos es reducir la varianza del proceso aleatorio y hacerlo muchas veces. Podemos recopilar los resultados de un buen número de carreras (digamos 10) y tomar el promedio. Por ejemplo, digamos que dividimos nuestro conjunto de datos 66% / 34%, ejecutamos nuestro algoritmo y obtuvimos una precisión, y lo hicimos 10 veces con 10 divisiones diferentes. Podríamos tener 10 puntajes de precisión de la siguiente manera: 87, 87, 88, 89, 88, 86, 88, 87, 88, 87. El rendimiento promedio de nuestro modelo es 87.5, con una desviación estándar de aproximadamente 0.85. Un problema con las pruebas de división múltiple es que es posible que algunas instancias de datos nunca se incluyan para capacitación o pruebas, mientras que otras pueden seleccionarse varias veces. El efecto es que esto puede sesgar los resultados y puede no dar una idea significativa de la precisión del algoritmo. Validación cruzada Una solución al problema de garantizar que cada instancia se use para entrenar y probar un número igual de veces mientras se reduce la varianza de un puntaje de precisión es usar la validación cruzada. Específicamente, la validación cruzada de k-fold, donde k es el número de divisiones a realizar en el conjunto de datos. Por ejemplo, elija un valor de k = 10 (muy común). Esto dividirá el conjunto de datos en 10 partes (10 pliegues) y el algoritmo se ejecutará 10 veces. Cada vez que se ejecuta el algoritmo, se entrenará en el 90% de los datos y se probará en el 10%, y cada ejecución del algoritmo cambiará en qué 10% de los datos se prueba el algoritmo. En este ejemplo, cada instancia de datos se utilizará como instancia de entrenamiento exactamente 9 veces y como instancia de prueba 1 vez. La precisión no será una media y una desviación estándar, sino que será una puntuación de precisión exacta de cuántas predicciones correctas se hicieron. El método de validación cruzada k-fold es el método de referencia para evaluar el rendimiento de un algoritmo en un conjunto de datos. Desea elegir valores k que le brinden un conjunto de datos de entrenamiento y prueba de buen tamaño para su algoritmo. No demasiado desproporcionado (demasiado grande o pequeño para entrenamiento o prueba). Si tiene muchos datos, es posible que deba recurrir a muestrear los datos o volver a una prueba dividida. La validación cruzada proporciona una estimación imparcial del rendimiento de los algoritmos en datos no vistos, pero ¿qué sucede si el algoritmo en sí usa aleatoriedad? El algoritmo produciría resultados diferentes para los mismos datos de entrenamiento cada vez que se entrenaba con una semilla de número aleatorio diferente (inicio de la secuencia de pseudoaleatoriedad). La validación cruzada no tiene en cuenta la variación en las predicciones del algoritmo. Otro punto de preocupación es que la validación cruzada en sí misma usa la aleatoriedad para decidir cómo dividir el conjunto de datos en k pliegues. La validación cruzada no estima cómo funciona el algoritmo con diferentes conjuntos de pliegues. Esto solo importa si desea comprender qué tan robusto es el algoritmo en el conjunto de datos. Validación cruzada múltiple Una forma de tener en cuenta la varianza en el algoritmo en sí mismo es ejecutar la validación cruzada varias veces y tomar la media y la desviación estándar de la precisión del algoritmo de cada ejecución. Esto le dará una estimación del rendimiento del algoritmo en el conjunto de datos y una estimación de cuán robusto (el tamaño de la desviación estándar) es el rendimiento. Si tiene una media y una desviación estándar para el algoritmo A y otra media y una desviación estándar para el algoritmo B y difieren (por ejemplo, el algoritmo A tiene una mayor precisión), ¿cómo sabe si la diferencia es significativa? Esto solo importa si desea comparar los resultados entre algoritmos. Significación estadística Una solución para comparar las medidas de rendimiento del algoritmo cuando se utilizan múltiples ejecuciones de validación cruzada k-fold es utilizar pruebas de significación estadística (como la prueba t de Student). Los resultados de múltiples ejecuciones de validación cruzada k-fold son una lista de números. Nos gusta resumir estos números usando la media y la desviación estándar. Puede pensar en estos números como una muestra de una población subyacente. Una prueba de significación estadística responde a la pregunta: ¿se toman dos muestras de la misma población? (ninguna diferencia). Si la respuesta es &quot;sí&quot;, entonces, incluso si la media y las desviaciones estándar difieren, se puede decir que la diferencia no es estadísticamente significativa. Podemos usar pruebas de significación estadística para dar significado a las diferencias (o la falta de ellas) entre los resultados del algoritmo cuando se usan múltiples ejecuciones (como múltiples ejecuciones de validación cruzada k-fold con diferentes semillas de números aleatorios). Esto puede ocurrir cuando queremos hacer afirmaciones precisas sobre los resultados (el algoritmo A fue mejor que el algoritmo B y la diferencia fue estadísticamente significativa). Este no es el final de la historia, porque hay diferentes pruebas de significación estadística (paramétricas y no paramétricas) y parámetros para esas pruebas (valor p). Voy a trazar la línea aquí porque si me has seguido hasta aquí, ahora sabes lo suficiente sobre cómo seleccionar opciones de prueba para producir resultados rigurosos (¡publicables!). Resumen En esta publicación, ha descubierto la diferencia entre las principales opciones de prueba disponibles al diseñar un arnés de prueba para evaluar algoritmos de aprendizaje automático. Específicamente, aprendió la utilidad y los problemas con: Entrenamiento y pruebas en el mismo conjunto de datos Pruebas divididas Pruebas divididas múltiples Validación cruzada Validación cruzada múltiple Significación estadística En caso de duda, use la validación cruzada k-fold (k = 10) y use múltiples corridas de k doble validación cruzada con pruebas de significación estadística cuando desee comparar significativamente algoritmos en su conjunto de datos."
34;news.mit.edu;http://news.mit.edu/2020/dreaming-big-small-country-misti-global-startup-labs-uruguay-0224;;Dreaming big in a small country;"When Miguel Brechner started planning a new ambitious plan to foster a new generation of data scientists in Uruguay and Latin America, he immediately thought of MIT. “There is no question that MIT is a world leader in science and technology. In Uruguay we are a small country, but we dream big.” Brechner is president of Plan Ceibal, an internationally awarded public initiative that has as main goals to distribute technology, promote knowledge, and generate social equity by widening access to digital technologies.

In 2019, Uruguayan public institutions like Plan Ceibal, ANII (Agencia Nacional de Investigación e Innovación), and UTEC (Universidad Tecnológica del Uruguay) began collaborating with MIT International Science and Technology Initiatives (MISTI) and the Abdul Latif Jameel World Education Lab (J-WEL). The partnership supports 60 Latin American students that are part of the Program in Data Science, a program which includes online courses from MITx and on-site workshops run by J-WEL and MISTI. Local students include CEOs, entrepreneurs, engineers, economists, medical professionals, and senior administrators.

The MISTI Global Startup Labs (GSL) program, now in its 20th year, has expanded its partnerships to include Uruguayan institutions to promote entrepreneurship and data science across Latin America. GSL is a unique program designed to offer the opportunity to blend digital technologies and entrepreneurship in emerging regions in the world. Since 1998, hundreds of MIT students have traveled to more than 15 countries to be part of the program that has benefited thousands of technology entrepreneurs around the world. GSL instructors are MIT graduate and undergraduate students, selected among many applicants from all over the institute. GSL programs in different countries are uniquely crafted based on the needs of the local partners, and MIT student instructors take the lead teaching app and web development, coding, data science, entrepreneurship, and intrapreneurship.

The new GSL, one of the first to be run over Independent Acitivities Period, took place during January in Montevideo. The Uruguay program focused specifically on machine learning and the business opportunities of the technology. The local student participants had previously taken courses from the MITx MicroMasters in Data Science, and the GSL workshop gave them the opportunity to experience project-based learning in data science. This hands-on experiential immersion in the subject matter is the core methodology of the GSL program.

More than 30 graduate and undergraduate students applied to be part of GSL in Uruguay this year, and 13 were selected to be part of the workshop in Montevideo. Eduardo Rivera, managing director for Uruguay, explained the process: “Recruiting students for GSL is always a challenge. We look for expertise and experience teaching, but also for team players and risk-takers. The team is composed of students from different disciplines and levels of studies, which makes the experience a unique opportunity for our students to learn from their MIT peers in new and challenging contexts.” Rivera adds, “At MIT, we are fortunate to have plenty of talented and passionate students, willing to cross borders and oceans to teach and learn.”

Over the course of a month, the local students were taught how to build prototypes, create business models, and pitch presentations. The class pursued projects ranging from predictive maintenance to autism detection to logistics optimization. The final results were presented in a pitch event hosted in Zonamerica, Uruguay's premier hub for technology and innovation.

""Working with our local students was a truly unique and unforgettable opportunity,"" says electrical engineering and computer science (EECS) senior Ryan Sander. ""I'm certain I learned just as much from the students as they learned from us. What really left an impression on me was observing not only how bright our students are, but also how passionate these people are about solving real-world problems with high impact.""

For MBA student Kenny Li, the opportunity to interact with the local students was broadening. “In today’s world, you need to be able to understand people’s cultures, how do they approach business, how they interact at work …GSL gave me a great learning opportunity to understand the global context of entrepreneurs.”

When not teaching classes, the MIT students were able to visit various places around Montevideo, including the beautiful beaches of Punta del Este, the neighboring city of Buenos Aires, and relaxing getaways to Colonia. After classes, the teaching team was steps away from the beach and could wind each day down with a beautiful sunset, soaking up the warm summer weather in January.

Rivera finds these cultural connections to be one of the major benefits of the program. “At MISTI, we are certain that international teaching activities contribute not only to the academic formation of the students but also give them valuable tools to interact in multicultural environments and confront new challenges in different locations. For future global leaders, this is a unique opportunity. We often hear from our students that MISTI experiences are life-changing, not only in professional life but also in their personal life.”

""The weekends and weekday evenings were a great way for us to bond with each other and our students,"" says Victoria Pisini, a senior in the MIT Sloan School of Management. “We went to beaches together, traveled to different cities, and shared a lot of unforgettable moments.""

The MIT students participating in this year’s GSL were Amauche Emenari (EECS PhD student), Devin Zhang (MBA student), Evan Mu (EECS PhD student), Geeticka Chauhan (EECS PhD student), Hans Nowak (MBA student), Julian Alverio (EECS MEng student), Kenny Li (MBA student), Madhav Kumar (MIT Sloan PhD candidate), Maya Murad (Leaders for Global Operations master's and PhD student), Ryan Sander (EECS student), Taylor Elise Baum (EECS PhD student), Tiffany Fung (MBA student), and Victoria Pisini (MBA student).

GSL programs are planned in multiple countries for summer 2020 and Independent Activities Period 2021, and there are still a small number of available opportunities for instructors this summer.";Soñando en grande en un país pequeño;"Cuando Miguel Brechner comenzó a planificar un nuevo plan ambicioso para fomentar una nueva generación de científicos de datos en Uruguay y América Latina, inmediatamente pensó en el MIT. “No hay duda de que MIT es un líder mundial en ciencia y tecnología. En Uruguay somos un país pequeño, pero soñamos en grande ”. Brechner es presidente de Plan Ceibal, una iniciativa pública internacionalmente reconocida que tiene como objetivos principales distribuir tecnología, promover el conocimiento y generar equidad social al ampliar el acceso a las tecnologías digitales. En 2019, las instituciones públicas uruguayas como Plan Ceibal, ANII (Agencia Nacional de Investigación e Innovación) y UTEC (Universidad Tecnológica del Uruguay) comenzaron a colaborar con las Iniciativas Internacionales de Ciencia y Tecnología (MISTI) del MIT y el Laboratorio Mundial de Educación Abdul Latif Jameel (J -WEL). La asociación apoya a 60 estudiantes latinoamericanos que forman parte del Programa de Ciencia de Datos, un programa que incluye cursos en línea de MITx y talleres en el lugar dirigidos por J-WEL y MISTI. Los estudiantes locales incluyen directores generales, empresarios, ingenieros, economistas, profesionales médicos y administradores superiores. El programa MISTI Global Startup Labs (GSL), ahora en su vigésimo año, ha ampliado sus alianzas para incluir a las instituciones uruguayas para promover el espíritu empresarial y la ciencia de datos en América Latina. GSL es un programa único diseñado para ofrecer la oportunidad de combinar tecnologías digitales y emprendimiento en las regiones emergentes del mundo. Desde 1998, cientos de estudiantes del MIT han viajado a más de 15 países para formar parte del programa que ha beneficiado a miles de emprendedores tecnológicos de todo el mundo. Los instructores de GSL son estudiantes graduados y de pregrado del MIT, seleccionados entre muchos solicitantes de todo el instituto. Los programas GSL en diferentes países están diseñados de manera única en función de las necesidades de los socios locales, y los instructores estudiantiles del MIT toman la aplicación principal de enseñanza y desarrollo web, codificación, ciencia de datos, emprendimiento e intraemprendimiento. El nuevo GSL, uno de los primeros en ejecutarse durante el Período de Actividades Independientes, tuvo lugar durante enero en Montevideo. El programa de Uruguay se centró específicamente en el aprendizaje automático y las oportunidades comerciales de la tecnología. Los estudiantes participantes locales habían tomado previamente cursos de MITx MicroMasters en ciencia de datos, y el taller GSL les dio la oportunidad de experimentar el aprendizaje basado en proyectos en ciencia de datos. Esta inmersión experiencial práctica en el tema es la metodología central del programa GSL. Más de 30 estudiantes de posgrado y pregrado solicitaron ser parte de GSL en Uruguay este año, y 13 fueron seleccionados para ser parte del taller en Montevideo. Eduardo Rivera, director gerente de Uruguay, explicó el proceso: “Reclutar estudiantes para GSL siempre es un desafío. Buscamos experiencia y enseñanza de experiencia, pero también para jugadores de equipo y tomadores de riesgos. El equipo está compuesto por estudiantes de diferentes disciplinas y niveles de estudios, lo que hace que la experiencia sea una oportunidad única para que nuestros estudiantes aprendan de sus compañeros del MIT en contextos nuevos y desafiantes &quot;. Rivera agrega: &quot;En el MIT, somos afortunados de tener muchos estudiantes talentosos y apasionados, dispuestos a cruzar las fronteras y los océanos para enseñar y aprender&quot;. En el transcurso de un mes, a los estudiantes locales se les enseñó cómo construir prototipos, crear modelos de negocios y presentaciones de presentación. La clase persiguió proyectos que iban desde mantenimiento predictivo hasta detección de autismo y optimización logística. Los resultados finales se presentaron en un evento de lanzamiento organizado en Zonamerica, el principal centro de tecnología e innovación de Uruguay. &quot;Trabajar con nuestros estudiantes locales fue una oportunidad verdaderamente única e inolvidable&quot;, dice Ryan Sander, estudiante de ingeniería eléctrica y ciencias de la computación (EECS). &quot;Estoy seguro de que aprendí tanto de los estudiantes como ellos aprendieron de nosotros. Lo que realmente me impresionó fue observar no solo cuán brillantes son nuestros estudiantes, sino también cuán apasionados son estas personas para resolver problemas del mundo real con alto impacto.&quot; Para el estudiante de MBA Kenny Li, se amplió la oportunidad de interactuar con los estudiantes locales. &quot;En el mundo de hoy, debes ser capaz de comprender las culturas de las personas, cómo abordan los negocios, cómo interactúan en el trabajo ... GSL me dio una gran oportunidad de aprendizaje para comprender el contexto global de los empresarios&quot;. Cuando no daban clases, los estudiantes del MIT podían visitar varios lugares alrededor de Montevideo, incluidas las hermosas playas de Punta del Este, la ciudad vecina de Buenos Aires, y relajantes escapadas a Colonia. Después de las clases, el equipo docente estaba a pasos de la playa y podía relajarse cada día con una hermosa puesta de sol, absorbiendo el cálido clima de verano en enero. Rivera considera que estas conexiones culturales son uno de los principales beneficios del programa. “En MISTI, estamos seguros de que las actividades docentes internacionales contribuyen no solo a la formación académica de los estudiantes, sino que también les brindan valiosas herramientas para interactuar en entornos multiculturales y enfrentar nuevos desafíos en diferentes lugares. Para los futuros líderes mundiales, esta es una oportunidad única. A menudo escuchamos de nuestros estudiantes que las experiencias MISTI cambian la vida, no solo en la vida profesional sino también en su vida personal ”. &quot;Los fines de semana y las noches entre semana fueron una excelente manera de relacionarnos entre nosotros y nuestros estudiantes&quot;, dice Victoria Pisini, estudiante de último año de la Escuela de Administración MIT Sloan. &quot;Fuimos juntos a las playas, viajamos a diferentes ciudades y compartimos muchos momentos inolvidables&quot;. Los estudiantes del MIT que participaron en el GSL de este año fueron Amauche Emenari (estudiante de doctorado EECS), Devin Zhang (estudiante de MBA), Evan Mu (doctorado de EECS). estudiante), Geeticka Chauhan (estudiante de doctorado EECS), Hans Nowak (estudiante de MBA), Julian Alverio (estudiante de MEng de EECS), Kenny Li (estudiante de MBA), Madhav Kumar (candidato de doctorado en MIT Sloan), Maya Murad (maestría de líderes de operaciones globales y estudiante de doctorado), Ryan Sander (estudiante de EECS), Taylor Elise Baum (estudiante de doctorado de EECS), Tiffany Fung (estudiante de MBA) y Victoria Pisini (estudiante de MBA). Los programas de GSL están planificados en varios países para el verano de 2020 y el período de actividades independientes 2021, y todavía hay un pequeño número de oportunidades disponibles para instructores este verano."
35;news.mit.edu;http://news.mit.edu/2020/age-founders-successful-startups-0320;;A business edge that comes with age;"Two years ago, MIT economist Pierre Azoulay started a lively discussion when a working paper he co-authored, “Age and High-Growth Entrepreneurship,” revealed a surprising fact about startup founders: Among firms in the top 1/10 of the top 1 percent, in terms of growth, the average founder’s age is 45. That’s contrary to the popular image of valuable startups being the sole domain of twentysomething founders, such as Mark Zuckerberg of Facebook.

The paper, written with Benjamin Jones of Northwestern University, J. Daniel Kim of the University of Pennsylvania, and Javier Miranda of the U.S. Bureau of the Census, has now been officially published, in the journal American Economics Review: Insights. MIT News spoke to Azoulay, the International Programs Professor of Management at the MIT Sloan School of Management, about the finding and the discussion it has generated.

Q: What has been the response of people to the study?

A: We’re documenting a fact, and that fact either accords with people’s intuitions, or it doesn’t. Some people are genuinely surprised because they’ve lived in the current zeitgeist, and then once they start thinking about it, they say, “Ah, it makes sense.” And then there are people who say, “Oh, I knew it all along!” But Silicon Valley venture capitalists have studiously avoided engaging with what we’ve done. And I don’t know why.

I think one line of [venture capitalist] skepticism is to say, “Well, you may well be right, but you’re studying the one-in-a-thousand firm, and we’re [investing in] the one-in-a-million firm.” Which is sort of like restating that Apple, Microsoft, and Google were founded by young people.

Yes, we already knew it is possible for very large, successful firms to be founded by very young people. The question is: Is it likely?

Q: As you continue to think about this subject, is it possible to say what accounts for the success of relatively older entrepreneurs? Experience, intellectual capital, greater business connections — what matters?

A: All of those things are not mutually exclusive, and they’re all likely to play a role. They just need to be studied separately, if you will. We have to remain agnostic. But there is one key point in my view. Forget experience: How about just knowledge? I like to say there is no such thing as a 25-year-old biotech entrepreneur. That person just doesn’t exist, because you need a PhD and three postdocs [to gain high-level knowledge]. There are lots of fields where if you want to make a contribution, you have to bring yourself to the frontier of knowledge in a domain, and that takes time. And that’s not going to be the realm of the 22-year-old.

Beyond the big platform IT companies, if you’re thinking about the broader swath of entrepreneurship across a multiplicity of sectors, then you have to acknowledge this point. In some sense, if you recognize the diversity of startups, the [founder’s] age number is going to be higher than if you’re only focused on super-high-value Silicon Valley internet companies. So we need a basic attitude adjustment.

Q. Even as people mythologize the young startup founder, there is also a tech-sector ethos that heralds serial entrepreneurship and tolerates failure, because you’re taking risks and learning by doing. Isn’t that one Silicon Valley notion that might correspond to what you discovered?

A: Yes, one thing that could explain our results is entrepreneurship being an activity you can learn to do better over time. That’s certainly something we’ve heard. We can’t pin it down, but if I had to think of the most likely stories, that’s certainly one. Even within a particular sector that demands a certain amount of intellectual capital, holding that constant, even within biotech or clean energy, you might think there is something about learning by founding, which is going to lead to a correlation between success and a higher age for founders.";Una ventaja comercial que viene con la edad.;"Hace dos años, el economista del MIT Pierre Azoulay comenzó una animada discusión cuando un documento de trabajo del cual fue coautor, &quot;Edad y emprendimiento de alto crecimiento&quot;, reveló un hecho sorprendente sobre los fundadores de startups: entre las empresas que se encuentran entre los primeros 1/10 de los primeros 1 por ciento, en términos de crecimiento, la edad promedio del fundador es de 45 años. Eso es contrario a la imagen popular de nuevas empresas valiosas que son el dominio exclusivo de los fundadores de veinte años, como Mark Zuckerberg de Facebook. El documento, escrito con Benjamin Jones de la Universidad Northwestern, J. Daniel Kim de la Universidad de Pensilvania y Javier Miranda de la Oficina del Censo de los EE. UU., Ahora se ha publicado oficialmente en la revista American Economics Review: Insights. MIT News habló con Azoulay, el Profesor de Programas Internacionales de Administración en la Escuela de Administración MIT Sloan, sobre el hallazgo y la discusión que ha generado. P: ¿Cuál ha sido la respuesta de las personas al estudio? R: Estamos documentando un hecho, y ese hecho concuerda con las intuiciones de las personas o no. Algunas personas están realmente sorprendidas porque han vivido en el zeitgeist actual, y luego, una vez que comienzan a pensar en ello, dicen: &quot;Ah, tiene sentido&quot;. Y luego hay personas que dicen: &quot;¡Oh, lo supe todo el tiempo!&quot; Pero los capitalistas de riesgo de Silicon Valley han evitado deliberadamente comprometerse con lo que hemos hecho. Y no sé por qué. Creo que una línea de escepticismo [capitalista de riesgo] es decir: &quot;Bueno, es posible que tengas razón, pero estás estudiando la empresa uno en mil, y estamos [invirtiendo en] el uno en -un millón de firmas. Lo cual es como reafirmar que Apple, Microsoft y Google fueron fundados por jóvenes. Sí, ya sabíamos que es posible que empresas muy grandes y exitosas sean fundadas por personas muy jóvenes. La pregunta es: ¿es probable? P: A medida que continúa pensando en este tema, ¿es posible decir qué explica el éxito de los empresarios relativamente mayores? Experiencia, capital intelectual, mayores conexiones comerciales, ¿qué importa? R: Todas esas cosas no son mutuamente excluyentes, y es probable que todas desempeñen un papel. Solo deben estudiarse por separado, si lo desea. Tenemos que permanecer agnósticos. Pero hay un punto clave en mi opinión. Olvídate de la experiencia: ¿qué tal solo el conocimiento? Me gusta decir que no existe un empresario biotecnológico de 25 años. Esa persona simplemente no existe, porque necesitas un doctorado y tres postdocs [para obtener un conocimiento de alto nivel]. Hay muchos campos en los que, si desea hacer una contribución, tiene que llevarse a la frontera del conocimiento en un dominio, y eso lleva tiempo. Y ese no será el reino de los 22 años. Más allá de las grandes empresas de TI de plataforma, si está pensando en la amplia gama de emprendimiento en una multiplicidad de sectores, entonces debe reconocer este punto. En cierto sentido, si reconoce la diversidad de nuevas empresas, el número de edad [del fundador] será mayor que si solo se concentra en las empresas de Internet de Silicon Valley de muy alto valor. Entonces necesitamos un ajuste básico de actitud. P. Incluso cuando la gente mitifica al joven fundador de startups, también existe un espíritu del sector tecnológico que anuncia el emprendimiento en serie y tolera el fracaso, porque estás tomando riesgos y aprendiendo al hacerlo. ¿No es esa una noción de Silicon Valley que podría corresponder a lo que descubriste? R: Sí, una cosa que podría explicar nuestros resultados es que el espíritu empresarial es una actividad que puede aprender a mejorar con el tiempo. Eso es ciertamente algo que hemos escuchado. No podemos precisarlo, pero si tuviera que pensar en las historias más probables, esa es ciertamente una. Incluso dentro de un sector en particular que exige una cierta cantidad de capital intelectual, manteniéndolo constante, incluso dentro de la biotecnología o la energía limpia, puede pensar que hay algo sobre aprender al fundar, lo que conducirá a una correlación entre el éxito y una edad superior para los fundadores"
36;machinelearningmastery.com;https://machinelearningmastery.com/how-to-predict-whether-eyes-are-open-or-closed-using-brain-waves/;2018-08-26;How to Predict Whether a Persons Eyes are Open or Closed Using Brain Waves;"# remove outliers from the EEG data

from pandas import read_csv

from numpy import mean

from numpy import std

from numpy import delete

from numpy import savetxt

# load the dataset.

data = read_csv ( 'EEG_Eye_State.csv' , header = None )

values = data . values

# step over each EEG column

for i in range ( values . shape [ 1 ] - 1 ) :

# calculate column mean and standard deviation

data_mean , data_std = mean ( values [ : , i ] ) , std ( values [ : , i ] )

# define outlier bounds

cut_off = data_std * 4

lower , upper = data_mean - cut_off , data_mean + cut_off

# remove too small

too_small = [ j for j in range ( values . shape [ 0 ] ) if values [ j , i ] < lower ]

values = delete ( values , too_small , 0 )

print ( '>deleted %d rows' % len ( too_small ) )

# remove too large

too_large = [ j for j in range ( values . shape [ 0 ] ) if values [ j , i ] > upper ]

values = delete ( values , too_large , 0 )

print ( '>deleted %d rows' % len ( too_large ) )

# save the results to a new file";Cómo predecir si los ojos de una persona están abiertos o cerrados usando ondas cerebrales;"# eliminar valores atípicos de los datos de EEG de pandas import read_csv de numpy importar significa de numpy import std de numpy import eliminar de numpy import savetxt # cargar el conjunto de datos. data = read_csv (&#39;EEG_Eye_State.csv&#39;, header = None) valores = datos. valores # paso sobre cada columna EEG para i en el rango (valores. forma [1] - 1): # calcular la media de la columna y la desviación estándar data_mean, data_std = mean (valores [:, i]), std (valores [:, i ]) # define límites atípicos cut_off = data_std * 4 lower, upper = data_mean - cut_off, data_mean + cut_off # remove too small too_small = [j para j en rango (valores. forma [0]) si los valores [j, i] &lt; valores inferiores] = eliminar (valores, demasiado pequeños, 0) imprimir (&#39;&gt; eliminado% d filas&#39;% len (demasiado pequeños)) # eliminar demasiado grande too_large = [j para j en rango (valores. forma [0]) si los valores [ j, i]&gt; valores superiores] = eliminar (valores, demasiado grande, 0) imprimir (&#39;&gt; eliminado% d filas&#39;% len (demasiado grande)) # guardar los resultados en un nuevo archivo"
37;machinelearningmastery.com;https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/;2017-05-16;How to Use the TimeDistributed Layer in Keras;"from numpy import array

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# prepare sequence

length = 5

seq = array ( [ i / float ( length ) for i in range ( length ) ] )

X = seq . reshape ( len ( seq ) , 1 , 1 )

y = seq . reshape ( len ( seq ) , 1 )

# define LSTM configuration

n_neurons = length

n_batch = length

n_epoch = 1000

# create LSTM

model = Sequential ( )

model . add ( LSTM ( n_neurons , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

print ( model . summary ( ) )

# train LSTM

model . fit ( X , y , epochs = n_epoch , batch_size = n_batch , verbose = 2 )

# evaluate

result = model . predict ( X , batch_size = n_batch , verbose = 0 )

for value in result :";Cómo usar la capa de tiempo distribuido en Keras;"de numpy array de importación de keras. Los modelos importan secuenciales de keras. Las capas importan densas de keras. capas importar LSTM # preparar secuencia longitud = 5 seq = matriz ([i / float (longitud) para i en rango (longitud)]) X = seq. remodelar (len (seq), 1, 1) y = seq. remodelar (len (seq), 1) # define la configuración LSTM n_neurons = length n_batch = length n_epoch = 1000 # create LSTM model = Sequential () model. modelo add (LSTM (n_neurons, input_shape = (1, 1)))). Añadir modelo (Denso (1)). compile (loss = &#39;mean_squared_error&#39;, optimizer = &#39;adam&#39;) print (model. summary ()) # train LSTM model. fit (X, y, epochs = n_epoch, batch_size = n_batch, verbose = 2) # evaluar resultado = modelo. predecir (X, batch_size = n_batch, verbose = 0) para el valor en el resultado:"
