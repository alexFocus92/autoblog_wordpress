;Unnamed: 0;periodico;url;fecha;titulo;cuerpo
0;0;machinelearningmastery.com;https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/;2019-06-23;How to use the UpSampling2D and Conv2DTranspose Layers in Keras;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are an architecture for training generative models, such as deep convolutional neural networks for generating images.

The GAN architecture is comprised of both a generator and a discriminator model. The generator is responsible for creating new outputs, such as images, that plausibly could have come from the original dataset. The generator model is typically implemented using a deep convolutional neural network and results-specialized layers that learn to fill in features in an image rather than extract features from an input image.

Two common types of layers that can be used in the generator model are a upsample layer (UpSampling2D) that simply doubles the dimensions of the input and the transpose convolutional layer (Conv2DTranspose) that performs an inverse convolution operation.

In this tutorial, you will discover how to use UpSampling2D and Conv2DTranspose Layers in Generative Adversarial Networks when generating images.

After completing this tutorial, you will know:

Generative models in the GAN architecture are required to upsample input data in order to generate an output image.

The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.

The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Need for Upsampling in GANs

How to Use the Upsampling Layer

How to Use the Transpose Convolutional Layer

Need for Upsampling in Generative Adversarial Networks

Generative Adversarial Networks are an architecture for neural networks for training a generative model.

The architecture is comprised of a generator and a discriminator model, both of which are implemented as a deep convolutional neural network. The discriminator is responsible for classifying images as either real (from the domain) or fake (generated). The generator is responsible for generating new plausible examples from the problem domain.

The generator works by taking a random point from the latent space as input and outputting a complete image, in a one-shot manner.

A traditional convolutional neural network for image classification, and related tasks, will use pooling layers to downsample input images. For example, an average pooling or max pooling layer will reduce the feature maps from a convolutional by half on each dimension, resulting in an output that is one quarter the area of the input.

Convolutional layers themselves also perform a form of downsampling by applying each filter across the input images or feature maps; the resulting activations are an output feature map that is smaller because of the border effects. Often padding is used to counter this effect.

The generator model in a GAN requires an inverse operation of a pooling layer in a traditional convolutional layer. It needs a layer to translate from coarse salient features to a more dense and detailed output.

A simple version of an unpooling or opposite pooling layer is called an upsampling layer. It works by repeating the rows and columns of the input.

A more elaborate approach is to perform a backwards convolutional operation, originally referred to as a deconvolution, which is incorrect, but is more commonly referred to as a fractional convolutional layer or a transposed convolutional layer.

Both of these layers can be used on a GAN to perform the required upsampling operation to transform a small input into a large image output.

In the following sections, we will take a closer look at each and develop an intuition for how they work so that we can use them effectively in our GAN models.

How to Use the UpSampling2D Layer

Perhaps the simplest way to upsample an input is to double each row and column.

For example, an input image with the shape 2×2 would be output as 4×4.

1, 2 Input = (3, 4) 1, 1, 2, 2 Output = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4 1 2 3 4 5 6 7 1, 2 Input = (3, 4) 1, 1, 2, 2 Output = (1, 1, 2, 2) 3, 3, 4, 4 3, 3, 4, 4

Worked Example Using the UpSampling2D Layer

The Keras deep learning library provides this capability in a layer called UpSampling2D.

It can be added to a convolutional neural network and repeats the rows and columns provided as input in the output. For example:

... # define model model = Sequential() model.add(UpSampling2D()) 1 2 3 4 . . . # define model model = Sequential ( ) model . add ( UpSampling2D ( ) )

We can demonstrate the behavior of this layer with a simple contrived example.

First, we can define a contrived input image that is 2×2 pixel"
1;1;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/;2018-09-23;How to Develop RNN Models for Human Activity Recognition Time Series Classification;"# convlstm model

from numpy import mean

from numpy import std

from numpy import dstack

from pandas import read_csv

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import Dropout

from keras . layers import LSTM

from keras . layers import TimeDistributed

from keras . layers import ConvLSTM2D

from keras . utils import to_categorical

from matplotlib import pyplot

# load a single file as a numpy array

def load_file ( filepath ) :

dataframe = read_csv ( filepath , header = None , delim_whitespace = True )

return dataframe . values

# load a list of files and return as a 3d numpy array

def load_group ( filenames , prefix = '' ) :

loaded = list ( )

for name in filenames :

data = load_file ( prefix + name )

loaded . append ( data )

# stack group so that features are the 3rd dimension

loaded = dstack ( loaded )

return loaded

# load a dataset group, such as train or test

def load_dataset_group ( group , prefix = '' ) :

filepath = prefix + group + '/Inertial Signals/'

# load all 9 files as a single array

filenames = list ( )

# total acceleration

filenames += [ 'total_acc_x_' + group + '.txt' , 'total_acc_y_' + group + '.txt' , 'total_acc_z_' + group + '.txt' ]

# body acceleration

filenames += [ 'body_acc_x_' + group + '.txt' , 'body_acc_y_' + group + '.txt' , 'body_acc_z_' + group + '.txt' ]

# body gyroscope

filenames += [ 'body_gyro_x_' + group + '.txt' , 'body_gyro_y_' + group + '.txt' , 'body_gyro_z_' + group + '.txt' ]

# load input data

X = load_group ( filenames , filepath )

# load class output

y = load_file ( prefix + group + '/y_' + group + '.txt' )

return X , y

# load the dataset, returns train and test X and y elements

def load_dataset ( prefix = '' ) :

# load all train

trainX , trainy = load_dataset_group ( 'train' , prefix + 'HARDataset/' )

print ( trainX . shape , trainy . shape )

# load all test

testX , testy = load_dataset_group ( 'test' , prefix + 'HARDataset/' )

print ( testX . shape , testy . shape )

# zero-offset class values

trainy = trainy - 1

testy = testy - 1

# one hot encode y

trainy = to_categorical ( trainy )

testy = to_categorical ( testy )

print ( trainX . shape , trainy . shape , testX . shape , testy . shape )

return trainX , trainy , testX , testy

# fit and evaluate a model

def evaluate_model ( trainX , trainy , testX , testy ) :

# define model

verbose , epochs , batch_size = 0 , 25 , 64

n_timesteps , n_features , n_outputs = trainX . shape [ 1 ] , trainX . shape [ 2 ] , trainy . shape [ 1 ]

# reshape into subsequences (samples, time steps, rows, cols, channels)

n_steps , n_length = 4 , 32

trainX = trainX . reshape ( ( trainX . shape [ 0 ] , n_steps , 1 , n_length , n_features ) )

testX = testX . reshape ( ( testX . shape [ 0 ] , n_steps , 1 , n_length , n_features ) )

# define model

model = Sequential ( )

model . add ( ConvLSTM2D ( filters = 64 , kernel_size = ( 1 , 3 ) , activation = 'relu' , input_shape = ( n_steps , 1 , n_length , n_features ) ) )

model . add ( Dropout ( 0.5 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 100 , activation = 'relu' ) )

model . add ( Dense ( n_outputs , activation = 'softmax' ) )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( trainX , trainy , epochs = epochs , batch_size = batch_size , verbose = verbose )

# evaluate model

_ , accuracy = model . evaluate ( testX , testy , batch_size = batch_size , verbose = 0 )

return accuracy

# summarize scores

def summarize_results ( scores ) :

print ( scores )

m , s = mean ( scores ) , std ( scores )

print ( 'Accuracy: %.3f%% (+/-%.3f)' % ( m , s ) )

# run an experiment

def run_experiment ( repeats = 10 ) :

# load data

trainX , trainy , testX , testy = load_dataset ( )

# repeat experiment

scores = list ( )

for r in range ( repeats ) :

score = evaluate_model ( trainX , trainy , testX , testy )

score = score * 100.0

print ( '>#%d: %.3f' % ( r + 1 , score ) )

scores . append ( score )

# summarize results

summarize_results ( scores )

# run the experiment"
2;2;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-and-demonstrate-competence-with-deep-learning-for-computer-vision/;2019-03-10;How to Develop Competence With Deep Learning for Computer Vision;"Tweet Share Share

Last Updated on July 5, 2019

Computer vision is perhaps one area that has been most impacted by developments in deep learning.

It can be difficult to both develop and to demonstrate competence with deep learning for problems in the field of computer vision. It is not clear how to get started, what the most important techniques are, and the types of problems and projects that can best highlight the value that deep learning can bring to the field.

On approach is to systematically develop, and at the same time demonstrate competence with, data handling, modeling techniques, and application domains and present your results in a public portfolio of completed projects. This approach allows you to compound your skills from project to project. It also provides the basis for real projects that can be presented and discussed with prospective employers in order to demonstrate your capabilities.

In this post, you will discover how to develop and demonstrate competence in deep learning applied to problems in computer vision.

After reading this post, you will know:

Developing a portfolio of completed small projects can both be leveraged on new projects in the future and demonstrate your competence with deep learning for computer vision projects.

Projects can be kept small in scope, although they can still demonstrate a systematic approach to problem-solving and the development of skillful models.

A three-level competence framework can be followed that includes data handling competence, technique competence, and application competence.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Deep Learning for Computer Vision Develop a Portfolio of Small Projects Deep Learning for Computer Vision Competence Framework

Deep Learning for Computer Vision

Perhaps one domain that has been the most impacted by developments in deep learning is computer vision.

Computer vision is a subfield of artificial intelligence concerned with understanding data in images, such as photos and videos.

Computer vision tasks such as recognizing handwritten digits and objects in photographs were some of the early case studies demonstrating the capability of modern deep learning techniques achieving state-of-the-art results.

As a practitioner, you may wish to develop and demonstrate your skills with deep learning in computer vision.

This does assume a few things, such as:

You are familiar with applied machine learning, meaning that you are able to work through a predictive modeling project end-to-end and deliver a skillful model.

You are familiar with deep learning techniques, meaning that you know the difference between the main methods and when to use them.

This does not mean that you are an expert, only that you have a working knowledge and are able to wok through problems systematically.

As a machine learning or even deep learning practitioner, how can you show competence with computer vision applications?

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Develop a Portfolio of Small Projects

Competence with deep learning for computer vision can be developed and demonstrated using a project-based approach.

Specifically, the skills can be built and demonstrated incrementally by completing and presenting small projects that use deep learning techniques on computer vision problems.

This requires you to develop a portfolio of completed projects. A portfolio helps you in two specific ways:

Skill Development : The code and findings from the projects in the portfolio can be leveraged by you on future projects, accelerating your progress and allowing you to take on larger and more challenging projects.

: The code and findings from the projects in the portfolio can be leveraged by you on future projects, accelerating your progress and allowing you to take on larger and more challenging projects. Skill Demonstration: The public presentation of the projects provides a demonstration of your capabilities, providing the basis for discussion of APIs, model selection, and design decisions with prospective employers.

Projects can be focused on standard and publicly available computer vision datasets, such as those developed and hosted by academics or those used in machine learning competitions.

Projects can be completed in a systematic manner, including aspects such as clear problem definition, review of relevant literature and models, model development and tuning, and the presentation of results and findings in a report, notebook, or even slideshow presentation format.

Projects are small, meaning that they can be completed in a workday, perh"
3;3;machinelearningmastery.com;https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/;2019-06-20;How to Implement GAN Hacks in Keras to Train Stable Models;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are challenging to train.

This is because the architecture involves both a generator and a discriminator model that compete in a zero-sum game. It means that improvements to one model come at the cost of a degrading of performance in the other model. The result is a very unstable training process that can often lead to failure, e.g. a generator that generates the same image all the time or generates nonsense.

As such, there are a number of heuristics or best practices (called “GAN hacks“) that can be used when configuring and training your GAN models. These heuristics are been hard won by practitioners testing and evaluating hundreds or thousands of combinations of configuration operations on a range of problems over many years.

Some of these heuristics can be challenging to implement, especially for beginners.

Further, some or all of them may be required for a given project, although it may not be clear which subset of heuristics should be adopted, requiring experimentation. This means a practitioner must be ready to implement a given heuristic with little notice.

In this tutorial, you will discover how to implement a suite of best practices or GAN hacks that you can copy-and-paste directly into your GAN project.

After reading this tutorial, you will know:

The best sources for practical heuristics or hacks when developing generative adversarial networks.

How to implement seven best practices for the deep convolutional GAN model architecture from scratch.

How to implement four additional best practices from Soumith Chintala’s GAN Hacks presentation and list.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Heuristics for Training Stable GANs Best Practices for Deep Convolutional GANs Downsample Using Strided Convolutions Upsample Using Strided Convolutions Use LeakyReLU Use Batch Normalization Use Gaussian Weight Initialization Use Adam Stochastic Gradient Descent Scale Images to the Range [-1,1] Soumith Chintala’s GAN Hacks Use a Gaussian Latent Space Separate Batches of Real and Fake Images Use Label Smoothing Use Noisy Labels

Heuristics for Training Stable GANs

GANs are difficult to train.

At the time of writing, there is no good theoretical foundation as to how to design and train GAN models, but there is established literature of heuristics, or “hacks,” that have been empirically demonstrated to work well in practice.

As such, there are a range of best practices to consider and implement when developing a GAN model.

Perhaps the two most important sources of suggested configuration and training parameters are:

Alec Radford, et al’s 2015 paper that introduced the DCGAN architecture. Soumith Chintala’s 2016 presentation and associated “GAN Hacks” list.

In this tutorial, we will explore how to implement the most important best practices from these two sources.

Best Practices for Deep Convolutional GANs

Perhaps one of the most important steps forward in the design and training of stable GAN models was the 2015 paper by Alec Radford, et al. titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.”

In the paper, they describe the Deep Convolutional GAN, or DCGAN, approach to GAN development that has become the de facto standard.

We will look at how to implement seven best practices for the DCGAN model architecture in this section.

1. Downsample Using Strided Convolutions

The discriminator model is a standard convolutional neural network model that takes an image as input and must output a binary classification as to whether it is real or fake.

It is standard practice with deep convolutional networks to use pooling layers to downsample the input and feature maps with the depth of the network.

This is not recommended for the DCGAN, and instead, they recommend downsampling using strided convolutions.

This involves defining a convolutional layer as per normal, but instead of using the default two-dimensional stride of (1,1) to change it to (2,2). This has the effect of downsampling the input, specifically halving the width and height of the input, resulting in output feature maps with one quarter the area.

The example below demonstrates this with a single hidden convolutional layer that uses downsampling strided convolutions by setting the ‘strides‘ argument to (2,2). The effect is the model will downsample the input from 64×64 to 32×32.

# example of downsampling with strided convolutions from keras.models import Sequential from keras.layers import Conv2D # define model model = Sequential() model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3))) # summarize model model.summary() 1 2 3 4 5 6 7 8 # example of"
4;4;machinelearningmastery.com;https://machinelearningmastery.com/how-to-evaluate-pixel-scaling-methods-for-image-classification/;2019-03-26;How to Evaluate Pixel Scaling Methods for Image Classification With CNNs;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113

# comparison of training-set based pixel scaling methods on MNIST from numpy import mean from numpy import std from matplotlib import pyplot from keras . datasets import mnist from keras . utils import to_categorical from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten # load train and test dataset def load_dataset ( ) : # load dataset ( trainX , trainY ) , ( testX , testY ) = mnist . load_data ( ) # reshape dataset to have a single channel width , height , channels = trainX . shape [ 1 ] , trainX . shape [ 2 ] , 1 trainX = trainX . reshape ( ( trainX . shape [ 0 ] , width , height , channels ) ) testX = testX . reshape ( ( testX . shape [ 0 ] , width , height , channels ) ) # one hot encode target values trainY = to_categorical ( trainY ) testY = to_categorical ( testY ) return trainX , trainY , testX , testY # define cnn model def define_model ( ) : model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , input_shape = ( 28 , 28 , 1 ) ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 64 , activation = 'relu' ) ) model . add ( Dense ( 10 , activation = 'softmax' ) ) # compile model model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] ) return model # normalize images def prep_normalize ( train , test ) : # convert from integers to floats train_norm = train . astype ( 'float32' ) test_norm = test . astype ( 'float32' ) # normalize to range 0-1 train_norm = train_norm / 255.0 test_norm = test_norm / 255.0 # return normalized images return train_norm , test_norm # center images def prep_center ( train , test ) : # convert from integers to floats train_cent = train . astype ( 'float32' ) test_cent = test . astype ( 'float32' ) # calculate statistics m = train_cent . mean ( ) # center datasets train_cent = train_cent - m test_cent = test_cent - m # return normalized images return train_cent , test_cent # standardize images def prep_standardize ( train , test ) : # convert from integers to floats train_stan = train . astype ( 'float32' ) test_stan = test . astype ( 'float32' ) # calculate statistics m = train_stan . mean ( ) s = train_stan . std ( ) # center datasets train_stan = ( train_stan - m ) / s test_stan = ( test_stan - m ) / s # return normalized images return train_stan , test_stan # repeated evaluation of model with data prep scheme def repeated_evaluation ( datapre_func , n_repeats = 10 ) : # prepare data trainX , trainY , testX , testY = load_dataset ( ) # repeated evaluation scores = list ( ) for i in range ( n_repeats ) : # define model model = define_model ( ) # prepare data prep_trainX , prep_testX = datapre_func ( trainX , testX ) # fit model model . fit ( prep_trainX , trainY , epochs = 5 , batch_size = 64 , verbose = 0 ) # evaluate model _ , acc = model . evaluate ( prep_testX , testY , verbose = 0 ) # store result scores . append ( acc ) print ( '> %d: %.3f' % ( i , acc * 100.0 ) ) return scores all_scores = list ( ) # normalization scores = repeated_evaluation ( prep_normalize ) print ( 'Normalization: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # center scores = repeated_evaluation ( prep_center ) print ( 'Centered: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # standardize scores = repeated_evaluation ( prep_standardize ) print ( 'Standardized: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) ) all_scores . append ( scores ) # box and whisker plots of results pyplot . boxplot ( all_scores , labels = [ 'norm' , 'cent' , 'stan' ] ) pyplot . show ( )"
5;5;machinelearningmastery.com;https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/;2017-06-06;The 5 Step Life-Cycle for Long Short-Term Memory Models in Keras;"# Example of LSTM to learn a sequence

from pandas import DataFrame

from pandas import concat

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# create sequence

length = 10

sequence = [ i / float ( length ) for i in range ( length ) ]

print ( sequence )

# create X/y pairs

df = DataFrame ( sequence )

df = concat ( [ df . shift ( 1 ) , df ] , axis = 1 )

df . dropna ( inplace = True )

# convert to LSTM friendly format

values = df . values

X , y = values [ : , 0 ] , values [ : , 1 ]

X = X . reshape ( len ( X ) , 1 , 1 )

# 1. define network

model = Sequential ( )

model . add ( LSTM ( 10 , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 ) )

# 2. compile network

model . compile ( optimizer = 'adam' , loss = 'mean_squared_error' )

# 3. fit network

history = model . fit ( X , y , epochs = 1000 , batch_size = len ( X ) , verbose = 0 )

# 4. evaluate network

loss = model . evaluate ( X , y , verbose = 0 )

print ( loss )

# 5. make predictions

predictions = model . predict ( X , verbose = 0 )"
6;6;machinelearningmastery.com;http://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/;2016-04-21;Bagging and Random Forest Ensemble Algorithms for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.

In this post you will discover the Bagging ensemble algorithm and the Random Forest algorithm for predictive modeling. After reading this post you will know about:

The bootstrap method for estimating statistical quantities from samples.

The Bootstrap Aggregation algorithm for creating multiple different models from a single training dataset.

The Random Forest algorithm that makes a small tweak to Bagging and results in a very powerful classifier.

This post was written for developers and assumes no background in statistics or mathematics. The post focuses on how the algorithm works and how to use it for predictive modeling problems.

If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Bootstrap Method

Before we get to Bagging, let’s take a quick look at an important foundation technique called the bootstrap.

The bootstrap is a powerful statistical method for estimating a quantity from a data sample. This is easiest to understand if the quantity is a descriptive statistic such as a mean or a standard deviation.

Let’s assume we have a sample of 100 values (x) and we’d like to get an estimate of the mean of the sample.

We can calculate the mean directly from the sample as:

mean(x) = 1/100 * sum(x)

We know that our sample is small and that our mean has error in it. We can improve the estimate of our mean using the bootstrap procedure:

Create many (e.g. 1000) random sub-samples of our dataset with replacement (meaning we can select the same value multiple times). Calculate the mean of each sub-sample. Calculate the average of all of our collected means and use that as our estimated mean for the data.

For example, let’s say we used 3 resamples and got the mean values 2.3, 4.5 and 3.3. Taking the average of these we could take the estimated mean of the data to be 3.367.

This process can be used to estimate other quantities like the standard deviation and even quantities used in machine learning algorithms, like learned coefficients.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Bootstrap Aggregation (Bagging)

Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.

An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.

Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees, like classification and regression trees (CART).

Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.

Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.

Let’s assume we have a sample dataset of 1000 instances (x) and we are using the CART algorithm. Bagging of the CART algorithm would work as follows.

Create many (e.g. 100) random sub-samples of our dataset with replacement. Train a CART model on each sample. Given a new dataset, calculate the average prediction from each model.

For example, if we had 5 bagged decision trees that made the following class predictions for a in input sample: blue, blue, red, blue and red, we would take the most frequent class and predict blue.

When bagging with decision trees, we are less concerned about individual trees overfitting the training data. For this reason and for efficiency, the individual decision trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees are not pruned. These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging.

The only parameters when bagging decision trees is the number of samples and hence the number of trees to include. This can be chosen by increasing the number of trees on run after run until the accuracy begins to stop showing improvement (e.g. on a cross validation test harness). Very large numbers of models may take a long time to prepare, but will not overfit the training data.

Just like the decision trees themselves, Bagging ca"
7;7;machinelearningmastery.com;https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/;2020-01-07;Tour of Evaluation Metrics for Imbalanced Classification;"Tweet Share Share

Last Updated on January 14, 2020

A classifier is only as good as the metric used to evaluate it.

If you choose the wrong metric to evaluate your models, you are likely to choose a poor model, or in the worst case, be misled about the expected performance of your model.

Choosing an appropriate metric is challenging generally in applied machine learning, but is particularly difficult for imbalanced classification problems. Firstly, because most of the standard metrics that are widely used assume a balanced class distribution, and because typically not all classes, and therefore, not all prediction errors, are equal for imbalanced classification.

In this tutorial, you will discover metrics that you can use for imbalanced classification.

After completing this tutorial, you will know:

About the challenge of choosing metrics for classification, and how it is particularly difficult when there is a skewed class distribution.

How there are three main types of metrics for evaluating classifier models, referred to as rank, threshold, and probability.

How to choose a metric for imbalanced classification if you don’t know where to start.

Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more in my new book, with 30 step-by-step tutorials and full Python source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Challenge of Evaluation Metrics Taxonomy of Classifier Evaluation Metrics How to Choose an Evaluation Metric

Challenge of Evaluation Metrics

An evaluation metric quantifies the performance of a predictive model.

This typically involves training a model on a dataset, using the model to make predictions on a holdout dataset not used during training, then comparing the predictions to the expected values in the holdout dataset.

For classification problems, metrics involve comparing the expected class label to the predicted class label or interpreting the predicted probabilities for the class labels for the problem.

Selecting a model, and even the data preparation methods together are a search problem that is guided by the evaluation metric. Experiments are performed with different models and the outcome of each experiment is quantified with a metric.

Evaluation measures play a crucial role in both assessing the classification performance and guiding the classifier modeling.

— Classification Of Imbalanced Data: A Review, 2009.

There are standard metrics that are widely used for evaluating classification predictive models, such as classification accuracy or classification error.

Standard metrics work well on most problems, which is why they are widely adopted. But all metrics make assumptions about the problem or about what is important in the problem. Therefore an evaluation metric must be chosen that best captures what you or your project stakeholders believe is important about the model or predictions, which makes choosing model evaluation metrics challenging.

This challenge is made even more difficult when there is a skew in the class distribution. The reason for this is that many of the standard metrics become unreliable or even misleading when classes are imbalanced, or severely imbalanced, such as 1:100 or 1:1000 ratio between a minority and majority class.

In the case of class imbalances, the problem is even more acute because the default, relatively robust procedures used for unskewed data can break down miserably when the data is skewed.

— Page 187, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

For example, reporting classification accuracy for a severely imbalanced classification problem could be dangerously misleading. This is the case if project stakeholders use the results to draw conclusions or plan new projects.

In fact, the use of common metrics in imbalanced domains can lead to sub-optimal classification models and might produce misleading conclusions since these measures are insensitive to skewed domains.

— A Survey of Predictive Modelling under Imbalanced Distributions, 2015.

Importantly, different evaluation metrics are often required when working with imbalanced classification.

Unlike standard evaluation metrics that treat all classes as equally important, imbalanced classification problems typically rate classification errors with the minority class as more important than those with the majority class. As such performance metrics may be needed that focus on the minority class, which is made challenging because it is the minority class where we lack observations required to train an effective model.

The main problem of imbalanced data sets lies on the fact that they are often associated with a user preference bias towards the performance on cases that are poorly represented in the available data sample.

— A Survey of Predictive Modelling under Imbalanced Distributions, 2015.

Now that we are familiar with the challenge of choosing a mod"
8;8;news.mit.edu;http://news.mit.edu/2020/how-to-stage-revolution-mit-history-class-0107;;How to stage a revolution;"Revolutions are monumental social upheavals that can remake whole nations, dismantling — often violently — old paradigms. But the stories of the epic struggles that leave their mark on the world’s history are frequently fragile, precarious, and idiosyncratic in their details, leaving some key questions only partially understood: Why and how do peoples overthrow their governments? Why do some revolutions succeed and others fail?

These are not simple questions, and, for 12 years, MIT students and faculty have set out to answer them in a survey course that spans centuries and continents.

Course 21H.001 (How to Stage a Revolution, or Revolutions for short) is an MIT history class that examines the roots, drivers, and complexities of how governments fall. Co-taught this past fall by three historians — History Section head Professor Jeffrey Ravel, Associate Professor Tanalís Padilla, and Lecturer Pouya Alimagham — the semester is divided into three parts, with each instructor covering, respectively, the French Revolution, the Mexican Revolution, and the Iranian Revolution.

During a mixture of lectures and breakout discussion sessions, students explore the causes, tactics, goals, and significant factors of each revolution, drawing insights from music, film, art, constitutions, declarations, and the writings of revolutionaries themselves.

A wide-angle approach

The topics covered this year span centuries, from the near-mythic French Revolution (1789–99) to the Mexican Revolution (1910-20) to events that have emerged in the students’ own lifetimes, such as the Arab Spring (2010-12). Alimagham brought the semester to a close with a focus on the Iranian Revolution; having students begin their exploration with the roots of American intervention in Iran the latter half of the 20th century, and tracking developments through to today’s western media narrative of the Sunni/Shia conflict.

“Revolutions are a surprisingly good way to learn about a culture,” says Quinn Bowers, a first-year student who took the opportunity to deepen his understanding of history as a parallel to his intended double major in mechanical engineering and aerospace engineering. “Revolutions draw attention to the values the culture holds. This class did a lot to dispel assumptions I didn’t even know I had.”

For another first-year student, Somaia Saba, the offering leapt out at her as she browsed the course catalog to plan her first semester at MIT. With an intended major in computation and cognition (Course 6.9), she was drawn to the class by a fascination with major political transformations, “especially because of the tense political climate in which we are currently living.”

The freedom and exploration in essay-writing was a transformative experience for Saba; essay prompts and writing assignments had never been her favorite aspect of the classroom. But, snagged by a brief mention in class about women’s roles during the Mexican Revolution, she found herself writing extensively on the subject, drawing on her personal attentiveness to women’s issues and roles in history.

“I did not realize the extent to which these issues mattered to me until [seeing the professor’s] comments on my essay.” She also notes that the class has given her ways of thinking and analyzing that allow her to be more engaged with current political events.

Ever-evolving

How to Stage a Revolution is also a chameleon course in that its subject matter fluxes from year to year depending on the expertise of the faculty instructors — a plan that allows a venerable course to cover any number of revolutionary histories. Two years ago, for instance, when Alimagham first taught the course, working alongside MIT historians Caley Horan and Malick Ghachem, the class consisted of modules on the Haitian Revolution, the American Civil War (as America’s second revolution), and the Iranian Revolution.

Not only is the course constantly transforming, Alimagham notes, but its three co-instructors are always adapting as well. “When you’re involved in a team-taught course that includes material in which you are not the primary expert, you evolve as an instructor. It keeps you on your toes.”

Ravel agrees: “One benefit of co-teaching is that we learn from each other. It’s a great conversation among the three of us.”

Ravel currently serves as the head of the MIT History Section, as president of the American Society for Eighteenth-Century Studies, and as a co-director for the Comédie-Française Registers Project, which is producing a collaborative, extensive history of one of France’s iconic theater groups. “Co-teaching reminds me of what it’s like to be a student again,” reflects Padilla. “It makes me more sensitive to how students are taking in information that, for me, is now second nature.”

Padilla is a historian of Latin America and a contributor to numerous publications and volumes surrounding the Mexican Revolution. Her current book project centers on how rural schoolteachers “went fro"
9;9;machinelearningmastery.com;http://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/;2016-09-01;Avoid Overfitting By Early Stopping With XGBoost In Python;"# plot learning curve

from numpy import loadtxt

from xgboost import XGBClassifier

from sklearn . model_selection import train_test_split

from sklearn . metrics import accuracy_score

from matplotlib import pyplot

# load data

dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" )

# split data into X and y

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# split data into train and test sets

X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.33 , random_state = 7 )

# fit model no training data

model = XGBClassifier ( )

eval_set = [ ( X_train , y_train ) , ( X_test , y_test ) ]

model . fit ( X_train , y_train , eval_metric = [ ""error"" , ""logloss"" ] , eval_set = eval_set , verbose = True )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )

print ( ""Accuracy: %.2f%%"" % ( accuracy * 100.0 ) )

# retrieve performance metrics

results = model . evals_result ( )

epochs = len ( results [ 'validation_0' ] [ 'error' ] )

x_axis = range ( 0 , epochs )

# plot log loss

fig , ax = pyplot . subplots ( )

ax . plot ( x_axis , results [ 'validation_0' ] [ 'logloss' ] , label = 'Train' )

ax . plot ( x_axis , results [ 'validation_1' ] [ 'logloss' ] , label = 'Test' )

ax . legend ( )

pyplot . ylabel ( 'Log Loss' )

pyplot . title ( 'XGBoost Log Loss' )

pyplot . show ( )

# plot classification error

fig , ax = pyplot . subplots ( )

ax . plot ( x_axis , results [ 'validation_0' ] [ 'error' ] , label = 'Train' )

ax . plot ( x_axis , results [ 'validation_1' ] [ 'error' ] , label = 'Test' )

ax . legend ( )

pyplot . ylabel ( 'Classification Error' )

pyplot . title ( 'XGBoost Classification Error' )"
10;10;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/;2019-06-27;How to Develop a GAN for Generating MNIST Handwritten Digits;"# example of training a gan on mnist

from numpy import expand_dims

from numpy import zeros

from numpy import ones

from numpy import vstack

from numpy . random import randn

from numpy . random import randint

from keras . datasets . mnist import load_data

from keras . optimizers import Adam

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Reshape

from keras . layers import Flatten

from keras . layers import Conv2D

from keras . layers import Conv2DTranspose

from keras . layers import LeakyReLU

from keras . layers import Dropout

from matplotlib import pyplot

# define the standalone discriminator model

def define_discriminator ( in_shape = ( 28 , 28 , 1 ) ) :

model = Sequential ( )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = in_shape ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

return model

# define the standalone generator model

def define_generator ( latent_dim ) :

model = Sequential ( )

# foundation for 7x7 image

n_nodes = 128 * 7 * 7

model . add ( Dense ( n_nodes , input_dim = latent_dim ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Reshape ( ( 7 , 7 , 128 ) ) )

# upsample to 14x14

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# upsample to 28x28

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Conv2D ( 1 , ( 7 , 7 ) , activation = 'sigmoid' , padding = 'same' ) )

return model

# define the combined generator and discriminator model, for updating the generator

def define_gan ( g_model , d_model ) :

# make weights in the discriminator not trainable

d_model . trainable = False

# connect them

model = Sequential ( )

# add generator

model . add ( g_model )

# add the discriminator

model . add ( d_model )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt )

return model

# load and prepare mnist training images

def load_real_samples ( ) :

# load mnist dataset

( trainX , _ ) , ( _ , _ ) = load_data ( )

# expand to 3d, e.g. add channels dimension

X = expand_dims ( trainX , axis = - 1 )

# convert from unsigned ints to floats

X = X . astype ( 'float32' )

# scale from [0,255] to [0,1]

X = X / 255.0

return X

# select real samples

def generate_real_samples ( dataset , n_samples ) :

# choose random instances

ix = randint ( 0 , dataset . shape [ 0 ] , n_samples )

# retrieve selected images

X = dataset [ ix ]

# generate 'real' class labels (1)

y = ones ( ( n_samples , 1 ) )

return X , y

# generate points in latent space as input for the generator

def generate_latent_points ( latent_dim , n_samples ) :

# generate points in the latent space

x_input = randn ( latent_dim * n_samples )

# reshape into a batch of inputs for the network

x_input = x_input . reshape ( n_samples , latent_dim )

return x_input

# use the generator to generate n fake examples, with class labels

def generate_fake_samples ( g_model , latent_dim , n_samples ) :

# generate points in latent space

x_input = generate_latent_points ( latent_dim , n_samples )

# predict outputs

X = g_model . predict ( x_input )

# create 'fake' class labels (0)

y = zeros ( ( n_samples , 1 ) )

return X , y

# create and save a plot of generated images (reversed grayscale)

def save_plot ( examples , epoch , n = 10 ) :

# plot images

for i in range ( n * n ) :

# define subplot

pyplot . subplot ( n , n , 1 + i )

# turn off axis

pyplot . axis ( 'off' )

# plot raw pixel data

pyplot . imshow ( examples [ i , : , : , 0 ] , cmap = 'gray_r' )

# save plot to file

filename = 'generated_plot_e%03d.png' % ( epoch + 1 )

pyplot . savefig ( filename )

pyplot . close ( )

# evaluate the discriminator, plot generated images, save generator model

def summarize_performance ( epoch , g_model , d_model , dataset , latent_dim , n_samples = 100 ) :

# prepare real samples

X_real , y_real = generate_real_samples ( dataset , n_samples )

# evaluate discriminator on real examples

_ , acc_real = d_model . evaluate ( X_real , y_real , verbose = 0 )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( g_model , latent_dim , n_samples )

# evaluate discriminator on fake examples

_ , acc_fake = d_model . evaluate ( x_fake , y_fake , verbose = 0 )

# summarize discriminator performance

print ( '>Accuracy rea"
11;11;machinelearningmastery.com;https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/;2017-05-23;A Gentle Introduction to Long Short-Term Memory Networks by the Experts;"Tweet Share Share

Last Updated on February 20, 2020

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.

This is a behavior required in complex problem domains like machine translation, speech recognition, and more.

LSTMs are a complex area of deep learning. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional and sequence-to-sequence relate to the field.

In this post, you will get insight into LSTMs using the words of research scientists that developed the methods and applied them to new and important problems.

There are few that are better at clearly and precisely articulating both the promise of LSTMs and how they work than the experts that developed them.

We will explore key questions in the field of LSTMs using quotes from the experts, and if you’re interested, you will be able to dive into the original papers from which the quotes were taken.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

The Promise of Recurrent Neural Networks

Recurrent neural networks are different from traditional feed-forward neural networks.

This difference in the addition of complexity comes with the promise of new behaviors that the traditional methods cannot achieve.

Recurrent networks … have an internal state that can represent context information. … [they] keep information about past inputs for an amount of time that is not fixed a priori, but rather depends on its weights and on the input data. … A recurrent network whose inputs are not fixed but rather constitute an input sequence can be used to transform an input sequence into an output sequence while taking into account contextual information in a flexible way.

— Yoshua Bengio, et al., Learning Long-Term Dependencies with Gradient Descent is Difficult, 1994.

The paper defines 3 basic requirements of a recurrent neural network:

That the system be able to store information for an arbitrary duration.

That the system be resistant to noise (i.e. fluctuations of the inputs that are random or irrelevant to predicting a correct output).

That the system parameters be trainable (in reasonable time).

The paper also describes the “minimal task” for demonstrating recurrent neural networks.

Context is key.

Recurrent neural networks must use context when making predictions, but to this extent, the context required must also be learned.

… recurrent neural networks contain cycles that feed the network activations from a previous time step as inputs to the network to influence predictions at the current time step. These activations are stored in the internal states of the network which can in principle hold long-term temporal contextual information. This mechanism allows RNNs to exploit a dynamically changing contextual window over the input sequence history

— Hassim Sak, et al., Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling, 2014

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

LSTMs Deliver on the Promise

The success of LSTMs is in their claim to be one of the first implements to overcome the technical problems and deliver on the promise of recurrent neural networks.

Hence standard RNNs fail to learn in the presence of time lags greater than 5 – 10 discrete time steps between relevant input events and target signals. The vanishing error problem casts doubt on whether standard RNNs can indeed exhibit significant practical advantages over time window-based feedforward networks. A recent model, “Long Short-Term Memory” (LSTM), is not affected by this problem. LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through “constant error carrousels” (CECs) within special units, called cells

— Felix A. Gers, et al., Learning to Forget: Continual Prediction with LSTM, 2000

The two technical problems overcome by LSTMs are vanishing gradients and exploding gradients, both related to how the network is trained.

Unfortunately, the range of contextual information that standard RNNs can access is in practice quite limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network’s recurrent connections. This shortcoming … referred to in the literature as the vanishing gradient problem … Long Short-Term Memory (LSTM) is an RNN architecture specifically designed to address the vanishing gradient problem.

— Alex Graves, et al., A Novel Connectionist System for Unconstrained Handwriting Reco"
12;12;machinelearningmastery.com;https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/;2018-09-03;How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course);"# univariate mlp example

from numpy import array

from keras . models import Sequential

from keras . layers import Dense

# define dataset

X = array ( [ [ 10 , 20 , 30 ] , [ 20 , 30 , 40 ] , [ 30 , 40 , 50 ] , [ 40 , 50 , 60 ] ] )

y = array ( [ 40 , 50 , 60 , 70 ] )

# define model

model = Sequential ( )

model . add ( Dense ( 100 , activation = 'relu' , input_dim = 3 ) )

model . add ( Dense ( 1 ) )

model . compile ( optimizer = 'adam' , loss = 'mse' )

# fit model

model . fit ( X , y , epochs = 2000 , verbose = 0 )

# demonstrate prediction

x_input = array ( [ 50 , 60 , 70 ] )

x_input = x_input . reshape ( ( 1 , 3 ) )

yhat = model . predict ( x_input , verbose = 0 )"
13;13;news.mit.edu;http://news.mit.edu/2020/cnt-nanosensor-smartphone-plant-stress-0415;;Nanosensor can alert a smartphone when plants are stressed;"MIT engineers have developed a way to closely track how plants respond to stresses such as injury, infection, and light damage, using sensors made of carbon nanotubes. These sensors can be embedded in plant leaves, where they report on hydrogen peroxide signaling waves.

Plants use hydrogen peroxide to communicate within their leaves, sending out a distress signal that stimulates leaf cells to produce compounds that will help them repair damage or fend off predators such as insects. The new sensors can use these hydrogen peroxide signals to distinguish between different types of stress, as well as between different species of plants.

“Plants have a very sophisticated form of internal communication, which we can now observe for the first time. That means that in real-time, we can see a living plant’s response, communicating the specific type of stress that it’s experiencing,” says Michael Strano, the Carbon P. Dubbs Professor of Chemical Engineering at MIT.

This kind of sensor could be used to study how plants respond to different types of stress, potentially helping agricultural scientists develop new strategies to improve crop yields. The researchers demonstrated their approach in eight different plant species, including spinach, strawberry plants, and arugula, and they believe it could work in many more.

Strano is the senior author of the study, which appears today in Nature Plants. MIT graduate student Tedrick Thomas Salim Lew is the lead author of the paper.

Embedded sensors

Over the past several years, Strano’s lab has been exploring the potential for engineering “nanobionic plants” — plants that incorporate nanomaterials that give the plants new functions, such as emitting light or detecting water shortages. In the new study, he set out to incorporate sensors that would report back on the plants’ health status.

Strano had previously developed carbon nanotube sensors that can detect various molecules, including hydrogen peroxide. About three years ago, Lew began working on trying to incorporate these sensors into plant leaves. Studies in Arabidopsis thaliana, often used for molecular studies of plants, had suggested that plants might use hydrogen peroxide as a signaling molecule, but its exact role was unclear.

Lew used a method called lipid exchange envelope penetration (LEEP) to incorporate the sensors into plant leaves. LEEP, which Strano’s lab developed several years ago, allows for the design of nanoparticles that can penetrate plant cell membranes. As Lew was working on embedding the carbon nanotube sensors, he made a serendipitous discovery.

“I was training myself to get familiarized with the technique, and in the process of the training I accidentally inflicted a wound on the plant. Then I saw this evolution of the hydrogen peroxide signal,” he says.

He saw that after a leaf was injured, hydrogen peroxide was released from the wound site and generated a wave that spread along the leaf, similar to the way that neurons transmit electrical impulses in our brains. As a plant cell releases hydrogen peroxide, it triggers calcium release within adjacent cells, which stimulates those cells to release more hydrogen peroxide.

“Like dominos successively falling, this makes a wave that can propagate much further than a hydrogen peroxide puff alone would,” Strano says. “The wave itself is powered by the cells that receive and propagate it.”

This flood of hydrogen peroxide stimulates plant cells to produce molecules called secondary metabolites, such as flavonoids or carotenoids, which help them to repair the damage. Some plants also produce other secondary metabolites that can be secreted to fend off predators. These metabolites are often the source of the food flavors that we desire in our edible plants, and they are only produced under stress.

A key advantage of the new sensing technique is that it can be used in many different plant species. Traditionally, plant biologists have done much of their molecular biology research in certain plants that are amenable to genetic manipulation, including Arabidopsis thaliana and tobacco plants. However, the new MIT approach is applicable to potentially any plant.

“In this study, we were able to quickly compare eight plant species, and you would not be able to do that with the old tools,” Strano says.

The researchers tested strawberry plants, spinach, arugula, lettuce, watercress, and sorrel, and found that different species appear to produce different waveforms — the distinctive shape produced by mapping the concentration of hydrogen peroxide over time. They hypothesize that each plant’s response is related to its ability to counteract the damage. Each species also appears to respond differently to different types of stress, including mechanical injury, infection, and heat or light damage.

“This waveform holds a lot of information for each species, and even more exciting is that the type of stress on a given plant is encoded in this waveform,” Strano"
14;14;machinelearningmastery.com;http://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/;2016-03-13;Parametric and Nonparametric Machine Learning Algorithms;"Tweet Share Share

Last Updated on October 25, 2019

What is a parametric machine learning algorithm and how is it different from a nonparametric machine learning algorithm?

In this post you will discover the difference between parametric and nonparametric machine learning algorithms.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Learning a Function

Machine learning can be summarized as learning a function (f) that maps input variables (X) to output variables (Y).

Y = f(x)

An algorithm learns this target mapping function from training data.

The form of the function is unknown, so our job as machine learning practitioners is to evaluate different machine learning algorithms and see which is better at approximating the underlying function.

Different algorithms make different assumptions or biases about the form of the function and how it can be learned.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Parametric Machine Learning Algorithms

Assumptions can greatly simplify the learning process, but can also limit what can be learned. Algorithms that simplify the function to a known form are called parametric machine learning algorithms.

A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.

— Artificial Intelligence: A Modern Approach, page 737

The algorithms involve two steps:

Select a form for the function. Learn the coefficients for the function from the training data.

An easy to understand functional form for the mapping function is a line, as is used in linear regression:

b0 + b1*x1 + b2*x2 = 0

Where b0, b1 and b2 are the coefficients of the line that control the intercept and slope, and x1 and x2 are two input variables.

Assuming the functional form of a line greatly simplifies the learning process. Now, all we need to do is estimate the coefficients of the line equation and we have a predictive model for the problem.

Often the assumed functional form is a linear combination of the input variables and as such parametric machine learning algorithms are often also called “linear machine learning algorithms“.

The problem is, the actual unknown underlying function may not be a linear function like a line. It could be almost a line and require some minor transformation of the input data to work right. Or it could be nothing like a line in which case the assumption is wrong and the approach will produce poor results.

Some more examples of parametric machine learning algorithms include:

Logistic Regression

Linear Discriminant Analysis

Perceptron

Naive Bayes

Simple Neural Networks

Benefits of Parametric Machine Learning Algorithms:

Simpler : These methods are easier to understand and interpret results.

: These methods are easier to understand and interpret results. Speed : Parametric models are very fast to learn from data.

: Parametric models are very fast to learn from data. Less Data: They do not require as much training data and can work well even if the fit to the data is not perfect.

Limitations of Parametric Machine Learning Algorithms:

Constrained : By choosing a functional form these methods are highly constrained to the specified form.

: By choosing a functional form these methods are highly constrained to the specified form. Limited Complexity : The methods are more suited to simpler problems.

: The methods are more suited to simpler problems. Poor Fit: In practice the methods are unlikely to match the underlying mapping function.

Nonparametric Machine Learning Algorithms

Algorithms that do not make strong assumptions about the form of the mapping function are called nonparametric machine learning algorithms. By not making assumptions, they are free to learn any functional form from the training data.

Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features.

— Artificial Intelligence: A Modern Approach, page 757

Nonparametric methods seek to best fit the training data in constructing the mapping function, whilst maintaining some ability to generalize to unseen data. As such, they are able to fit a large number of functional forms.

An easy to understand nonparametric model is the k-nearest neighbors algorithm that makes predictions based on the k most similar training patterns for a new data instance. The method does not assume anything about the form of the mapping function other than patterns that are close "
15;15;news.mit.edu;http://news.mit.edu/2020/bringing-deep-learning-to-life-0224;;Bringing deep learning to life;"Gaby Ecanow loves listening to music, but never considered writing her own until taking 6.S191 (Introduction to Deep Learning). By her second class, the second-year MIT student had composed an original Irish folk song with the help of a recurrent neural network, and was considering how to adapt the model to create her own Louis the Child-inspired dance beats.

“It was cool,” she says. “It didn’t sound at all like a machine had made it.”

This year, 6.S191 kicked off as usual, with students spilling into the aisles of Stata Center’s Kirsch Auditorium during Independent Activities Period (IAP). But the opening lecture featured a twist: a recorded welcome from former President Barack Obama. The video was quickly revealed to be an AI-generated fabrication, one of many twists that Alexander Amini ’17 and Ava Soleimany ’16 introduce throughout their for-credit course to make the equations and code come alive.

As hundreds of their peers look on, Amini and Soleimany take turns at the podium. If they appear at ease, it’s because they know the material cold; they designed the curriculum themselves, and have taught it for the past three years. The course covers the technical foundations of deep learning and its societal implications through lectures and software labs focused on real-world applications. On the final day, students compete for prizes by pitching their own ideas for research projects. In the weeks leading up to class, Amini and Soleimany spend hours updating the labs, refreshing their lectures, and honing their presentations.

A branch of machine learning, deep learning harnesses massive data and algorithms modeled loosely on how the brain processes information to make predictions. The class has been credited with helping to spread machine-learning tools into research labs across MIT. That’s by design, says Amini, a graduate student in MIT’s Department of Electrical Engineering and Computer Science (EECS), and Soleimany, a graduate student at MIT and Harvard University.

Both are using machine learning in their own research — Amini in engineering robots, and Soleimany in developing diagnostic tools for cancer — and they wanted to make sure the curriculum would prepare students to do the same. In addition to the lab on developing a music-generating AI, they offer labs on building a face-recognition model with convolutional neural networks and a bot that uses reinforcement learning to play the vintage Atari video game, Pong. After students master the basics, those taking the class for credit go on to create applications of their own.

This year, 23 teams presented projects. Among the prize winners was Carmen Martin, a graduate student in the Harvard-MIT Program in Health Sciences and Technology (HST), who proposed using a type of neural net called a graph convolutional network to predict the spread of coronavirus. She combined several data streams: airline ticketing data to measure population fluxes, real-time confirmation of new infections, and a ranking of how well countries are equipped to prevent and respond to a pandemic.

“The goal is to train the model to predict cases to guide national governments and the World Health Organization in their recommendations to limit new cases and save lives,” she says.

A second prize winner, EECS graduate student Samuel Sledzieski, proposed building a model to predict protein interactions using only their amino acid sequences. Predicting protein behavior is key to designing drug targets, among other clinical applications, and Sledzieski wondered if deep learning could speed up the search for viable protein pairs.

“There’s still work to be done, but I’m excited by how far I was able to get in three days,” he says. “Having easy-to-follow examples in TensorFlow and Keras helped me understand how to actually build and train these models myself.” He plans to continue the work in his current lab rotation with Bonnie Berger, the Simons Professor of Mathematics in EECS and the Computer Science and Artificial Intelligence Laboratory (CSAIL).

Each year, students also hear about emerging deep-learning applications from companies sponsoring the course. David Cox, co-director of the MIT-IBM Watson AI Lab, covered neuro-symbolic AI, a hybrid approach that combines symbolic programs with deep learning’s expert pattern-matching ability. Alex Wiltschko, a senior researcher at Google Brain, spoke about using a network analysis tool to predict the scent of small molecules. Chuan Li, chief scientific officer at Lambda Labs, discussed neural rendering, a tool for reconstructing and generating graphics scenes. Animesh Garg, a senior researcher at NVIDIA, covered strategies for developing robots that perceive and act more human-like.

With 350 students taking the live course each year, and more than a million people who have watched the lectures online, Amini and Soleimany have become prominent ambassadors for deep learning. Yet, it was tennis that first brought them together.

Amini c"
16;16;machinelearningmastery.com;http://machinelearningmastery.com/use-classification-machine-learning-algorithms-weka/;2016-07-24;How To Use Classification Machine Learning Algorithms in Weka;"Tweet Share Share

Last Updated on August 22, 2019

Weka makes a large number of classification algorithms available.

The large number of machine learning algorithms available is one of the benefits of using the Weka platform to work through your machine learning problems.

In this post you will discover how to use 5 top machine learning algorithms in Weka.

After reading this post you will know:

About 5 top machine learning algorithms that you can use on your classification problems.

How to use 5 top classification algorithms in Weka.

The key configuration parameters for 5 top classification algorithms.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Classification Algorithm Tour Overview

We are going to take a tour of 5 top classification algorithms in Weka.

Each algorithm that we cover will be briefly described in terms of how it works, key algorithm parameters will be highlighted and the algorithm will be demonstrated in the Weka Explorer interface.

The 5 algorithms that we will review are:

Logistic Regression Naive Bayes Decision Tree k-Nearest Neighbors Support Vector Machines

These are 5 algorithms that you can try on your classification problem as a starting point.

A standard machine learning classification problem will be used to demonstrate each algorithm. Specifically, the Ionosphere binary classification problem. This is a good dataset to demonstrate classification algorithms because the input variables are numeric and all have the same scale the problem only has two classes to discriminate.

Each instance describes the properties of radar returns from the atmosphere and the task is to predict whether or not there is structure in the ionosphere or not. There are 34 numerical input variables of generally the same scale. You can learn more about this dataset on the UCI Machine Learning Repository. Top results are in the order of 98% accuracy.

Start the Weka Explorer:

Open the Weka GUI Chooser. Click the “Explorer” button to open the Weka Explorer. Load the Ionosphere dataset from the data/ionosphere.arff file. Click “Classify” to open the Classify tab.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Logistic Regression

Logistic regression is a binary classification algorithm.

It assumes the input variables are numeric and have a Gaussian (bell curve) distribution. This last point does not have to be true, as logistic regression can still achieve good results if your data is not Gaussian. In the case of the Ionosphere dataset, some input attributes have a Gaussian-like distribution, but many do not.

The algorithm learns a coefficient for each input value, which are linearly combined into a regression function and transformed using a logistic (s-shaped) function. Logistic regression is a fast and simple technique, but can be very effective on some problems.

The logistic regression only supports binary classification problems, although the Weka implementation has been adapted to support multi-class classification problems.

Choose the logistic regression algorithm:

Click the “Choose” button and select “Logistic” under the “functions” group. Click on the name of the algorithm to review the algorithm configuration.

The algorithm can run for a fixed number of iterations (maxIts), but by default will run until it is estimated that the algorithm has converged.

The implementation uses a ridge estimator which is a type of regularization. This method seeks to simplify the model during training by minimizing the coefficients learned by the model. The ridge parameter defines how much pressure to put on the algorithm to reduce the size of the coefficients. Setting this to 0 will turn off this regularization.

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Ionosphere dataset.

You can see that with the default configuration that logistic regression achieves an accuracy of 88%.

Naive Bayes

Naive Bayes is a classification algorithm. Traditionally it assumes that the input values are nominal, although it numerical inputs are supported by assuming a distribution.

Naive Bayes uses a simple implementation of Bayes Theorem (hence naive) where the prior probability for each class is calculated from the training data and assumed to be independent of each other (technically called conditionally independent).

This is an unrealistic assumption because we expect the variables to interact and be dependent, although this assumption makes the probabilities fast and easy to calculate. Even under this unrealistic assumption, Naive Bayes has been shown to be a very effective classification algorithm.

Naive Bayes ca"
17;17;machinelearningmastery.com;http://machinelearningmastery.com/compare-the-performance-of-machine-learning-algorithms-in-r/;2016-02-25;Compare The Performance of Machine Learning Algorithms in R;"# prepare training scheme

control < - trainControl ( method = ""repeatedcv"" , number = 10 , repeats = 3 )

# CART

set . seed ( 7 )

fit . cart < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""rpart"" , trControl = control )

# LDA

set . seed ( 7 )

fit . lda < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""lda"" , trControl = control )

# SVM

set . seed ( 7 )

fit . svm < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""svmRadial"" , trControl = control )

# kNN

set . seed ( 7 )

fit . knn < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""knn"" , trControl = control )

# Random Forest

set . seed ( 7 )

fit . rf < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""rf"" , trControl = control )

# collect resamples"
18;18;machinelearningmastery.com;https://machinelearningmastery.com/how-to-handle-big-p-little-n-p-n-in-machine-learning/#comments;2020-04-14;How to Handle Big-p, Little-n (p >> n) in Machine Learning;"Tweet Share Share

What if I have more Columns than Rows in my dataset?

Machine learning datasets are often structured or tabular data comprised of rows and columns.

The columns that are fed as input to a model are called predictors or “p” and the rows are samples “n“. Most machine learning algorithms assume that there are many more samples than there are predictors, denoted as p << n.

Sometimes, this is not the case, and there are many more predictors than samples in the dataset, referred to as “big-p, little-n” and denoted as p >> n. These problems often require specialized data preparation and modeling algorithms to address them correctly.

In this tutorial, you will discover the challenge of big-p, little n or p >> n machine learning problems.

After completing this tutorial, you will know:

Most machine learning problems have many more samples than predictors and most machine learning algorithms make this assumption during the training process.

Some modeling problems have many more predictors than samples, referred to as p >> n.

Algorithms to explore when modeling machine learning datasets with more predictors than samples.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Predictors (p) and Samples (n)

Machine Learning Assumes p << n

How to Handle p >> n

Predictors (p) and Samples (n)

Consider a predictive modeling problem, such as classification or regression.

The dataset is structured data or tabular data, like what you might see in an Excel spreadsheet.

There are columns and rows. Most of the columns would be used as inputs to a model and one column would represent the output or variable to be predicted.

The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable—in this case, sales—is often called the response or dependent variable, and is typically denoted using the symbol Y.

— Page 15, An Introduction to Statistical Learning with Applications in R, 2017.

Each column represents a variable or one aspect of a sample. The columns that represent the inputs to the model are called predictors.

Each row represents one sample with values across each of the columns or features.

Predictors : Input columns of a dataset, also called input variables or features.

: Input columns of a dataset, also called input variables or features. Samples: Rows of a dataset, also called an observation, example, or instance.

It is common to describe a training dataset in machine learning in terms of the predictors and samples.

The number of predictors in a dataset is described using the term “p” and the number of samples in a dataset is described using the term “n” or sometimes “N“.

p : The number of predictors in a dataset.

: The number of predictors in a dataset. n: The number of samples in a dataset.

To make this concrete, let’s take a look at the iris flowers classification problem.

Below is a sample of the first five rows of this dataset.

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ... 1 2 3 4 5 6 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ...

This dataset has five columns and 150 rows.

The first four columns are inputs and the fifth column is the output, meaning that there are four predictors.

We would describe the iris flowers dataset as:

p=4, n=150.

Machine Learning Assumes p << n

It is almost always the case that the number of predictors (p) will be smaller than the number of samples (n).

Often much smaller.

We can summarize this expectation as p << n, where “<<” is a mathematical inequality that means “much less than.”

p << n: Typically we have fewer predictors than samples.

To demonstrate this, let’s look at a few more standard machine learning datasets:

Most machine learning algorithms operate based on the assumption that there are many more samples than predictors.

One way to think about predictors and samples is to take a geometrical perspective.

Consider a hypercube where the number of predictors (p) defines the number of dimensions of the hypercube. The volume of this hypercube is the scope of possible samples that could be drawn from the domain. The number of samples (n) are the actual samples drawn from the domain that you must use to model your predictive modeling problem.

This is a rationale for the axiom “get as much data as possible” in applied machine learning. It is a desire to gather a sufficiently representative sample of the p-dimensional problem domain.

As the number of dimensions (p) increases, the volume of the domain increases exponentially. This, in turn, requires more samples (n) from the domain to provide effective coverage of the domain for a learning algorithm. We don’t need full coverage of the domain, just what is likely to be observable.

Th"
19;19;machinelearningmastery.com;http://machinelearningmastery.com/practice-machine-learning-with-small-in-memory-datasets-from-the-uci-machine-learning-repository/;2015-08-20;Practice Machine Learning with Datasets from the UCI Machine Learning Repository;"Tweet Share Share

Last Updated on July 5, 2019

Where can you get good datasets to practice machine learning?

Datasets that are real-world so that they are interesting and relevant, although small enough for you to review in Excel and work through on your desktop.

In this post you will discover a database of high-quality, real-world, and well understood machine learning datasets that you can use to practice applied machine learning.

This database is called the UCI machine learning repository and you can use it to structure a self-study program and build a solid foundation in machine learning.

Why Do We Need Practice Datasets?

If you are interested in practicing applied machine learning, you need datasets on which to practice.

This problem can stop you dead.

Which dataset should you use?

Should you collect your own or use one off the shelf?

Which one and why?

I teach a top-down approach to machine learning where I encourage you to learn a process for working a problem end-to-end, map that process onto a tool and practice the process on data in a targeted way. For more information see my post “Machine Learning for Programmers: Leap from developer to machine learning practitioner“.

So How Do You Practice In A Targeted Way?

I teach that the best way to get started is to practice on datasets that have specific traits.

I recommend you select traits that you will encounter and need to address when you start working on problems of your own such as:

Different types of supervised learning such as classification and regression.

Different sized datasets from tens, hundreds, thousands and millions of instances.

Different numbers of attributes from less than ten, tens, hundreds and thousands of attributes

Different attribute types from real, integer, categorical, ordinal and mixtures

Different domains that force you to quickly understand and characterize a new problem in which you have no previous experience.

You can create a program of traits to study and learn about and the algorithm you need to address them, by designing a program of test problem datasets to work through.

Such a program has a number of practical requirements, for example:

Real-World : The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data.

: The datasets should be drawn from the real world (rather than being contrived). This will keep them interesting and introduce the challenges that come with real data. Small : The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle.

: The datasets need to be small so that you can inspect and understand them and that you can run many models quickly to accelerate your learning cycle. Well-Understood : There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation.

: There should be a clear idea of what the data contains, why it was collected, what the problem is that needs to be solved so that you can frame your investigation. Baseline : It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken).

: It is also important to have an idea of what algorithms are known to perform well and the scores they achieved so that you have a useful point of comparison. This is important when you are getting started and learning because you need quick feedback as to how well you are performing (close to state-of-the-art or something is broken). Plentiful: You need many datasets to choose from, both to satisfy the traits you would like to investigate and (if possible) your natural curiosity and interests.

For beginners, you can get everything you need and more in terms of datasets to practice on from the UCI Machine Learning Repository.

What is the UCI Machine Learning Repository?

The UCI Machine Learning Repository is a database of machine learning problems that you can access for free.

It is hosted and maintained by the Center for Machine Learning and Intelligent Systems at the University of California, Irvine. It was originally created by David Aha as a graduate student at UC Irvine.

For more than 25 years it has been the go-to place for machine learning researchers and machine learning practitioners that need a dataset.

Each dataset gets its own webpage that lists all the details known about it including any relevant publications that investigate it. The datasets themselves can be downloaded as ASCII files, often the useful CSV format.

For example, here is the webpage for the Abalone Data Set that requires the prediction of the "
20;20;news.mit.edu;http://news.mit.edu/2020/origins-earth-magnetic-field-mystery-0408;;Origins of Earth’s magnetic field remain a mystery;"Microscopic minerals excavated from an ancient outcrop of Jack Hills, in Western Australia, have been the subject of intense geological study, as they seem to bear traces of the Earth’s magnetic field reaching as far back as 4.2 billion years ago. That’s almost 1 billion years earlier than when the magnetic field was previously thought to originate, and nearly back to the time when the planet itself was formed.

But as intriguing as this origin story may be, an MIT-led team has now found evidence to the contrary. In a paper published today in Science Advances, the team examined the same type of crystals, called zircons, excavated from the same outcrop, and have concluded that zircons they collected are unreliable as recorders of ancient magnetic fields.

In other words, the jury is still out on whether the Earth’s magnetic field existed earlier than 3.5 billion years ago.

“There is no robust evidence of a magnetic field prior to 3.5 billion years ago, and even if there was a field, it will be very difficult to find evidence for it in Jack Hills zircons,” says Caue Borlina, a graduate student in MIT’s Department of Earth, Atmospheric, and Planetary Sciences (EAPS). “It’s an important result in the sense that we know what not to look for anymore.”

Borlina is the paper’s first author, which also includes EAPS Professor Benjamin Weiss, Principal Research Scientist Eduardo Lima, and Research Scientist Jahandar Ramezan of MIT, along with others from Cambridge University, Harvard University, the University of California at Los Angeles, the University of Alabama, and Princeton University.

A field, stirred up

Earth’s magnetic field is thought to play an important role in making the planet habitable. Not only does a magnetic field set the direction of our compass needles, it also acts as a shield of sorts, deflecting away solar wind that might otherwise eat away at the atmosphere.

Scientists know that today the Earth’s magnetic field is powered by the solidification of the planet’s liquid iron core. The cooling and crystallization of the core stirs up the surrounding liquid iron, creating powerful electric currents that generate a magnetic field stretching far out into space. This magnetic field is known as the geodynamo.

Multiple lines of evidence have shown that the Earth’s magnetic field existed at least 3.5 billion years ago. However, the planet’s core is thought to have started solidifying just 1 billion years ago, meaning that the magnetic field must have been driven by some other mechanism prior to 1 billion years ago. Pinning down exactly when the magnetic field formed could help scientists figure out what generated it to begin with.

Borlina says the origin of Earth’s magnetic field could also illuminate the early conditions in which Earth’s first life forms took hold.

“In the Earth’s first billion years, between 4.4 billion and 3.5 billion years, that’s when life was emerging,” Borlina says. “Whether you have a magnetic field at that time has different implications for the environment in which life emerged on Earth. That’s the motivation for our work.”

“Can’t trust zircon”

Scientists have traditionally used minerals in ancient rocks to determine the orientation and intensity of Earth’s magnetic field back through time. As rocks form and cool, the electrons within individual grains can shift in the direction of the surrounding magnetic field. Once the rock cools past a certain temperature, known as the Curie temperature, the orientations of the electrons are set in stone, so to speak. Scientists can determine their age and use standard magnetometers to measure their orientation, to estimate the strength and orientation of the Earth’s magnetic field at a given point in time.

Since 2001, Weiss and his group have been studying the magnetization of the Jack Hills rocks and zircon grains, with the challenging goal of establishing whether they contain ancient records of the Earth’s magnetic field.

“The Jack Hills zircons are some of the most weakly magnetic objects studied in the history of paleomagnetism,” Weiss says. “Furthermore, these zircons include the oldest known Earth materials, meaning that there are many geological events that could have reset their magnetic records.”

In 2015, a separate research group that had also started studying the Jack Hills zircons argued that they found evidence of magnetic material in zircons that they dated to be 4.2 billion years old — the first evidence that Earth’s magnetic field may have existed prior to 3.5 billion years ago.

But Borlina notes that the team did not confirm whether the magnetic material they detected actually formed during or after the zircon crystal formed 4.2 billion years ago — a goal that he and his team took on for their new paper.

Borlina, Weiss, and their colleagues had collected rocks from the same Jack Hills outcrop, and from those samples, extracted 3,754 zircon grains, each around 150 micrometers long — about the width of a human "
21;21;news.mit.edu;http://news.mit.edu/2020/amid-shutdowns-supply-chains-pivot-demand-for-specialized-talent-intensifies-mitx-micromasters-0413;;Amid shutdowns, supply chains pivot and global demand for specialized talent intensifies;"The global landscape of supply chain management has changed drastically in the past several weeks. Businesses, organizations, and people are rapidly innovating to improve supply chains and upskill and reskill the workforce and themselves to accommodate disruptions caused by the global Covid-19 health crisis. Online retailers and logistics providers are announcing vast hiring initiatives, while companies and organizations grapple with the logistics demands of supplying for vital services.

Even in the middle of these disruptions, a cohort of 383 dedicated online learners concluded nine to 18 months of learning to pass their comprehensive final exams, earning their MITx MicroMasters program credentials in supply chain management. These new credential-holders bring the total number of holders to 2,243 from 115 countries. The majority of credential-holders hail from the United States, India, Brazil, Spain, and China, some of the world’s most influential economies. While credential holders’ median age is 31, holders range in age from 21 to 74, practicing diverse business functions. They are currently employed at more than 700 companies worldwide, ranging from the largest multinational corporations to local, family-owned businesses.

Given the volatile nature of logistics during disruptions, the comprehensive theoretical and practical knowledge gained from these courses is already having an impact. “The program significantly changed my mindset to be proactive. This helped me improvise ahead of the current pandemic challenges to provide visibility across my supply chain,” says learner Mohamed El Tayeb, a demand planner in Saudi Arabia. “Technically, everything I learned in the program is coming in handy now.” Similarly, Matthias Stolz, a supply chain management project manager from Germany, claims “The MicroMasters program helped me to be able to make back-of-the-envelope calculations to quantify effects and evaluate risks and opportunities fast. This allowed me to confidently prepare decisions for the top management which have already enabled the company to respond quickly.”

Learners like El Tayeb and Stolz are leading the way on the ground, along with contributors from across the supply chain, to be cited as everyday heroes by MIT’s Center for Transportation and Logistics and the U.S. Department of Agriculture, among many others.

MIT will recognize the contributions of credential holders and program participants in a public online completion celebration on April 15 at 11 a.m. EDT. “Our goal is to pioneer supply chain digital education to shape the leaders of the future,” says program Director Eva Ponce. “We are bringing MIT education to anyone from anywhere to improve the capabilities and prospects of professionals through our massive open online courses. It is my distinct pleasure to thank the committed and passionate team responsible for the development and delivery of this program and to welcome this learner cohort to the credential-holder community, who are the future of the supply chain profession.”

As a new normal becomes apparent in the foreseeable future, experts agree that the global disruptions should serve as a wake-up call for supply chain and logistics managers. They foresee that practitioners will need an array of practical and analytical tools at their disposal to accommodate rapidly changing demands. Teaching supply chain management online is one strategy to meet this dynamic demand. The MITx MicroMasters Program in Supply Chain Management is becoming recognized as a go-to knowledge baseline for individuals and organizations to meet their global demand for talent."
22;22;machinelearningmastery.com;https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/;2020-01-14;Random Oversampling and Undersampling for Imbalanced Classification;"# example of random oversampling to balance the class distribution

from collections import Counter

from sklearn . datasets import make_classification

from imblearn . over_sampling import RandomOverSampler

# define dataset

X , y = make_classification ( n_samples = 10000 , weights = [ 0.99 ] , flip_y = 0 )

# summarize class distribution

print ( Counter ( y ) )

# define oversampling strategy

oversample = RandomOverSampler ( sampling_strategy = 'minority' )

# fit and apply the transform

X_over , y_over = oversample . fit_resample ( X , y )

# summarize class distribution"
23;23;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/;2018-11-11;How to Develop Convolutional Neural Network Models for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Convolutional Neural Network models, or CNNs for short, can be applied to time series forecasting.

There are many types of CNN models that can be used for each specific type of time series forecasting problem.

In this tutorial, you will discover how to develop a suite of CNN models for a range of standard time series forecasting problems.

The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem.

After completing this tutorial, you will know:

How to develop CNN models for univariate time series forecasting.

How to develop CNN models for multivariate time series forecasting.

How to develop CNN models for multi-step time series forecasting.

This is a large and important post; you may want to bookmark it for future reference.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

In this tutorial, we will explore how to develop a suite of different types of CNN models for time series forecasting.

The models are demonstrated on small contrived time series problems intended to give the flavor of the type of time series problem being addressed. The chosen configuration of the models is arbitrary and not optimized for each problem; that was not the goal.

This tutorial is divided into four parts; they are:

Univariate CNN Models Multivariate CNN Models Multi-Step CNN Models Multivariate Multi-Step CNN Models

Univariate CNN Models

Although traditionally developed for two-dimensional image data, CNNs can be used to model univariate time series forecasting problems.

Univariate time series are datasets comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence.

This section is divided into two parts; they are:

Data Preparation CNN Model

Data Preparation

Before a univariate series can be modeled, it must be prepared.

The CNN model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn.

Consider a given univariate sequence:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.

X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ... 1 2 3 4 5 X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ...

The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.

# split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on our small contrived dataset above.

The complete example is listed below.

# univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # univariate data preparation from numpy import array # split a univ"
24;24;machinelearningmastery.com;http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/;2016-08-08;How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras;"# Use scikit-learn to grid search the number of neurons

import numpy

from sklearn . model_selection import GridSearchCV

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Dropout

from keras . wrappers . scikit_learn import KerasClassifier

from keras . constraints import maxnorm

# Function to create model, required for KerasClassifier

def create_model ( neurons = 1 ) :

# create model

model = Sequential ( )

model . add ( Dense ( neurons , input_dim = 8 , kernel_initializer = 'uniform' , activation = 'linear' , kernel_constraint = maxnorm ( 4 ) ) )

model . add ( Dropout ( 0.2 ) )

model . add ( Dense ( 1 , kernel_initializer = 'uniform' , activation = 'sigmoid' ) )

# Compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

return model

# fix random seed for reproducibility

seed = 7

numpy . random . seed ( seed )

# load dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# create model

model = KerasClassifier ( build_fn = create_model , epochs = 100 , batch_size = 10 , verbose = 0 )

# define the grid search parameters

neurons = [ 1 , 5 , 10 , 15 , 20 , 25 , 30 ]

param_grid = dict ( neurons = neurons )

grid = GridSearchCV ( estimator = model , param_grid = param_grid , n_jobs = - 1 , cv = 3 )

grid_result = grid . fit ( X , Y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :"
25;25;machinelearningmastery.com;https://machinelearningmastery.com/scale-machine-learning-data-scratch-python/;2016-10-13;How to Scale Machine Learning Data From Scratch With Python;"from csv import reader

from math import sqrt

# Load a CSV file

def load_csv ( filename ) :

file = open ( filename , ""rb"" )

lines = reader ( file )

dataset = list ( lines )

return dataset

# Convert string column to float

def str_column_to_float ( dataset , column ) :

for row in dataset :

row [ column ] = float ( row [ column ] . strip ( ) )

# calculate column means

def column_means ( dataset ) :

means = [ 0 for i in range ( len ( dataset [ 0 ] ) ) ]

for i in range ( len ( dataset [ 0 ] ) ) :

col_values = [ row [ i ] for row in dataset ]

means [ i ] = sum ( col_values ) / float ( len ( dataset ) )

return means

# calculate column standard deviations

def column_stdevs ( dataset , means ) :

stdevs = [ 0 for i in range ( len ( dataset [ 0 ] ) ) ]

for i in range ( len ( dataset [ 0 ] ) ) :

variance = [ pow ( row [ i ] - means [ i ] , 2 ) for row in dataset ]

stdevs [ i ] = sum ( variance )

stdevs = [ sqrt ( x / ( float ( len ( dataset ) - 1 ) ) ) for x in stdevs ]

return stdevs

# standardize dataset

def standardize_dataset ( dataset , means , stdevs ) :

for row in dataset :

for i in range ( len ( row ) ) :

row [ i ] = ( row [ i ] - means [ i ] ) / stdevs [ i ]

# Load pima-indians-diabetes dataset

filename = 'pima-indians-diabetes.csv'

dataset = load_csv ( filename )

print ( 'Loaded data file {0} with {1} rows and {2} columns' ) . format ( filename , len ( dataset ) , len ( dataset [ 0 ] ) )

# convert string columns to float

for i in range ( len ( dataset [ 0 ] ) ) :

str_column_to_float ( dataset , i )

print ( dataset [ 0 ] )

# Estimate mean and standard deviation

means = column_means ( dataset )

stdevs = column_stdevs ( dataset , means )

# standardize dataset

standardize_dataset ( dataset , means , stdevs )"
26;26;machinelearningmastery.com;https://machinelearningmastery.com/gentle-introduction-generative-long-short-term-memory-networks/;2017-08-24;Gentle Introduction to Generative Long Short-Term Memory Networks;"Tweet Share Share

Last Updated on August 14, 2019

The Long Short-Term Memory recurrent neural network was developed for sequence prediction.

In addition to sequence prediction problems. LSTMs can also be used as a generative model

In this post, you will discover how LSTMs can be used as generative models.

After completing this post, you will know:

About generative models, with a focus on generative models for text called language modeling.

Examples of applications where LSTM Generative models have been used.

Examples of how to model text for generative models with LSTMs.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Generative Models

LSTMs can be used as a generative model.

Given a large corpus of sequence data, such as text documents, LSTM models can be designed to learn the general structural properties of the corpus, and when given a seed input, can generate new sequences that are representative of the original corpus.

The problem of developing a model to generalize a corpus of text is called language modeling in the field of natural language processing. A language model may work at the word level and learn the probabilistic relationships between words in a document in order to accurately complete a sentence and generate entirely new sentences. At its most challenging, language models work at the character level, learning from sequences of characters, and generating new sequences one character at a time.

The goal of character-level language modeling is to predict the next character in a sequence.

— Generating Text with Recurrent Neural Networks, 2011.

Although more challenging, the added flexibility of a character-level model allows new words to be generated, punctuation added, and the generation of any other structures that may exist in the text data.

… predicting one character at a time is more interesting from the perspective of sequence generation, because it allows the network to invent novel words and strings.

— Generating Sequences With Recurrent Neural Networks, 2013.

Language modeling is by far the most studied application of Generative LSTMs, perhaps because of the use of standard datasets where model performance can be quantified and compared. This approach has been used to generate text on a suite of interesting language modeling problems, such as:

Generating Wikipedia articles (including markup).

Generating snippets from great authors like Shakespeare.

Generating technical manuscripts (including markup).

Generating computer source code.

Generating article headlines.

The quality of the results vary; for example, the markup or source code may require manual intervention to render or compile. Nevertheless, the results are impressive.

The approach has also been applied to different domains where a large corpus of existing sequence information is available and new sequences can be generated one step at a time, such as:

Handwriting generation.

Music generation.

Speech generation.

Generative LSTMs

A Generative LSTM is not really architecture, it is more a change in perspective about what an LSTM predictive model learns and how the model is used.

We could conceivably use any LSTM architecture as a generative model. In this case, we will use a simple Vanilla LSTM.

In the case of a character-level language model, the alphabet of all possible characters is fixed. A one hot encoding is used both for learning input sequences and predicting output sequences.

A one-to-one model is used where one step is predicted for each input time step. This means that input sequences may require specialized handling in order to be vectorized or formatted for efficiently training a supervised model.

For example, given the sequence:

""hello world"" 1 ""hello world""

A dataset would need to be constructed such as:

'h' => 'e' 'e' => 'l' 'l' => 'l' ... 1 2 3 4 'h' => 'e' 'e' => 'l' 'l' => 'l' ...

This could be presented as-is as a dataset of one time step samples, which could be quite limiting to the network (e.g. no BPTT).

Alternately, it could be vectorized to a fixed-length input sequence for a many-to-one time step model, such as:

['h', 'e', 'l'] => 'l' ['e', 'l', 'l'] => 'o' ['l', 'l', 'o'] => ' ' ... 1 2 3 4 ['h', 'e', 'l'] => 'l' ['e', 'l', 'l'] => 'o' ['l', 'l', 'o'] => ' ' ...

Or, a fixed-length output sequence for a one-to-many time step model:

'h' => ['e', 'l', 'l'] 'e' => ['l', 'l', 'o'] 'l' => ['l', 'o', ' '] ... 1 2 3 4 'h' => ['e', 'l', 'l'] 'e' => ['l', 'l', 'o'] 'l' => ['l', 'o', ' '] ...

Or some variation on these approaches.

Note that the same vectorized representation would be required when "
27;27;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/;2019-05-12;How to Develop a CNN From Scratch for CIFAR-10 Photo Classification;"# baseline model with dropout and data augmentation on the cifar10 dataset

import sys

from matplotlib import pyplot

from keras . datasets import cifar10

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Conv2D

from keras . layers import MaxPooling2D

from keras . layers import Dense

from keras . layers import Flatten

from keras . optimizers import SGD

from keras . preprocessing . image import ImageDataGenerator

from keras . layers import Dropout

from keras . layers import BatchNormalization

# load train and test dataset

def load_dataset ( ) :

# load dataset

( trainX , trainY ) , ( testX , testY ) = cifar10 . load_data ( )

# one hot encode target values

trainY = to_categorical ( trainY )

testY = to_categorical ( testY )

return trainX , trainY , testX , testY

# scale pixels

def prep_pixels ( train , test ) :

# convert from integers to floats

train_norm = train . astype ( 'float32' )

test_norm = test . astype ( 'float32' )

# normalize to range 0-1

train_norm = train_norm / 255.0

test_norm = test_norm / 255.0

# return normalized images

return train_norm , test_norm

# define cnn model

def define_model ( ) :

model = Sequential ( )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' , input_shape = ( 32 , 32 , 3 ) ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.2 ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.3 ) )

model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( BatchNormalization ( ) )

model . add ( Dropout ( 0.5 ) )

model . add ( Dense ( 10 , activation = 'softmax' ) )

# compile model

opt = SGD ( lr = 0.001 , momentum = 0.9 )

model . compile ( optimizer = opt , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )

return model

# plot diagnostic learning curves

def summarize_diagnostics ( history ) :

# plot loss

pyplot . subplot ( 211 )

pyplot . title ( 'Cross Entropy Loss' )

pyplot . plot ( history . history [ 'loss' ] , color = 'blue' , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , color = 'orange' , label = 'test' )

# plot accuracy

pyplot . subplot ( 212 )

pyplot . title ( 'Classification Accuracy' )

pyplot . plot ( history . history [ 'accuracy' ] , color = 'blue' , label = 'train' )

pyplot . plot ( history . history [ 'val_accuracy' ] , color = 'orange' , label = 'test' )

# save plot to file

filename = sys . argv [ 0 ] . split ( '/' ) [ - 1 ]

pyplot . savefig ( filename + '_plot.png' )

pyplot . close ( )

# run the test harness for evaluating a model

def run_test_harness ( ) :

# load dataset

trainX , trainY , testX , testY = load_dataset ( )

# prepare pixel data

trainX , testX = prep_pixels ( trainX , testX )

# define model

model = define_model ( )

# create data generator

datagen = ImageDataGenerator ( width_shift_range = 0.1 , height_shift_range = 0.1 , horizontal_flip = True )

# prepare iterator

it_train = datagen . flow ( trainX , trainY , batch_size = 64 )

# fit model

steps = int ( trainX . shape [ 0 ] / 64 )

history = model . fit_generator ( it_train , steps_per_epoch = steps , epochs = 400 , validation_data = ( testX , testY ) , verbose = 0 )

# evaluate model

_ , acc = model . evaluate ( testX , testY , verbose = 0 )

print ( '> %.3f' % ( acc * 100.0 ) )

# learning curves

summarize_diagnostics ( history )

# entry point, run the test harness"
28;28;news.mit.edu;http://news.mit.edu/2020/engineers-3d-print-brain-implants-0330;;Engineers 3D print soft, rubbery brain implants;"The brain is one of our most vulnerable organs, as soft as the softest tofu. Brain implants, on the other hand, are typically made from metal and other rigid materials that over time can cause inflammation and the buildup of scar tissue.

MIT engineers are working on developing soft, flexible neural implants that can gently conform to the brain’s contours and monitor activity over longer periods, without aggravating surrounding tissue. Such flexible electronics could be softer alternatives to existing metal-based electrodes designed to monitor brain activity, and may also be useful in brain implants that stimulate neural regions to ease symptoms of epilepsy, Parkinson’s disease, and severe depression.

Led by Xuanhe Zhao, a professor of mechanical engineering and of civil and environmental engineering, the research team has now developed a way to 3D print neural probes and other electronic devices that are as soft and flexible as rubber.

The devices are made from a type of polymer, or soft plastic, that is electrically conductive. The team transformed this normally liquid-like conducting polymer solution into a substance more like viscous toothpaste — which they could then feed through a conventional 3D printer to make stable, electrically conductive patterns.

The team printed several soft electronic devices, including a small, rubbery electrode, which they implanted in the brain of a mouse. As the mouse moved freely in a controlled environment, the neural probe was able to pick up on the activity from a single neuron. Monitoring this activity can give scientists a higher-resolution picture of the brain’s activity, and can help in tailoring therapies and long-term brain implants for a variety of neurological disorders.

“We hope by demonstrating this proof of concept, people can use this technology to make different devices, quickly,” says Hyunwoo Yuk, a graduate student in Zhao’s group at MIT. “They can change the design, run the printing code, and generate a new design in 30 minutes. Hopefully this will streamline the development of neural interfaces, fully made of soft materials.”

Yuk and Zhao have published their results today in the journal Nature Communications. Their co-authors include Baoyang Lu and Jingkun Xu of the Jiangxi Science and Technology Normal University, along with Shen Lin and Jianhong Luo of Zheijiang University’s School of Medicine.

The team printed several soft electronic devices, including a small, rubbery electrode.

From soap water to toothpaste

Conducting polymers are a class of materials that scientists have eagerly explored in recent years for their unique combination of plastic-like flexibility and metal-like electrical conductivity. Conducting polymers are used commercially as antistatic coatings, as they can effectively carry away any electrostatic charges that build up on electronics and other static-prone surfaces.

“These polymer solutions are easy to spray on electrical devices like touchscreens,” Yuk says. “But the liquid form is mostly for homogenous coatings, and it’s difficult to use this for any two-dimensional, high-resolution patterning. In 3D, it’s impossible.”

Yuk and his colleagues reasoned that if they could develop a printable conducting polymer, they could then use the material to print a host of soft, intricately patterned electronic devices, such as flexible circuits, and single-neuron electrodes.

In their new study, the team report modifying poly (3,4-ethylenedioxythiophene) polystyrene sulfonate, or PEDOT:PSS, a conducting polymer typically supplied in the form of an inky, dark-blue liquid. The liquid is a mixture of water and nanofibers of PEDOT:PSS. The liquid gets its conductivity from these nanofibers, which, when they come in contact, act as a sort of tunnel through which any electrical charge can flow.

If the researchers were to feed this polymer into a 3D printer in its liquid form, it would simply bleed across the underlying surface. So the team looked for a way to thicken the polymer while retaining the material’s inherent electrical conductivity.

They first freeze-dried the material, removing the liquid and leaving behind a dry matrix, or sponge, of nanofibers. Left alone, these nanofibers would become brittle and crack. So the researchers then remixed the nanofibers with a solution of water and an organic solvent, which they had previously developed, to form a hydrogel — a water-based, rubbery material embedded with nanofibers.

They made hydrogels with various concentrations of nanofibers, and found that a range between 5 to 8 percent by weight of nanofibers produced a toothpaste-like material that was both electrically conductive and suitable for feeding into a 3D printer.

“Initially, it’s like soap water,” Zhao says. “We condense the nanofibers and make it viscous like toothpaste, so we can squeeze it out as a thick, printable liquid.”

Implants on demand

The researchers fed the new conducting polymer into a conventional 3D prin"
29;29;news.mit.edu;http://news.mit.edu/2020/warning-labels-fake-news-trustworthy-0303;;The catch to putting warning labels on fake news;"After the 2016 U.S. presidential election, Facebook began putting warning tags on news stories fact-checkers judged to be false. But there’s a catch: Tagging some stories as false makes readers more willing to believe other stories and share them with friends, even if those additional, untagged stories also turn out to be false.

That is the main finding of a new study co-authored by an MIT professor, based on multiple experiments with news consumers. The researchers call this unintended consequence — in which the selective labeling of false news makes other news stories seem more legitimate — the “implied-truth effect” in news consumption.

“Putting a warning on some content is going to make you think, to some extent, that all of the other content without the warning might have been checked and verified,” says David Rand, the Erwin H. Schell Professor at the MIT Sloan School of Management and co-author of a newly published paper detailing the study.

“There’s no way the fact-checkers can keep up with the stream of misinformation, so even if the warnings do really reduce belief in the tagged stories, you still have a problem, because of the implied truth effect,” Rand adds.

Moreover, Rand observes, the implied truth effect “is actually perfectly rational” on the part of readers, since there is ambiguity about whether untagged stories were verified or just not yet checked. “That makes these warnings potentially problematic,” he says. “Because people will reasonably make this inference.”

Even so, the findings also suggest a solution: Placing “Verified” tags on stories found to be true eliminates the problem.

The paper, “The Implied Truth Effect,” has just appeared in online form in the journal Management Science. In addition to Rand, the authors are Gordon Pennycook, an assistant professor of psychology at the University of Regina; Adam Bear, a postdoc in the Cushman Lab at Harvard University; and Evan T. Collins, an undergraduate researcher on the project from Yale University.

BREAKING: More labels are better

To conduct the study, the researchers conducted a pair of online experiments with a total of 6,739 U.S. residents, recruited via Amazon’s Mechanical Turk platform. Participants were given a variety of true and false news headlines in a Facebook-style format. The false stories were chosen from the website Snopes.com and included headlines such as “BREAKING NEWS: Hillary Clinton Filed for Divorce in New York Courts” and “Republican Senator Unveils Plan To Send All Of America’s Teachers Through A Marine Bootcamp.”

The participants viewed an equal mix of true stories and false stories, and were asked whether they would consider sharing each story on social media. Some participants were assigned to a control group in which no stories were labeled; others saw a set of stories where some of the false ones displayed a “FALSE” label; and some participants saw a set of stories with warning labels on some false stories and “TRUE” verification labels for some true stories.

In the first place, stamping warnings on false stories does make people less likely to consider sharing them. For instance, with no labels being used at all, participants considered sharing 29.8 percent of false stories in the sample. That figure dropped to 16.1 percent of false stories that had a warning label attached.

However, the researchers also saw the implied truth effect take effect. Readers were willing to share 36.2 percent of the remaining false stories that did not have warning labels, up from 29.8 percent.

“We robustly observe this implied-truth effect, where if false content doesn’t have a warning, people believe it more and say they would be more likely to share it,” Rand notes.

But when the warning labels on some false stories were complemented with verification labels on some of the true stories, participants were less likely to consider sharing false stories, across the board. In those circumstances, they shared only 13.7 percent of the headlines labeled as false, and just 26.9 percent of the nonlabeled false stories.

“If, in addition to putting warnings on things fact-checkers find to be false, you also put verification panels on things fact-checkers find to be true, then that solves the problem, because there’s no longer any ambiguity,” Rand says. “If you see a story without a label, you know it simply hasn’t been checked.”

Policy implications

The findings come with one additional twist that Rand emphasizes, namely, that participants in the survey did not seem to reject warnings on the basis of ideology. They were still likely to change their perceptions of stories with warning or verifications labels, even if discredited news items were “concordant” with their stated political views.

“These results are not consistent with the idea that our reasoning powers are hijacked by our partisanship,” Rand says.

Rand notes that, while continued research on the subject is important, the current study suggests a straightfo"
30;30;machinelearningmastery.com;https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/;2018-12-09;Use Early Stopping to Halt the Training of Neural Networks At the Right Time;"# generate two moons dataset

from sklearn . datasets import make_moons

from matplotlib import pyplot

from pandas import DataFrame

# generate 2d classification dataset

X , y = make_moons ( n_samples = 100 , noise = 0.2 , random_state = 1 )

# scatter plot, dots colored by class value

df = DataFrame ( dict ( x = X [ : , 0 ] , y = X [ : , 1 ] , label = y ) )

colors = { 0 : 'red' , 1 : 'blue' }

fig , ax = pyplot . subplots ( )

grouped = df . groupby ( 'label' )

for key , group in grouped :

group . plot ( ax = ax , kind = 'scatter' , x = 'x' , y = 'y' , label = key , color = colors [ key ] )"
31;31;machinelearningmastery.com;http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/;2016-03-20;Overfitting and Underfitting With Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

The cause of poor performance in machine learning is either overfitting or underfitting the data.

In this post, you will discover the concept of generalization in machine learning and the problems of overfitting and underfitting that go along with it.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Approximate a Target Function in Machine Learning

Supervised machine learning is best understood as approximating a target function (f) that maps input variables (X) to an output variable (Y).

Y = f(X)

This characterization describes the range of classification and prediction problems and the machine algorithms that can be used to address them.

An important consideration in learning the target function from the training data is how well the model generalizes to new data. Generalization is important because the data we collect is only a sample, it is incomplete and noisy.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Generalization in Machine Learning

In machine learning we describe the learning of the target function from training data as inductive learning.

Induction refers to learning general concepts from specific examples which is exactly the problem that supervised machine learning problems aim to solve. This is different from deduction that is the other way around and seeks to learn specific concepts from general rules.

Generalization refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning.

The goal of a good machine learning model is to generalize well from the training data to any data from the problem domain. This allows us to make predictions in the future on data the model has never seen.

There is a terminology used in machine learning when we talk about how well a machine learning model learns and generalizes to new data, namely overfitting and underfitting.

Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms.

Statistical Fit

In statistics, a fit refers to how well you approximate a target function.

This is good terminology to use in machine learning, because supervised machine learning algorithms seek to approximate the unknown underlying mapping function for the output variables given the input variables.

Statistics often describe the goodness of fit which refers to measures used to estimate how well the approximation of the function matches the target function.

Some of these methods are useful in machine learning (e.g. calculating the residual errors), but some of these techniques assume we know the form of the target function we are approximating, which is not the case in machine learning.

If we knew the form of the target function, we would use it directly to make predictions, rather than trying to learn an approximation from samples of noisy training data.

Overfitting in Machine Learning

Overfitting refers to a model that models the training data too well.

Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.

Overfitting is more likely with nonparametric and nonlinear models that have more flexibility when learning a target function. As such, many nonparametric machine learning algorithms also include parameters or techniques to limit and constrain how much detail the model learns.

For example, decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by pruning a tree after it has learned in order to remove some of the detail it has picked up.

Underfitting in Machine Learning

Underfitting refers to a model that can neither model the training data nor generalize to new data.

An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.

Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.

A Good Fit in Machine Learning

Ideally, you want to select a model at the sweet spot between underfitting and overfitting.

This is the goal, but is very difficult"
32;32;machinelearningmastery.com;http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/;2016-04-14;K-Nearest Neighbors for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

In this post you will discover the k-Nearest Neighbors (KNN) algorithm for classification and regression. After reading this post you will know.

The model representation used by KNN.

How a model is learned using KNN (hint, it’s not).

How to make predictions using KNN

The many names for KNN including how different fields refer to it.

How to prepare your data to get the most from KNN.

Where to look to learn more about the KNN algorithm.

This post was written for developers and assumes no background in statistics or mathematics. The focus is on how the algorithm works and how to use it for predictive modeling problems. If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

KNN Model Representation

The model representation for KNN is the entire training dataset.

It is as simple as that.

KNN has no model other than storing the entire dataset, so there is no learning required.

Efficient implementations can store the data using complex data structures like k-d trees to make look-up and matching of new patterns during prediction efficient.

Because the entire training dataset is stored, you may want to think carefully about the consistency of your training data. It might be a good idea to curate it, update it often as new data becomes available and remove erroneous and outlier data.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Making Predictions with KNN

KNN makes predictions using the training dataset directly.

Predictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.

To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance.

Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j.

EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )

Other popular distance measures include:

Hamming Distance : Calculate the distance between binary vectors (more).

: Calculate the distance between binary vectors (more). Manhattan Distance : Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more).

: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more). Minkowski Distance: Generalization of Euclidean and Manhattan distance (more).

There are many other distance measures that can be used, such as Tanimoto, Jaccard, Mahalanobis and cosine distance. You can choose the best distance metric based on the properties of your data. If you are unsure, you can experiment with different distance metrics and different values of K together and see which mix results in the most accurate models.

Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.).

The value for K can be found by algorithm tuning. It is a good idea to try many different values for K (e.g. values from 1 to 21) and see what works best for your problem.

The computational complexity of KNN increases with the size of the training dataset. For very large training sets, KNN can be made stochastic by taking a sample from the training dataset from which to calculate the K-most similar instances.

KNN has been around for a long time and has been very well studied. As such, different disciplines have different names for it, for example:

Instance-Based Learning : The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain).

: The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain). Lazy Learning : No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorith"
33;33;machinelearningmastery.com;https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/;2020-01-02;How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification;"# calculates precision for 1:100 dataset with 90 tp and 30 fp

from sklearn . metrics import precision_score

# define actual

act_pos = [ 1 for _ in range ( 100 ) ]

act_neg = [ 0 for _ in range ( 10000 ) ]

y_true = act_pos + act_neg

# define predictions

pred_pos = [ 0 for _ in range ( 10 ) ] + [ 1 for _ in range ( 90 ) ]

pred_neg = [ 1 for _ in range ( 30 ) ] + [ 0 for _ in range ( 9970 ) ]

y_pred = pred_pos + pred_neg

# calculate prediction

precision = precision_score ( y_true , y_pred , average = 'binary' )"
34;34;machinelearningmastery.com;https://machinelearningmastery.com/statistical-tolerance-intervals-in-machine-learning/;2018-05-31;A Gentle Introduction to Statistical Tolerance Intervals in Machine Learning;"# parametric tolerance interval

from numpy . random import seed

from numpy . random import randn

from numpy import mean

from numpy import sqrt

from scipy . stats import chi2

from scipy . stats import norm

# seed the random number generator

seed ( 1 )

# generate dataset

data = 5 * randn ( 100 ) + 50

# specify degrees of freedom

n = len ( data )

dof = n - 1

# specify data coverage

prop = 0.95

prop_inv = ( 1.0 - prop ) / 2.0

gauss_critical = norm . isf ( prop_inv )

print ( 'Gaussian critical value: %.3f (coverage=%d%%)' % ( gauss_critical , prop* 100 ) )

# specify confidence

prob = 0.99

chi_critical = chi2 . isf ( q = prob , df = dof )

print ( 'Chi-Squared critical value: %.3f (prob=%d%%, dof=%d)' % ( chi_critical , prob* 100 , dof ) )

# tolerance

interval = sqrt ( ( dof * ( 1 + ( 1 / n ) ) * gauss_critical* * 2 ) / chi_critical )

print ( 'Tolerance Interval: %.3f' % interval )

# summarize

data_mean = mean ( data )

lower , upper = data_mean - interval , data_mean + interval"
35;35;news.mit.edu;http://news.mit.edu/2020/searching-covid-19-protein-test-hadley-sikes-0417;;3 Questions: Hadley Sikes on searching for a Covid-19 protein test;"Before the world was alerted to the threat of a novel coronavirus spreading out from China, Hadley Sikes was already well acquainted with developing molecular technology to improve diagnosis and treatment of diseases. Now working on a crucial diagnostic test to find people with Covid-19, the Esther and Harold E. Edgerton Associate Professor of chemical engineering at MIT and principal investigator of the Antimicrobial Resistance Interdisciplinary Research Group at Singapore-MIT Alliance for Research and Technology (SMART), and her collaborators have managed to condense months of work into a matter of a few weeks.

Q: Where does your work fit in with the global Covid-19 research effort?

A: The Covid-19 pandemic has presented a huge challenge for the world’s capacity for diagnostic testing. It has given us a clearer picture of what our actual capabilities are because all the countries in this effort are as motivated as they could ever be to deploy technologies that can test populations as quickly and accurately as possible.

Scientists have only been able to deploy one kind of test so far to identify people who have Covid-19. The coronavirus that causes this illness is made out of proteins and RNA, and so far, we only detect its RNA. RNA tests are complicated and can take hours, or even days, for doctors to receive the results. A faster version of an RNA test was just announced, but it also requires laboratory equipment and it is difficult to produce as many tests are needed.

What I’ve been working on at MIT and SMART, MIT’s research enterprise in Singapore, is developing protein tests that are quick to run and don’t require laboratories. These tests can find out if viral proteins are present in bodily fluids and also if a patient’s immune system has responded to the SARS-CoV-2 virus.

That is information that is critical, especially in this situation whereby countries are shutting down. If you know who has had the infection and recovered, and thus now has immunity, you have the potential to keep things running without putting more people at risk.

Before this outbreak, we had been working with support from the Deshpande Center on protein tests to diagnose malaria, dengue, and tuberculosis. Our goal was to find a way to capture more of the proteins made by these pathogens by developing binding reagents that concentrate the proteins within a testing device. We also wanted our tests to be affordable and easy to produce in large quantities.

Developing the reagents is a slow but crucial part of the process of developing a clinical protein diagnostic, and typically takes longer than for nucleic acid tests.

Dr. Eric Miller, who is in my lab at MIT, and Dr. Patthara Kongsuphol at SMART, have been working on engineering reagents that capture more of the scarce viral proteins in a patient’s bodily fluids. If more of these viral proteins can be captured, the test can be more sensitive.

With the Covid-19 pandemic as his motivation, Dr. Miller figured out how to engineer these binding agents in just two weeks — much sooner than the several months it might typically take. On the SMART side, Dr. Kongsuphol has been leading our efforts in Singapore to integrate these agents into diverse test formats that can be challenged with clinical samples.

We are aiming to create a test that can work in 10 minutes and doesn’t require specialized instruments or laboratory infrastructure. In this way, it can be carried out at an airport or a clinic to accurately show if a person either has or is immune to Covid-19. It’s challenging to make a test that is sensitive and accurate enough, and also a huge challenge to scale up production of such a test fast enough to have an impact when a new pathogen emerges.

Q: What influence has Singapore had on your work?

A: I have been at MIT for 10 years and started working with SMART two years ago. Joining an interdisciplinary research team in Singapore has given me a really great chance to work on a pressing medical problem of our time, which is antimicrobial resistance. It allows me to work with world-class clinicians and government agencies that are international leaders in public health, and with top researchers at Nanyang Technological University, the National University of Singapore, and the Agency for Science, Technology and Research.

I spent January in Singapore and went back again at the beginning of March, just as the outbreak was emerging in the United States. I really wanted to learn more about how Singapore’s experience during the SARS outbreak in the early 2000s allowed them to respond so effectively to this outbreak, particularly with diagnostic testing.

It was nerve-wracking being separated from my family at this time. I have three young children and a husband who has a full-time job. Because of the 12-hour time difference with Boston, we had a lot of late-night and early-morning FaceTime chats.

They have been really supportive of the work my team and I are trying to do. I t"
36;36;machinelearningmastery.com;https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/#comments;2020-03-31;Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost;"# gradient boosting for classification in scikit-learn

from numpy import mean

from numpy import std

from sklearn . datasets import make_classification

from sklearn . ensemble import GradientBoostingClassifier

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from matplotlib import pyplot

# define dataset

X , y = make_classification ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_redundant = 5 , random_state = 1 )

# evaluate the model

model = GradientBoostingClassifier ( )

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

n_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' )

print ( 'Accuracy: %.3f (%.3f)' % ( mean ( n_scores ) , std ( n_scores ) ) )

# fit the model on the whole dataset

model = GradientBoostingClassifier ( )

model . fit ( X , y )

# make a single prediction

row = [ [ 2.56999479 , - 0.13019997 , 3.16075093 , - 4.35936352 , - 1.61271951 , - 1.39352057 , - 2.48924933 , - 1.93094078 , 3.26130366 , 2.05692145 ] ]

yhat = model . predict ( row )"
37;37;machinelearningmastery.com;https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/;2020-02-11;Bagging and Random Forest for Imbalanced Classification;"# bagged decision trees on an imbalanced classification problem

from numpy import mean

from sklearn . datasets import make_classification

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from sklearn . ensemble import BaggingClassifier

# generate dataset

X , y = make_classification ( n_samples = 10000 , n_features = 2 , n_redundant = 0 ,

n_clusters_per_class = 1 , weights = [ 0.99 ] , flip_y = 0 , random_state = 4 )

# define model

model = BaggingClassifier ( )

# define evaluation procedure

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

# evaluate model

scores = cross_val_score ( model , X , y , scoring = 'roc_auc' , cv = cv , n_jobs = - 1 )

# summarize performance"
38;38;machinelearningmastery.com;http://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/;2014-02-06;Why you should be Spot-Checking Algorithms on your Machine Learning Problems;"Tweet Share Share

Last Updated on April 7, 2018

Spot-checking algorithms is about getting a quick assessment of a bunch of different algorithms on your machine learning problem so that you know what algorithms to focus on and what to discard.

In this post you will discover the 3 benefits of spot-checking algorithms, 5 tips for spot-checking on your next problem and the top 10 most popular data mining algorithms that you could use in your suite of algorithms to spot-check.

Spot-Checking Algorithms

Spot-checking algorithms is a part of the process of applied machine learning. On a new problem, you need to quickly determine which type or class of algorithms is good at picking out the structure in your problem and which are not.

The alternative to spot checking is that you feel overwhelmed by the vast number of algorithms and algorithm types that you could try that you end up trying very few or going with what has worked for you in the past. This results in wasted time and sub-par results.

Benefits of Spot-Checking Algorithms

There are 3 key benefits of spot-checking algorithms on your machine learning problems:

Speed : You could spend a lot of time playing around with different algorithms, tuning parameters and thinking about what algorithms will do well on your problem. I have been there and end up testing the same algorithms over and over because I have not been systematic. A single spot-check experiment can save hours, days and even weeks of noodling around.

: You could spend a lot of time playing around with different algorithms, tuning parameters and thinking about what algorithms will do well on your problem. I have been there and end up testing the same algorithms over and over because I have not been systematic. A single spot-check experiment can save hours, days and even weeks of noodling around. Objective : There is a tendency to go with what has worked for you before. We pick our favorite algorithm (or algorithms) and apply them to every problem we see. The power of machine learning is that there are so many different ways to approach a given problem. A spot-check experiment allows you to automatically and objectively discover those algorithms that are the best at picking out the structure in the problem so you can focus your attention.

: There is a tendency to go with what has worked for you before. We pick our favorite algorithm (or algorithms) and apply them to every problem we see. The power of machine learning is that there are so many different ways to approach a given problem. A spot-check experiment allows you to automatically and objectively discover those algorithms that are the best at picking out the structure in the problem so you can focus your attention. Results: Spot-checking algorithms gets you usable results, fast. You may discover a good enough solution in the first spot experiment. Alternatively, you may quickly learn that your dataset does not expose enough structure for any mainstream algorithm to do well. Spot-checking gives you the results you need to decide whether to move forward and optimize a given model or backward and revisit the presentation of the problem.

I think spot checking mainstream algorithms on your problem is a no-brainer first step.

Tips for Spot-Checking Algorithms

There are some things you can do when you are spot-checking algorithms to ensure you are getting useful and actionable results.

Below are 5 tips to ensure you are getting the most from spot-checking machine learning algorithms on your problem.

Algorithm Diversity : You want a good mix of algorithm types. I like to include instance based methods (live LVQ and knn), functions and kernels (like neural nets, regression and SVM), rule systems (like Decision Table and RIPPER) and decision trees (like CART, ID3 and C4.5).

: You want a good mix of algorithm types. I like to include instance based methods (live LVQ and knn), functions and kernels (like neural nets, regression and SVM), rule systems (like Decision Table and RIPPER) and decision trees (like CART, ID3 and C4.5). Best Foot Forward : Each algorithm needs to be given a chance to put it’s best foot forward. This does not mean performing a sensitivity analysis on the parameters of each algorithm, but using experiments and heuristics to give each algorithm a fair chance. For example if kNN is in the mix, give it 3 chances with k values of 1, 5 and 7.

: Each algorithm needs to be given a chance to put it’s best foot forward. This does not mean performing a sensitivity analysis on the parameters of each algorithm, but using experiments and heuristics to give each algorithm a fair chance. For example if kNN is in the mix, give it 3 chances with k values of 1, 5 and 7. Formal Experiment : Don’t play. There is a huge temptation to try lots of different things in an informal manner, to play around with algorithms on your problem. The idea of spot-checking is to get to the methods that do well on the problem, fast. Design the experimen"
39;39;machinelearningmastery.com;https://machinelearningmastery.com/applications-of-deep-learning-for-natural-language-processing/;2017-09-19;7 Applications of Deep Learning for Natural Language Processing;"Tweet Share Share

Last Updated on August 7, 2019

The field of natural language processing is shifting from statistical methods to neural network methods.

There are still many challenging problems to solve in natural language. Nevertheless, deep learning methods are achieving state-of-the-art results on some specific language problems.

It is not just the performance of deep learning models on benchmark problems that is most interesting; it is the fact that a single model can learn word meaning and perform language tasks, obviating the need for a pipeline of specialized and hand-crafted methods.

In this post, you will discover 7 interesting natural language processing tasks where deep learning methods are achieving some headway.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we will look at the following 7 natural language processing problems.

Text Classification Language Modeling Speech Recognition Caption Generation Machine Translation Document Summarization Question Answering

I have tried to focus on the types of end-user problems that you may be interested in, as opposed to more academic or linguistic sub-problems where deep learning does well such as part-of-speech tagging, chunking, named entity recognition, and so on.

Each example provides a description of the problem, an example, and references to papers that demonstrate the methods and results. Most references are drawn from Goldberg’s excellent 2015 primer on deep learning for NLP researchers.

Do you have a favorite NLP application for deep learning that is not listed?

Let me know in the comments below.

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Text Classification

Given an example of text, predict a predefined class label.

The goal of text categorization is to classify the topic or theme of a document.

— Page 575, Foundations of Statistical Natural Language Processing, 1999.

A popular classification example is sentiment analysis where class labels represent the emotional tone of the source text such as “positive” or “negative“.

Below are a 3 more examples:

Spam filtering, classifying email text as spam or not.

Language identification, classifying the language of the source text.

Genre classification, classifying the genre of a fictional story.

Further, the problem may be framed in a way that requires multiple classes assigned to a text, so-called multi-label classification. Such as predicting multiple hashtags for a source tweet.

For more on the general topic, see:

Below are 3 examples of deep learning papers for text classification:

2. Language Modeling

Language modeling is really a subtask of more interesting natural language problems, specifically those that condition the language model on some other input.

… the problem is to predict the next word given the previous words. The task is fundamental to speech or optical character recognition, and is also used for spelling correction, handwriting recognition, and statistical machine translation.

— Page 191, Foundations of Statistical Natural Language Processing, 1999.

In addition to the academic interest in language modeling, it is a key component of many deep learning natural language processing architectures.

A language model learns the probabilistic relationship between words such that new sequences of words can be generated that are statistically consistent with the source text.

Alone, language models can be used for text or speech generation; for example:

Generating new article headlines.

Generating new sentences, paragraphs, or documents.

Generating suggested continuation of a sentence.

For more in language modeling, see:

Below is an example of deep learning for language modeling (only):

Language model of English texts, books and news articles. A Neural Probabilistic Language Model, 2003



3. Speech Recognition

Speech recognition is the problem of understanding what was said.

The task of speech recognition is to map an acoustic signal containing a spoken natural language utterance into the corresponding sequence of words intended by the speaker.

— Page 458, Deep Learning, 2016.

Given an utterance of text as audio data, the model must produce human readable text.

Given the automatic nature of the process, the problem may also be called Automatic Speech Recognition (ASR).

A language model is used to create the text output that is conditioned on the audio data.

Some examples include:

Transcribing a speech.

Creating text captions for a movie or TV show.

Issuing commands to the radio while driving.

For more on speech recognition, see:

Below are 3 examples of deep learning for speech recognition.

4. Caption Generati"
40;40;machinelearningmastery.com;https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/;2018-01-09;How to Develop a Neural Machine Translation System from Scratch;"from pickle import load

from numpy import array

from numpy import argmax

from keras . preprocessing . text import Tokenizer

from keras . preprocessing . sequence import pad_sequences

from keras . models import load_model

from nltk . translate . bleu_score import corpus_bleu

# load a clean dataset

def load_clean_sentences ( filename ) :

return load ( open ( filename , 'rb' ) )

# fit a tokenizer

def create_tokenizer ( lines ) :

tokenizer = Tokenizer ( )

tokenizer . fit_on_texts ( lines )

return tokenizer

# max sentence length

def max_length ( lines ) :

return max ( len ( line . split ( ) ) for line in lines )

# encode and pad sequences

def encode_sequences ( tokenizer , length , lines ) :

# integer encode sequences

X = tokenizer . texts_to_sequences ( lines )

# pad sequences with 0 values

X = pad_sequences ( X , maxlen = length , padding = 'post' )

return X

# map an integer to a word

def word_for_id ( integer , tokenizer ) :

for word , index in tokenizer . word_index . items ( ) :

if index == integer :

return word

return None

# generate target given source sequence

def predict_sequence ( model , tokenizer , source ) :

prediction = model . predict ( source , verbose = 0 ) [ 0 ]

integers = [ argmax ( vector ) for vector in prediction ]

target = list ( )

for i in integers :

word = word_for_id ( i , tokenizer )

if word is None :

break

target . append ( word )

return ' ' . join ( target )

# evaluate the skill of the model

def evaluate_model ( model , tokenizer , sources , raw_dataset ) :

actual , predicted = list ( ) , list ( )

for i , source in enumerate ( sources ) :

# translate encoded source text

source = source . reshape ( ( 1 , source . shape [ 0 ] ) )

translation = predict_sequence ( model , eng_tokenizer , source )

raw_target , raw_src = raw_dataset [ i ]

if i < 10 :

print ( 'src=[%s], target=[%s], predicted=[%s]' % ( raw_src , raw_target , translation ) )

actual . append ( [ raw_target . split ( ) ] )

predicted . append ( translation . split ( ) )

# calculate BLEU score

print ( 'BLEU-1: %f' % corpus_bleu ( actual , predicted , weights = ( 1.0 , 0 , 0 , 0 ) ) )

print ( 'BLEU-2: %f' % corpus_bleu ( actual , predicted , weights = ( 0.5 , 0.5 , 0 , 0 ) ) )

print ( 'BLEU-3: %f' % corpus_bleu ( actual , predicted , weights = ( 0.3 , 0.3 , 0.3 , 0 ) ) )

print ( 'BLEU-4: %f' % corpus_bleu ( actual , predicted , weights = ( 0.25 , 0.25 , 0.25 , 0.25 ) ) )

# load datasets

dataset = load_clean_sentences ( 'english-german-both.pkl' )

train = load_clean_sentences ( 'english-german-train.pkl' )

test = load_clean_sentences ( 'english-german-test.pkl' )

# prepare english tokenizer

eng_tokenizer = create_tokenizer ( dataset [ : , 0 ] )

eng_vocab_size = len ( eng_tokenizer . word_index ) + 1

eng_length = max_length ( dataset [ : , 0 ] )

# prepare german tokenizer

ger_tokenizer = create_tokenizer ( dataset [ : , 1 ] )

ger_vocab_size = len ( ger_tokenizer . word_index ) + 1

ger_length = max_length ( dataset [ : , 1 ] )

# prepare data

trainX = encode_sequences ( ger_tokenizer , ger_length , train [ : , 1 ] )

testX = encode_sequences ( ger_tokenizer , ger_length , test [ : , 1 ] )

# load model

model = load_model ( 'model.h5' )

# test on some training sequences

print ( 'train' )

evaluate_model ( model , eng_tokenizer , trainX , train )

# test on some test sequences

print ( 'test' )"
41;41;machinelearningmastery.com;http://machinelearningmastery.com/how-to-choose-the-right-test-options-when-evaluating-machine-learning-algorithms/;2014-02-18;How To Choose The Right Test Options When Evaluating Machine Learning Algorithms;"Tweet Share Share

Last Updated on June 21, 2016

The test options you use when evaluating machine learning algorithms can mean the difference between over-learning, a mediocre result and a usable state-of-the-art result that you can confidently shout from the roof tops (you really do feel like doing that sometimes).

In this post you will discover the standard test options you can use in your algorithm evaluation test harness and how to choose the right options next time.

Randomness

The root of the difficulty in choosing the right test options is randomness. Most (almost all) machine learning algorithms use randomness in some way. The randomness may be explicit in the algorithm or may be in the sample of the data selected to train the algorithm.

This does not mean that the algorithms produce random results, it means that they produce results with some noise or variance. We call this type of limited variance, stochastic and the algorithms that exploit it, stochastic algorithms.

Train and Test on Same Data

If you have a dataset, you may want to train the model on the dataset and then report the results of the model on that dataset. That’s how good the model is, right?

The problem with this approach of evaluating algorithms is that you indeed will know the performance of the algorithm on the dataset, but do not have any indication of how the algorithm will perform on data that the model was not trained on (so-called unseen data).

This matters, only if you want to use the model to make predictions on unseen data.

Split Test

A simple way to use one dataset to both train and estimate the performance of the algorithm on unseen data is to split the dataset. You take the dataset, and split it into a training dataset and a test dataset. For example, you randomly select 66% of the instances for training and use the remaining 34% as a test dataset.

The algorithm is run on the training dataset and a model is created and assessed on the test dataset and you get a performance accuracy, lets say 87% classification accuracy.

Spit tests are fast and great when you have a lot of data or when training a model is expensive (it resources or time). A split test on a very very large dataset can produce an accurate estimate of the actual performance of the algorithm.

How good is the algorithm on the data? Can we confidently say it can achieve an accuracy of 87%?

A problem is that if we spit the training dataset again into a different 66%/34% split, we would get a different result from our algorithm. This is called model variance.

Multiple Split Tests

A solution to our problem with the split test getting different results on different splits of the dataset is to reduce the variance of the random process and do it many times. We can collect the results from a fair number of runs (say 10) and take the average.

For example, let’s say we split our dataset 66%/34%, ran our algorithm and got an accuracy and we did this 10 times with 10 different splits. We might have 10 accuracy scores as follows: 87, 87, 88, 89, 88, 86, 88, 87, 88, 87.

The average performance of our model is 87.5, with a standard deviation of about 0.85.

A problem with multiple split tests is that it is possible that some data instance are never included for training or testing, where as others may be selected multiple times. The effect is that this may skew results and may not give an meaningful idea of the accuracy of the algorithm.

Cross Validation

A solution to the problem of ensuring each instance is used for training and testing an equal number of times while reducing the variance of an accuracy score is to use cross validation. Specifically k-fold cross validation, where k is the number of splits to make in the dataset.

For example, let’s choose a value of k=10 (very common). This will split the dataset into 10 parts (10 folds) and the algorithm will be run 10 times. Each time the algorithm is run, it will be trained on 90% of the data and tested on 10%, and each run of the algorithm will change which 10% of the data the algorithm is tested on.

In this example, each data instance will be used as a training instance exactly 9 times and as a test instance 1 time. The accuracy will not be a mean and a standard deviation, but instead will be an exact accuracy score of how many correct predictions were made.

The k-fold cross validation method is the go-to method for evaluating the performance of an algorithm on a dataset. You want to choose k-values that give you a good sized training and test dataset for your algorithm. Not too disproportionate (too large or small for training or test). If you have a lot of data, you may may have to resort to either sampling the data or reverting to a split test.

Cross validation does give an unbiased estimation of the algorithms performance on unseen data, but what if the algorithm itself uses randomness. The algorithm would produce different results for the same training data each time it was trained with a"
42;42;news.mit.edu;http://news.mit.edu/2020/dreaming-big-small-country-misti-global-startup-labs-uruguay-0224;;Dreaming big in a small country;"When Miguel Brechner started planning a new ambitious plan to foster a new generation of data scientists in Uruguay and Latin America, he immediately thought of MIT. “There is no question that MIT is a world leader in science and technology. In Uruguay we are a small country, but we dream big.” Brechner is president of Plan Ceibal, an internationally awarded public initiative that has as main goals to distribute technology, promote knowledge, and generate social equity by widening access to digital technologies.

In 2019, Uruguayan public institutions like Plan Ceibal, ANII (Agencia Nacional de Investigación e Innovación), and UTEC (Universidad Tecnológica del Uruguay) began collaborating with MIT International Science and Technology Initiatives (MISTI) and the Abdul Latif Jameel World Education Lab (J-WEL). The partnership supports 60 Latin American students that are part of the Program in Data Science, a program which includes online courses from MITx and on-site workshops run by J-WEL and MISTI. Local students include CEOs, entrepreneurs, engineers, economists, medical professionals, and senior administrators.

The MISTI Global Startup Labs (GSL) program, now in its 20th year, has expanded its partnerships to include Uruguayan institutions to promote entrepreneurship and data science across Latin America. GSL is a unique program designed to offer the opportunity to blend digital technologies and entrepreneurship in emerging regions in the world. Since 1998, hundreds of MIT students have traveled to more than 15 countries to be part of the program that has benefited thousands of technology entrepreneurs around the world. GSL instructors are MIT graduate and undergraduate students, selected among many applicants from all over the institute. GSL programs in different countries are uniquely crafted based on the needs of the local partners, and MIT student instructors take the lead teaching app and web development, coding, data science, entrepreneurship, and intrapreneurship.

The new GSL, one of the first to be run over Independent Acitivities Period, took place during January in Montevideo. The Uruguay program focused specifically on machine learning and the business opportunities of the technology. The local student participants had previously taken courses from the MITx MicroMasters in Data Science, and the GSL workshop gave them the opportunity to experience project-based learning in data science. This hands-on experiential immersion in the subject matter is the core methodology of the GSL program.

More than 30 graduate and undergraduate students applied to be part of GSL in Uruguay this year, and 13 were selected to be part of the workshop in Montevideo. Eduardo Rivera, managing director for Uruguay, explained the process: “Recruiting students for GSL is always a challenge. We look for expertise and experience teaching, but also for team players and risk-takers. The team is composed of students from different disciplines and levels of studies, which makes the experience a unique opportunity for our students to learn from their MIT peers in new and challenging contexts.” Rivera adds, “At MIT, we are fortunate to have plenty of talented and passionate students, willing to cross borders and oceans to teach and learn.”

Over the course of a month, the local students were taught how to build prototypes, create business models, and pitch presentations. The class pursued projects ranging from predictive maintenance to autism detection to logistics optimization. The final results were presented in a pitch event hosted in Zonamerica, Uruguay's premier hub for technology and innovation.

""Working with our local students was a truly unique and unforgettable opportunity,"" says electrical engineering and computer science (EECS) senior Ryan Sander. ""I'm certain I learned just as much from the students as they learned from us. What really left an impression on me was observing not only how bright our students are, but also how passionate these people are about solving real-world problems with high impact.""

For MBA student Kenny Li, the opportunity to interact with the local students was broadening. “In today’s world, you need to be able to understand people’s cultures, how do they approach business, how they interact at work …GSL gave me a great learning opportunity to understand the global context of entrepreneurs.”

When not teaching classes, the MIT students were able to visit various places around Montevideo, including the beautiful beaches of Punta del Este, the neighboring city of Buenos Aires, and relaxing getaways to Colonia. After classes, the teaching team was steps away from the beach and could wind each day down with a beautiful sunset, soaking up the warm summer weather in January.

Rivera finds these cultural connections to be one of the major benefits of the program. “At MISTI, we are certain that international teaching activities contribute not only to the academic formation of the students but also"
43;43;news.mit.edu;http://news.mit.edu/2020/arnold-demain-professor-emeritus-biology-dies-0414;;Professor Emeritus Arnold Demain, a pioneer in the development of antibiotics, dies at 92;"Arnold Lester Demain, professor emeritus of biology, passed away on Apr. 3 at the age of 92 from complications due to Covid-19. He was just shy of celebrating his 93rd birthday.

Demain advanced the field of fermentation biology, and made major contributions to the study of antibiotics like penicillin, cephalosporin, and beta-lactam. Over the course of his 60-year career, he made a name for himself as one of the world’s leading industrial microbiologists, and mentored hundreds of budding scientists around the world.

“Arny was a prolific industrial biologist, as well as a colleague and friend,” says Alan Grossman, head of the Department of Biology and the Praecis Professor of Biology. “His work on antibiotic fermentations spurred a new wave. He was kind and supportive to all, and a dedicated mentor to many students and postdocs.”

Demain was born on April 26, 1927 in Brooklyn, New York, and grew up during the Great Depression. He graduated from high school at the age of 16, and attended Michigan State College (now Michigan State University). At 17, he put his education on pause to join the U.S. Navy and fight in World War II. After the war ended, he returned to Michigan State and resumed his studies, earning his bachelor’s degree in 1949 and his master’s in microbiology in 1950, with a focus on food fermentation — specifically, how pickles spoil. During this time, Demain met his wife Joanna (Kaye) Demain, and they were married on August 2, 1952.

Demain began his PhD in food science at the University of California at Berkeley, but relocated to Davis when that campus opened. Under the guidance of his research advisor and prominent yeast scholar, Herman Jan Phaff, Demain studied the degradation of pectic acid by an extracellular enzyme in the yeast Klyveromyces fragilis, publishing four papers, including one in Nature. Demain and Phaff were also among the first researchers to perform affinity chromatography, which later became a standard biochemical procedure.

After earning his PhD in 1954, Demain was recruited by Merck Sharp & Dohme Research Laboratories, first to research penicillin biosynthesis and later to study cephalosporin C. In 1965, he established Merck’s Fermentation Microbiology Department.

After 16 years at Merck, Demain joined MIT’s former Department of Nutrition and Food Science, and in 1988 he joined the Department of Biology. When he first arrived, no one at MIT was conducting research on antibiotics. He was eager to continue investigating penicillins and cephalosporins, and his hard work culminated in the breakthrough discovery of a key enzyme in cephalosporin biosynthesis: deacetoxycephalosporin C synthase.

Rich Losick PhD ’69 was finishing his graduate work at MIT when Demain arrived on campus. Demain later interacted with Losick’s wife Janice Pero and former postdoc John Perkins because all three shared an interest in vitamin B2 research. “I was drawn to Arny due to his warm and engaging personality and my interest in microbiology,” Losick recalls. “He pioneered industrial production of vitamins, antibiotics, and fine chemicals, and was revered for his many contributions to industrial microbiology. He was a big-hearted human being, an excellent and productive scientist, and a dedicated teacher. He will be greatly missed.”

While at MIT, Demain also helped catalyze the biotech industry by serving as the founding consultant for the biotech company, Cetus Corporation. By the mid-1990s, he’d spearheaded a series of NASA-sponsored experiments to probe the effects of simulated microgravity on secondary metabolism. Toward the end of his 32 years at MIT, he began examining Clostridium tetani and Clostridium difficile bacteria in hopes of devising tetanus and antibiotic-associated diarrhea vaccines. He ultimately authored more than 500 publications and 21 U.S. patents.

“Arny had a keen mind and a gentle disposition that put you at ease,” says Gerald Fink, professor in the Department of Biology and founding member and former director of the Whitehead Institute. When Fink started at MIT, Demain was the first to greet him. “He dropped into my office in Building 56 and he said, ‘You are going to like it here,’” Fink recalls.

“Arny was a wonderful colleague,” adds Robert Sauer, the Salvador E. Luria Professor of Biology. “He was always upbeat and happy to talk about science or anything else on your mind.”

Professor of Biology Anthony Sinskey shared an office with Demain, and remembers him as a pioneer who applied genetics and biochemistry to improve antibiotic production processes. He says Demain was instrumental in forming important interdisciplinary programs at MIT — including using anaerobic microorganisms to convert cellulose to fuels, as well as strategies for cell free synthesis of antibiotics and other projects.

“I learned a tremendous amount from our interactions,” Sinskey says. “He taught industrial microbiology and fermentation technology to hundreds of students both at MIT and from in"
44;44;news.mit.edu;http://news.mit.edu/2020/age-founders-successful-startups-0320;;A business edge that comes with age;"Two years ago, MIT economist Pierre Azoulay started a lively discussion when a working paper he co-authored, “Age and High-Growth Entrepreneurship,” revealed a surprising fact about startup founders: Among firms in the top 1/10 of the top 1 percent, in terms of growth, the average founder’s age is 45. That’s contrary to the popular image of valuable startups being the sole domain of twentysomething founders, such as Mark Zuckerberg of Facebook.

The paper, written with Benjamin Jones of Northwestern University, J. Daniel Kim of the University of Pennsylvania, and Javier Miranda of the U.S. Bureau of the Census, has now been officially published, in the journal American Economics Review: Insights. MIT News spoke to Azoulay, the International Programs Professor of Management at the MIT Sloan School of Management, about the finding and the discussion it has generated.

Q: What has been the response of people to the study?

A: We’re documenting a fact, and that fact either accords with people’s intuitions, or it doesn’t. Some people are genuinely surprised because they’ve lived in the current zeitgeist, and then once they start thinking about it, they say, “Ah, it makes sense.” And then there are people who say, “Oh, I knew it all along!” But Silicon Valley venture capitalists have studiously avoided engaging with what we’ve done. And I don’t know why.

I think one line of [venture capitalist] skepticism is to say, “Well, you may well be right, but you’re studying the one-in-a-thousand firm, and we’re [investing in] the one-in-a-million firm.” Which is sort of like restating that Apple, Microsoft, and Google were founded by young people.

Yes, we already knew it is possible for very large, successful firms to be founded by very young people. The question is: Is it likely?

Q: As you continue to think about this subject, is it possible to say what accounts for the success of relatively older entrepreneurs? Experience, intellectual capital, greater business connections — what matters?

A: All of those things are not mutually exclusive, and they’re all likely to play a role. They just need to be studied separately, if you will. We have to remain agnostic. But there is one key point in my view. Forget experience: How about just knowledge? I like to say there is no such thing as a 25-year-old biotech entrepreneur. That person just doesn’t exist, because you need a PhD and three postdocs [to gain high-level knowledge]. There are lots of fields where if you want to make a contribution, you have to bring yourself to the frontier of knowledge in a domain, and that takes time. And that’s not going to be the realm of the 22-year-old.

Beyond the big platform IT companies, if you’re thinking about the broader swath of entrepreneurship across a multiplicity of sectors, then you have to acknowledge this point. In some sense, if you recognize the diversity of startups, the [founder’s] age number is going to be higher than if you’re only focused on super-high-value Silicon Valley internet companies. So we need a basic attitude adjustment.

Q. Even as people mythologize the young startup founder, there is also a tech-sector ethos that heralds serial entrepreneurship and tolerates failure, because you’re taking risks and learning by doing. Isn’t that one Silicon Valley notion that might correspond to what you discovered?

A: Yes, one thing that could explain our results is entrepreneurship being an activity you can learn to do better over time. That’s certainly something we’ve heard. We can’t pin it down, but if I had to think of the most likely stories, that’s certainly one. Even within a particular sector that demands a certain amount of intellectual capital, holding that constant, even within biotech or clean energy, you might think there is something about learning by founding, which is going to lead to a correlation between success and a higher age for founders."
45;45;machinelearningmastery.com;https://machinelearningmastery.com/how-to-predict-whether-eyes-are-open-or-closed-using-brain-waves/;2018-08-26;How to Predict Whether a Persons Eyes are Open or Closed Using Brain Waves;"# remove outliers from the EEG data

from pandas import read_csv

from numpy import mean

from numpy import std

from numpy import delete

from numpy import savetxt

# load the dataset.

data = read_csv ( 'EEG_Eye_State.csv' , header = None )

values = data . values

# step over each EEG column

for i in range ( values . shape [ 1 ] - 1 ) :

# calculate column mean and standard deviation

data_mean , data_std = mean ( values [ : , i ] ) , std ( values [ : , i ] )

# define outlier bounds

cut_off = data_std * 4

lower , upper = data_mean - cut_off , data_mean + cut_off

# remove too small

too_small = [ j for j in range ( values . shape [ 0 ] ) if values [ j , i ] < lower ]

values = delete ( values , too_small , 0 )

print ( '>deleted %d rows' % len ( too_small ) )

# remove too large

too_large = [ j for j in range ( values . shape [ 0 ] ) if values [ j , i ] > upper ]

values = delete ( values , too_large , 0 )

print ( '>deleted %d rows' % len ( too_large ) )

# save the results to a new file"
46;46;machinelearningmastery.com;https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/;2017-05-16;How to Use the TimeDistributed Layer in Keras;"from numpy import array

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# prepare sequence

length = 5

seq = array ( [ i / float ( length ) for i in range ( length ) ] )

X = seq . reshape ( len ( seq ) , 1 , 1 )

y = seq . reshape ( len ( seq ) , 1 )

# define LSTM configuration

n_neurons = length

n_batch = length

n_epoch = 1000

# create LSTM

model = Sequential ( )

model . add ( LSTM ( n_neurons , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

print ( model . summary ( ) )

# train LSTM

model . fit ( X , y , epochs = n_epoch , batch_size = n_batch , verbose = 2 )

# evaluate

result = model . predict ( X , batch_size = n_batch , verbose = 0 )

for value in result :"
47;47;news.mit.edu;http://news.mit.edu/2020/posh-chatbots-0417;;Deploying more conversational chatbots;"The comedian Bill Burr has said he refuses to call into automated customer service lines for fear that, years later on his death bed, all he’ll be able to think about are the moments he wasted dealing with chatbots.

Indeed, the frustrating experience of trying to complete even the most straightforward task through an automated customer service line is enough to make anyone question the purpose of life.

Now the startup Posh is trying to make conversations with chatbots more natural and less maddening. It’s accomplishing this with an artificial intelligence-powered system that uses “conversational memory” to help users complete tasks.

“We noticed bots in general would take what the user said at face value, without connecting the dots of what was said before in the conversation,” says Posh co-founder and CEO Karan Kashyap ’17, SM ’17. “If you think about your conversations with humans, especially in places like banks with tellers or in customer service, what you said in the past is very important, so we focused on making bots more humanlike by giving them the ability to remember historical information in a conversation.”

Posh’s chatbots are currently used by over a dozen credit unions across voice- and text-based channels. The well-defined customer base has allowed the company to train its system on only the most relevant data, improving performance.

The founders plan to gradually partner with companies in other sectors to gather industry-specific data and expand the use of their system without compromising performance. Down the line, Kashyap and Posh co-founder and CTO Matt McEachern ’17, SM ’18 plan to provide their chatbots as a platform for developers to build on.

The expansion plans should attract businesses in a variety of sectors: Kashyap says some credit unions have successfully resolved more than 90 percent of customer calls with Posh’s platform. The company’s expansion may also help alleviate the mind-numbing experience of calling into traditional customer service lines.

“When we deploy our telephone product, there’s no notion of ‘Press one or press two,’” Kashyap explains. “There’s no dial tone menu. We just say, ‘Welcome to whatever credit union, how can I help you today?’ In a few words, you let us know. We prompt users to describe their problems via natural speech instead of waiting for menu options to be read out.”

Bootstrapping better bots

Kashyap and McEachern became friends while pursuing their degrees in MIT’s Department of Electrical Engineering and Computer Science. They also worked together in the same research lab at the Computer Science and Artificial Intelligence Laboratory (CSAIL).

But their relationship quickly grew outside of MIT. In 2016, the students began software consulting, in part designing chatbots for companies to handle customer inquiries around medical devices, flight booking, personal fitness, and more. Kashyap says they used their time consulting to learn about and take business risks.

“That was a great learning experience, because we got real-world experience in designing these bots using the tools that were available,” Kashyap says. “We saw the market need for a bot platform and for better bot experiences.”

From the start, the founders executed a lean business strategy that made it clear the engineering undergrads were thinking long term. Upon graduation, the founders used their savings from consulting to fund Posh’s early operations, giving themselves salaries and even hiring some contacts from MIT.

It also helped that they were accepted into the delta v accelerator, run by the Martin Trust Center for MIT Entrepreneurship, which gave them a summer of guidance and free rent. Following delta v, Posh was accepted into the DCU Fintech Innovation Center, connecting it with one of the largest credit unions in the country and netting the company another 12 months of free rent.



With DCU serving as a pilot customer, the founders got a “crash course” in the credit union industry, Kashyap says. From there they began a calculated expansion to ensure they didn’t grow faster than Posh’s revenue allowed, freeing them from having to raise venture capital.

The disciplined growth strategy at times forced Posh to get creative. Last year, as the founders were looking to build out new features and grow their team, they secured about $1.5 million in prepayments from eight credit unions in exchange for discounts on their service along with a peer-driven profit-sharing incentive. In total, the company has raised $2.5 million using that strategy.

Now on more secure financial footing, the founders are poised to accelerate Posh’s growth.

Pushing the boundaries

Even referring to today’s automated messaging platforms as chatbots seems generous. Most of the ones on the market today are only designed to understand what a user is asking for, something known as intent recognition.

The result is that many of the virtual agents in our lives, from the robotic telecom operator to Am"
48;48;machinelearningmastery.com;https://machinelearningmastery.com/benefits-of-implementing-machine-learning-algorithms-from-scratch/;2014-09-09;Benefits of Implementing Machine Learning Algorithms From Scratch;"Tweet Share Share

Last Updated on August 12, 2019

Machine Learning can be difficult to understand when getting started. There are a lot of algorithms and processes that are prescribed and used, many with difficult to penetrate explanations for how and why the work.

It can feel overwhelming.

An approach that you can use to get handle on machine learning algorithms and practices is to implement them from scratch. This will give you a deep understanding of how the algorithm works and all of the micro decision points within the method that can be parameterized or modified to tune it to a specific problem.

In this post you will discover the benefits and limitations of implementing machine learning algorithms from scratch and how you can accelerate this process by completing algorithm tutorials.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Implement Machine Learning Algorithms

Implementing machine learning algorithms from scratch can give you a deep understanding of the algorithm and a sense of confidence and ownership that are difficult to achieve by just applying the method.

Benefits

The benefits of implementing algorithms from scratch are:

Understanding : You will gain a deep appreciate for how the algorithm works. You understand how the mathematical description of the method relates to vectors and matrices of numbers that you code operates on. You will also know how all of the parameters are used, their effects and even have insights into how it could be further parameterized to specialize it for a problem.

: You will gain a deep appreciate for how the algorithm works. You understand how the mathematical description of the method relates to vectors and matrices of numbers that you code operates on. You will also know how all of the parameters are used, their effects and even have insights into how it could be further parameterized to specialize it for a problem. Starting Point : Your implementation will provide the basis for more advanced extensions and even an operational system that uses the algorithm. Your deep knowledge of the algorithm and you implementation can give you advantages of knowing the space and time complexity of your own code over using an opaque off-the-shelf library.

: Your implementation will provide the basis for more advanced extensions and even an operational system that uses the algorithm. Your deep knowledge of the algorithm and you implementation can give you advantages of knowing the space and time complexity of your own code over using an opaque off-the-shelf library. Ownership: The implementation is your own giving you confidence with the method and ownership over how it is realized as a system. It is no longer just a machine learning algorithm, but a method that is now in your toolbox.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Extensions

Once you have implemented an algorithm you can explore making improvements to the implementation. Some examples of improvements you could explore include:

Experimentation : You can expose many of the micro-decisions you made in the algorithms implementation as parameters and perform studies on variations of those parameters. This can lead to new insights and disambiguation of algorithm implementations that you can share and promote.

: You can expose many of the micro-decisions you made in the algorithms implementation as parameters and perform studies on variations of those parameters. This can lead to new insights and disambiguation of algorithm implementations that you can share and promote. Optimization : You can explore opportunities to make the implementation more efficient by using tools, libraries, different languages, different data structures, patterns and internal algorithms. Knowledge you have of algorithms and data structures for classical computer science can be very beneficial in this type of work.

: You can explore opportunities to make the implementation more efficient by using tools, libraries, different languages, different data structures, patterns and internal algorithms. Knowledge you have of algorithms and data structures for classical computer science can be very beneficial in this type of work. Specialization : You may explore ways of making the algorithm more specific to a problem. This can be required when creating production systems and is a valuable skill. Making an algorithm more problem specific can also lead to increases in efficiency (such as running time) and efficacy (such as accuracy or other performance measures).

: You may explore ways of making the algorithm more specific to a problem. This can be required when creating production systems and is a valuable skill. Maki"
49;49;machinelearningmastery.com;https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/;2019-02-12;How to Control Neural Network Model Capacity With Nodes and Layers;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52

# study of mlp learning curves given different number of layers for multi-class classification from sklearn . datasets import make_blobs from keras . models import Sequential from keras . layers import Dense from keras . optimizers import SGD from keras . utils import to_categorical from matplotlib import pyplot # prepare multi-class classification dataset def create_dataset ( ) : # generate 2d classification dataset X , y = make_blobs ( n_samples = 1000 , centers = 20 , n_features = 100 , cluster_std = 2 , random_state = 2 ) # one hot encode output variable y = to_categorical ( y ) # split into train and test n_train = 500 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] return trainX , trainy , testX , testy # fit model with given number of layers, returns test set accuracy def evaluate_model ( n_layers , trainX , trainy , testX , testy ) : # configure the model based on the data n_input , n_classes = trainX . shape [ 1 ] , testy . shape [ 1 ] # define model model = Sequential ( ) model . add ( Dense ( 10 , input_dim = n_input , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) for _ in range ( 1 , n_layers ) : model . add ( Dense ( 10 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( n_classes , activation = 'softmax' ) ) # compile model opt = SGD ( lr = 0.01 , momentum = 0.9 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] ) # fit model history = model . fit ( trainX , trainy , epochs = 100 , verbose = 0 ) # evaluate model on test set _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) return history , test_acc # get dataset trainX , trainy , testX , testy = create_dataset ( ) # evaluate model and plot learning curve of model with given number of layers all_history = list ( ) num_layers = [ 1 , 2 , 3 , 4 , 5 ] for n_layers in num_layers : # evaluate model with a given number of layers history , result = evaluate_model ( n_layers , trainX , trainy , testX , testy ) print ( 'layers=%d: %.3f' % ( n_layers , result ) ) # plot learning curve pyplot . plot ( history . history [ 'loss' ] , label = str ( n_layers ) ) pyplot . legend ( ) pyplot . show ( )"
50;50;machinelearningmastery.com;http://machinelearningmastery.com/normalize-standardize-time-series-data-python/;2016-12-11;How to Normalize and Standardize Time Series Data in Python;"# Normalize time series data

from pandas import read_csv

from sklearn . preprocessing import MinMaxScaler

# load the dataset and print the first 5 rows

series = read_csv ( 'daily-minimum-temperatures-in-me.csv' , header = 0 , index_col = 0 )

print ( series . head ( ) )

# prepare data for normalization

values = series . values

values = values . reshape ( ( len ( values ) , 1 ) )

# train the normalization

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

scaler = scaler . fit ( values )

print ( 'Min: %f, Max: %f' % ( scaler . data_min_ , scaler . data_max_ ) )

# normalize the dataset and print the first 5 rows

normalized = scaler . transform ( values )

for i in range ( 5 ) :

print ( normalized [ i ] )

# inverse transform and print the first 5 rows

inversed = scaler . inverse_transform ( normalized )

for i in range ( 5 ) :"
51;51;machinelearningmastery.com;https://machinelearningmastery.com/promise-recurrent-neural-networks-time-series-forecasting/;2017-05-21;The Promise of Recurrent Neural Networks for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Recurrent neural networks are a type of neural network that add the explicit handling of order in input observations.

This capability suggests that the promise of recurrent neural networks is to learn the temporal context of input sequences in order to make better predictions. That is, that the suite of lagged observations required to make a prediction no longer must be diagnosed and specified as in traditional time series forecasting, or even forecasting with classical neural networks. Instead, the temporal dependence can be learned, and perhaps changes to this dependence can also be learned.

In this post, you will discover the promised capability of recurrent neural networks for time series forecasting. After reading this post, you will know:

The focus and implicit, if not explicit, limitations on traditional time series forecasting methods.

The capabilities provided in using traditional feed-forward neural networks for time series forecasting.

The additional promise that recurrent neural networks make on top of traditional neural nets and hints of what this may mean in practice.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Time Series Forecasting

Time series forecasting is difficult.

Unlike the simpler problems of classification and regression, time series problems add the complexity of order or temporal dependence between observations.

This can be difficult as the specialized handling of the data is required when fitting and evaluating models. It also aids in modeling, providing additional structure like trends and seasonality that can be leveraged to improve model skill.

Traditionally, time series forecasting has been dominated by linear methods like ARIMA because they are well understood and effective on many problems. But these traditional methods also suffer from some limitations, such as:

Focus on complete data : missing or corrupt data is generally unsupported.

: missing or corrupt data is generally unsupported. Focus on linear relationships : assuming a linear relationship excludes more complex joint distributions.

: assuming a linear relationship excludes more complex joint distributions. Focus on fixed temporal dependence : the relationship between observations at different times, and in turn the number of lag observations provided as input, must be diagnosed and specified.

: the relationship between observations at different times, and in turn the number of lag observations provided as input, must be diagnosed and specified. Focus on univariate data : many real-world problems have multiple input variables.

: many real-world problems have multiple input variables. Focus on one-step forecasts: many real-world problems require forecasts with a long time horizon.

Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field.

— John Gamboa, Deep Learning for Time-Series Analysis, 2017

Note that some specialized techniques have been developed to address some of these limitations.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Neural Networks for Time Series

Neural networks approximate a mapping function from input variables to output variables.

This general capability is valuable for time series for a number of reasons.

Robust to Noise . Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values.

. Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values. Nonlinear. Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships.

… one important contribution of neural networks – namely their elegant ability to approximate arbitrary non-linear functions. This property is of high value in time series processing and promises more powerful applications, especially in the subfeld of forecasting …

— Georg Dorffner, Neural Networks for Time Series Processing, 1996.

More specifically, neural networks can be configured to support an arbitrary defined but fixed number of inputs and outputs in the mapping function. This means that:

Multivariate Inputs . An arbitrary number of input features can be specified, providing direct support for multivariate forecasting.

. An arbitrary number of input features can be specified, providing direct support for multivariate forecasting. Multi-Step Forecasts. An arbitrary number of output values can be specified, providing direct support for multi-step and even mu"
52;52;machinelearningmastery.com;https://machinelearningmastery.com/handle-missing-timesteps-sequence-prediction-problems-python/;2017-06-20;How to Handle Missing Timesteps in Sequence Prediction Problems with Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47

from random import random from numpy import array from pandas import concat from pandas import DataFrame from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import Masking # generate a sequence of random values def generate_sequence ( n_timesteps ) : return [ random ( ) for _ in range ( n_timesteps ) ] # generate data for the lstm def generate_data ( n_timesteps ) : # generate sequence sequence = generate_sequence ( n_timesteps ) sequence = array ( sequence ) # create lag df = DataFrame ( sequence ) df = concat ( [ df . shift ( 1 ) , df ] , axis = 1 ) # replace missing values with -1 df . fillna ( - 1 , inplace = True ) values = df . values # specify input and output data X , y = values , values [ : , 1 ] # reshape X = X . reshape ( len ( X ) , 2 , 1 ) y = y . reshape ( len ( y ) , 1 ) return X , y n_timesteps = 10 # define model model = Sequential ( ) model . add ( Masking ( mask_value = - 1 , input_shape = ( 2 , 1 ) ) ) model . add ( LSTM ( 5 ) ) model . add ( Dense ( 1 ) ) model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' ) # fit model for i in range ( 500 ) : X , y = generate_data ( n_timesteps ) model . fit ( X , y , epochs = 1 , batch_size = 1 , verbose = 2 ) # evaluate model on new data X , y = generate_data ( n_timesteps ) yhat = model . predict ( X ) for i in range ( len ( X ) ) : print ( 'Expected' , y [ i , 0 ] , 'Predicted' , yhat [ i , 0 ] )"
53;53;machinelearningmastery.com;http://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/;2013-12-26;How to Evaluate Machine Learning Algorithms;"Tweet Share Share

Last Updated on March 22, 2020

Once you have defined your problem and prepared your data you need to apply machine learning algorithms to the data in order to solve your problem.

You can spend a lot of time choosing, running and tuning algorithms. You want to make sure you are using your time effectively to get closer to your goal.

In this post you will step through a process to rapidly test algorithms and discover whether or not there is structure in your problem for the algorithms to learn and which algorithms are effective.

Test Harness

You need to define a test harness. The test harness is the data you will train and test an algorithm against and the performance measure you will use to assess its performance. It is important to define your test harness well so that you can focus on evaluating different algorithms and thinking deeply about the problem.

The goal of the test harness is to be able to quickly and consistently test algorithms against a fair representation of the problem being solved. The outcome of testing multiple algorithms against the harness will be an estimation of how a variety of algorithms perform on the problem against a chosen performance measure. You will know which algorithms might be worth tuning on the problem and which should not be considered further.

The results will also give you an indication of how learnable the problem is. If a variety of different learning algorithms universally perform poorly on the problem, it may be an indication of a lack of structure available to algorithms to learn. This may be because there actually is a lack of learnable structure in the selected data or it may be an opportunity to try different transforms to expose the structure to the learning algorithms.

Performance Measure

The performance measure is the way you want to evaluate a solution to the problem. It is the measurement you will make of the predictions made by a trained model on the test dataset.

Performance measures are typically specialized to the class of problem you are working with, for example classification, regression, and clustering. Many standard performance measures will give you a score that is meaningful to your problem domain. For example, classification accuracy for classification (total correct correction divided by the total predictions made multiple by 100 to turn it into a percentage).

You may also want a more detailed breakdown of performance, for example, you may want to know about the false positives on a spam classification problem because good email will be marked as spam and cannot be read.

There are many standard performance measures to choose from. You rarely have to devise a new performance measure yourself as you can generally find or adapt one that best captures the requirements of the problem being solved. Look to similar problems you uncovered and at the performance measures used to see if any can be adopted.

Test and Train Datasets

From the transformed data, you will need to select a test set and a training set. An algorithm will be trained on the training dataset and will be evaluated against the test set. This may be as simple as selecting a random split of data (66% for training, 34% for testing) or may involve more complicated sampling methods.

A trained model is not exposed to the test dataset during training and any predictions made on that dataset are designed to be indicative of the performance of the model in general. As such you want to make sure the selection of your datasets are representative of the problem you are solving.

Cross Validation

A more sophisticated approach than using a test and train dataset is to use the entire transformed dataset to train and test a given algorithm. A method you could use in your test harness that does this is called cross validation.

It first involves separating the dataset into a number of equally sized groups of instances (called folds). The model is then trained on all folds exception one that was left out and the prepared model is tested on that left out fold. The process is repeated so that each fold get’s an opportunity at being left out and acting as the test dataset. Finally, the performance measures are averaged across all folds to estimate the capability of the algorithm on the problem.

For example, a 3-fold cross validation would involve training and testing a model 3 times:

#1: Train on folds 1+2, test on fold 3

#2: Train on folds 1+3, test on fold 2

#3: Train on folds 2+3, test on fold 1

The number of folds can vary based on the size of your dataset, but common numbers are 3, 5, 7 and 10 folds. The goal is to have a good balance between the size and representation of data in your train and test sets.

When you’re just getting started, stick with a simple split of train and test data (such as 66%/34%) and move onto cross validation once you have more confidence.

Testing Algorithms

When starting with a problem and having defined a test harness you are h"
54;54;machinelearningmastery.com;https://machinelearningmastery.com/memory-in-a-long-short-term-memory-network/;2017-05-11;Demonstration of Memory with a Long Short-Term Memory Network in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72

from pandas import DataFrame from keras . models import Sequential from keras . layers import Dense from keras . layers import LSTM # binary encode an input pattern, return a list of binary vectors def encode ( pattern , n_unique ) : encoded = list ( ) for value in pattern : row = [ 0.0 for x in range ( n_unique ) ] row [ value ] = 1.0 encoded . append ( row ) return encoded # create input/output pairs of encoded vectors, returns X, y def to_xy_pairs ( encoded ) : X , y = list ( ) , list ( ) for i in range ( 1 , len ( encoded ) ) : X . append ( encoded [ i - 1 ] ) y . append ( encoded [ i ] ) return X , y # convert sequence to x/y pairs ready for use with an LSTM def to_lstm_dataset ( sequence , n_unique ) : # one hot encode encoded = encode ( sequence , n_unique ) # convert to in/out patterns X , y = to_xy_pairs ( encoded ) # convert to LSTM friendly format dfX , dfy = DataFrame ( X ) , DataFrame ( y ) lstmX = dfX . values lstmX = lstmX . reshape ( lstmX . shape [ 0 ] , 1 , lstmX . shape [ 1 ] ) lstmY = dfy . values return lstmX , lstmY # define sequences seq1 = [ 3 , 0 , 1 , 2 , 3 ] seq2 = [ 4 , 0 , 1 , 2 , 4 ] # convert sequences into required data format n_unique = len ( set ( seq1 + seq2 ) ) seq1X , seq1Y = to_lstm_dataset ( seq1 , n_unique ) seq2X , seq2Y = to_lstm_dataset ( seq2 , n_unique ) # define LSTM configuration n_neurons = 20 n_batch = 1 n_epoch = 250 n_features = n_unique # create LSTM model = Sequential ( ) model . add ( LSTM ( n_neurons , batch_input_shape = ( n_batch , 1 , n_features ) , stateful = True ) ) model . add ( Dense ( n_unique , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' ) # train LSTM for i in range ( n_epoch ) : model . fit ( seq1X , seq1Y , epochs = 1 , batch_size = n_batch , verbose = 1 , shuffle = False ) model . reset_states ( ) model . fit ( seq2X , seq2Y , epochs = 1 , batch_size = n_batch , verbose = 0 , shuffle = False ) model . reset_states ( ) # test LSTM on sequence 1 print ( 'Sequence 1' ) result = model . predict_classes ( seq1X , batch_size = n_batch , verbose = 0 ) model . reset_states ( ) for i in range ( len ( result ) ) : print ( 'X=%.1f y=%.1f, yhat=%.1f' % ( seq1 [ i ] , seq1 [ i + 1 ] , result [ i ] ) ) # test LSTM on sequence 2 print ( 'Sequence 2' ) result = model . predict_classes ( seq2X , batch_size = n_batch , verbose = 0 ) model . reset_states ( ) for i in range ( len ( result ) ) : print ( 'X=%.1f y=%.1f, yhat=%.1f' % ( seq2 [ i ] , seq2 [ i + 1 ] , result [ i ] ) )"
55;55;machinelearningmastery.com;https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/;2018-12-04;How to Reduce Overfitting With Dropout Regularization in Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29

# mlp with dropout on the two circles dataset from sklearn . datasets import make_circles from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from matplotlib import pyplot # generate 2d classification dataset X , y = make_circles ( n_samples = 100 , noise = 0.1 , random_state = 1 ) # split into train and test n_train = 30 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] # define model model = Sequential ( ) model . add ( Dense ( 500 , input_dim = 2 , activation = 'relu' ) ) model . add ( Dropout ( 0.4 ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 4000 , verbose = 0 ) # evaluate the model _ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 ) _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) ) # plot history pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' ) pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' ) pyplot . legend ( ) pyplot . show ( )"
56;56;machinelearningmastery.com;http://machinelearningmastery.com/crash-course-recurrent-neural-networks-deep-learning/;2016-07-07;Crash Course in Recurrent Neural Networks for Deep Learning;"Tweet Share Share

Last Updated on August 14, 2019

There is another type of neural network that is dominating difficult machine learning problems that involve sequences of inputs called recurrent neural networks.

Recurrent neural networks have connections that have loops, adding feedback and memory to the networks over time. This memory allows this type of network to learn and generalize across sequences of inputs rather than individual patterns.

A powerful type of Recurrent Neural Network called the Long Short-Term Memory Network has been shown to be particularly effective when stacked into a deep configuration, achieving state-of-the-art results on a diverse array of problems from language translation to automatic captioning of images and videos.

In this post you will get a crash course in recurrent neural networks for deep learning, acquiring just enough understanding to start using LSTM networks in Python with Keras.

After reading this post, you will know:

The limitations of Multilayer Perceptrons that are addressed by recurrent neural networks.

The problems that must be addressed to make Recurrent Neural networks useful.

The details of the Long Short-Term Memory networks used in applied deep learning.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Support For Sequences in Neural Networks

There are some problem types that are best framed involving either a sequence as an input or an output.

For example, consider a univariate time series problem, like the price of a stock over time. This dataset can be framed as a prediction problem for a classical feedforward multilayer Perceptron network by defining a windows size (e.g. 5) and training the network to learn to make short term predictions from the fixed sized window of inputs.

This would work, but is very limited. The window of inputs adds memory to the problem, but is limited to just a fixed number of points and must be chosen with sufficient knowledge of the problem. A naive window would not capture the broader trends over minutes, hours and days that might be relevant to making a prediction. From one prediction to the next, the network only knows about the specific inputs it is provided.

Univariate time series prediction is important, but there are even more interesting problems that involve sequences.

Consider the following taxonomy of sequence problems that require a mapping of an input to an output (taken from Andrej Karpathy).

One-to-Many : sequence output, for image captioning.

: sequence output, for image captioning. Many-to-One : sequence input, for sentiment classification.

: sequence input, for sentiment classification. Many-to-Many : sequence in and out, for machine translation.

: sequence in and out, for machine translation. Synched Many-to-Many: synced sequences in and out, for video classification.

We can also see that a one-to-one example of input to output would be an example of a classical feedforward neural network for a prediction task like image classification.

Support for sequences in neural networks is an important class of problem and one where deep learning has recently shown impressive results State-of-the art results have been using a type of network specifically designed for sequence problems called recurrent neural networks.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Recurrent Neural Networks

Recurrent Neural Networks or RNNs are a special type of neural network designed for sequence problems.

Given a standard feed-forward multilayer Perceptron network, a recurrent neural network can be thought of as the addition of loops to the architecture. For example, in a given layer, each neuron may pass its signal latterly (sideways) in addition to forward to the next layer. The output of the network may feedback as an input to the network with the next input vector. And so on.

The recurrent connections add state or memory to the network and allow it to learn broader abstractions from the input sequences.

The field of recurrent neural networks is well established with popular methods. For the techniques to be effective on real problems, two major issues needed to be resolved for the network to be useful.

How to train the network with Backpropagation. How to stop gradients vanishing or exploding during training.

1. How to Train Recurrent Neural Networks

The staple technique for training feedforward neural networks is to back propagate error and update the network weights.

Backpropagation breaks down in a recurrent neural network, because of the recurrent or loop connections.

This was addressed with a modification of the Backpropagation technique called Backpropagation Throug"
57;57;machinelearningmastery.com;http://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/;2016-09-06;How to Tune the Number and Size of Decision Trees with XGBoost in Python;"# XGBoost on Otto dataset, Tune n_estimators

from pandas import read_csv

from xgboost import XGBClassifier

from sklearn . model_selection import GridSearchCV

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import LabelEncoder

import matplotlib

matplotlib . use ( 'Agg' )

from matplotlib import pyplot

# load data

data = read_csv ( 'train.csv' )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 94 ]

y = dataset [ : , 94 ]

# encode string class values as integers

label_encoded_y = LabelEncoder ( ) . fit_transform ( y )

# grid search

model = XGBClassifier ( )

n_estimators = range ( 50 , 400 , 50 )

param_grid = dict ( n_estimators = n_estimators )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 )

grid_search = GridSearchCV ( model , param_grid , scoring = ""neg_log_loss"" , n_jobs = - 1 , cv = kfold )

grid_result = grid_search . fit ( X , label_encoded_y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :

print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )

# plot

pyplot . errorbar ( n_estimators , means , yerr = stds )

pyplot . title ( ""XGBoost n_estimators vs Log Loss"" )

pyplot . xlabel ( 'n_estimators' )

pyplot . ylabel ( 'Log Loss' )"
58;58;news.mit.edu;http://news.mit.edu/2020/explained-cement-vs-concrete-understanding-differences-and-sustainability-opportunities-0403;;Explained: Cement vs. concrete — their differences, and opportunities for sustainability;"There’s a lot the average person doesn’t know about concrete. For example, it’s porous; it’s the world’s most-used material after water; and, perhaps most fundamentally, it’s not cement.

Though many use ""cement"" and ""concrete"" interchangeably, they actually refer to two different — but related — materials: Concrete is a composite made from several materials, one of which is cement.

Cement production begins with limestone, a sedimentary rock. Once quarried, it is mixed with a silica source, such as industrial byproducts slag or fly ash, and gets fired in a kiln at 2,700 degrees Fahrenheit. What comes out of the kiln is called clinker. Cement plants grind clinker down to an extremely fine powder and mix in a few additives. The final result is cement.

“Cement is then brought to sites where it is mixed with water, where it becomes cement paste,” explains Professor Franz-Josef Ulm, faculty director of the MIT Concrete Sustainability Hub (CSHub). “If you add sand to that paste it becomes mortar. And if you add to the mortar large aggregates — stones of a diameter of up to an inch — it becomes concrete.”

What makes concrete so strong is the chemical reaction that occurs when cement and water mix — a process known as hydration.

“Hydration occurs when cement and water react,” says Ulm. “During hydration, the clinker dissolves into the calcium and recombines with water and silica to form calcium silica hydrates.”

Calcium silica hydrates, or CSH, are the key to cement’s solidity. As they form, they combine, developing tight bonds that lend strength to the material. These connections have a surprising byproduct — they make cement incredibly porous.

Within the spaces between the bonds of CSH, tiny pores develop — on the scale of 3 nanometers. These are known as gel pores. On top of this, any water that hasn’t reacted to form CSH during the hydration process remains in the cement, creating another set of larger pores, called capillary pores.

According to research conducted by CSHub, the French National Center for Scientific Research, and Aix-Marseille University, cement paste is so porous that 96 percent of its pores are connected.

Despite this porosity, cement possesses excellent strength and binding properties. Of course, by decreasing this porosity, one can create a denser and even stronger final product.

Starting in the 1980s, engineers designed a material — high-performance concrete (HPC) — that did just that.

“High-performance concrete developed in the 1980s when people realized that the capillary pores can be reduced in part by reducing the water-to-cement ratio,” says Ulm. “With the addition of certain ingredients as well, this created more CSH and reduced the water that remained after hydration. Essentially, it reduced the larger pores filled with water and increased the strength of the material.”

Of course, notes Ulm, reducing the water-to-cement ratio for HPC also requires more cement. And depending on how that cement is produced, this can increase the material’s environmental impact. This is in part because when calcium carbonate is fired in a kiln to produce conventional cement, a chemical reaction occurs that produces carbon dioxide (CO 2 ).

Another source of cement’s CO 2 emissions come from heating cement kilns. This heating must be done using fossil fuels because of the extremely high temperatures required in the kiln (2,700 F). The electrification of kilns is being studied, but it is currently not technically or economically feasible.

Since concrete is the most popular material in the world and cement is the primary binder used in concrete, these two sources of CO 2 are the main reason that cement contributes around 8 percent of global emissions.

CSHub’s Executive Director Jeremy Gregory, however, sees concrete’s scale as an opportunity to mitigate climate change.

“Concrete is the most-used building material in the world. And because we use so much of it, any reductions we make in its footprint will have a big impact on global emissions.”

Many of the technologies needed to reduce concrete’s footprint exist today, he notes.

“When it comes to reducing the emissions of cement, we can increase the efficiency of cement kilns by increasing our use of waste materials as energy sources rather than fossil fuels,” explains Gregory.

“We can also use blended cements that have less clinker, such as Portland limestone cement, which mixes unheated limestone in the final grinding step of cement production. The last thing we can do is capture and store or utilize the carbon emitted during cement production.”

Carbon capture, utilization, and storage has significant potential to reduce cement and concrete’s environmental impact while creating large market opportunities. According to the Center for Climate and Energy Solutions, carbon utilization in concrete will have a $400 billion global market by 2030. Several companies, like Solidia Technologies and Carbon Cure, are getting ahead of the curve by de"
59;59;machinelearningmastery.com;http://machinelearningmastery.com/use-regression-machine-learning-algorithms-weka/;2016-07-21;How To Use Regression Machine Learning Algorithms in Weka;"Tweet Share Share

Last Updated on August 22, 2019

Weka has a large number of regression algorithms available on the platform.

The large number of machine learning algorithms supported by Weka is one of the biggest benefits of using the platform.

In this post you will discover how to use top regression machine learning algorithms in Weka.

After reading this post you will know:

About 5 top regression algorithms supported by Weka.

How to use regression machine learning algorithms for predictive modeling in Weka.

About the key configuration options of regression algorithms in Weka.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Regression Algorithms Overview

We are going to take a tour of 5 top regression algorithms in Weka.

Each algorithm that we cover will be briefly described in terms of how it works, key algorithm parameters will be highlighted and the algorithm will be demonstrated in the Weka Explorer interface.

The 5 algorithms that we will review are:

Linear Regression k-Nearest Neighbors Decision Tree Support Vector Machines Multi-Layer Perceptron

These are 5 algorithms that you can try on your regression problem as a starting point.

A standard machine learning regression problem will be used to demonstrate each algorithm.

Specifically, the Boston House Price Dataset. Each instance describes the properties of a Boston suburb and the task is to predict the house prices in thousands of dollars. There are 13 numerical input variables with varying scales describing the properties of suburbs. You can learn more about this dataset on the UCI Machine Learning Repository.

Start the Weka Explorer:

Open the Weka GUI Chooser. Click the “Explorer” button to open the Weka Explorer. Load the Boston house price dataset from the housing.arff file. Click “Classify” to open the Classify tab.

Let’s start things off by looking at the linear regression algorithm.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Linear Regression

Linear regression only supports regression type problems.

It works by estimating coefficients for a line or hyperplane that best fits the training data. It is a very simple regression algorithm, fast to train and can have great performance if the output variable for your data is a linear combination of your inputs.

It is good idea to evaluate linear regression on your problem before moving onto more complex algorithms in case it performs well.

Choose the linear regression algorithm:

Click the “Choose” button and select “LinearRegression” under the “functions” group. Click on the name of the algorithm to review the algorithm configuration.

The performance of linear regression can be reduced if your training data has input attributes that are highly correlated. Weka can detect and remove highly correlated input attributes automatically by setting eliminateColinearAttributes to True, which is the default.

Additionally, attributes that are unrelated to the output variable can also negatively impact performance. Weka can automatically perform feature selection to only select those relevant attributes by setting the attributeSelectionMethod. This is enabled by default and can be disabled.

Finally, the Weka implementation uses a ridge regularization technique in order to reduce the complexity of the learned model. It does this by minimizing the square of the absolute sum of the learned coefficients, which will prevent any specific coefficient from becoming too large (a sign of complexity in regression models).

Click “OK” to close the algorithm configuration. Click the “Start” button to run the algorithm on the Boston house price dataset.

You can see that with the default configuration that linear regression achieves an RMSE of 4.9.

k-Nearest Neighbors

The k-nearest neighbors algorithm supports both classification and regression. It is also called kNN for short. It works by storing the entire training dataset and querying it to locate the k most similar training patterns when making a prediction.

As such, there is no model other than the raw training dataset and the only computation performed is the querying of the training dataset when a prediction is requested.

It is a simple algorithm, but one that does not assume very much about the problem other than that the distance between data instances is meaningful in making predictions. As such, it often achieves very good performance.

When making predictions on regression problems, KNN will take the mean of the k most similar instances in the training dataset. Choose the KNN algorithm:

Click the “Choose” button and select “IBk” under the “lazy” group. Click on the name of the algorithm to"
60;60;machinelearningmastery.com;http://machinelearningmastery.com/introduction-python-deep-learning-library-tensorflow/;2016-05-04;Introduction to the Python Deep Learning Library TensorFlow;"import tensorflow as tf

import numpy as np

# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3

x_data = np . random . rand ( 100 ) . astype ( np . float32 )

y_data = x_data * 0.1 + 0.3

# Try to find values for W and b that compute y_data = W * x_data + b

# (We know that W should be 0.1 and b 0.3, but Tensorflow will

# figure that out for us.)

W = tf . Variable ( tf . random_uniform ( [ 1 ] , - 1.0 , 1.0 ) )

b = tf . Variable ( tf . zeros ( [ 1 ] ) )

y = W * x_data + b

# Minimize the mean squared errors.

loss = tf . reduce_mean ( tf . square ( y - y_data ) )

optimizer = tf . train . GradientDescentOptimizer ( 0.5 )

train = optimizer . minimize ( loss )

# Before starting, initialize the variables. We will 'run' this first.

init = tf . initialize_all_variables ( )

# Launch the graph.

sess = tf . Session ( )

sess . run ( init )

# Fit the line.

for step in xrange ( 201 ) :

sess . run ( train )

if step % 20 == 0 :

print ( step , sess . run ( W ) , sess . run ( b ) )"
61;61;machinelearningmastery.com;https://machinelearningmastery.com/why-training-a-neural-network-is-hard/;2019-02-28;Why Training a Neural Network Is Hard;"Tweet Share Share

Last Updated on August 6, 2019

Or, Why Stochastic Gradient Descent Is Used to Train Neural Networks.

Fitting a neural network involves using a training dataset to update the model weights to create a good mapping of inputs to outputs.

This training process is solved using an optimization algorithm that searches through a space of possible values for the neural network model weights for a set of weights that results in good performance on the training dataset.

In this post, you will discover the challenge of training a neural network framed as an optimization problem.

After reading this post, you will know:

Training a neural network involves using an optimization algorithm to find a set of weights to best map inputs to outputs.

The problem is hard, not least because the error surface is non-convex and contains local minima, flat spots, and is highly multidimensional.

The stochastic gradient descent algorithm is the best general algorithm to address this challenging problem.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

Learning as Optimization Challenging Optimization Features of the Error Surface Implications for Training

Learning as Optimization

Deep learning neural network models learn to map inputs to outputs given a training dataset of examples.

The training process involves finding a set of weights in the network that proves to be good, or good enough, at solving the specific problem.

This training process is iterative, meaning that it progresses step by step with small updates to the model weights each iteration and, in turn, a change in the performance of the model each iteration.

The iterative training process of neural networks solves an optimization problem that finds for parameters (model weights) that result in a minimum error or loss when evaluating the examples in the training dataset.

Optimization is a directed search procedure and the optimization problem that we wish to solve when training a neural network model is very challenging.

This raises the question as to what exactly is so challenging about this optimization problem?

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Challenging Optimization

Training deep learning neural networks is very challenging.

The best general algorithm known for solving this problem is stochastic gradient descent, where model weights are updated each iteration using the backpropagation of error algorithm.

Optimization in general is an extremely difficult task. […] When training neural networks, we must confront the general non-convex case.

— Page 282, Deep Learning, 2016.

An optimization process can be understood conceptually as a search through a landscape for a candidate solution that is sufficiently satisfactory.

A point on the landscape is a specific set of weights for the model, and the elevation of that point is an evaluation of the set of weights, where valleys represent good models with small values of loss.

This is a common conceptualization of optimization problems and the landscape is referred to as an “error surface.”

In general, E(w) [the error function of the weights] is a multidimensional function and impossible to visualize. If it could be plotted as a function of w [the weights], however, E [the error function] might look like a landscape with hills and valleys …

— Page 113, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

The optimization algorithm iteratively steps across this landscape, updating the weights and seeking out good or low elevation areas.

For simple optimization problems, the shape of the landscape is a big bowl and finding the bottom is easy, so easy that very efficient algorithms can be designed to find the best solution.

These types of optimization problems are referred to mathematically as convex.

The error surface we wish to navigate when optimizing the weights of a neural network is not a bowl shape. It is a landscape with many hills and valleys.

These type of optimization problems are referred to mathematically as non-convex.

In fact, there does not exist an algorithm to solve the problem of finding an optimal set of weights for a neural network in polynomial time. Mathematically, the optimization problem solved by training a neural network is referred to as NP-complete (e.g. they are very hard to solve).

We prove this problem NP-complete and thus demonstrate that learning in neural networks has no efficient general solution.

— Neural Network Design and the Complexity of Learning, 1988.

Key Features of the Error Surface

There are many types of non-convex optimization pr"
62;62;machinelearningmastery.com;http://machinelearningmastery.com/process-for-working-through-machine-learning-problems/;2014-02-11;Applied Machine Learning Process;"Tweet Share Share

Last Updated on July 5, 2019

The Systematic Process For Working Through Predictive Modeling Problems

That Delivers Above Average Results

Over time, working on applied machine learning problems you develop a pattern or process for quickly getting to good robust results.

Once developed, you can use this process again and again on project after project. The more robust and developed your process, the faster you can get to reliable results.

In this post, I want to share with you the skeleton of my process for working a machine learning problem.

You can use this as a starting point or template on your next project.

5-Step Systematic Process

I liked to use a 5-step process:

Define the Problem Prepare Data Spot Check Algorithms Improve Results Present Results

There is a lot of flexibility in this process. For example, the “prepare data” step is typically broken down into analyze data (summarize and graph) and prepare data (prepare samples for experiments). The “Spot Checks” step may involve multiple formal experiments.

It’s a great big production line that I try to move through in a linear manner. The great thing in using automated tools is that you can go back a few steps (say from “Improve Results” back to “Prepare Data”) and insert a new transform of the dataset and re-run experiments in the intervening steps to see what interesting results come out and how they compare to the experiments you executed before.

The process I use has been adapted from the standard data mining process of knowledge discovery in databases (or KDD), See the post What is Data Mining and KDD for more details.

1. Define the Problem

I like to use a three step process to define the problem. I like to move quickly and I use this mini process to see the problem from a few different perspectives very quickly:

Step 1: What is the problem? Describe the problem informally and formally and list assumptions and similar problems.

Describe the problem informally and formally and list assumptions and similar problems. Step 2: Why does the problem need to be solved? List your motivation for solving the problem, the benefits a solution provides and how the solution will be used.

List your motivation for solving the problem, the benefits a solution provides and how the solution will be used. Step 3: How would I solve the problem? Describe how the problem would be solved manually to flush domain knowledge.

You can learn more about this process in the post:

2. Prepare Data

I preface data preparation with a data analysis phase that involves summarizing the attributes and visualizing them using scatter plots and histograms. I also like to describe in detail each attribute and relationships between attributes. This grunt work forces me to think about the data in the context of the problem before it is lost to the algorithms

The actual data preparation process is three step as follows:

Step 1: Data Selection : Consider what data is available, what data is missing and what data can be removed.

: Consider what data is available, what data is missing and what data can be removed. Step 2: Data Preprocessing : Organize your selected data by formatting, cleaning and sampling from it.

: Organize your selected data by formatting, cleaning and sampling from it. Step 3: Data Transformation: Transform preprocessed data ready for machine learning by engineering features using scaling, attribute decomposition and attribute aggregation.

You can learn more about this process for preparing data in the post:

3. Spot Check Algorithms

I use 10 fold cross validation in my test harnesses by default. All experiments (algorithm and dataset combinations) are repeated 10 times and the mean and standard deviation of the accuracy is collected and reported. I also use statistical significance tests to flush out meaningful results from noise. Box-plots are very useful for summarizing the distribution of accuracy results for each algorithm and dataset pair.

I spot check algorithms, which means loading up a bunch of standard machine learning algorithms into my test harness and performing a formal experiment. I typically run 10-20 standard algorithms from all the major algorithm families across all the transformed and scaled versions of the dataset I have prepared.

The goal of spot checking is to flush out the types of algorithms and dataset combinations that are good at picking out the structure of the problem so that they can be studied in more detail with focused experiments.

More focused experiments with well-performing families of algorithms may be performed in this step, but algorithm tuning is left for the next step.

You can discover more about defining your test harness in the post:

You can discover the importance of spot checking algorithms in the post:

4. Improve Results

After spot checking, it’s time to squeeze out the best result from the rig. I do this by running an automated sensitivity analysis on the parameters of the top"
63;63;machinelearningmastery.com;https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/;2019-04-16;How Do Convolutional Layers Work in Deep Learning Neural Networks?;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32

# example of calculation 2d convolutions from numpy import asarray from keras . models import Sequential from keras . layers import Conv2D # define input data data = [ [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ] data = asarray ( data ) data = data . reshape ( 1 , 8 , 8 , 1 ) # create model model = Sequential ( ) model . add ( Conv2D ( 1 , ( 3 , 3 ) , input_shape = ( 8 , 8 , 1 ) ) ) # define a vertical line detector detector = [ [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] , [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] , [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ] weights = [ asarray ( detector ) , asarray ( [ 0.0 ] ) ] # store the weights in the model model . set_weights ( weights ) # confirm they were stored print ( model . get_weights ( ) ) # apply filter to input data yhat = model . predict ( data ) for r in range ( yhat . shape [ 1 ] ) : # print each column in the row print ( [ yhat [ 0 , r , c , 0 ] for c in range ( yhat . shape [ 2 ] ) ] )"
64;64;machinelearningmastery.com;https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/;2019-04-21;A Gentle Introduction to Pooling Layers for Convolutional Neural Networks;"# example of vertical line detection with a convolutional layer

from numpy import asarray

from keras . models import Sequential

from keras . layers import Conv2D

# define input data

data = [ [ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ,

[ 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 ] ]

data = asarray ( data )

data = data . reshape ( 1 , 8 , 8 , 1 )

# create model

model = Sequential ( )

model . add ( Conv2D ( 1 , ( 3 , 3 ) , activation = 'relu' , input_shape = ( 8 , 8 , 1 ) ) )

# summarize model

model . summary ( )

# define a vertical line detector

detector = [ [ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ,

[ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ,

[ [ [ 0 ] ] , [ [ 1 ] ] , [ [ 0 ] ] ] ]

weights = [ asarray ( detector ) , asarray ( [ 0.0 ] ) ]

# store the weights in the model

model . set_weights ( weights )

# apply filter to input data

yhat = model . predict ( data )

# enumerate rows

for r in range ( yhat . shape [ 1 ] ) :

# print each column in the row"
65;65;news.mit.edu;http://news.mit.edu/2020/mit-christine-soh-integrates-computer-science-and-linguistics-0305;;MIT senior Christine Soh integrates computer science and linguistics;"Christine Soh fell in love with MIT the summer before her senior year of high school while attending the Women’s Technology Program run by MIT’s Department of Electrical Engineering and Computer Science. That’s when she discovered that learning to program in Python is just like learning a new language — and Soh loves languages.

Growing up in Colorado, Soh spoke both English and Korean; she learned French and Latin in school. This June, Soh will graduate from MIT, where she has happily combined her passions by majoring in computer science and engineering (Course 6-3) and linguistics (Course 24). She plans to begin working toward a PhD in linguistics next year.

With fluency in both technical and humanistic modes of thinking, Soh exemplifies a ""bilingual"" perspective. ""Dual competence is a good model for undergraduates at MIT,"" says engineer/historian David Mindell, who encourages MIT students to ""master two fundamental ways of thinking about the world, one technical and one humanistic or social. Sometimes these two modes will be at odds with each other, which raises critical questions. Other times they will be synergistic and energizing.""



The challenge of natural language and computation

“The really cool thing about language is that it’s universal,” says Soh, who has added ancient Greek, Chinese, and the programming language Java to her credits since that summer. “I can have a really interesting conversation with anybody, even if they don’t have a linguistics background, because everyone has experience with language.”

That said, natural language is difficult for computers to comprehend — something Soh finds fascinating. “It’s really interesting to think about how we understand language,” she says. “How is it that computers have such a hard time understanding what we find so easy?”

Tools from computational linguistics to improve speech

Pairing linguistics with computer science has allowed Soh to explore cutting-edge research combining the two disciplines. Thanks to MIT’s Advanced Undergraduate Research Opportunities Program, Soh got the chance to explore whether speech analysis software can be used as a tool for the clinical diagnosis of speech impairments.

“It’s very difficult to correctly diagnose a child because a speech impairment can be caused by a ton of different things,” says Soh. Working with the Speech Communication Group in MIT’s Research Laboratory of Electronics, Soh has been developing a tool that can listen to a child’s speech and extract linguistic information, such where in the mouth the sound was produced, thus identifying modifications from the proper formation of the word. “We can then use computational techniques to see if there are patterns to the modifications that have been made and see if these patterns can distinguish one underlying condition from another.”

A natural leader

Even if the team isn’t able to find such patterns, Soh says the tool could be used by speech pathologists to learn more about what linguistic modifications a child might need to make to improve speech. In December, Soh presented a poster on this work at the annual meeting of the Acoustical Society of America and was honored with a first-place prize in her category (signal processing in acoustics).

Exploring such real-world applications for computational linguistics helped inspire Soh to apply to doctoral programs in linguistics for next year. “I’ll be doing research that will be integrating computer science and linguistics,” she says, noting that possible applications of computational linguistics include working to improve speech-recognition software or to make machine-produced speech sound more natural. “I look forward to using the knowledge and skills I’ve learned at MIT in doing that research.”

“Christine’s unique interests, energy, and deep interests in both linguistics and computer science should enable her to accomplish great things,” says Suzanne Flynn, a professor of linguistics who has had Soh as a student. “She is a natural leader.”



From field methods to neurolinguistics

Looking back at her time at MIT, Soh recalls particularly enjoying two linguistics classes: 24.909 (Field Methods in Linguistics) which explores the structure of an unfamiliar language through direct work with a native speaker (in Soh’s year, the class centered on Wolof, which is spoken in Senegal, the Gambia, and Mauritania), and 24.906 (The Linguistic Study of Bilingualism).

In the latter class, Soh says, “We looked at neurolinguistics, what’s happening in the brain as the bilingual brain developed. We looked at topics in sociolinguistics: In communities that are bilingual, like Quebec, what kind of impact does it have on society, such as how schools are run? … We got to see a spectrum of linguistics. It was really cool.”

Building community at MIT

Outside class, Soh says she found community at MIT through the Asian Christian Fellowship and the Society of Women Engineers (SWE), which she served last year as vice pre"
66;66;news.mit.edu;http://news.mit.edu/press/for-journalists;;For Journalists;"MIT is home to extraordinary work in science, technology, and other fields of inquiry, and to the world-class faculty and students who make that true. Those of us charged with sharing the Institute's news with the media hope that we can be your guides, whether you are interested in pursuing a specific research advance, or simply getting to know the Institute as a whole.

MIT News is the companion website to the MIT Media Relations website (which is where you are right now). It features articles about MIT teaching, research and innovation written by journalists on staff at the MIT News Office. The website was launched in September 2009, and it currently serves an international audience of about 400,000 monthly unique visitors.

To receive press releases and other MIT news

The MIT News Office's Media Relations team usually distributes several press releases tied into research advances each week, along with other MIT news and announcements. Please sign up here to receive press releases on a regular basis, and let us know what topics interest you.

Events

In any given week there are a variety of talks, symposia, and other more eclectic events happening on the MIT campus. We highlight some of these on our website, and will also notify you if we think there's something going on here that you'd want to know about. MIT also maintains an extensive events calendar.

Experts

Many of MIT's 1,000+ faculty members and researchers are available to comment on their research or other related topics. See Experts Guide.

Film crews

Film and documentary crews are welcome to film on the MIT campus, provided the focus of their work is MIT-related content and they adhere to certain guidelines.

General MIT background

See the MIT Facts page for general information about MIT, including enrollment numbers, admissions statistics, and a history of the Institute. Click here for a listing of MIT research areas by topic.

MIT studio (ISDN)

MIT Audio-Visual Services operates a studio in Building 46 equipped with an ISDN line for broadcast radio. For pricing and availability, producers should email avorders@mit.edu or call 617-253-2808.

MIT studio (satellite uplink)

MIT Video Productions (MVP) operates a studio in Building 24 that allows for MIT community members to be connected live to news outlets. The studio offers a number of visual backgrounds, including images that place the interviewee at MIT. This service is $600/hour with a one-hour minimum. ($750/hour during off hours) To arrange to use the studio, please contact MVP.

Photography

We can usually provide journalists with high-resolution images to illustrate research stories. These are available for download off of individual press releases, or by request. We also have an archive of thousands of additional images, so if you cannot find what you are looking for, please contact us."
67;67;news.mit.edu;http://news.mit.edu/2016/forbes-30-under-30-lists-0107;;25 from MIT named to Forbes 30 Under 30 lists in 2016;"According to Forbes magazine, their fifth annual 30 Under 30 lists showcase “America’s most important young entrepreneurs, creative leaders and brightest stars” who are less than than 30 years old. Twenty-five MIT students, researchers, and alumni made the 2016 lists.

Some 600 selections covering 20 categories were whittled down from an initial screening of more than 15,000 nominations. Following are the MIT affiliates who were selected.

Peter Bailis (Enterprise Tech)

Postdoc in the MIT Computer Science and Artificial Intelligence Laboratory

""26-year-old Bailis finished his PhD in computer science at Berkeley and accepted a tenure-track assistant professor position in Stanford's computer science department, starting in fall 2016. He wrote his PhD thesis on large-scale data management and is a recipient of a National Science Foundation Graduate Research Fellowship.""

Sampriti Bhattacharyya (Manufacturing and Industry)

Grad student in the Department of Mechanical Engineering and founder of Hydroswarm

“Bhattacharyya has developed underwater drones that are capable of scanning the ocean for lost planes, or measure oil spills or radiation under the sea.”

Jonathan Birnbaum ’08 (Finance)

Vice president at Morgan Stanley

“Chief operating officer of bank’s U.S. credit trading group, managing 100 bankers trading investment grade, high yield and distressed debt.”

Natalya Brikner PhD ’15, Louis Perna ’09, SM ’14 (Manufacturing and Industry)

Co-founders of Accion Systems

“Accion is working to commercialize ion propulsion technology for small satellites using a liquid ionic propellant that is non-toxic and non-explosive.”

Arnav Chhabra (Science)

Grad student with Harvard-MIT Health Sciences and Technology

“Chhabra published his first paper in high school. He's now pursuing his PhD, and his most recent thesis is focused on building a “liver on a chip” — a miniaturized liver model scientists hope could one day replace the use of animals for disease research.”

Abe Davis (Science)

Grad student in the Department of Electrical Engineering and Computer Science

“Davis is best known for the subject of his TED Talk, in which he showed that he could capture information from video based on vibrations in the room.”

Adam Elmachtoub PhD ’14 (Science)

Assistant professor at Columbia University

“Elmachtoub’s research is focused on using data science and optimizing algorithms to make businesses more efficient.”

Teasha Feldman-Fitzthum ’14 (Energy)

Cofounder of EverVest

“EverVest provides advanced software for analyzing, valuing, and financing renewable energy projects.”

Brian Fiske PhD ’15 (Healthcare)

Senior associate at Flagship Ventures

“This MIT PhD has put this technique to work making genetic alterations to cells to find proteins that can be hit with new drugs.”

Michael Grinich ’11, Christine Spang ’10 (Enterprise Tech)

Co-founders of Nylas

“While studying computer science and physics at MIT, Grinich wrote his thesis on the fundamental tools for syncing email. He founded Nylas (formerly known as Inbox), with fellow MIT alumna Spang in 2013 … Spang, who wrote the core mail synchronization engine, runs its platform team, while Grinich is CEO.”

Ben Harvatine ’12 (Healthcare)

Founder of Jolt

“Harvatine aims to catch concussions as they happen. His sensor can be attached to anything on the head — headband, helmet, hair clip — and sends feedback to smartphones in real time.”

Noel Hollingsworth ’13, MEng ’14 (Sports)

Director of data at Second Spectrum

“Hollingsworth won the 2014 Best Research Award at the MIT Sloan Sports Analytics Conference. Almost all NBA championship contenders use his work to gain an edge.”

Steven Keating SM ’12 (Healthcare)

Grad student in the department of Mechanical Engineering

“Keating found out that he had brain cancer after volunteering for an MRI experiment … the MIT researcher wants to know: why can my doctors see my tumor genome and not me?”

Andrew Leone ’09 (Finance)

Vice president at Nomura

“Heads one of Wall Street’s biggest VIX and structured volatility market making desks.”

Andrej Lenert SM ’10, PhD ’14 (Science)

Postdoc at the University of Michigan

“One avenue of his research was to develop a hybrid solar power system, combining the best of photovoltaic and solar thermal systems without their drawbacks.”

Maxim Lobovsky SM ’11 (Manufacturing and Industry)

Co-founder of Formlabs

“Formlabs’ printers are designed to allow for more precise parts to be created for more complicated 3-D printing projects.”

Carl Schoellhammer PhD ’15 (Healthcare)

CEO of Suono Bio

“Schoellhammer, a student of MIT professors Daniel Blankschtein and Robert Langer, invented a pill that can inject drugs directly into the gastrointestinal tract. The gadget helped him win the prestigious Lemelson-MIT Student Prize.”

Harbaljit Sohal (Science)

Postdoc at the McGovern Institute for Brain Research at MIT

""Sohal's research focus is on building neural implants for the treatment of brai"
68;68;machinelearningmastery.com;https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/;2020-03-31;Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost;"# gradient boosting for classification in scikit-learn

from numpy import mean

from numpy import std

from sklearn . datasets import make_classification

from sklearn . ensemble import GradientBoostingClassifier

from sklearn . model_selection import cross_val_score

from sklearn . model_selection import RepeatedStratifiedKFold

from matplotlib import pyplot

# define dataset

X , y = make_classification ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_redundant = 5 , random_state = 1 )

# evaluate the model

model = GradientBoostingClassifier ( )

cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 )

n_scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' )

print ( 'Accuracy: %.3f (%.3f)' % ( mean ( n_scores ) , std ( n_scores ) ) )

# fit the model on the whole dataset

model = GradientBoostingClassifier ( )

model . fit ( X , y )

# make a single prediction

row = [ [ 2.56999479 , - 0.13019997 , 3.16075093 , - 4.35936352 , - 1.61271951 , - 1.39352057 , - 2.48924933 , - 1.93094078 , 3.26130366 , 2.05692145 ] ]

yhat = model . predict ( row )"
69;69;machinelearningmastery.com;https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/;2018-02-25;How to Calculate the SVD from Scratch with Python;"from numpy import array

from numpy import diag

from numpy import zeros

from scipy . linalg import svd

# define a matrix

A = array ( [

[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] ,

[ 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 ] ,

[ 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 ] ] )

print ( A )

# Singular-value decomposition

U , s , VT = svd ( A )

# create m x n Sigma matrix

Sigma = zeros ( ( A . shape [ 0 ] , A . shape [ 1 ] ) )

# populate Sigma with n x n diagonal matrix

Sigma [ : A . shape [ 0 ] , : A . shape [ 0 ] ] = diag ( s )

# select

n_elements = 2

Sigma = Sigma [ : , : n_elements ]

VT = VT [ : n_elements , : ]

# reconstruct

B = U . dot ( Sigma . dot ( VT ) )

print ( B )

# transform

T = U . dot ( Sigma )

print ( T )

T = A . dot ( VT . T )"
70;70;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-machine-learning-models-for-multivariate-multi-step-air-pollution-time-series-forecasting/;2018-10-18;How to Develop Multivariate Multi-Step Time Series Forecasting Models for Air Pollution;"# spot check nonlinear algorithms

from numpy import load

from numpy import loadtxt

from numpy import nan

from numpy import isnan

from numpy import count_nonzero

from numpy import unique

from numpy import array

from sklearn . base import clone

from sklearn . neighbors import KNeighborsRegressor

from sklearn . tree import DecisionTreeRegressor

from sklearn . tree import ExtraTreeRegressor

from sklearn . svm import SVR

from sklearn . ensemble import AdaBoostRegressor

from sklearn . ensemble import BaggingRegressor

from sklearn . ensemble import RandomForestRegressor

from sklearn . ensemble import ExtraTreesRegressor

from sklearn . ensemble import GradientBoostingRegressor

# split the dataset by 'chunkID', return a list of chunks

def to_chunks ( values , chunk_ix = 0 ) :

chunks = list ( )

# get the unique chunk ids

chunk_ids = unique ( values [ : , chunk_ix ] )

# group rows by chunk id

for chunk_id in chunk_ids :

selection = values [ : , chunk_ix ] == chunk_id

chunks . append ( values [ selection , : ] )

return chunks

# return true if the array has any non-nan values

def has_data ( data ) :

return count_nonzero ( isnan ( data ) ) < len ( data )

# return a list of relative forecast lead times

def get_lead_times ( ) :

return [ 1 , 2 , 3 , 4 , 5 , 10 , 17 , 24 , 48 , 72 ]

# fit a single model

def fit_model ( model , X , y ) :

# clone the model configuration

local_model = clone ( model )

# fit the model

local_model . fit ( X , y )

return local_model

# fit one model for each variable and each forecast lead time [var][time][model]

def fit_models ( model , train ) :

# prepare structure for saving models

models = [ [ list ( ) for _ in range ( train . shape [ 1 ] ) ] for _ in range ( train . shape [ 0 ] ) ]

# enumerate vars

for i in range ( train . shape [ 0 ] ) :

# enumerate lead times

for j in range ( train . shape [ 1 ] ) :

# get data

data = train [ i , j ]

X , y = data [ : , : - 1 ] , data [ : , - 1 ]

# fit model

local_model = fit_model ( model , X , y )

models [ i ] [ j ] . append ( local_model )

return models

# return forecasts as [chunks][var][time]

def make_predictions ( models , test ) :

lead_times = get_lead_times ( )

predictions = list ( )

# enumerate chunks

for i in range ( test . shape [ 0 ] ) :

# enumerate variables

chunk_predictions = list ( )

for j in range ( test . shape [ 1 ] ) :

# get the input pattern for this chunk and target

pattern = test [ i , j ]

# assume a nan forecast

forecasts = array ( [ nan for _ in range ( len ( lead_times ) ) ] )

# check we can make a forecast

if has_data ( pattern ) :

pattern = pattern . reshape ( ( 1 , len ( pattern ) ) )

# forecast each lead time

forecasts = list ( )

for k in range ( len ( lead_times ) ) :

yhat = models [ j ] [ k ] [ 0 ] . predict ( pattern )

forecasts . append ( yhat [ 0 ] )

forecasts = array ( forecasts )

# save forecasts for each lead time for this variable

chunk_predictions . append ( forecasts )

# save forecasts for this chunk

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# convert the test dataset in chunks to [chunk][variable][time] format

def prepare_test_forecasts ( test_chunks ) :

predictions = list ( )

# enumerate chunks to forecast

for rows in test_chunks :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 3 , rows . shape [ 1 ] ) :

yhat = rows [ : , j ]

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# calculate the error between an actual and predicted value

def calculate_error ( actual , predicted ) :

# give the full actual value if predicted is nan

if isnan ( predicted ) :

return abs ( actual )

# calculate abs difference

return abs ( actual - predicted )

# evaluate a forecast in the format [chunk][variable][time]

def evaluate_forecasts ( predictions , testset ) :

lead_times = get_lead_times ( )

total_mae , times_mae = 0.0 , [ 0.0 for _ in range ( len ( lead_times ) ) ]

total_c , times_c = 0 , [ 0 for _ in range ( len ( lead_times ) ) ]

# enumerate test chunks

for i in range ( len ( test_chunks ) ) :

# convert to forecasts

actual = testset [ i ]

predicted = predictions [ i ]

# enumerate target variables

for j in range ( predicted . shape [ 0 ] ) :

# enumerate lead times

for k in range ( len ( lead_times ) ) :

# skip if actual in nan

if isnan ( actual [ j , k ] ) :

continue

# calculate error

error = calculate_error ( actual [ j , k ] , predicted [ j , k ] )

# update statistics

total_mae += error

times_mae [ k ] += error

total_c += 1

times_c [ k ] += 1

# normalize summed absolute errors

total_mae /= total_c

times_mae = [ times_mae [ i ] / times_c [ i ] for i in range ( len ( times_mae ) ) ]

return total_mae , times_mae

# summarize scores

def summarize_error ( name , total_mae ) :

print ( '%s: %.3f MAE' "
71;71;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-convolutional-neural-networks-for-multi-step-time-series-forecasting/;2018-10-07;How to Develop Convolutional Neural Networks for Multi-Step Time Series Forecasting;"# multi headed multi-step cnn

from math import sqrt

from numpy import split

from numpy import array

from pandas import read_csv

from sklearn . metrics import mean_squared_error

from matplotlib import pyplot

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers . convolutional import Conv1D

from keras . layers . convolutional import MaxPooling1D

from keras . models import Model

from keras . layers import Input

from keras . layers . merge import concatenate

# split a univariate dataset into train/test sets

def split_dataset ( data ) :

# split into standard weeks

train , test = data [ 1 : - 328 ] , data [ - 328 : - 6 ]

# restructure into windows of weekly data

train = array ( split ( train , len ( train ) / 7 ) )

test = array ( split ( test , len ( test ) / 7 ) )

return train , test

# evaluate one or more weekly forecasts against expected values

def evaluate_forecasts ( actual , predicted ) :

scores = list ( )

# calculate an RMSE score for each day

for i in range ( actual . shape [ 1 ] ) :

# calculate mse

mse = mean_squared_error ( actual [ : , i ] , predicted [ : , i ] )

# calculate rmse

rmse = sqrt ( mse )

# store

scores . append ( rmse )

# calculate overall RMSE

s = 0

for row in range ( actual . shape [ 0 ] ) :

for col in range ( actual . shape [ 1 ] ) :

s += ( actual [ row , col ] - predicted [ row , col ] ) * * 2

score = sqrt ( s / ( actual . shape [ 0 ] * actual . shape [ 1 ] ) )

return score , scores

# summarize scores

def summarize_scores ( name , score , scores ) :

s_scores = ', ' . join ( [ '%.1f' % s for s in scores ] )

print ( '%s: [%.3f] %s' % ( name , score , s_scores ) )

# convert history into inputs and outputs

def to_supervised ( train , n_input , n_out = 7 ) :

# flatten data

data = train . reshape ( ( train . shape [ 0 ] * train . shape [ 1 ] , train . shape [ 2 ] ) )

X , y = list ( ) , list ( )

in_start = 0

# step over the entire history one time step at a time

for _ in range ( len ( data ) ) :

# define the end of the input sequence

in_end = in_start + n_input

out_end = in_end + n_out

# ensure we have enough data for this instance

if out_end <= len ( data ) :

X . append ( data [ in_start : in_end , : ] )

y . append ( data [ in_end : out_end , 0 ] )

# move along one time step

in_start += 1

return array ( X ) , array ( y )

# plot training history

def plot_history ( history ) :

# plot loss

pyplot . subplot ( 2 , 1 , 1 )

pyplot . plot ( history . history [ 'loss' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' )

pyplot . title ( 'loss' , y = 0 , loc = 'center' )

pyplot . legend ( )

# plot rmse

pyplot . subplot ( 2 , 1 , 2 )

pyplot . plot ( history . history [ 'rmse' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_rmse' ] , label = 'test' )

pyplot . title ( 'rmse' , y = 0 , loc = 'center' )

pyplot . legend ( )

pyplot . show ( )

# train the model

def build_model ( train , n_input ) :

# prepare data

train_x , train_y = to_supervised ( train , n_input )

# define parameters

verbose , epochs , batch_size = 0 , 25 , 16

n_timesteps , n_features , n_outputs = train_x . shape [ 1 ] , train_x . shape [ 2 ] , train_y . shape [ 1 ]

# create a channel for each variable

in_layers , out_layers = list ( ) , list ( )

for i in range ( n_features ) :

inputs = Input ( shape = ( n_timesteps , 1 ) )

conv1 = Conv1D ( filters = 32 , kernel_size = 3 , activation = 'relu' ) ( inputs )

conv2 = Conv1D ( filters = 32 , kernel_size = 3 , activation = 'relu' ) ( conv1 )

pool1 = MaxPooling1D ( pool_size = 2 ) ( conv2 )

flat = Flatten ( ) ( pool1 )

# store layers

in_layers . append ( inputs )

out_layers . append ( flat )

# merge heads

merged = concatenate ( out_layers )

# interpretation

dense1 = Dense ( 200 , activation = 'relu' ) ( merged )

dense2 = Dense ( 100 , activation = 'relu' ) ( dense1 )

outputs = Dense ( n_outputs ) ( dense2 )

model = Model ( inputs = in_layers , outputs = outputs )

# compile model

model . compile ( loss = 'mse' , optimizer = 'adam' )

# fit network

input_data = [ train_x [ : , : , i ] . reshape ( ( train_x . shape [ 0 ] , n_timesteps , 1 ) ) for i in range ( n_features ) ]

model . fit ( input_data , train_y , epochs = epochs , batch_size = batch_size , verbose = verbose )

return model

# make a forecast

def forecast ( model , history , n_input ) :

# flatten data

data = array ( history )

data = data . reshape ( ( data . shape [ 0 ] * data . shape [ 1 ] , data . shape [ 2 ] ) )

# retrieve last observations for input data

input_x = data [ - n_input : , : ]

# reshape into n input arrays

input_x = [ input_x [ : , i ] . reshape ( ( 1 , input_x . shape [ 0 ] , 1 ) ) for i in range ( input_x . shape [ 1 ] ) ]

# forecast the next week

yhat = model . predict ( input_x , verbose = 0 )

# we only want the vector forecast

yhat = yhat [ 0 ]

return yhat

# evaluate a single"
72;72;machinelearningmastery.com;https://machinelearningmastery.com/how-to-learn-a-machine-learning-algorithm/;2014-01-10;How to Learn a Machine Learning Algorithm;"Tweet Share Share

Last Updated on August 12, 2019

The question of how to learn a machine learning algorithm has come up a few times on the email list.

In this post I’ll share with you the strategy I have been using for years to learn and build up a structured description of an algorithm in a step-by-step manner that I can add to, refine and refer back to again and again. I even used it to write a book.

This was just a strategy I used personally and I’ve been really surprised by the positive feedback.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Algorithm Descriptions are Broken

Learning a machine learning algorithm can be overwhelming. There are so many papers, books and websites describing how the algorithm works mathematically and textually. If you are really lucky you might find a pseudocode description of the algorithm.

If you are really really lucky you might find some suggested ways to configure the method for different situations. These descriptions are rare and typically buried deep in the original publication or in technical notes by the original authors.

A fact you learn quickly when you want to implement a method from research papers is that algorithms are almost never described in sufficient detail for you to reproduce them. The reasons vary, from the micro-decisions that are left out of the paper, to whole procedures that are summarized ambiguously in text, to symbols that are used inconsistently.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Piece it Together

To understand an algorithm you have to piece together an understanding yourself from disparate descriptions. This is the only tactic that I have found to be successful.

Disparate descriptions means resources such as the original descriptions of the method in the primary sources as well as authoritative secondary interpretations made of original descriptions in review papers and books.

It is common for there to be prototype implementations of a method released with the primary sources and reading this code (typically C, FORTRAN, R or Matlab) can be very enlightening for the details you need to reproduce an algorithm.

Algorithm Descriptions

An algorithm is an island of research and in all reality it can be difficult to pin down the canonical definition. For example, is it the version described in the primary source or is it the version that includes all the fixes and enhancements that are “best practice”.

A solution is to consider a given algorithm from multiple perspectives, each of which can serve a different purpose. For example, the abstract information processing description of the algorithm could be realized by a variety of different specific computational implementations.

I like this approach because it defends the need to telescope in on a specific case of the algorithm from many possible cases at each step of the description while also leaving the option open for the description of variations.

There are many descriptions you could use of varying specificity depending on your needs. Some that I like to use include: inspiration for the algorithm, metaphor or analogy for the strategy, information processing objectives, pseudocode and code.

Algorithm Prescriptions

When I started my own research projects, I thought the answer to this problem was to read everything on an algorithm and create the definitive implementation in code. Nice idea perhaps, but code is just one way to communicate an algorithm, and it is limited.

There is more to an algorithm description than the computation. There is meta information around an algorithm that can be invaluable for certain use cases.

For example, usage heuristics for an algorithm are embedded in papers. Having a summary of usage heuristics collected together in one place can mean the difference of getting a good enough result quickly and running sensitivity analysis on the algorithm for days or weeks.

Other examples include the standard experimental datasets used to test the algorithm, the general classes of problem to which the algorithm is suited, and known limitations that have been identified and described for the algorithm.

Design an Algorithm Description Template

An algorithm description template provides a structured way for you to learn about a machine learning algorithm.

You can start with a blank document and list out the section headings for the types of descriptions you need of the algorithm, for example applied, implementation, or your own personal reference cheat sheet.

To figure out what sections to include in your template, list out questions you would like to answer about the algorithm, or algorithms if you are lookin"
73;73;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-probabilistic-forecasting-model-to-predict-air-pollution-days/;2018-09-06;How to Develop a Probabilistic Forecasting Model to Predict Air Pollution Days;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68

# tune the gbm configuration from numpy import loadtxt from numpy import mean from matplotlib import pyplot from sklearn . base import clone from sklearn . metrics import brier_score_loss from sklearn . ensemble import BaggingClassifier from sklearn . ensemble import ExtraTreesClassifier from sklearn . ensemble import GradientBoostingClassifier from sklearn . ensemble import RandomForestClassifier # evaluate a sklearn model def evaluate_once ( bs_ref , template , trainX , trainy , testX , testy ) : # fit model model = clone ( template ) model . fit ( trainX , trainy ) # predict probabilities for 0 and 1 probs = model . predict_proba ( testX ) # keep the probabilities for class=1 only yhat = probs [ : , 1 ] # calculate brier score bs = brier_score_loss ( testy , yhat ) # calculate brier skill score bss = ( bs - bs_ref ) / ( 0 - bs_ref ) return bss # evaluate an sklearn model n times def evaluate ( bs_ref , model , trainX , trainy , testX , testy , n = 10 ) : scores = [ evaluate_once ( bs_ref , model , trainX , trainy , testX , testy ) for _ in range ( n ) ] print ( '>%s, bss=%.6f' % ( type ( model ) , mean ( scores ) ) ) return scores # load datasets train = loadtxt ( 'train.csv' , delimiter = ',' ) test = loadtxt ( 'test.csv' , delimiter = ',' ) # split into inputs/outputs trainX , trainy , testX , testy = train [ : , : - 1 ] , train [ : , - 1 ] , test [ : , : - 1 ] , test [ : , - 1 ] # estimate naive probabilistic forecast naive = sum ( train [ : , - 1 ] ) / train . shape [ 0 ] # forecast the test dataset yhat = [ naive for _ in range ( len ( test ) ) ] # calculate naive bs bs_ref = brier_score_loss ( testy , yhat ) # evaluate a suite of ensemble tree methods scores , names = list ( ) , list ( ) # base model = GradientBoostingClassifier ( learning_rate = 0.1 , n_estimators = 100 , subsample = 1.0 , max_depth = 3 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'base' ) # learning rate model = GradientBoostingClassifier ( learning_rate = 0.01 , n_estimators = 500 , subsample = 1.0 , max_depth = 3 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'lr' ) # depth model = GradientBoostingClassifier ( learning_rate = 0.1 , n_estimators = 100 , subsample = 0.7 , max_depth = 7 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'depth' ) # all model = GradientBoostingClassifier ( learning_rate = 0.01 , n_estimators = 500 , subsample = 0.7 , max_depth = 7 ) avg_bss = evaluate ( bs_ref , model , trainX , trainy , testX , testy ) scores . append ( avg_bss ) names . append ( 'all' ) # plot results pyplot . boxplot ( scores , labels = names ) pyplot . show ( )"
74;74;machinelearningmastery.com;https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/;2019-05-26;How to Perform Object Detection With YOLOv3 in Keras;"# create a YOLOv3 Keras model and save it to file

# based on https://github.com/experiencor/keras-yolo3

import struct

import numpy as np

from keras . layers import Conv2D

from keras . layers import Input

from keras . layers import BatchNormalization

from keras . layers import LeakyReLU

from keras . layers import ZeroPadding2D

from keras . layers import UpSampling2D

from keras . layers . merge import add , concatenate

from keras . models import Model

def _conv_block ( inp , convs , skip = True ) :

x = inp

count = 0

for conv in convs :

if count == ( len ( convs ) - 2 ) and skip :

skip_connection = x

count += 1

if conv [ 'stride' ] > 1 : x = ZeroPadding2D ( ( ( 1 , 0 ) , ( 1 , 0 ) ) ) ( x ) # peculiar padding as darknet prefer left and top

x = Conv2D ( conv [ 'filter' ] ,

conv [ 'kernel' ] ,

strides = conv [ 'stride' ] ,

padding = 'valid' if conv [ 'stride' ] > 1 else 'same' , # peculiar padding as darknet prefer left and top

name = 'conv_' + str ( conv [ 'layer_idx' ] ) ,

use_bias = False if conv [ 'bnorm' ] else True ) ( x )

if conv [ 'bnorm' ] : x = BatchNormalization ( epsilon = 0.001 , name = 'bnorm_' + str ( conv [ 'layer_idx' ] ) ) ( x )

if conv [ 'leaky' ] : x = LeakyReLU ( alpha = 0.1 , name = 'leaky_' + str ( conv [ 'layer_idx' ] ) ) ( x )

return add ( [ skip_connection , x ] ) if skip else x

def make_yolov3_model ( ) :

input_image = Input ( shape = ( None , None , 3 ) )

# Layer 0 => 4

x = _conv_block ( input_image , [ { 'filter' : 32 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 0 } ,

{ 'filter' : 64 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 1 } ,

{ 'filter' : 32 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 2 } ,

{ 'filter' : 64 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 3 } ] )

# Layer 5 => 8

x = _conv_block ( x , [ { 'filter' : 128 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 5 } ,

{ 'filter' : 64 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 6 } ,

{ 'filter' : 128 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 7 } ] )

# Layer 9 => 11

x = _conv_block ( x , [ { 'filter' : 64 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 9 } ,

{ 'filter' : 128 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 10 } ] )

# Layer 12 => 15

x = _conv_block ( x , [ { 'filter' : 256 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 12 } ,

{ 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 13 } ,

{ 'filter' : 256 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 14 } ] )

# Layer 16 => 36

for i in range ( 7 ) :

x = _conv_block ( x , [ { 'filter' : 128 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 16 + i* 3 } ,

{ 'filter' : 256 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 17 + i* 3 } ] )

skip_36 = x

# Layer 37 => 40

x = _conv_block ( x , [ { 'filter' : 512 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 37 } ,

{ 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 38 } ,

{ 'filter' : 512 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 39 } ] )

# Layer 41 => 61

for i in range ( 7 ) :

x = _conv_block ( x , [ { 'filter' : 256 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 41 + i* 3 } ,

{ 'filter' : 512 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 42 + i* 3 } ] )

skip_61 = x

# Layer 62 => 65

x = _conv_block ( x , [ { 'filter' : 1024 , 'kernel' : 3 , 'stride' : 2 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 62 } ,

{ 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 63 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 64 } ] )

# Layer 66 => 74

for i in range ( 3 ) :

x = _conv_block ( x , [ { 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 66 + i* 3 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 67 + i* 3 } ] )

# Layer 75 => 79

x = _conv_block ( x , [ { 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 75 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 76 } ,

{ 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 77 } ,

{ 'filter' : 1024 , 'kernel' : 3 , 'stride' : 1 , 'bnorm' : True , 'leaky' : True , 'layer_idx' : 78 } ,

{ 'filter' : 512 , 'kernel' : 1 , 'stride' : 1 , 'bno"
75;75;news.mit.edu;http://news.mit.edu/2020/mit-csail-sprayabletech-sprayable-user-interfaces-0408;;Sprayable user interfaces;"For decades researchers have envisioned a world where digital user interfaces are seamlessly integrated with the physical environment, until the two are virtually indistinguishable from one another.

This vision, though, is held up by a few boundaries. First, it’s difficult to integrate sensors and display elements into our tangible world due to various design constraints. Second, most methods to do so are limited to smaller scales, bound by the size of the fabricating device.

Recently, a group of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with SprayableTech, a system that lets users create room-sized interactive surfaces with sensors and displays. The system, which uses airbrushing of functional inks, enables various displays, like interactive sofas with embedded sensors to control your television, and sensors for adjusting lighting and temperature through your walls.

SprayableTech lets users channel their inner Picassos: After designing your interactive artwork in the 3D editor, it automatically generates stencils for airbrushing the layout onto a surface. Once they’ve created the stencils from cardboard, a user can then add sensors to the desired surface, whether it’s a sofa, a wall, or even a building, to control various appliances like your lamp or television. (An alternate option to stenciling is projecting them digitally.)

“Since SprayableTech is so flexible in its application, you can imagine using this type of system beyond walls and surfaces to power larger-scale entities like interactive smart cities and interactive architecture in public places,” says Michael Wessely, postdoc in CSAIL and lead author on a new paper about SprayableTech. “We view this as a tool that will allow humans to interact with and use their environment in newfound ways.”

The race for the smartest home has now been in the works for some time, with a large interest in sensor technology. It’s a big advance from the enormous glass wall displays with quick-shifting images and screens we’ve seen in countless dystopian films.

The MIT researchers’ approach is focusing on scale, and creative expression. By using the airbrush technology, they’re no longer limited to the size of the printer, the area of the screen-printing net, or the size of the hydrographic bath — and there’s thousands of possible design options.

Let’s say a user wanted to design a tree symbol on their wall to control the ambient light in the room. To start the process, they would use a toolkit in a 3D editor to design their digital object, and customize for things like proximity sensors, touch buttons, sliders, and electroluminescent displays.

Then, the toolkit would output the choice of stencils: fabricated stencils cut from cardboard, which are great for high-precision spraying on simple, flat, surfaces, or projected stencils, which are less precise, but better for doubly-curved surfaces.

Designers can then spray on the functional ink, which is ink with electrically functional elements, using an airbrush. As a final step to get the system going, a microcontroller is attached that connects the interface to the board that runs the code for sensing and visual output.

The team tested the system on a variety of items, including:

a musical interface on a concrete pillar;

an interactive sofa that’s connected to a television;

a wall display for controlling light; and

a street post with a touchable display that provides audible information on subway stations and local attractions.

Since the stencils need to be created in advance via the digital editor, it reduces the opportunity for spontaneous exploration. Looking forward, the team wants to explore so-called “modular” stencils that create touch buttons of different sizes, as well as shape-changing stencils that adjust themselves based on a desired user interface shape.

“In the future, we aim to collaborate with graffiti artists and architects to explore the future potential for large-scale user interfaces in enabling the internet of things for smart cities and interactive homes,” says Wessely.

Wessely wrote the paper alongside MIT PhD student Ticha Sethapakdi, MIT undergraduate students Carlos Castillo and Jackson C. Snowden, MIT postdoc Isabel P.S. Qamar, MIT Professor Stefanie Mueller, University of Bristol PhD student Ollie Hanton, University of Bristol Professor Mike Fraser, and University of Bristol Associate Professor Anne Roudaut."
76;76;machinelearningmastery.com;https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/;2018-12-18;Ensemble Learning Methods for Deep Learning Neural Networks;"Tweet Share Share

Last Updated on August 6, 2019

How to Improve Performance By Combining Predictions From Multiple Models.

Deep learning neural networks are nonlinear methods.

They offer increased flexibility and can scale in proportion to the amount of training data available. A downside of this flexibility is that they learn via a stochastic training algorithm which means that they are sensitive to the specifics of the training data and may find a different set of weights each time they are trained, which in turn produce different predictions.

Generally, this is referred to as neural networks having a high variance and it can be frustrating when trying to develop a final model to use for making predictions.

A successful approach to reducing the variance of neural network models is to train multiple models instead of a single model and to combine the predictions from these models. This is called ensemble learning and not only reduces the variance of predictions but also can result in predictions that are better than any single model.

In this post, you will discover methods for deep learning neural networks to reduce variance and improve prediction performance.

After reading this post, you will know:

Neural network models are nonlinear and have a high variance, which can be frustrating when preparing a final model for making predictions.

Ensemble learning combines the predictions from multiple neural network models to reduce the variance of predictions and reduce generalization error.

Techniques for ensemble learning can be grouped by the element that is varied, such as training data, the model, and how predictions are combined.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

High Variance of Neural Network Models Reduce Variance Using an Ensemble of Models How to Ensemble Neural Network Models Summary of Ensemble Techniques

High Variance of Neural Network Models

Training deep neural networks can be very computationally expensive.

Very deep networks trained on millions of examples may take days, weeks, and sometimes months to train.

Google’s baseline model […] was a deep convolutional neural network […] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores.

— Distilling the Knowledge in a Neural Network, 2015.

After the investment of so much time and resources, there is no guarantee that the final model will have low generalization error, performing well on examples not seen during training.

… train many different candidate networks and then to select the best, […] and to discard the rest. There are two disadvantages with such an approach. First, all of the effort involved in training the remaining networks is wasted. Second, […] the network which had best performance on the validation set might not be the one with the best performance on new test data.

— Pages 364-365, Neural Networks for Pattern Recognition, 1995.

Neural network models are a nonlinear method. This means that they can learn complex nonlinear relationships in the data. A downside of this flexibility is that they are sensitive to initial conditions, both in terms of the initial random weights and in terms of the statistical noise in the training dataset.

This stochastic nature of the learning algorithm means that each time a neural network model is trained, it may learn a slightly (or dramatically) different version of the mapping function from inputs to outputs, that in turn will have different performance on the training and holdout datasets.

As such, we can think of a neural network as a method that has a low bias and high variance. Even when trained on large datasets to satisfy the high variance, having any variance in a final model that is intended to be used to make predictions can be frustrating.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Reduce Variance Using an Ensemble of Models

A solution to the high variance of neural networks is to train multiple models and combine their predictions.

The idea is to combine the predictions from multiple good but different models.

A good model has skill, meaning that its predictions are better than random chance. Importantly, the models must be good in different ways; they must make different prediction errors.

The reason that model averaging works is that different models will usually not make all the same errors on the test set.

— Page 256, Deep Learning, 2016.

Combining the predictions from multiple neural networks adds a bias that in turn counters the variance of a single trained neural network model. The results are predictions that are "
77;77;machinelearningmastery.com;https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/;2017-07-11;How to One Hot Encode Sequence Data in Python;"from numpy import argmax

# define input string

data = 'hello world'

print ( data )

# define universe of possible input values

alphabet = 'abcdefghijklmnopqrstuvwxyz '

# define a mapping of chars to integers

char_to_int = dict ( ( c , i ) for i , c in enumerate ( alphabet ) )

int_to_char = dict ( ( i , c ) for i , c in enumerate ( alphabet ) )

# integer encode input data

integer_encoded = [ char_to_int [ char ] for char in data ]

print ( integer_encoded )

# one hot encode

onehot_encoded = list ( )

for value in integer_encoded :

letter = [ 0 for _ in range ( len ( alphabet ) ) ]

letter [ value ] = 1

onehot_encoded . append ( letter )

print ( onehot_encoded )

# invert encoding

inverted = int_to_char [ argmax ( onehot_encoded [ 0 ] ) ]"
78;78;machinelearningmastery.com;http://machinelearningmastery.com/object-recognition-convolutional-neural-networks-keras-deep-learning-library/;2016-06-30;Object Classification with CNNs using the Keras Deep Learning Library;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55

# Large CNN model for the CIFAR-10 Dataset from keras . datasets import cifar10 from keras . models import Sequential from keras . layers import Dense from keras . layers import Dropout from keras . layers import Flatten from keras . constraints import maxnorm from keras . optimizers import SGD from keras . layers . convolutional import Conv2D from keras . layers . convolutional import MaxPooling2D from keras . utils import np_utils # load data ( X_train , y_train ) , ( X_test , y_test ) = cifar10 . load_data ( ) # normalize inputs from 0-255 to 0.0-1.0 X_train = X_train . astype ( 'float32' ) X_test = X_test . astype ( 'float32' ) X_train = X_train / 255.0 X_test = X_test / 255.0 # one hot encode outputs y_train = np_utils . to_categorical ( y_train ) y_test = np_utils . to_categorical ( y_test ) num_classes = y_test . shape [ 1 ] # Create the model model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , input_shape = ( 32 , 32 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Conv2D ( 128 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' ) ) model . add ( MaxPooling2D ( ) ) model . add ( Flatten ( ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Dense ( 1024 , activation = 'relu' , kernel_constraint = maxnorm ( 3 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Dense ( 512 , activation = 'relu' , kernel_constraint = maxnorm ( 3 ) ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Dense ( num_classes , activation = 'softmax' ) ) # Compile model epochs = 25 lrate = 0.01 decay = lrate / epochs sgd = SGD ( lr = lrate , momentum = 0.9 , decay = decay , nesterov = False ) model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ] ) model . summary ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = epochs , batch_size = 64 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Accuracy: %.2f%%"" % ( scores [ 1 ] * 100 ) )"
79;79;news.mit.edu;http://news.mit.edu/2020/three-mit-awarded-2020-guggenheim-fellowships-0414;;Three from MIT awarded 2020 Guggenheim Fellowships;"MIT faculty members Sabine Iatridou, Jonathan Gruber, and Rebecca Saxe are among 175 scientists, artists, and scholars awarded 2020 fellowships from the John Simon Guggenheim Foundation. Appointed on the basis of prior achievement and exceptional promise, the 2020 Guggenheim Fellows were selected from almost 3,000 applicants.

“It’s exceptionally encouraging to be able to share such positive news at this terribly challenging time” says Edward Hirsch, president of the foundation. “A Guggenheim Fellowship has always offered practical assistance, helping fellows do their work, but for many of the new fellows, it may be a lifeline at a time of hardship, a survival tool as well as a creative one.”

Since 1925, the foundation has granted more the $375 million in fellowships to over 18,000 individuals, including Nobel laureates, Fields medalists, poets laureate, and winners of the Pulitzer Prize, among other internationally recognized honors. This year’s MIT recipients include a linguist, an economist, and a cognitive neuroscientist.

Sabine Iatridou is professor of linguistics in MIT's Department of Linguistics and Philosophy. Her work focuses on syntax and the syntax-semantics interface, as well as comparative linguistics. She is the author and coauthor of a series of innovative papers about tense and modality that opened up whole new domains of research for the field. Since those publications, she has made foundational contributions to many branches of linguistics that connect form with meaning. She is the recipient of the National Young Investigator Award (USA), of an honorary doctorate from the University of Crete in Greece, and of an award from the Royal Dutch Academy of Sciences. She was elected fellow of the Linguistic Society of America. She is co-founder and co-director of the CreteLing Summer School of Linguistics.

Jonathan Gruber is the Ford Professor of Economics at MIT, the director of the Health Care Program at the National Bureau of Economic Research, and the former president of the American Society of Health Economists. He has published more than 175 research articles, has edited six research volumes, and is the author of “Public Finance and Public Policy,” a leading undergraduate text; “Health Care Reform,” a graphic novel; and “Jump-Starting America: How Breakthrough Science Can Revive Economic Growth and the American Dream.” In 2006 he received the American Society of Health Economists Inaugural Medal for the best health economist in the nation aged 40 and under. He served as deputy sssistant secretary for economic policy at the U.S. Department of the Treasury. He was a key architect of Massachusetts' ambitious health reform effort, and became an inaugural member of the Health Connector Board, the main implementing body for that effort. He served as a technical consultant to the Obama administration and worked with both the administration and Congress to help craft the Affordable Care Act. In 2011, he was named “One of the Top 25 Most Innovative and Practical Thinkers of Our Time” by Slate magazine.

Rebecca Saxe is an associate investigator of the McGovern Institute and the John W. Jarve (1978) Professor in Brain and Cognitive Sciences. She studies human social cognition, using a combination of behavioral testing and brain imaging technologies. She is best known for her work on brain regions specialized for abstract concepts such as “theory of mind” tasks that involve understanding the mental states of other people. She also studies the development of the human brain during early infancy. She obtained her PhD from MIT and was a Harvard University junior fellow before joining the MIT faculty in 2006. Saxe was chosen in 2012 as a Young Global Leader by the World Economic Forum, and she received the 2014 Troland Award from the National Academy of Sciences. Her TED Talk, “How we read each other’s minds” has been viewed over 3 million times.

“As we grapple with the difficulties of the moment, it is also important to look to the future,” says Hirsch. “The artists, writers, scholars, and scientific researchers supported by the fellowship will help us understand and learn from what we are enduring individually and collectively, and it is an honor for the foundation to help them do their essential work.”"
80;80;machinelearningmastery.com;http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/;2016-07-03;How to Predict Sentiment From Movie Reviews Using Deep Learning (Text Classification);"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

# CNN for the IMDB problem from keras . datasets import imdb from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers . convolutional import Conv1D from keras . layers . convolutional import MaxPooling1D from keras . layers . embeddings import Embedding from keras . preprocessing import sequence # load the dataset but only keep the top n words, zero the rest top_words = 5000 ( X_train , y_train ) , ( X_test , y_test ) = imdb . load_data ( num_words = top_words ) # pad dataset to a maximum review length in words max_words = 500 X_train = sequence . pad_sequences ( X_train , maxlen = max_words ) X_test = sequence . pad_sequences ( X_test , maxlen = max_words ) # create the model model = Sequential ( ) model . add ( Embedding ( top_words , 32 , input_length = max_words ) ) model . add ( Conv1D ( 32 , 3 , padding = 'same' , activation = 'relu' ) ) model . add ( MaxPooling1D ( ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 250 , activation = 'relu' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) model . summary ( ) # Fit the model model . fit ( X_train , y_train , validation_data = ( X_test , y_test ) , epochs = 2 , batch_size = 128 , verbose = 2 ) # Final evaluation of the model scores = model . evaluate ( X_test , y_test , verbose = 0 ) print ( ""Accuracy: %.2f%%"" % ( scores [ 1 ] * 100 ) )"
81;81;machinelearningmastery.com;https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/;2017-05-09;Multi-step Time Series Forecasting with Long Short-Term Memory Networks in Python;"from pandas import DataFrame

from pandas import Series

from pandas import concat

from pandas import read_csv

from pandas import datetime

from sklearn . metrics import mean_squared_error

from sklearn . preprocessing import MinMaxScaler

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from math import sqrt

from matplotlib import pyplot

from numpy import array

# date-time parsing function for loading the dataset

def parser ( x ) :

return datetime . strptime ( '190' + x , '%Y-%m' )

# convert time series into supervised learning problem

def series_to_supervised ( data , n_in = 1 , n_out = 1 , dropnan = True ) :

n_vars = 1 if type ( data ) is list else data . shape [ 1 ]

df = DataFrame ( data )

cols , names = list ( ) , list ( )

# input sequence (t-n, ... t-1)

for i in range ( n_in , 0 , - 1 ) :

cols . append ( df . shift ( i ) )

names += [ ( 'var%d(t-%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ]

# forecast sequence (t, t+1, ... t+n)

for i in range ( 0 , n_out ) :

cols . append ( df . shift ( - i ) )

if i == 0 :

names += [ ( 'var%d(t)' % ( j + 1 ) ) for j in range ( n_vars ) ]

else :

names += [ ( 'var%d(t+%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ]

# put it all together

agg = concat ( cols , axis = 1 )

agg . columns = names

# drop rows with NaN values

if dropnan :

agg . dropna ( inplace = True )

return agg

# create a differenced series

def difference ( dataset , interval = 1 ) :

diff = list ( )

for i in range ( interval , len ( dataset ) ) :

value = dataset [ i ] - dataset [ i - interval ]

diff . append ( value )

return Series ( diff )

# transform series into train and test sets for supervised learning

def prepare_data ( series , n_test , n_lag , n_seq ) :

# extract raw values

raw_values = series . values

# transform data to be stationary

diff_series = difference ( raw_values , 1 )

diff_values = diff_series . values

diff_values = diff_values . reshape ( len ( diff_values ) , 1 )

# rescale values to -1, 1

scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) )

scaled_values = scaler . fit_transform ( diff_values )

scaled_values = scaled_values . reshape ( len ( scaled_values ) , 1 )

# transform into supervised learning problem X, y

supervised = series_to_supervised ( scaled_values , n_lag , n_seq )

supervised_values = supervised . values

# split into train and test sets

train , test = supervised_values [ 0 : - n_test ] , supervised_values [ - n_test : ]

return scaler , train , test

# fit an LSTM network to training data

def fit_lstm ( train , n_lag , n_seq , n_batch , nb_epoch , n_neurons ) :

# reshape training into [samples, timesteps, features]

X , y = train [ : , 0 : n_lag ] , train [ : , n_lag : ]

X = X . reshape ( X . shape [ 0 ] , 1 , X . shape [ 1 ] )

# design network

model = Sequential ( )

model . add ( LSTM ( n_neurons , batch_input_shape = ( n_batch , X . shape [ 1 ] , X . shape [ 2 ] ) , stateful = True ) )

model . add ( Dense ( y . shape [ 1 ] ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

# fit network

for i in range ( nb_epoch ) :

model . fit ( X , y , epochs = 1 , batch_size = n_batch , verbose = 0 , shuffle = False )

model . reset_states ( )

return model

# make one forecast with an LSTM,

def forecast_lstm ( model , X , n_batch ) :

# reshape input pattern to [samples, timesteps, features]

X = X . reshape ( 1 , 1 , len ( X ) )

# make forecast

forecast = model . predict ( X , batch_size = n_batch )

# convert to array

return [ x for x in forecast [ 0 , : ] ]

# evaluate the persistence model

def make_forecasts ( model , n_batch , train , test , n_lag , n_seq ) :

forecasts = list ( )

for i in range ( len ( test ) ) :

X , y = test [ i , 0 : n_lag ] , test [ i , n_lag : ]

# make forecast

forecast = forecast_lstm ( model , X , n_batch )

# store the forecast

forecasts . append ( forecast )

return forecasts

# invert differenced forecast

def inverse_difference ( last_ob , forecast ) :

# invert first forecast

inverted = list ( )

inverted . append ( forecast [ 0 ] + last_ob )

# propagate difference forecast using inverted first value

for i in range ( 1 , len ( forecast ) ) :

inverted . append ( forecast [ i ] + inverted [ i - 1 ] )

return inverted

# inverse data transform on forecasts

def inverse_transform ( series , forecasts , scaler , n_test ) :

inverted = list ( )

for i in range ( len ( forecasts ) ) :

# create array from forecast

forecast = array ( forecasts [ i ] )

forecast = forecast . reshape ( 1 , len ( forecast ) )

# invert scaling

inv_scale = scaler . inverse_transform ( forecast )

inv_scale = inv_scale [ 0 , : ]

# invert differencing

index = len ( series ) - n_test + i - 1

last_ob = series . values [ index ]

inv_diff = inverse_difference ( last_ob , inv_scale )

# store

inverted . append ( inv_diff )

return inverted

# evaluate the RMSE for each forecast time step

def evaluate_forecasts ( te"
82;82;news.mit.edu;http://news.mit.edu/2020/mit-idss-promoting-women-data-science-0417;;Annual Women in Data Science conference discusses fake news;"What do radiation waves from space, the U.S. Census, and the human genome have in common? All three, like so many things today, involve massive amounts of data. These data can unlock insights and lead to new solutions and better decision-making — for those who have the knowledge and tools to analyze it.

The impressive variety of applications for data science tools and techniques were on display at the Women in Data Science Conference (WiDS Cambridge), held at the Microsoft NERD Center in early March, before MIT and the Commonwealth of Massachusetts began to de-densify in response to the Covid-19 emergency. Co-hosted by the Institute for Data, Systems, and Society (IDSS), the Harvard Institute for Applied Computational Science, and Microsoft Research New England, WiDS Cambridge is one of dozens of satellite WiDS events around the world. The program showcases women who are not only using data science tools in their research or business, but who are leaders refining those tools and recruiting more women into the field.

The day’s signature event was a panel discussion on data science and fake news called “Data weaponized, data scrutinized: a war on information.” The panel was moderated by Manon Revel, a doctoral student in the IDSS Social and Engineering Systems (SES) program whose research has analyzed popup ads to see how exposure influences readers' assessment of news credibility. Addressing current challenges, Manon shared: “Understanding the effect of false information and combatting it is crucial. It requires thinking through the technology design, but also the regulatory framework and the political and social context.”

The panel also included Camille Francois, chief information officer for Graphika, a social network analysis startup that uses AI to understand online communities. “We don’t know how to measure the impact of foreign interference for many complicated reasons,” said Francois. “The aim of a foreign interference campaign is not necessarily to impact a vote. It’s to divide, it’s to confuse, and it’s to create chaos. How do you measure chaos?”

In addition to the discussion on misinformation, WiDS Cambridge featured a wide variety of insights from industry and academia. Asu Ozdaglar, deputy dean of academics, head of the Department of Electrical Engineering and Computer Science, and faculty member in IDSS and the Laboratory for Information and Decision Systems (LIDS), highlighted robustness in machine learning. Citing the common example of image classification system errors, she explored how ‘perturbed’ data can, with small variations, disrupt otherwise accurate models, and offered a ‘minmax’ approach using generative adversarial networks (GANs) to increase robustness.

For an industry perspective, Jess Stauth, managing director at Fidelity Labs, provided ways to apply basic research principles to modern data science business problems. Data science is a collection of tools from statistics to computing, she says, and businesses require infrastructure to use them to create tangible business value. “A data scientist alone in a room with a laptop is probably not going to be all that successful,” she muses.

The conference provided opportunities for participants to network and job search, with sponsor companies hosting recruiting tables and answering questions. WiDS also empowered newer practitioners with a student and postdoc poster session and lightning talks. Over 30 poster presenters participated, showcasing work in fields as diverse as demographic bias in natural language processing, crime prediction, neurodegenerative disease, and sustainable buildings.

“WiDS is a wonderful event where you can interact with your peers, present your research, and build confidence,” says Marie Charpignon, a graduate student in MIT’s SES PhD program who presented a poster on using causal inference on electronic health records to explore repurposing diabetes medication to treat dementia. “The conference brings together students, professors, industry researchers, and even venture capitalists in search of promising ideas. WiDS gives you a sense of the myriad paths you could take after graduation.”"
83;83;news.mit.edu;http://news.mit.edu/2020/3-questions-ron-rivest-trusting-electronic-voting-systems-0226;2020-03-19;3 Questions: Ron Rivest on trusting electronic voting systems;"Ron Rivest is an MIT Institute Professor in the Department of Electrical Engineering and Computer Science. He’s an authority on algorithms and an inventor of the RSA public-key cryptosystem, one of the most widely used algorithms to securely transmit data. Since the 1980s, he’s taught students how to use cryptography to help secure voting systems. Then, in 2000, an historic recount in Florida determined the outcome of the U.S. presidential election, and the Caltech / MIT Voting Technology Project was founded with the mission to secure future elections, pulling in Rivest, who has been involved since, as well as other MIT faculty from the Department of Political Science and the MIT Sloan School of Management.

For five years, Rivest advised the U.S. Election Assistance Commission, where he helped set standards for voting system certification. In that time, he became an advocate for keeping paper ballots and auditing election outcomes based on a statistical analysis of a random sample of ballots, recommended steps to verify the reported outcome. In his research, he’s also developed technologies to use cryptography for voting, helping to secure elections dependent on electronic records.

As election security becomes a top concern in the United States, Rivest continues applying his cryptography expertise to help improve voting systems. Here, he discusses the major issues with securing all-electronic voting systems and explains why he prefers keeping paper ballots as backup to verify voter intentions have been recorded — and that the election outcome isn’t based on a computer bug.

Q: If an electronic voting system has been certified, does that mean it’s secure?

A: You have to be careful with what you expect from the certification process. You could go into it thinking, well, I’m going to have these systems certified, and because they’re certified, they’re secure, and therefore, because the voting systems are secure, I can trust the election outcomes are right. And that turns out not to be a terribly good mode of thought. For one thing, you can’t really show that something is secure by testing.

Security relates to the absence of ways an adversary can affect the election outcome. Testing a voting system may show that certain adversarial attacks don’t work, but it doesn’t reveal that there are no attacks that work. Furthermore, commercial software is well-known to have several bugs per thousand lines of code, any of which could be a security hole. Finding all bugs is well-nigh impossible, even with a vigorous testing, so certification will never provide a guarantee that a voting system is secure.

Q: Can an electronic voting system ever be secure?

A: One major problem is that you never know that the system that is running is actually the system that was tested. There are procedures in place that are supposed to ensure that, but a common way to attack a system is to attack the supply chain, so that the voting system somebody installs is not what they think it is.

You never want to be in a position where you have to say, “I trust the election outcomes because I trust the computer.” Because computers, in the end, aren’t that trustworthy. They can be manipulated. They can have their programming changed. Every day new breaches of major computer systems are reported. Computer systems just are very difficult to make secure, especially for something that's very important, like elections.

All-electronic voting systems are therefore next-to-impossible to secure. A voting system founded instead on voter-verifiable paper ballots (preferably hand-marked paper ballots) provides a basis for checking that election outcomes are correctly derived from expressed voter intentions, instead of from some computer bug.

Q: What’s your new philosophy when it comes to securing U.S. elections?

A: The new philosophy and the change of perspective I’ve adopted is not to believe an election outcome is right because you believe that machinery is doing the right thing, but rather to check that the outcome is right for each election.

You look at a sample of the paper ballots, you use some statistics, and you confirm with high confidence that the reported election outcome is consistent with that sample. A number of us have been working on that technology; Philip Stark at University of California at Berkeley is the leading statistician involved with this and the inventor of such “risk-limiting audits.”

There was a panel I was on recently for the National Academy of Sciences that produced a report called “Securing the Vote” (September 2018). That report recommended two things strongly: using paper ballots, and performing statistical post-election audits to check the tabulation of the paper ballots. In conjunction with other procedures (that, for example, ensure that the paper ballots checked are those that are cast and tabulated), one can develop confidence that our election outcomes are indeed correct."
84;84;news.mit.edu;http://news.mit.edu/2019/emily-richmond-pollock-opera-zero-hour-1002;;A new act for opera;"In November 1953, the Nationaltheater in Mannheim, Germany, staged a new opera, the composer Boris Blacher’s “Abstrakte Oper Nr. 1,” which had debuted just months previously. As it ran, music fans were treated to both a performance and a raging controversy about the work, which one critic called “a monstrosity of musical progress,” and another termed “a stillbirth.”

Some of this vitriol stemmed from Blacher’s experimental composition, which had jazz and pop sensibilities, few words in the libretto (but some nonsense syllables), and no traditional storyline. The controversy was heightened by the Mannheim production, which projected images of postwar ruins and other related tropes onto the backdrop.

“The staging was very political,” says MIT music scholar Emily Richmond Pollock, author of a new book about postwar German opera. “Putting these very concrete images behind [the stage], that people had just lived through, produced a very uncomfortable feeling.”

It wasn’t just critics who were dubious: One audience member wrote to the Mannheim morning newspaper to say that Blacher’s “cacophonous concoction is actually approaching absolute zero and is not even original in doing so.”

In short, “Abstrakte Oper Nr. 1” hardly fit its genre’s traditions. Blacher’s work was introduced soon after the supposed “Zero Hour” in German society — the years after World War Two ended in 1945. Germany had instigated the deadliest war in history, and the country was supposed to be building itself entirely anew on political, civic, and cultural fronts. But the reaction to “Abstrakte Oper Nr. 1” shows the limits of that concept; Germans also craved continuity.

“There is this mythology of the Zero Hour, that Germans had to start all over again,” says Pollock, an associate professor in MIT’s Music and Theater Arts Section.

Pollock’s new book, “Opera after the Zero Hour,” just published by Oxford University Press, explores these tensions in rich detail. In the work, Pollock closely scrutinizes five postwar German operas while examining the varied reactions they produced. Rather than participating in a total cultural teardown, she concludes, many Germans were attempting to construct a useable past and build a future connected to it.

“Opera in general is a conservative art form,” Pollock says. “It has often been identified very closely with whomever is in power.” For that reason, she adds, “Opera is a really good place to examine why tradition was a problem [after 1945], and how different artists chose to approach that problem.”

The politics of cultural nationalism

Rebuilding Germany after 1945 was a monumental task, even beyond creating a new political state. A significant part of Germany lay in rubble; for that matter, most large opera houses had been bombed.

Nonetheless, opera soon bloomed again in Germany. There were 170 new operas staged in Germany from 1945 to 1965. Operationally, as Pollock notes in the book, this inevitably meant including former Nazis in the opera business — efforts at “denazification” of society, she thinks, were of limited effectiveness. Substantively, meanwhile, the genre’s sense of tradition set audience expectations that could be difficult to alter.

“There’s a lot of investment in opera, but it’s not [usually] going to be avant-garde,” Pollock says, noting there were “hundreds of years of opera tradition pressing down” on composers, as well as “a bourgeois restored German culture that doesn’t want to do anything too radical.” However, she notes, after 1945, “There are a lot of traditions of music-making as part of the culture of being German that feel newly problematic [to socially-aware observers].”

Thus a substantial portion of those 170 new operas — besides “Abstrakte Oper Nr. 1” — contained distinctive blends of innovation and tradition. Consider Carl Orff’s “Oedipus der Tyrann,” a 1958 work of musical innovation with a traditional theme. Orff was one of Germany’s best-known composers (he wrote “Carmina Burana” in 1937) and had professional room to experiment. “Oedipus der Tyrann” strips away operatic musical form, with scant melody or symphonic expression, though Pollock’s close reading of the score shows some remaining links to mainstream operatic tradition. But the subject of the opera is classical: Orff uses the German poet Friedrich Holderlin’s 1804 translation of Sophocles’ “Oedipus” as his content. As Pollock notes, in 1958, this could be a problematic theme.

“When Germans claim special ownership of Greek culture, they’re saying they’re better than other countries — it’s cultural nationalism,” Pollock observes. “So what does it mean that a German composer is taking Greek tropes and reinterpreting them for a postwar context? Only recently, [there had been] events like the Berlin Olympics, where the Third Reich was specifically mobilizing an identification between Germans and the Greeks.”

In this case, Pollock says, “I think Orff was not able to think clearly about the potential politi"
85;85;machinelearningmastery.com;https://machinelearningmastery.com/neural-networks-tricks-of-the-trade-review/;2019-02-19;Neural Networks: Tricks of the Trade Review;"Tweet Share Share

Last Updated on August 6, 2019

Deep learning neural networks are challenging to configure and train.

There are decades of tips and tricks spread across hundreds of research papers, source code, and in the heads of academics and practitioners.

The book “Neural Networks: Tricks of the Trade” originally published in 1998 and updated in 2012 at the cusp of the deep learning renaissance ties together the disparate tips and tricks into a single volume. It includes advice that is required reading for all deep learning neural network practitioners.

In this post, you will discover the book “Neural Networks: Tricks of the Trade” that provides advice by neural network academics and practitioners on how to get the most out of your models.

After reading this post, you will know:

The motivation for why the book was written.

A breakdown of the chapters and topics in the first and second editions.

A list and summary of the must-read chapters for every neural network practitioner.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

Neural Networks: Tricks of the Trade is a collection of papers on techniques to get better performance from neural network models.

The first edition was published in 1998 comprised of five parts and 17 chapters. The second edition was published right on the cusp of the new deep learning renaissance in 2012 and includes three more parts and 13 new chapters.

If you are a deep learning practitioner, then it is a must read book.

I own and reference both editions.

Motivation

The motivation for the book was to collate the empirical and theoretically grounded tips, tricks, and best practices used to get the best performance from neural network models in practice.

The author’s concern is that many of the useful tips and tricks are tacit knowledge in the field, trapped in peoples heads, code bases, or at the end of conference papers and that beginners to the field should be aware of them.

It is our belief that researchers and practitioners acquire, through experience and word-of-mouth, techniques and heuristics that help them successfully apply neural networks to difficult real-world problems. […] they are usually hidden in people’s heads or in the back pages of space-constrained conference papers.

The book is an effort to try to group the tricks together, after the success of a workshop at the 1996 NIPS conference with the same name.

This book is an outgrowth of a 1996 NIPS workshop called Tricks of the Trade whose goal was to begin the process of gathering and documenting these tricks. The interest that the workshop generated motivated us to expand our collection and compile it into this book.

— Page 1, Neural Networks: Tricks of the Trade, Second Edition, 2012.

Breakdown of First Edition

The first edition of the book was put together (edited) by Genevieve Orr and Klaus-Robert Muller comprised of five parts and 17 chapters and was published 20 years ago in 1998.

Each part includes a useful preface that summarizes what to expect in the upcoming chapters, and each chapter written by one or more academics in the field.

The breakdown of this first edition was as follows:

Part 1: Speeding Learning

Chapter 1: Efficient BackProp

Part 2: Regularization Techniques to Improve Generalization

Chapter 2: Early Stopping – But When?

Chapter 3: A Simple Trick for Estimating the Weight Decay Parameter

Chapter 4: Controlling the Hyperparameter Search on MacKay’s Bayesian Neural Network Framework

Chapter 5: Adaptive Regularization in Neural Network Modeling

Chapter 6: Large Ensemble Averaging

Part 3: Improving Network Models and Algorithmic Tricks

Chapter 7: Square Unit Augmented, Radically Extended, Multilayer Perceptrons

Chapter 8: A Dozen Tricks with Multitask Learning

Chapter 9: Solving the Ill-Conditioning on Neural Network Learning

Chapter 10: Centering Neural Network Gradient Factors

Chapter 11: Avoiding Roundoff Error in Backpropagating Derivatives

Part 4: Representation and Incorporating PRior Knowledge in Neural Network Training

Chapter 12: Transformation Invariance in Pattern Recognition – Tangent Distance and Tangent Propagation

Chapter 13: Combining Neural Networks and Context-Driven Search for On-Line Printed Handwriting Recognition in the Newton

Chapter 14: Neural Network Classification and Prior Class Probabilities

Chapter 15: Applying Divide and Conquer to Large Scale Pattern Recognition Tasks

Part 5: Tricks for Time Series

Chapter 16: Forecasting the Economy with Neural Nets: A Survey of Challenges and Solutions

Chapter 17: How to Train Neural Networks

It is an expensive book, and if you can pick-up a cheap second-hand copy of this first edition, then I highly recommend it.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-u"
86;86;news.mit.edu;http://news.mit.edu/2020/remote-control-hormone-release-nanoparticles-0410;;Researchers achieve remote control of hormone release;"Abnormal levels of stress hormones such as adrenaline and cortisol are linked to a variety of mental health disorders, including depression and posttraumatic stress disorder (PTSD). MIT researchers have now devised a way to remotely control the release of these hormones from the adrenal gland, using magnetic nanoparticles.

This approach could help scientists to learn more about how hormone release influences mental health, and could eventually offer a new way to treat hormone-linked disorders, the researchers say.

“We’re looking how can we study and eventually treat stress disorders by modulating peripheral organ function, rather than doing something highly invasive in the central nervous system,” says Polina Anikeeva, an MIT professor of materials science and engineering and of brain and cognitive sciences.

To achieve control over hormone release, Dekel Rosenfeld, an MIT-Technion postdoc in Anikeeva’s group, has developed specialized magnetic nanoparticles that can be injected into the adrenal gland. When exposed to a weak magnetic field, the particles heat up slightly, activating heat-responsive channels that trigger hormone release. This technique can be used to stimulate an organ deep in the body with minimal invasiveness.

Anikeeva and Alik Widge, an assistant professor of psychiatry at the University of Minnesota and a former research fellow at MIT’s Picower Institute for Learning and Memory, are the senior authors of the study. Rosenfeld is the lead author of the paper, which appears today in Science Advances.

Controlling hormones

Anikeeva’s lab has previously devised several novel magnetic nanomaterials, including particles that can release drugs at precise times in specific locations in the body.

In the new study, the research team wanted to explore the idea of treating disorders of the brain by manipulating organs that are outside the central nervous system but influence it through hormone release. One well-known example is the hypothalamic-pituitary-adrenal (HPA) axis, which regulates stress response in mammals. Hormones secreted by the adrenal gland, including cortisol and adrenaline, play important roles in depression, stress, and anxiety.

“Some disorders that we consider neurological may be treatable from the periphery, if we can learn to modulate those local circuits rather than going back to the global circuits in the central nervous system,” says Anikeeva, who is a member of MIT’s Research Laboratory of Electronics and McGovern Institute for Brain Research.

As a target to stimulate hormone release, the researchers decided on ion channels that control the flow of calcium into adrenal cells. Those ion channels can be activated by a variety of stimuli, including heat. When calcium flows through the open channels into adrenal cells, the cells begin pumping out hormones. “If we want to modulate the release of those hormones, we need to be able to essentially modulate the influx of calcium into adrenal cells,” Rosenfeld says.

Unlike previous research in Anikeeva’s group, in this study magnetothermal stimulation was applied to modulate the function of cells without artificially introducing any genes.

To stimulate these heat-sensitive channels, which naturally occur in adrenal cells, the researchers designed nanoparticles made of magnetite, a type of iron oxide that forms tiny magnetic crystals about 1/5000 the thickness of a human hair. In rats, they found these particles could be injected directly into the adrenal glands and remain there for at least six months. When the rats were exposed to a weak magnetic field — about 50 millitesla, 100 times weaker than the fields used for magnetic resonance imaging (MRI) — the particles heated up by about 6 degrees Celsius, enough to trigger the calcium channels to open without damaging any surrounding tissue.

The heat-sensitive channel that they targeted, known as TRPV1, is found in many sensory neurons throughout the body, including pain receptors. TRPV1 channels can be activated by capsaicin, the organic compound that gives chili peppers their heat, as well as by temperature. They are found across mammalian species, and belong to a family of many other channels that are also sensitive to heat.

This stimulation triggered a hormone rush — doubling cortisol production and boosting noradrenaline by about 25 percent. That led to a measurable increase in the animals’ heart rates.

Treating stress and pain

The researchers now plan to use this approach to study how hormone release affects PTSD and other disorders, and they say that eventually it could be adapted for treating such disorders. This method would offer a much less invasive alternative to potential treatments that involve implanting a medical device to electrically stimulate hormone release, which is not feasible in organs such as the adrenal glands that are soft and highly vascularized, the researchers say.

Another area where this strategy could hold promise is in the treatment of pain,"
87;87;machinelearningmastery.com;https://machinelearningmastery.com/load-machine-learning-data-scratch-python/;2016-10-11;How to Load Machine Learning Data From Scratch In Python;"from csv import reader

# Load a CSV file

def load_csv ( filename ) :

file = open ( filename , ""rb"" )

lines = reader ( file )

dataset = list ( lines )

return dataset

# Convert string column to float

def str_column_to_float ( dataset , column ) :

for row in dataset :

row [ column ] = float ( row [ column ] . strip ( ) )

# Load pima-indians-diabetes dataset

filename = 'pima-indians-diabetes.csv'

dataset = load_csv ( filename )

print ( 'Loaded data file {0} with {1} rows and {2} columns' ) . format ( filename , len ( dataset ) , len ( dataset [ 0 ] ) )

print ( dataset [ 0 ] )

# convert string columns to float

for i in range ( len ( dataset [ 0 ] ) ) :

str_column_to_float ( dataset , i )"
88;88;news.mit.edu;http://news.mit.edu/2020/deep-learning-mechanical-property-metallic-0316;;Deep learning for mechanical property evaluation;"A standard method for testing some of the mechanical properties of materials is to poke them with a sharp point. This “indentation technique” can provide detailed measurements of how the material responds to the point’s force, as a function of its penetration depth.

With advances in nanotechnology during the past two decades, the indentation force can be measured to a resolution on the order of one-billionth of a Newton (a measure of the force approximately equivalent to the force you feel when you hold a medium-sized apple in your hand), and the sharp tip’s penetration depth can be captured to a resolution as small as a nanometer, or about 1/100,000 the diameter of a human hair. Such instrumented nanoindentation tools have provided new opportunities for probing physical properties in a wide variety of materials, including metals and alloys, plastics, ceramics, and semiconductors.

But while indentation techniques, including nanoindentation, work well for measuring some properties, they exhibit large errors when probing plastic properties of materials — the kind of permanent deformation that happens, for example, if you press your thumb into a piece of silly putty and leave a dent, or when you permanently bend a paper clip using your fingers. Such tests can be important in a wide variety of industrial applications, including conventional and digital manufacturing (3-D printing) of metallic structures, material quality assurance of engineering parts, and optimization of performance and cost. However, conventional indentation tests and existing methods to extract critical properties can be highly inaccurate.

Now, an international research team comprising researchers from MIT, Brown University, and Nanyang Technological University (NTU) in Singapore has developed a new analytical technique that can improve the estimation of mechanical properties of metallic materials from instrumented indention, with as much as 20 times greater accuracy than existing methods. Their findings are described today in the Proceedings of the National Academy of Sciences, in a paper combining indentation experiments with computational modeling of materials using the latest machine learning tools.

The team includes co-lead and senior author Ming Dao, a principal research scientist at MIT, and senior author Subra Suresh, MIT Vannevar Bush Professor Emeritus who is president and distinguished university professor at NTU Singapore. Their co-authors are doctoral student Lu Lu and Professor George Em Karniadakis of Brown University and research fellow Punit Kumar and Professor Upadrasta Ramamurty of NTU Singapore.

Animation showing schematically the process of extracting mechanical properties from indentation tests. It is a challenging task to accurately obtain the yield strength and nonlinear mechanical behavior from indention tests. Courtesy of the researchers.

“Small” challenges beyond elasticity

“Indentation is a very good method for testing mechanical properties,” Dao says, especially in cases where only small samples are available for testing. “When you try to develop new materials, you often have only a small quantity, and you can use indentation or nanoindentation to test really small quantities of materials,” he says.

Such testing can be quite accurate for elastic properties — that is, situations where the material bounces back to its original shape after having been poked. But when the applied force goes beyond the material’s “yield strength” — the point at which the poking leaves a lasting mark on the surface — this is called plastic deformation, and traditional indentation testing becomes much less accurate. “In fact, there's no widely available method that's being used” that can produce reliable information in such cases, Dao says.

Indentation can be used to determine hardness, but Dao explains that “hardness is only a combination of a material’s elastic and plastic properties. It's not a ‘clean’ parameter that can be used directly for design purposes. … But properties at or beyond yield strength, the strength denoting the point at which the material begins to deform irreversibly, are important to access the material’s suitability for engineering applications.”

Technique demands smaller amounts of high-quality data

The new method does not require any changes to experimental equipment or operation, but rather provides a way to work with the data to improve the accuracy of its predictions. By using an advanced neural network machine-learning system, the team found that a carefully planned integration of both real experimental data and computer-generated “synthetic” data of different levels of accuracy (a so-called multifidelity approach to deep learning) can produce the kind of quick and simple yet highly accurate data that industrial applications require for testing materials.

Traditional machine learning approaches require large amounts of high-quality data. However, detailed experiments on actual material samples "
89;89;towardsdatascience.com;https://towardsdatascience.com/data-augmentation-for-end-to-end-speech-translation-be4a1bd9ffbf?source=collection_home---4------4-----------------------;2020-04-19;Data Augmentation for End-to-End Speech Translation;"Data Augmentation for End-to-End Speech Translation

How to solve data scarcity with audio and text augmentation techniques

Photo by Alexander Sinn on Unsplash

End-to-end (or direct) speech translation is an approach to speech translation (ST) that is gaining high interest from the research world in the last few years. It consists in using a single deep learning model that learns to generate translated text of the input audio in an end-to-end fashion. Its surge in popularity is due to the scientific interest of achieving such a difficult task, but also to the expected effectiveness in practical applications.

The use of a single model is appealing for many reasons:

Decoding with a single model is faster than decoding with a pipeline of (at least) an automatic speech recognition (ASR) and a machine translation (MT) system. The absence of a transcription step can prevent the propagation of early bad decisions (error propagation), possibly resulting in superior quality. A single system is easier to manage and use than a cascaded system, and could save memory and GPU resources when deployed.

There is one big problem with this. End-to-end speech translation requires different data from ASR and MT, and while these two tasks have been studied for decades and have plenty of data at their disposal (at least for some languages), very little is publicly available for the new approach. In ST, we only have some hundreds of thousands segment pairs for the best-resourced languages, while a neural machine translation system (an easier task because the input is textual) is usually trained on tens or hundreds of millions of sentence pairs.

Then, how can we train a system with such small data? The answer so far is data augmentation: creating synthetic data through transformation of the existing data.

Data augmentation is a commonly used technique in deep learning, as this approach is famous to be data hungry and models can improve significantly their quality by adding data. The classic approach is to alter the input sample while keeping fixed its class label. One example for computer vision is to rotate images: the input image will be different as it is viewed by a different angle, but the content will be the same. A dog stays a dog and a cat stays a cat, though rotated. A practical example can be found here:

In this post, I want to focus on text and audio augmentation techniques that have been proposed for speech translation but can also be used for other tasks involving these types of data. Obviously, rotating and shifting pixels are two techniques that do not apply to text and audio, so we need more sophisticated techniques, sometimes involving the use of other machine learning systems.

Audio Augmentation

The first approach is similar to what happens with images: alter the input but keep the labels. A broadly used approach also by the community of ASR is speed perturbation, which consists in using a tool like SoX for perturbing the audio speed while keeping the translation fixed. It was used, for example, in the winning submission at IWSLT 2019 [1], an international workshop on speech translation that organizes a competition every year. One common practice is to multiply the speed (or the time duration, which is equivalent for our purpose) by a random factor in the range [0.9–1.1], which usually produces an audio that is still human-like but sounds different from the original. This kind of transformation is usually applied offline when the dataset is being built.

Another type of audio transformation happens online. Only some types of transformations can be applied online because current speech translation systems receive as input spectrograms, not wave forms. In order to occupy less disk space and save computational time during training, the datasets are usually stored as spectrograms, which require power to be computed and are more compact than the waveforms.

Wave form (above) and spectrogram (below) for the same utterance. The wave form is a time series, while the spectrogram is like a matrix: the x-axis is for time, the y-axis is for frequencies.

SpecAugment [2] is a popular spectrogram augmentation technique that was proposed last year and gathered a great success. It consists of three steps:

i) time warp is a complex and computational expensive algorithm that shifts a portion of the spectrogram along the time axis; ii) frequency masking applies a horizontal mask that covers some frequencies for the whole temporal dimension; iii) time masking applies a vertical mask that covers all the frequencies for some adjacent time steps. Time and frequency masking are the two most effective components of SpecAugment and basically force the model to predict the target sequence while being deaf to some frequencies or portions of the audio. The two types of masks have different width and positions at every iteration, so that the model learns to use them all.

SpecAugment applied to the original input (topmost image). F"
90;90;machinelearningmastery.com;https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/;2017-10-29;How to Develop a Deep Convolutional Neural Network for Sentiment Analysis (Text Classification);"from string import punctuation

from os import listdir

from numpy import array

from numpy import asarray

from numpy import zeros

from keras . preprocessing . text import Tokenizer

from keras . preprocessing . sequence import pad_sequences

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import Embedding

from keras . layers . convolutional import Conv1D

from keras . layers . convolutional import MaxPooling1D

# load doc into memory

def load_doc ( filename ) :

# open the file as read only

file = open ( filename , 'r' )

# read all text

text = file . read ( )

# close the file

file . close ( )

return text

# turn a doc into clean tokens

def clean_doc ( doc , vocab ) :

# split into tokens by white space

tokens = doc . split ( )

# remove punctuation from each token

table = str . maketrans ( '' , '' , punctuation )

tokens = [ w . translate ( table ) for w in tokens ]

# filter out tokens not in vocab

tokens = [ w for w in tokens if w in vocab ]

tokens = ' ' . join ( tokens )

return tokens

# load all docs in a directory

def process_docs ( directory , vocab , is_trian ) :

documents = list ( )

# walk through all files in the folder

for filename in listdir ( directory ) :

# skip any reviews in the test set

if is_trian and filename . startswith ( 'cv9' ) :

continue

if not is_trian and not filename . startswith ( 'cv9' ) :

continue

# create the full path of the file to open

path = directory + '/' + filename

# load the doc

doc = load_doc ( path )

# clean doc

tokens = clean_doc ( doc , vocab )

# add to list

documents . append ( tokens )

return documents

# load embedding as a dict

def load_embedding ( filename ) :

# load embedding into memory, skip first line

file = open ( filename , 'r' )

lines = file . readlines ( )

file . close ( )

# create a map of words to vectors

embedding = dict ( )

for line in lines :

parts = line . split ( )

# key is string word, value is numpy array for vector

embedding [ parts [ 0 ] ] = asarray ( parts [ 1 : ] , dtype = 'float32' )

return embedding

# create a weight matrix for the Embedding layer from a loaded embedding

def get_weight_matrix ( embedding , vocab ) :

# total vocabulary size plus 0 for unknown words

vocab_size = len ( vocab ) + 1

# define weight matrix dimensions with all 0

weight_matrix = zeros ( ( vocab_size , 100 ) )

# step vocab, store vectors using the Tokenizer's integer mapping

for word , i in vocab . items ( ) :

vector = embedding . get ( word )

if vector is not None :

weight_matrix [ i ] = vector

return weight_matrix

# load the vocabulary

vocab_filename = 'vocab.txt'

vocab = load_doc ( vocab_filename )

vocab = vocab . split ( )

vocab = set ( vocab )

# load all training reviews

positive_docs = process_docs ( 'txt_sentoken/pos' , vocab , True )

negative_docs = process_docs ( 'txt_sentoken/neg' , vocab , True )

train_docs = negative_docs + positive_docs

# create the tokenizer

tokenizer = Tokenizer ( )

# fit the tokenizer on the documents

tokenizer . fit_on_texts ( train_docs )

# sequence encode

encoded_docs = tokenizer . texts_to_sequences ( train_docs )

# pad sequences

max_length = max ( [ len ( s . split ( ) ) for s in train_docs ] )

Xtrain = pad_sequences ( encoded_docs , maxlen = max_length , padding = 'post' )

# define training labels

ytrain = array ( [ 0 for _ in range ( 900 ) ] + [ 1 for _ in range ( 900 ) ] )

# load all test reviews

positive_docs = process_docs ( 'txt_sentoken/pos' , vocab , False )

negative_docs = process_docs ( 'txt_sentoken/neg' , vocab , False )

test_docs = negative_docs + positive_docs

# sequence encode

encoded_docs = tokenizer . texts_to_sequences ( test_docs )

# pad sequences

Xtest = pad_sequences ( encoded_docs , maxlen = max_length , padding = 'post' )

# define test labels

ytest = array ( [ 0 for _ in range ( 100 ) ] + [ 1 for _ in range ( 100 ) ] )

# define vocabulary size (largest integer value)

vocab_size = len ( tokenizer . word_index ) + 1

# load embedding from file

raw_embedding = load_embedding ( 'glove.6B.100d.txt' )

# get vectors in the right order

embedding_vectors = get_weight_matrix ( raw_embedding , tokenizer . word_index )

# create the embedding layer

embedding_layer = Embedding ( vocab_size , 100 , weights = [ embedding_vectors ] , input_length = max_length , trainable = False )

# define model

model = Sequential ( )

model . add ( embedding_layer )

model . add ( Conv1D ( filters = 128 , kernel_size = 5 , activation = 'relu' ) )

model . add ( MaxPooling1D ( pool_size = 2 ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

print ( model . summary ( ) )

# compile network

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( Xtrain , ytrain , epochs = 10 , verbose = 2 )

# evaluate

loss , acc = model . evaluate ( Xtest , ytest , verbo"
91;91;machinelearningmastery.com;https://machinelearningmastery.com/multi-output-regression-models-with-python/;2020-03-26;How to Develop Multi-Output Regression Models with Python;"# linear regression for multioutput regression

from sklearn . datasets import make_regression

from sklearn . linear_model import LinearRegression

# create datasets

X , y = make_regression ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_targets = 2 , random_state = 1 )

# define model

model = LinearRegression ( )

# fit model

model . fit ( X , y )

# make a prediction

data_in = [ [ - 2.02220122 , 0.31563495 , 0.82797464 , - 0.30620401 , 0.16003707 , - 1.44411381 , 0.87616892 , - 0.50446586 , 0.23009474 , 0.76201118 ] ]

yhat = model . predict ( data_in )

# summarize prediction"
92;92;towardsdatascience.com;https://towardsdatascience.com/a-must-have-algorithm-for-your-machine-learning-toolbox-xgboost-3e295cf8d69b?source=collection_home---4------1-----------------------;2020-04-19;A Must-have Algorithm for Your Machine Learning Toolbox: XGBoost;"Image by WikiImages from Pixabay

One of the most performant machine learning algorithms

XGBoost is a supervised learning algorithm that can be used for both regression & classification. Like all algorithms, it has its virtues & draws, of which we’ll be sure to walk through.

For this post, we’ll just be learning about XGBoost from the context of classification problems. For the regression portion, be sure to keep an eye on my blog at datasciencelessons.com.

Supervised learning catch up

I won't dive in too deep here, but for those who need a quick refresher on supervised learning; supervised learning is when you have a specific thing in mind you’d like to predict. For instance, you want to predict future home prices; so you have what you want to predict, the next step would be labeling historic data as a means to predict the future. To dive deeper into this example; let's say you wanted to sell your home but wanted to know what price you should pay, you could potentially accumulate data points about the homes as well as what their sale price was during that same time period. From this point, you would train a model to which you would pass the datapoints about your own home to generate a prediction about the value of your home. For the classification example where your predicting class; let's say your Gmail and wanting to predict spam… this requires a model to train on many emails that were ‘labeled’ as spam as well as a corresponding amount that were not.

What exactly is it?

XGBoost uses what is called an ensemble method. Without going into too much detail on ensemble methods, what makes XGBoost unique is how it leverages the outputs of many models to generate its prediction! XGBoost makes use of what are known as many ‘weak learners’ to produce a ‘strong learner’.

The XGBoost process looks something like this:

It iteratively trains many weak models

Weights each prediction according to performance

Combines the many weighted predictions to come up with a final output.

What makes XGBoost so popular?

Accuracy

Speed

This algorithm makes good use of modern computation allows itself to be parallelized

Consistent outperformance of other algorithms

Your First XGBoost Model

Let's break down the steps!

You’ll start by importing the XGBoost package in python

Break out your dependent & independent variables using y & X respectively

Break out train test split

Instantiate your classifier

Train your classifier

Predict y for your test set

Assess accuracy!

For this example, we’ll be classifying survival on the titanic.

import xgboost as xgb from sklearn.model_selection

import train_test_split X, y = titanic.iloc[:,:-1], titanic.iloc[:,-1] X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=.2, random_state=44) xgb_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators= 7, seed=44) xgb_model.fit(X_train, y_train) pred = xgb_model.predict(X_test) accuracy = float(np.sum(pred == y_test)) / y_test.shape[0]

Well done! That was a great first pass! Let's learn a bit more about how to assess the quality of our model.

Assessing Performance

We’ve already seen accuracy in the code snippet above, but there are also the other aspects of a confusion matrix, precision & recall. I won't talk about those two here, but if you’d like more info, jump over to this post on the random forest algorithm: https://datasciencelessons.com/2019/08/13/random-forest-for-classification-in-r/

Apart from those, I’d like to talk about AUC.

To put it simply; if you chose one positive datapoint & one negative datapoint, AUC is the probability that the positive datapoint would be more highly ranked than the negative datapoint.

XGBoost allows you to run cross validation testing & to specify metrics you care about in the core algorithm call itself. This is partly done by creating a data structure called a dmatrix which consists of your X & y values.

The core difference this time is that we’ll create a dmatrix, specify parameters of our model, then generate an output where we specify the metric as AUC.

titanic_dm = xgb.DMatrix(data=X, label=y) params = {""objective"":""reg:logistic"", ""max_depth"":3} output = xgb.cv(dtrain=titanic_dm, params=params, nfold=3, num_boost_round=5, metrics=""auc"", as_pandas=True, seed=123) print((output[""test-auc-mean""]).iloc[-1])

How often should you use it?

XGBoost is not to be used every time you need to predict something. It can, however, be very useful in the right scenarios:

You have a lot of training data

You don’t only have categorical data, rather a good mix of numeric & categorical variables or just numeric variables.

You’ll definitely want to keep XGBoost away from computer vision & nlp related tasks… or if you have some very limited data.

As always, I hope this proves useful in your data science endeavors! Be sure to check out my other posts at datasciencelessons.com!

Happy Data Science-ing!"
93;93;machinelearningmastery.com;https://machinelearningmastery.com/multi-output-regression-models-with-python/#comments;2020-03-26;How to Develop Multi-Output Regression Models with Python;"# linear regression for multioutput regression

from sklearn . datasets import make_regression

from sklearn . linear_model import LinearRegression

# create datasets

X , y = make_regression ( n_samples = 1000 , n_features = 10 , n_informative = 5 , n_targets = 2 , random_state = 1 )

# define model

model = LinearRegression ( )

# fit model

model . fit ( X , y )

# make a prediction

data_in = [ [ - 2.02220122 , 0.31563495 , 0.82797464 , - 0.30620401 , 0.16003707 , - 1.44411381 , 0.87616892 , - 0.50446586 , 0.23009474 , 0.76201118 ] ]

yhat = model . predict ( data_in )

# summarize prediction"
94;94;news.mit.edu;http://news.mit.edu/2020/enzyme-square-dance-helps-generate-dna-building-blocks-0330;;Newly discovered enzyme “square dance” helps generate DNA building blocks;"How do you capture a cellular process that transpires in the blink of an eye? Biochemists at MIT have devised a way to trap and visualize a vital enzyme at the moment it becomes active — informing drug development and revealing how biological systems store and transfer energy.

The enzyme, ribonucleotide reductase (RNR), is responsible for converting RNA building blocks into DNA building blocks, in order to build new DNA strands and repair old ones. RNR is a target for anti-cancer therapies, as well as drugs that treat viral diseases like HIV/AIDS. But for decades, scientists struggled to determine how the enzyme is activated because it happens so quickly. Now, for the first time, researchers have trapped the enzyme in its active state and observed how the enzyme changes shape, bringing its two subunits closer together and transferring the energy needed to produce the building blocks for DNA assembly.

Before this study, many believed RNR’s two subunits came together and fit with perfect symmetry, like a key into a lock. “For 30 years, that’s what we thought,” says Catherine Drennan, an MIT professor of chemistry and biology and a Howard Hughes Medical Institute investigator. “But now, we can see the movement is much more elegant. The enzyme is actually performing a ‘molecular square dance,’ where different parts of the protein hook onto and swing around other parts. It’s really quite beautiful.”

Drennan and JoAnne Stubbe, professor emerita of chemistry and biology at MIT, are the senior authors on the study, which appeared in the journal Science on March 26. Former graduate student Gyunghoon ""Kenny"" Kang PhD ’19 is the lead author.

All proteins, including RNR, are composed of fundamental units known as amino acids. For over a decade, Stubbe’s lab has been experimenting with substituting RNR’s natural amino acids for synthetic ones. In doing so, the lab realized they could trap the enzyme in its active state and slow down its return to normal. However, it wasn’t until the Drennan lab gained access to a key technological advancement — cryo-electron microscopy — that they could snap high-resolution images of these “trapped” enzymes from the Stubbe lab and get a closer look.

“We really hadn’t done any cryo-electron microscopy at the point that we actively started trying to do the impossible: get the structure of RNR in its active state,” Drennan says. “I can’t believe it worked; I’m still pinching myself.”

The combination of these techniques allowed the team to visualize the complex molecular dance that allows the enzyme to transport the catalytic “firepower” from one subunit to the next, in order to generate DNA building blocks. This firepower is derived from a highly reactive unpaired electron (a radical), which must be carefully controlled to prevent damage to the enzyme.

According to Drennan, the team “wanted to see how RNR does the equivalent of playing with fire without getting burned.”

First author Kang says slowing down the radical transfer allowed them to observe parts of the enzyme no one had been able to see before in full. “Before this study, we knew this molecular dance was happening, but we’d never seen the dance in action,” he says. “But now that we have a structure for RNR in its active state, we have a much better idea about how the different components of the enzyme are moving and interacting in order to transfer the radical across long distances.”

Although this molecular dance brings the subunits together, there is still considerable distance between them: The radical must travel 35-40 angstroms from the first subunit to the second. This journey is roughly 10 times farther than the average radical transfer, according to Drennan. The radical must then travel back to its starting place and be stored safely, all within a fraction of a second before the enzyme returns to its normal conformation.

Because RNR is a target for drugs treating cancer and certain viruses, knowing its active-state structure could help researchers devise more effective treatments. Understanding the enzyme’s active state could also provide insight into biological electron transport for applications like biofuels. Drennan and Kang hope their study will encourage others to capture fleeting cellular events that have been difficult to observe in the past.

“We may need to reassess decades of past results,” Drennan says. “This study could open more questions than it answers; it’s more of a beginning than an end.”

This research was funded by the National Institutes of Health, a David H. Koch Graduate Fellowship, and the Howard Hughes Medical Institute."
95;95;news.mit.edu;http://news.mit.edu/2019/dr-martin-luther-king-jr-visiting-professors-and-scholars-program-1008;;Meet the 2019-20 MLK Visiting Professors and Scholars;"Founded in 1990, the Martin Luther King Jr. (MLK) Visiting Professors and Scholars Program honors the life and legacy of Martin Luther King by increasing the presence of, and recognizing the contributions of, underrepresented minority scholars at MIT. MLK Visiting Professors and Scholars enhance their scholarship through intellectual engagement with the MIT community and enrich the cultural, academic, and professional experience of students. The program hosts between four and eight scholars each year with financial and institutional support from the Office of the Provost and oversight from the Institute Community and Equity Office. Six scholars are visiting MIT this academic year as part of the program.

Kasso Okoudjou is returning for a second year as an MLK Visiting Professor in the Department of Mathematics. Originally from Benin, he moved to the United States in 1998 and earned a PhD in mathematics from Georgia Tech. Okoudjou joins MIT from the University of Maryland College Park, where he is a professor. His research interests include applied and pure harmonic analysis, especially time-frequency and time-scale analysis; frame theory; and analysis and differential equations on fractals. He is interested in broadening the participation of underrepresented minorities in (undergraduate) research in the mathematical sciences.

Matthew Schumaker joins MIT for another year in the Music and Theater Arts Section within the School of Humanities, Arts, and Social Sciences. Schumaker received his doctorate in music composition from the University of California at Berkeley. At MIT, he teaches a new course, 21M.380 (Composing for Solo Instrument and Live Electronics), a hands-on music technology composition seminar combining instrumental writing with real-time computer music. Additionally, The Radius Ensemble in Cambridge, Massachusetts has commissioned Schumaker to write a new piece of music that seeks to translate into music the vibrant, curved gestures and slashed markings in the abstract landscapes of celebrated Ethiopian-born painter Julie Mehretu.

Jamie Macbeth is visiting from Smith College, where he is an assistant professor in computer science. He received his PhD in computer science from University of California at Los Angeles. Although this is his first year as an MLK Visiting Scholar, he is not new to MIT, since he has been a visiting scientist since 2017. He is hosted by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). Macbeth’s research is focused on building and studying intelligent computing systems that demonstrate a human-like capability for in-depth understanding and production of natural language, and thus can achieve richer interactions with human users. He is especially keen on building systems that decompose the meaning of language into complex conceptual structures that reflect humans’ embodied cognition, memory, imagery and knowledge about social situations.

Ben McDonald has been a postdoc in the Department of Chemistry since 2018 and is now an MLK Visiting Scholar. McDonald received his PhD in synthetic organic chemistry from Northwestern University. His research focused on the total synthesis of flavonolignan natural products and the development of reverse-polarity carbon-carbon bond forming reactions. As a member of the department’s Chemistry Alliance for Inclusion and Diversity, he is focused on advancing diversity, equity and inclusion efforts. One of the initiatives he seeks to establish is a summer research program, which recruits talented future scientists from underrepresented backgrounds.

Tina Opie is an associate professor in the Management Division at Babson College. Opie obtained her PhD in management (with a concentration in organizational behavior) from New York University’s Stern School of Business. As an MLK Visiting Scholar in MIT Sloan School of Management, along with access to MIT’s Behavioral Research Lab, she is conducting research to develop the construct of Shared Sisterhood. “Shared Sisterhood examines how high-quality relationships (e.g., relationships characterized by trust, emotional vulnerability) between black, white, and Latinx women at work facilitate workplace inclusion and equity.” Though her work has a specific focus, people of all genders and racioethnic backgrounds can be “sisters” and can contribute to fostering a more inclusive work environment. Opie established Opie Consulting Group, a diversity-and-inclusion consultancy that incorporates Shared Sisterhood in creating inclusive workplaces.

Rhonda Williams, an MLK Visiting Professor hosted by the Department of History, joins MIT from Vanderbilt University, where she was recently appointed the John L. Seigenthaler Chair in American History. She is the founder of the Social Justice Institute at Case Western Reserve University. Her essay titled “Black Women Who Educate for Justice and Put Their Time, Lives, and Spirits on the Line"" was recently published in ""Black Women And Soc"
96;96;machinelearningmastery.com;http://machinelearningmastery.com/how-to-use-machine-learning-results/;2013-12-31;How to Use Machine Learning Results;"Tweet Share Share

Last Updated on June 7, 2016

Once you have found and tuned a viable model of your problem it is time to make use of that model. You may need to revisit your why and remind yourself what form you need a solution for the problem you are solving.

The problem is not addressed until you do something with the results. In this post you will learn tactics for presenting your results in answer to a question and considerations when turning your prototype model into a production system.

Depending on the type of problem you are trying to solve, the presentation of results will be very different. There are two main facets to making use of the results of your machine learning endeavor:

Report the results

Operationalize the system

Report Results

Once you have discovered a good model and a good enough result (or not, as the case may be), you will want to summarize what was learned and present it to stakeholders. This may be yourself, a client or the company for which you work.

Use a powerpoint template and address the sections listed below. You might like to write up a one-pager and use part section as a section header. Try to follow this process even on small experimental projects you do for yourself such as tutorials and competitions. It is easy to spend an inordinate number of hours on a project and you want to make sure to capture all the great things you learn along the way.

Below are the sections you can complete when reporting the results for a project.

Context (Why): Define the environment in which the problem exists and set up the motivation for the research question.

(Why): Define the environment in which the problem exists and set up the motivation for research question. Problem (Question): Concisely describe the problem as a question that you went out and answered.

(Question): Concisely describe the problem as a question that you went out and answered. Solution (Answer): Concisely describe the solution as an answer to the question you posed in the previous section. Be specific.

(Answer): Concisely describe the solution as an answer to the question you posed in the previous section. Be specific. Findings : Bulleted lists of discoveries you made along the way that interest the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey.

: Bulleted lists of discoveries you made along the way that interest the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey. Limitations : Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel.

: Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel. Conclusions (Why+Question+Answer): Revisit the why, research question and the answer you discovered in a tight little package that is easy to remember and repeat for yourself and others.

The type of audience you are presenting to will define the amount of detail you go into. Having the discipline to complete your projects with a report on results, even on small side projects will accelerate your learning in field. On such small side projects, I highly recommend sharing results of side projects on blogs or with communities and elicit feedback that you can capture and take with you into the start of your next project.

Operationalize

You have found a model that is good enough at addressing the problem you face that you would like to put it into production. This may be an operational installation on your workstation in the case of a fun side project, all the way up to integrating the model into an existing enterprise application. The scope is enormous. In this section you will learn three key aspects to operationalizing a model that you could consider carefully before putting a system into production.

Three areas that you should think carefully about are the algorithm implementation, the automated testing of your model and the tracking of the models performance through time. These three issues will very likely influence the type of model you choose.

Algorithm Implementation

It is likely that you were using a research library to discover the best performing method. The algorithm implementations in research libraries can be excellent, but they can also be written for the general case of the problem rather than the specific case with which you are working.

Think very hard about the dependencies and technical debt you may be creating by putting such an implementation directly into production. Consider locating a production-level library that supports the method you wish to use. You may have to repeat the process o"
97;97;machinelearningmastery.com;https://machinelearningmastery.com/youre-wrong-machine-learning-not-hard/;2018-03-27;Why Machine Learning Does Not Have to Be So Hard;"Tweet Share Share

Last Updated on July 13, 2019

Technical topics like mathematics, physics, and even computer science are taught using a bottom-up approach.

This approach involves laying out the topics in an area of study in a logical way with a natural progression in complexity and capability.

The problem is, humans are not robots executing a learning program. We require motivation, excitement, and most importantly, a connection of the topic to tangible results.

Useful skills we use every day like reading, driving, and programming were not learned this way and were in fact learned using an inverted top-down approach. This top-down approach can be used to learn technical subjects directly such as machine learning, which can make you a lot more productive a lot sooner, and be a lot of fun.

In this post, you will discover the concrete difference between the top-down and bottom-up approaches to learning technical material and why this is the approach that practitioners should use to learn machine learning and even related mathematics.

After reading this post, you will know:

The bottom-up approach used in universities to teach technical subjects and the problems with it.

How people learn to read, drive, and program in a top-down manner and how the top-down approach works.

The frame of machine learning and even mathematics using the top-down approach to learning and how to start to make rapid progress as a practitioner.

Let’s get started.

Overview

This is an important blog post, because I think it can really help to shake you out of the bottom-up, university-style way of learning machine learning.

This post is divided into seven parts; they are:

Bottom-Up Learning Learning to Read Learning to Drive Learning to Code Top-Down Learning Learn Machine Learning Learning Mathematics

Bottom-Up Learning

Take a field of study, such as mathematics.

There is a logical way to lay out the topics in mathematics that build on each other and lead through a natural progression in skills, capability, and understanding.

The problem is, this logical progression might only make sense to those who are already on the other side and can intuit the relationships between the topics.

Most of school is built around this bottom-up natural progression through material. A host of technical and scientific fields of study are taught this way.

Think back to high-school or undergraduate studies and the fundamental fields you may have worked through: examples such as:

Mathematics, as mentioned.

Biology.

Chemistry.

Physics.

Computer Science.

Think about how the material was laid out, week-by-week, semester-by-semester, year-by-year. Bottom-up, logical progression.

The problem is, the logical progression through the material may not be the best way to learn the material in order to be productive.

We are not robots executing a learning program. We are emotional humans that need motivation, interest, attention, encouragement, and results.

You can learn technical subjects from the bottom-up, and a small percentage of people do prefer things this way, but it is not the only way.

Now, if you have completed a technical subject, think back to how to you actually learned it. I bet it was not bottom-up.

Learning to Read

Think back; how did you learn to read?

My son is starting to read. Without thinking too much, here are the general techniques he’s using (really the school and us as parents):

Start by being read to in order to generate interest and show benefits.

Get the alphabet down and making the right sounds.

Memorize the most frequent words, their sounds, and how to spell them.

Learn the “spell-out-the-word” heuristic to deal with unknown words.

Read through books with supervision.

Read through books without supervision.

It is important that he continually knows why reading is important, connected to very tangible things he wants to do, like:

Read captions on TV shows.

Read stories on topics he loves, like Star Wars.

Read signs and menus when we are out and about.

So on…

It is also important that he gets results that he can track and in which he can see improvement.

Larger vocabulary.

Smoother reading style

Books of increasing complexity.

Here’s how he did not learn to read:

Definitions of word types (verbs, nouns, adverbs, etc.)

Rules of grammar.

Rules of punctuation.

Theory of human languages.

Learning to Drive

Do you drive?

It’s cool if you don’t, but most adults do out of necessity. Society and city design is built around personal mobility.

How did you learn to drive?

I remember some written tests and maybe a test on a computer. I have no memory of studying for them, though I very likely did. Here’s what I do remember.

I remember hiring a driving instructor and doing driving lessons. Every single lesson was practical, in the car, practicing the skill I was required to master, driving the vehicle in traffic.

Here’s what I did not study or discuss with my driving instructor:

The history of "
98;98;machinelearningmastery.com;http://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/;2016-09-08;A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

Gradient boosting is one of the most powerful techniques for building predictive models.

In this post you will discover the gradient boosting machine learning algorithm and get a gentle introduction into where it came from and how it works.

After reading this post, you will know:

The origin of boosting from learning theory and AdaBoost.

How gradient boosting works including the loss function, weak learners and the additive model.

How to improve performance over the base algorithm with various regularization schemes.

Discover how to configure, fit, tune and evaluation gradient boosting models with XGBoost in my new book, with 15 step-by-step tutorial lessons, and full python code.

Let’s get started.

Need help with XGBoost in Python? Take my free 7-day email course and discover configuration, tuning and more (with sample code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

The Origin of Boosting

The idea of boosting came out of the idea of whether a weak learner can be modified to become better.

Michael Kearns articulated the goal as the “Hypothesis Boosting Problem” stating the goal from a practical standpoint as:

… an efficient algorithm for converting relatively poor hypotheses into very good hypotheses

— Thoughts on Hypothesis Boosting [PDF], 1988

A weak hypothesis or weak learner is defined as one whose performance is at least slightly better than random chance.

These ideas built upon Leslie Valiant’s work on distribution free or Probability Approximately Correct (PAC) learning, a framework for investigating the complexity of machine learning problems.

Hypothesis boosting was the idea of filtering observations, leaving those observations that the weak learner can handle and focusing on developing new weak learns to handle the remaining difficult observations.

The idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified. … Note, however, it is not obvious at all how this can be done

— Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World, page 152, 2013

AdaBoost the First Boosting Algorithm

The first realization of boosting that saw great success in application was Adaptive Boosting or AdaBoost for short.

Boosting refers to this general problem of producing a very accurate prediction rule by combining rough and moderately inaccurate rules-of-thumb.

— A decision-theoretic generalization of on-line learning and an application to boosting [PDF], 1995

The weak learners in AdaBoost are decision trees with a single split, called decision stumps for their shortness.

AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns.

This means that samples that are difficult to classify receive increasing larger weights until the algorithm identifies a model that correctly classifies these samples

— Applied Predictive Modeling, 2013

Predictions are made by majority vote of the weak learners’ predictions, weighted by their individual accuracy. The most successful form of the AdaBoost algorithm was for binary classification problems and was called AdaBoost.M1.

You can learn more about the AdaBoost algorithm in the post:

Generalization of AdaBoost as Gradient Boosting

AdaBoost and related algorithms were recast in a statistical framework first by Breiman calling them ARCing algorithms.

Arcing is an acronym for Adaptive Reweighting and Combining. Each step in an arcing algorithm consists of a weighted minimization followed by a recomputation of [the classifiers] and [weighted input].

— Prediction Games and Arching Algorithms [PDF], 1997

This framework was further developed by Friedman and called Gradient Boosting Machines. Later called just gradient boosting or gradient tree boosting.

The statistical framework cast boosting as a numerical optimization problem where the objective is to minimize the loss of the model by adding weak learners using a gradient descent like procedure.

This class of algorithms were described as a stage-wise additive model. This is because one new weak learner is added at a time and existing weak learners in the model are frozen and left unchanged.

Note that this stagewise strategy is different from stepwise approaches that readjust previously entered terms when new ones are added.

— Greedy Function Approximation: A Gradient Boosting Machine [PDF], 1999

The generalization allowed arbitrary differentiable loss functions to be used, expanding the technique beyond binary classification problems to support regression, multi-class classification and more.

How Gradient Boosting Works

Gradient boosting involves "
99;99;news.mit.edu;http://news.mit.edu/2020/solve-pandemic-challenge-covid-0413;;MIT Solve rises to meet health security and pandemic challenge;"In the face of the Covid-19 pandemic, MIT Solve launched a new Global Challenge: How can communities around the world prepare for, detect, and respond to emerging pandemics and health security threats? Solve’s mission is to foster innovation, seeking out tech-based social entrepreneurs and helping them scale up their ideas.

The new Solve Health Security and Pandemics Challenge is designed to produce both short-term solutions to the impact of the current pandemic as well as longer-term strategies for future crises. “The reason that this pandemic is costing so many lives is that we were unprepared,” says Pooja Wagh ’06, Solve’s director of health community and results measurement. “We need stronger health care supply chains and better disease surveillance. This will happen again, and we need to be better positioned to mitigate the impact on human lives.”

MIT Solve hopes to leverage its extensive community to identify tech innovations that will make a difference. “We had a role to play because of our massive network of innovators, member organizations, MIT students and faculty, and all the people we reach through our communications,” says Wagh.

“Solve was built on the ethos that great ideas can come from anywhere,” she explains. “The idea was to democratize access to the resources we have at MIT, since a lack of resources keeps many great solutions from coming to fruition.”

There are several rounds of review in the Solve selection process, with 50 to 60 experts from both inside and outside MIT reading every application. “We draw on the experience of our community,” says Wagh. “Decision makers are a panel of judges with deep expertise in the relevant areas. For this challenge, judges will include representatives from public health agencies and corporations.”

In addition to the new health challenge, Solve issued its 2020 Global Challenges earlier this year related to jobs and entrepreneurship among marginalized populations, sustainable food systems, maternal and newborn health, and access to education for marginalized girls and women.

“We solicit applications from people all over the world,” says Wagh. “Our network of more than 150,000 people includes centers at MIT as well as 130 Solver teams and more than 120 Solve Members with a wide diversity of interests. What binds them together is their motivation to help our innovators succeed.” A small number of Solver teams are chosen and then matched with partners, including funders, who can help them make their ideas a reality.

“Given the immediacy and urgency of this crisis, we want to build a pathway for scale for all the Solver teams we select,” Wagh says. “We’ve been talking to potential partners and funders, and dozens of people have reached out to offer support for the future Solver teams that are selected. Others want to help us choose the teams. People should know that we’re looking for both applicants and supporters — there is a role for anyone who wants to engage.”

There has been an enthusiastic response to the new challenge, with applications pouring in. Individuals and groups with a tech-based solution to health security threats are invited to submit a proposal by June 18, while potential supporters of the teams chosen can investigate options for partnership with Solve."
100;100;machinelearningmastery.com;http://machinelearningmastery.com/what-is-holding-you-back-from-your-machine-learning-goals/;2014-06-03;What Is Holding You Back From Your Machine Learning Goals?;"Tweet Share Share

Last Updated on December 24, 2016

Identify and Tackle Your Self-Limiting Beliefs and

Finally Make Progress

I get a lot of email from developers and students looking to get started in machine learning.

The first question I ask them is what is stopping them from getting started?

I try to get to the heart of what they are struggling with, and almost always it is a self-limiting belief that has halted their progress.

In this post, I want to touch on some self-limiting beliefs I see crop up in my email exchanges and discussions with coaching students.

Maybe you will see yourself in one or more of these beliefs. If so, I urge you to challenge your assumptions.

Self-Limiting Belief

A self-limiting belief is something that you assume to be true that is limiting your progress. You presuppose something about yourself or about the thing you want to achieve. The problem is you hold that belief to be true and you don’t question it.

Steve Pavlina lists 3 types of self-limiting beliefs in is post: Dissolving Limiting Beliefs:

If-then Beliefs : e.g. If I get started in machine learning, I will fail because I am not good enough.

: e.g. If I get started in machine learning, I will fail because I am not good enough. Universal Beliefs : e.g. All Data Scientists have a Ph.D. and are mathematics rock gods.

: e.g. All Data Scientists have a Ph.D. and are mathematics rock gods. Personal and Self-Esteem Beliefs: e.g. I’m not good enough to be a machine learner.

You’re probably a logical and rational thinker. Apply those skills to your own beliefs about your goals and aspirations in machine learning and challenge them.

Waiting To Get Started

I think the biggest class of limiting belief I see is the belief that you cannot get started until you have some specific prior knowledge. The problem is that the prior knowledge you think you need is either not required or is so vast in scope that even experts in that subject don’t know it all.

For example: “I need to KNOW statistics“. See how ambiguous that belief is. How much statistics, what areas of statistics and why do you need to know them before you can start your investigation into machine learning?

Below are some of the more common self-limiting beliefs of skills or prior knowledge that must be obtained before you can get started in machine learning.

I can’t get into machine learning until…

…I get a degree or higher degree

…I complete a course

…I am good at linear algebra

…I know statistics and probability theory

…I have mastered the R programming language

You can get started in machine learning today, right now. Run your first classifier in 5 minutes. You’re in. Now, start blocking out what it is from machine learning that you really want?

I have written about some of these before, for example:

Awaiting Perfect Conditions

Another class of self-limiting belief is where you are waiting for the perfect environment or conditions before taking the leap. Things will never be perfect, leap and make a mess, then leap again.

I can’t get started in machine learning because…

…I don’t have the time right now

…I don’t have a fast CPU, GPU or a bazillion MB of RAM

…I am just a student right now

…I am not a good programmer at the moment

…I am very busy at work right now

It does take a lot of time and effort to get good at machine learning, but not all at once and not all at the beginning.

You can make good progress with a few hours a week, or tens of minutes per day. There are plenty of small snack-sized tasks you could take on to get started in machine learning. You can get started, it is just going to take some sacrifice, like all good things in life.

Struggling or Tried and Failed

The third class of limiting belief is that where you have made a small start but you are struggling or have failed to achieve your goal.

This is a tough one. Machine learning is hard but no harder than other technical skills like programming. It takes persistence and dedication. It’s applied and empirical and demands trial and error.

I can’t get into machine learning because…

…I feel overwhelmed

…I don’t understand x

…I will never be as good as y

…I don’t know what to do next

…I can’t get my program to work

My advice is to cut scope or change direction. I advocate small projects as often as I can because the methodology has been so successful for me.

What is your self-limiting belief?

Do you have a self-limiting belief? Think about it. What are your goals and why do you think you are not there yet?

Do you have a goal to get into machine learning, to become a data scientist or a machine learning engineer but have not taken the first step?

Are you waiting to acquire some perfect set of skills before getting started?

Are you waiting for the perfect conditions before getting started?

Have you taken a first step and abandoned the trail?

Where do you want to be and what are you struggling with?"
101;101;machinelearningmastery.com;http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/;2015-08-18;8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset;"Tweet Share Share

Last Updated on January 14, 2020

Has this happened to you?

You are working on your dataset. You create a classification model and get 90% accuracy immediately. “Fantastic” you think. You dive a little deeper and discover that 90% of the data belongs to one class. Damn!

This is an example of an imbalanced dataset and the frustrating results it can cause.

In this post you will discover the tactics that you can use to deliver great results on machine learning datasets with imbalanced data.

Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more in my new book, with 30 step-by-step tutorials and full Python source code.

Let’s get started.

Coming To Grips With Imbalanced Data

I get emails about class imbalance all the time, for example:

I have a binary classification problem and one class is present with 60:1 ratio in my training set. I used the logistic regression and the result seems to just ignores one class.

And this:

I am working on a classification model. In my dataset I have three different labels to be classified, let them be A, B and C. But in the training dataset I have A dataset with 70% volume, B with 25% and C with 5%. Most of time my results are overfit to A. Can you please suggest how can I solve this problem?

I write long lists of techniques to try and think about the best ways to get past this problem. I finally took the advice of one of my students:

Perhaps one of your upcoming blog posts could address the problem of training a model to perform against highly imbalanced data, and outline some techniques and expectations.

Frustration!

Imbalanced data can cause you a lot of frustration.

You feel very frustrated when you discovered that your data has imbalanced classes and that all of the great results you thought you were getting turn out to be a lie.

The next wave of frustration hits when the books, articles and blog posts don’t seem to give you good advice about handling the imbalance in your data.

Relax, there are many options and we’re going to go through them all. It is possible, you can build predictive models for imbalanced data.

Want to Get Started With Imbalance Classification? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

What is Imbalanced Data?

Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally.

For example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.

This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.

You can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either.

The remaining discussions will assume a two-class classification problem because it is easier to think about and describe.

Imbalance is Common

Most classification data sets do not have exactly equal number of instances in each class, but a small difference often does not matter.

There are problems where a class imbalance is not just common, it is expected. For example, in datasets like those that characterize fraudulent transactions are imbalanced. The vast majority of the transactions will be in the “Not-Fraud” class and a very small minority will be in the “Fraud” class.

Another example is customer churn datasets, where the vast majority of customers stay with the service (the “No-Churn” class) and a small minority cancel their subscription (the “Churn” class).

When there is a modest class imbalance like 4:1 in the example above it can cause problems.

Accuracy Paradox

The accuracy paradox is the name for the exact situation in the introduction to this post.

It is the case where your accuracy measures tell the story that you have excellent accuracy (such as 90%), but the accuracy is only reflecting the underlying class distribution.

It is very common, because classification accuracy is often the first measure we use when evaluating models on our classification problems.

Put it All On Red!

What is going on in our models when we train on an imbalanced dataset?

As you might have guessed, the reason we get 90% accuracy on an imbalanced data (with 90% of the instances in Class-1) is because our models look at the data and cleverly decide that the best thing to do is to always predict “Class-1” and achieve high accuracy.

This is best seen when using a simple rule based algorithm. If you print out the rule in the final model you will see that it is very likely predicting one class regardless of the data it is asked to predict.

8 Tactics To Combat Imbalanced Training Data

We now understand what class imbalance is and why it provides misleading classific"
102;102;machinelearningmastery.com;https://machinelearningmastery.com/how-to-get-started-with-generative-adversarial-networks-7-day-mini-course/;2019-07-10;How to Get Started With Generative Adversarial Networks (7-Day Mini-Course);". . .

# define the discriminator model

model = Sequential ( )

# downsample to 14x14

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , input_shape = ( 28 , 28 , 3 ) ) )

model . add ( BatchNormalization ( ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample to 7x7

model . add ( Conv2D ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( BatchNormalization ( ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# classify

model . add ( Flatten ( ) )"
103;103;machinelearningmastery.com;http://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/;2016-12-13;Basic Feature Engineering With Time Series Data in Python;"# create date time features of a dataset

from pandas import read_csv

from pandas import DataFrame

series = read_csv ( 'daily-minimum-temperatures.csv' , header = 0 , index_col = 0 , parse_dates = True , squeeze = True )

dataframe = DataFrame ( )

dataframe [ 'month' ] = [ series . index [ i ] . month for i in range ( len ( series ) ) ]

dataframe [ 'day' ] = [ series . index [ i ] . day for i in range ( len ( series ) ) ]

dataframe [ 'temperature' ] = [ series [ i ] for i in range ( len ( series ) ) ]"
104;104;machinelearningmastery.com;https://machinelearningmastery.com/truncated-backpropagation-through-time-in-keras/;2017-06-27;How to Prepare Sequence Prediction for Truncated BPTT in Keras;"Tweet Share Share

Last Updated on August 14, 2019

Recurrent neural networks are able to learn the temporal dependence across multiple timesteps in sequence prediction problems.

Modern recurrent neural networks like the Long Short-Term Memory, or LSTM, network are trained with a variation of the Backpropagation algorithm called Backpropagation Through Time. This algorithm has been modified further for efficiency on sequence prediction problems with very long sequences and is called Truncated Backpropagation Through Time.

An important configuration parameter when training recurrent neural networks like LSTMs using Truncated Backpropagation Through Time is deciding how many timesteps to use as input. That is, how exactly to split up your very long input sequences into subsequences in order to get the best performance.

In this post, you will discover 6 different ways you can split up very long input sequences to effectively train recurrent neural networks using Truncated Backpropagation Through Time in Python with Keras.

After reading this post, you will know:

What Truncated Backpropagation Through Time is and how it has been implemented in the Python deep learning library Keras.

How exactly the choice of the number of input timesteps affects learning within recurrent neural networks.

6 different techniques you can use to split up your very long sequence prediction problems to make best use of the Truncated Backpropagation Through Time training algorithm.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Truncated Backpropagation Through Time

Backpropagation is the training algorithm used to update the weights in a neural network in order to minimize the error between the expected output and the predicted output for a given input.

For sequence prediction problems where there is an order dependence between observations, recurrent neural networks are used instead of classical feed-forward neural networks. Recurrent neural networks are trained using a variation of the Backpropagation algorithm called Backpropagation Through Time, or BPTT for short.

In effect, BPTT unrolls the recurrent neural network and propagates the error backward over the entire input sequence, one timestep at a time. The weights are then updated with the accumulated gradients.

BPTT can be slow to train recurrent neural networks on problems with very long input sequences. In addition to speed, the accumulation of gradients over so many timesteps can result in a shrinking of values to zero, or a growth of values that eventually overflow, or explode.

A modification of BPTT is to limit the number of timesteps used on the backward pass and in effect estimate the gradient used to update the weights rather than calculate it fully.

This variation is called Truncated Backpropagation Through Time, or TBPTT.

The TBPTT training algorithm has two parameters:

k1 : Defines the number of timesteps shown to the network on the forward pass.

: Defines the number of timesteps shown to the network on the forward pass. k2: Defines the number of timesteps to look at when estimating the gradient on the backward pass.

As such, we can use the notation TBPTT(k1, k2) when considering how to configure the training algorithm, where k1 = k2 = n, where n is the input sequence length for classical non-truncated BPTT.

Impact of TBPTT Configuration on the RNN Sequence Model

Modern recurrent neural networks like LSTMs can use their internal state to remember over very long input sequences. Such as over thousands of timesteps.

This means that the configuration of TBPTT does not necessarily define the memory of the network that you are optimizing with the choice of the number of timesteps. You can choose when the internal state of the network is reset separately from the regime used to update network weights.

Instead, the choice of TBPTT parameters influences how the network estimates the error gradient used to update the weights. More generally, the configuration defines the number of timesteps from which the network may be considered to model your sequence problem.

We can state this formally as something like:

yhat(t) = f(X(t), X(t-1), X(t-2), ... X(t-n)) 1 yhat(t) = f(X(t), X(t-1), X(t-2), ... X(t-n))

Where yhat is the output for a specific timestep, f(…) is the relationship that the recurrent neural network is approximating, and X(t) are observations at specific timesteps.

It is conceptually similar (but quite different in practice) to the window size on Multilayer Perceptrons trained on time series problems or to the p and q parameters of linear time series models like ARIMA. The TBPTT defines the scope of the input sequence for the model during training.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and a"
105;105;news.mit.edu;http://news.mit.edu/2020/why-workers-smaller-share-GDP-0311;;Why are workers getting smaller pieces of the pie?;"It’s one of the biggest economic changes in recent decades: Workers get a smaller slice of company revenue, while a larger share is paid to capital owners and distributed as profits. Or, as economists like to say, there has been a fall in labor’s share of gross domestic product, or GDP.

A new study co-authored by MIT economists uncovers a major reason for this trend: Big companies that spend more on capital and less on workers are gaining market share, while smaller firms that spend more on workers and less on capital are losing market share. That change, the researchers say, is a key reason why the labor share of GDP in the U.S. has dropped from around 67 percent in 1980 to 59 percent today, following decades of stability.

“To understand this phenomenon, you need to understand the reallocation of economic activity across firms,” says MIT economist David Autor, co-author of the paper. “That’s our key point.”

To be sure, many economists have suggested other hypotheses, including new generations of software and machines that substitute directly for workers, the effects of international trade and outsourcing, and the decline of labor union power. The current study does not entirely rule out all of those explanations, but it does highlight the importance of what the researchers term “superstar firms” as a primary factor.

“We feel this is an incredibly important and robust fact pattern that you have to grapple with,” adds Autor, the Ford Professor of Economics in MIT’s Department of Economics.

The paper, “The Fall of the Labor Share and the Rise of Superstar Firms,” appears in advance online form in the Quarterly Journal of Economics. In addition to Autor, the other authors are David Dorn, a professor of economics at the University of Zurich; Lawrence Katz, a professor of economics at Harvard University; Christina Patterson, PhD ’19, a postdoc at Northwestern University who will join the faculty at the University of Chicago’s Booth School of Business in July; and John Van Reenen, the Gordon Y. Billard Professor of Management and Economics at MIT.

An economic “miracle” vanishes

For much of the 20th century, labor’s share of GDP was notably consistent. As the authors note, John Maynard Keynes once called it “something of a miracle” in the face of economic changes, and the British economist Nicholas Kaldor included labor’s steady portion of GDP as one of his often-cited six “stylized facts” of growth.

To conduct the study, the researchers scrutinized data for the U.S. and other countries in the Organization of Economic Cooperation and Development (OECD). The scholars used U.S. Economic Census data from 1982 to 2012 to study six economic sectors that account for about 80 percent of employment and GDP: manufacturing, retail trade, wholesale trade, services, utilities and transportation, and finance. The data includes payroll, total output, and total employment.

The researchers also used information from the EU KLEMS database, housed at the Vienna Institute for International Economic Studies, to examine the other OECD countries.

The increase in market dominance for highly competitive top firms in many of those sectors is evident in the data. In the retail trade, for instance, the top four firms accounted for just under 15 percent of sales in 1981, but that grew to around 30 percent of sales in 2011. In utilities and transportation, those figures moved from 29 percent to 41 percent in the same time frame. In manufacturing, this top-four sales concentration grew from 39 percent in 1981 to almost 44 percent in 2011.

At the same time, the average payroll-to-sales ratio declined in five of those sectors — with finance being the one exception. In manufacturing, the payroll-to-sales ratio decreased from roughly 18 percent in 1981 to about 12 percent in 2011. On aggregate, the labor share of GDP declined at most times except the period from 1997 to 2002, the final years of an economic expansion with high employment.

But surprisingly, labor’s share is not falling at the typical firm. Rather, reallocation of market share between firms is the key. In general, says Autor, the picture is of a “winner-take-most setting, where a smaller number of firms are accounting for a larger amount of economic activity, and those are firms where workers historically got a smaller share of the pie.”

A key insight provided by the study is that the dynamics within industry sectors has powered the drop in the labor share of GDP. The overall change is not just the result of, say, an increase in the deployment of technology in manufacturing, which some economists have suggested. While manufacturing is important to the big picture, the same phenomenon is unfolding across and within many sectors of the economy.

As far as testing the remaining alternate hypotheses, the study found no special pattern within industries linked to changes in trade policy — a subject Autor has studied extensively in the past. And while the decline in union pow"
106;106;machinelearningmastery.com;https://machinelearningmastery.com/promise-of-deep-learning-for-computer-vision/;2019-03-25;A Gentle Introduction to the Promise of Deep Learning for Computer Vision;"Tweet Share Share

Last Updated on July 5, 2019

The promise of deep learning in the field of computer vision is better performance by models that may require more data but less digital signal processing expertise to train and operate.

There is a lot of hype and large claims around deep learning methods, but beyond the hype, deep learning methods are achieving state-of-the-art results on challenging problems. Notably, on computer vision tasks such as image classification, object recognition, and face detection.

In this post, you will discover the specific promises that deep learning methods have for tackling computer vision problems.

After reading this post, you will know:

The promises of deep learning for computer vision.

Examples of where deep learning has or is delivering on its promises.

Key deep learning methods and applications for computer vision.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Promises of Deep Learning Types of Deep Learning Networks Models Types of Computer Vision Problems

Promises of Deep Learning

Deep learning methods are popular, primarily because they are delivering on their promise.

That is not to say that there is no hype around the technology, but that the hype is based on very real results that are being demonstrated across a suite of very challenging artificial intelligence problems from computer vision and natural language processing.

Some of the first large demonstrations of the power of deep learning were in computer vision, specifically image recognition. More recently in object detection and face recognition.

In this post, we will look at five specific promises of deep learning methods in the field of computer vision.

In summary, they are:

The Promise of Automatic Feature Extraction . Features can be automatically learned and extracted from raw image data.

. Features can be automatically learned and extracted from raw image data. The Promise of End-to-End Models . Single end-to-end models can replace pipelines of specialized models.

. Single end-to-end models can replace pipelines of specialized models. The Promise of Model Reuse . Learned features and even entire models can be reused across tasks.

. Learned features and even entire models can be reused across tasks. The Promise of Superior Performance . Techniques demonstrate better skill on challenging tasks.

. Techniques demonstrate better skill on challenging tasks. The Promise of General Method. A single general method can be used on a range of related tasks.

We will now take a closer look at each.

There are other promises of deep learning for computer vision; these were just the five that I chose to highlight.

What do you think the promise of deep learning is for computer vision?

Let me know in the comments below.

Promise 1: Automatic Feature Extraction

A major focus of study in the field of computer vision is on techniques to detect and extract features from digital images.

Extracted features provide the context for inference about an image, and often the richer the features, the better the inference.

Sophisticated hand-designed features such as scale-invariant feature transform (SIFT), Gabor filters, and histogram of oriented gradients (HOG) have been the focus of computer vision for feature extraction for some time, and have seen good success.

The promise of deep learning is that complex and useful features can be automatically learned directly from large image datasets. More specifically, that a deep hierarchy of rich features can be learned and automatically extracted from images, provided by the multiple deep layers of neural network models.

They have deeper architectures with the capacity to learn more complex features than the shallow ones. Also the expressivity and robust training algorithms allow to learn informative object representations without the need to design features manually.

— Object Detection with Deep Learning: A Review, 2018.

Deep neural network models are delivering on this promise, most notably demonstrated by the transition away from sophisticated hand-crafted feature detection methods such as SIFT toward deep convolutional neural networks on standard computer vision benchmark datasets and competitions, such as the ImageNet Large Scale Visual Recognition Competition (ILSVRC).

ILSVRC over the past five years has paved the way for several breakthroughs in computer vision. The field of categorical object recognition has dramatically evolved […] starting from coded SIFT features and evolving to large-scale convolutional neural networks dominating at all three tasks of image classification, single-object localization, and object detection.

— ImageNet Large Scale Visual Recognition Challenge, 2015.

Want Results with Deep Learning for Compute"
107;107;machinelearningmastery.com;http://machinelearningmastery.com/learning-vector-quantization-for-machine-learning/;2016-04-17;Learning Vector Quantization for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

A downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset.

The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that lets you choose how many training instances to hang onto and learns exactly what those instances should look like.

In this post you will discover the Learning Vector Quantization algorithm. After reading this post you will know:

The representation used by the LVQ algorithm that you actually save to a file.

The procedure that you can use to make predictions with a learned LVQ model.

How to learn an LVQ model from training data.

The data preparation to use to get the best performance from the LVQ algorithm.

Where to look for more information on LVQ.

This post was written for developers and assumes no background in statistics or mathematics. The post focuses on how the algorithm works and how to use it for predictive modeling problems.

If you have any questions on LVQ, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

LVQ Model Representation

The representation for LVQ is a collection of codebook vectors.

LVQ was developed and is best understood as a classification algorithm. It supports both binary (two-class) and multi-class classification problems.

A codebook vector is a list of numbers that have the same input and output attributes as your training data. For example, if your problem is a binary classification with classes 0 and 1, and the inputs width, length height, then a codebook vector would be comprised of all four attributes: width, length, height and class.

The model representation is a fixed pool of codebook vectors, learned from the training data. They look like training instances, but the values of each attribute have been adapted based on the learning procedure.

In the language of neural networks, each codebook vector may be called a neuron, each attribute on a codebook vector is called a weight and the collection of codebook vectors is called a network.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Making Predictions with an LVQ Model

Predictions are made using the LVQ codebook vectors in the same way as K-Nearest Neighbors.

Predictions are made for a new instance (x) by searching through all codebook vectors for the K most similar instances and summarizing the output variable for those K instances. For classification this is the mode (or most common) class value.

Typically predictions are made with K=1, and the codebook vector that matches is called the Best Matching Unit (BMU).

To determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance. Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) for each attribute j.

EuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )

Learning an LVQ Model From Data

The LVQ algorithm learns the codebook vectors from the training data.

You must choose the number of codebook vectors to use, such as 20 or 40. You can find the best number of codebook vectors to use by testing different configurations on your training dataset.

The learning algorithm starts with a pool of random codebook vectors. These could be randomly selected instances from the training data, or randomly generated vectors with the same scale as the training data. Codebook vectors have the same number of input attributes as the training data. They also have an output class variable.

The instances in the training dataset are processed one at a time. For a given training instance, the most similar codebook vector is selected from the pool.

If the codebook vector has the same output as the training instance, the codebook vector is moved closer to the training instance. If it does not match, it is moved further away. The amount that the vector is moved is controlled by an algorithm parameter called the learning_rate.

For example, the input variable (x) of a codebook vector is moved closer to the training input value (t) by the amount in the learning_rate if the classes match as follows:

x = x + learning_rate * (t – x)

The opposite case of moving the input variables of a codebook variable away from a training instance is calculated as:

x = x – learning_rate * (t – x)

This would be repeated for each input variable.

Because one codebook vector is selected for modification for each training instance the algo"
108;108;machinelearningmastery.com;http://machinelearningmastery.com/ladder-approach-to-becoming-a-machine-learning-consultant/;2015-03-02;Get Paid To Apply Machine Learning;"Tweet Share Share

Last Updated on September 27, 2016

The Ladder Approach That You Can Use To Become a

Machine Learning Consultant

Do you want to do machine learning and get paid for it?

Be careful what you wish for.

In this post I outline a blueprint that you can use to learn enough machine learning to help small businesses and start-ups with their general data needs.

It’s not easy, you will have to work hard outside of your comfort zone. You will have to talk to real people in the real world!

Blueprint

The blueprint presented in this post will take you from a passionate interest in machine learning and the dedication to learn through to being capable and confident to work through the general data problems in a small to medium business or start-up and deliver a solution.

The blueprint for this path is as follows:

Build a foundation Build a portfolio Deliver solutions

Given your background and interests, you can tailor the roadmap to your needs.

To be clear, we are only interested in applied machine learning. We are only interested in theory and tools as much as they allow you to better understand your problem and achieve better results on the problem you are working.

This is a counter-intuitive but very productive view. Learn what you need just-in-time and focus on delivering results. It is about reliably achieving good results, not perfection.

1. Build a Foundation

You need to learn enough applied machine learning to have the confidence to work a problem from start to finish. To define it accurately and deliver a model or report required as an outcome for the project.

Pick and learn a process. Learn a step-by-step process that you can follow that will take you from problem definition through to delivering a result. Some examples include KDD, Crisp-DM, OSEMN, and others. Pick and learn a tool. Learn a tool or libraries that you can use to complete your selected process. I recommend one of Weka, scikit-learn, R depending on your interests and preference. Practice on small datasets. Download small datasets on which you can practice. Spend a lot of time on the UCI ML repository.

You are ready to move on when you are confident and capable enough to pick an arbitrary in-memory problem and use your tool to work it from start to finish.

2. Build a Portfolio

Once you have a foundation capability to work problems you need objective indicators that others can use to evaluate your capability. You need completed projects that demonstrate your ability to deliver.

You can do this by building a portfolio of completed machine learning projects.

Interlude on Mindset

Pause for a moment and take on the mindset of a manager or small business owner with a data problem.

As such a person, you are hiring programmers based on their ability to deliver results on project at other companies and in open source. You are hiring marketers based on their ability to lift conversions to attack the bottomline. If such a manager needed a “data guy” to deliver a report or a model, what would they look at to evaluate that a candidate could deliver a result?

Me in that position, I would want to see evidence of completed projects. More than that, I would want to see evidence of completed projects that are very close to the result I am looking for.

Your Portfolio

Pick a theme. This is the type of projects that you want to work on. A no-brainer would be reports on customer data (high-value customers, predictions of prospects that convert, etc.). Find open datasets. You need to locate datasets that you can practice on that are close to or on your theme. Look on competition websites like Kaggle and KDDCup as a starting point. There are a lot of public access datasets these days that you can practice on! Complete projects. Treat each dataset like a project with a client and apply your process to it in order to deliver a result. This may require you to assume the role of the client and take an educated guess as to the outcome they are looking for (model or report on a specific question, etc.) Write-up. Write-up your findings as a semi-formal work product and host it publicly online.

This last point is key and I will elaborate it.

Ideally, make each part of your process scripted so that you can re-execute it any time as you find bugs or gain insight. Consider uploading all of your code and scripts to a public github account for the project.

Write up the result of each project as a technical report or a power point. Consider recording a short video presenting your findings. Host the report on github, your blog, or somewhere. Write up the project on your public LinkedIn profile.

Your goal is to have a place that you can point someone and they can see all of the projects you have completed at a glance, and dive down into one and see what you did and what you delivered.

You are ready to move on when you can objectively convince someone that you are able to deliver results on your theme. I think 3-5 modest sized completed proj"
109;109;machinelearningmastery.com;http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/;2016-08-18;How to Develop Your First XGBoost Model in Python with scikit-learn;"# First XGBoost model for Pima Indians dataset

from numpy import loadtxt

from xgboost import XGBClassifier

from sklearn . model_selection import train_test_split

from sklearn . metrics import accuracy_score

# load data

dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" )

# split data into X and y

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# split data into train and test sets

seed = 7

test_size = 0.33

X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = test_size , random_state = seed )

# fit model no training data

model = XGBClassifier ( )

model . fit ( X_train , y_train )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )"
110;110;news.mit.edu;http://news.mit.edu/2020/psfc-receives-arpa-e-funding-to-explore-practical-fusion-paths-0415;;Plasma Science and Fusion Center receives $1.25M from ARPA-E to explore practical paths to fusion;"The Plasma Science and Fusion Center (PSFC) will receive $1.25 million in funding from the U.S. Department of Energy’s Advanced Research Projects Agency-Energy (ARPA-E). The award is part of the $32 million Breakthrough Enabling Thermonuclear-fusion Energy program established to explore lower-cost approaches to creating energy from nuclear fusion.

Fusion requires confining plasmas at extraordinarily high temperatures, up to 200 million degrees Celsius. One of the most promising ways to heat plasmas to these temperatures is with electromagnetic waves. Fusion power plants will need tens of thousands of kilowatts to be launched and absorbed by the plasma — with each kilowatt roughly the power of a conventional microwave oven. Complex analytic theory and computer simulations are required to design effective and efficient plasma heating scenarios. MIT’s project seeks to apply established state-of-the-art theoretical and simulation tools, developed and tested by the fusion community on more traditional concepts like tokamaks and stellarators, to explore the potential of novel, lower-cost fusion concepts.

The principal investigator for the project, PSFC Principal Research Scientist John Wright, along with PSFC Principal Research Scientist Abhay Ram, comprise the MIT team that will be working with colleagues from Oak Ridge National Laboratory, Lawrence Livermore National Laboratory, and CompX, a private company. They are excited about leveraging MIT radio-frequency (RF) heating expertise in the service of new, or renewed, approaches to fusion.

“The main fusion program pursues the tokamak and the stellarator,” says Wright, “but there are other magnetic geometries considered by the program in the past that have been set aside due to difficulties at the time. They may deserve a second look, to see if any of these have a shorter or cheaper path to fusion. These include the mirror concept, for which there are two ARPA-E experimental awards.”

While a tokamak confines hot plasma in a toroidal chamber, using high magnetic fields to steer it away from the interior walls, a mirror contains the plasma in a linear device that features higher-strength magnets at each end. The PSFC will help explore the mirror’s potential both as a fusion concept and as a source of neutrons, which are needed to study the effect of neutrons on materials used in long-lived fusion or fission devices.

The PSFC will work closely with two other projects funded in this ARPA-E funding round — one on magnetic mirrors led by the University of Wisconsin at Madison, and another led by Commonwealth Fusion Systems (CFS) on fast-ramping high-temperature superconducting solenoids. The PSFC is a significant collaborator on the CFS-led project, responsible for testing the performance of the novel magnets built by CFS. If successfully demonstrated, these magnets will reduce the cost and complexity of commercial tokamak power plants, further accelerating the advent of commercial fusion. An immediate application for these magnets would be for the SPARC project, now under design by a joint team from MIT and CFS. The collaboration with the University of Wisconsin grew out of discussions about applications of high-temperature superconducting magnets to other fusion experimental configurations.

“The tools we have developed in the tokamak community can help other fusion concepts, including the mirror, to do as well as possible,” says Wright. “We are excited to take our knowledge of RF in tokamak and stellarator geometries and see what surprises there are as we travel other paths to fusion.”"
111;111;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/;2019-05-16;How to Classify Photos of Dogs and Cats (with 97% accuracy);"Tweet Share Share

Last Updated on October 3, 2019

Develop a Deep Convolutional Neural Network Step-by-Step to Classify Photographs of Dogs and Cats

The Dogs vs. Cats dataset is a standard computer vision dataset that involves classifying photos as either containing a dog or cat.

Although the problem sounds simple, it was only effectively addressed in the last few years using deep learning convolutional neural networks. While the dataset is effectively solved, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch.

This includes how to develop a robust test harness for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.

In this tutorial, you will discover how to develop a convolutional neural network to classify photos of dogs and cats.

After completing this tutorial, you will know:

How to load and prepare photos of dogs and cats for modeling.

How to develop a convolutional neural network for photo classification from scratch and improve model performance.

How to develop a model for photo classification using transfer learning.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Updated Oct/2019: Updated for Keras 2.3 and TensorFlow 2.0.

Tutorial Overview

This tutorial is divided into six parts; they are:

Dogs vs. Cats Prediction Problem Dogs vs. Cats Dataset Preparation Develop a Baseline CNN Model Develop Model Improvements Explore Transfer Learning How to Finalize the Model and Make Predictions

Dogs vs. Cats Prediction Problem

The dogs vs cats dataset refers to a dataset used for a Kaggle machine learning competition held in 2013.

The dataset is comprised of photos of dogs and cats provided as a subset of photos from a much larger dataset of 3 million manually annotated photos. The dataset was developed as a partnership between Petfinder.com and Microsoft.

The dataset was originally used as a CAPTCHA (or Completely Automated Public Turing test to tell Computers and Humans Apart), that is, a task that it is believed a human finds trivial, but cannot be solved by a machine, used on websites to distinguish between human users and bots. Specifically, the task was referred to as “Asirra” or Animal Species Image Recognition for Restricting Access, a type of CAPTCHA. The task was described in the 2007 paper titled “Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization“.

We present Asirra, a CAPTCHA that asks users to identify cats out of a set of 12 photographs of both cats and dogs. Asirra is easy for users; user studies indicate it can be solved by humans 99.6% of the time in under 30 seconds. Barring a major advance in machine vision, we expect computers will have no better than a 1/54,000 chance of solving it.

— Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization, 2007.

At the time that the competition was posted, the state-of-the-art result was achieved with an SVM and described in a 2007 paper with the title “Machine Learning Attacks Against the Asirra CAPTCHA” (PDF) that achieved 80% classification accuracy. It was this paper that demonstrated that the task was no longer a suitable task for a CAPTCHA soon after the task was proposed.

… we describe a classifier which is 82.7% accurate in telling apart the images of cats and dogs used in Asirra. This classifier is a combination of support-vector machine classifiers trained on color and texture features extracted from images. […] Our results suggest caution against deploying Asirra without safeguards.

— Machine Learning Attacks Against the Asirra CAPTCHA, 2007.

The Kaggle competition provided 25,000 labeled photos: 12,500 dogs and the same number of cats. Predictions were then required on a test dataset of 12,500 unlabeled photographs. The competition was won by Pierre Sermanet (currently a research scientist at Google Brain) who achieved a classification accuracy of about 98.914% on a 70% subsample of the test dataset. His method was later described as part of the 2013 paper titled “OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.”

The dataset is straightforward to understand and small enough to fit into memory. As such, it has become a good “hello world” or “getting started” computer vision dataset for beginners when getting started with convolutional neural networks.

As such, it is routine to achieve approximately 80% accuracy with a manually designed convolutional neural network and 90%+ accuracy using transfer learning on this task.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and"
112;112;machinelearningmastery.com;https://machinelearningmastery.com/types-of-classification-in-machine-learning/#comments;2020-04-07;4 Types of Classification Tasks in Machine Learning;"# example of binary classification task

from numpy import where

from collections import Counter

from sklearn . datasets import make_blobs

from matplotlib import pyplot

# define dataset

X , y = make_blobs ( n_samples = 1000 , centers = 2 , random_state = 1 )

# summarize dataset shape

print ( X . shape , y . shape )

# summarize observations by class label

counter = Counter ( y )

print ( counter )

# summarize first few examples

for i in range ( 10 ) :

print ( X [ i ] , y [ i ] )

# plot the dataset and color the by class label

for label , _ in counter . items ( ) :

row_ix = where ( y == label ) [ 0 ]

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( label ) )

pyplot . legend ( )"
113;113;machinelearningmastery.com;http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/;2016-06-26;Handwritten Digit Recognition using Convolutional Neural Networks in Python with Keras;"Tweet Share Share

Last Updated on September 13, 2019

A popular demonstration of the capability of deep learning techniques is object recognition in image data.

The “hello world” of object recognition for machine learning and deep learning is the MNIST dataset for handwritten digit recognition.

In this post you will discover how to develop a deep learning model to achieve near state of the art performance on the MNIST handwritten digit recognition task in Python using the Keras deep learning library.

After completing this tutorial, you will know:

How to load the MNIST dataset in Keras.

How to develop and evaluate a baseline neural network model for the MNIST problem.

How to implement and evaluate a simple Convolutional Neural Network for MNIST.

How to implement a close to state-of-the-art deep learning model for MNIST.

Discover how to develop deep learning models for a range of predictive modeling problems with just a few lines of code in my new book, with 18 step-by-step tutorials and 9 projects.

Let’s get started.

Update Oct/2016 : Updated for Keras 1.1.0, TensorFlow 0.10.0 and scikit-learn v0.18.

: Updated for Keras 1.1.0, TensorFlow 0.10.0 and scikit-learn v0.18. Update Mar/2017 : Updated for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0.

: Updated for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0. Update Sep/2019: Updated for Keras 2.2.5 API.

Note, for an extended version of this tutorial see:

Description of the MNIST Handwritten Digit Recognition Problem

The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes and Christopher Burges for evaluating machine learning models on the handwritten digit classification problem.

The dataset was constructed from a number of scanned document dataset available from the National Institute of Standards and Technology (NIST). This is where the name for the dataset comes from, as the Modified NIST or MNIST dataset.

Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required.

Each image is a 28 by 28 pixel square (784 pixels total). A standard split of the dataset is used to evaluate and compare models, where 60,000 images are used to train a model and a separate set of 10,000 images are used to test it.

It is a digit recognition task. As such there are 10 digits (0 to 9) or 10 classes to predict. Results are reported using prediction error, which is nothing more than the inverted classification accuracy.

Excellent results achieve a prediction error of less than 1%. State-of-the-art prediction error of approximately 0.2% can be achieved with large Convolutional Neural Networks. There is a listing of the state-of-the-art results and links to the relevant papers on the MNIST and other datasets on Rodrigo Benenson’s webpage.

Need help with Deep Learning in Python? Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Loading the MNIST dataset in Keras

The Keras deep learning library provides a convenience method for loading the MNIST dataset.

The dataset is downloaded automatically the first time this function is called and is stored in your home directory in ~/.keras/datasets/mnist.pkl.gz as a 15MB file.

This is very handy for developing and testing deep learning models.

To demonstrate how easy it is to load the MNIST dataset, we will first write a little script to download and visualize the first 4 images in the training dataset.

# Plot ad hoc mnist instances from keras.datasets import mnist import matplotlib.pyplot as plt # load (downloaded if needed) the MNIST dataset (X_train, y_train), (X_test, y_test) = mnist.load_data() # plot 4 images as gray scale plt.subplot(221) plt.imshow(X_train[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(X_train[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(X_train[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(X_train[3], cmap=plt.get_cmap('gray')) # show the plot plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Plot ad hoc mnist instances from keras . datasets import mnist import matplotlib . pyplot as plt # load (downloaded if needed) the MNIST dataset ( X_train , y_train ) , ( X_test , y_test ) = mnist . load_data ( ) # plot 4 images as gray scale plt . subplot ( 221 ) plt . imshow ( X_train [ 0 ] , cmap = plt . get_cmap ( 'gray' ) ) plt . subplot ( 222 ) plt . imshow ( X_train [ 1 ] , cmap = plt . get_cmap ( 'gray' ) ) plt . subplot ( 223 ) plt . imshow ( X_train [ 2 ] , cmap = plt . get_cmap ( 'gray' ) ) plt . subplot ( 224 ) plt . imshow ( X_train [ 3 ] , cmap = plt . get_cmap ( 'gray' ) ) # show the plot plt . show ( )

You can see that downloading and loading the MNIST dataset is as easy as calling the mnist.loa"
114;114;machinelearningmastery.com;https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/;2020-04-09;Stacking Ensemble Machine Learning With Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65

# compare ensemble to each baseline classifier from numpy import mean from numpy import std from sklearn . datasets import make_classification from sklearn . model_selection import cross_val_score from sklearn . model_selection import RepeatedStratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . neighbors import KNeighborsClassifier from sklearn . tree import DecisionTreeClassifier from sklearn . svm import SVC from sklearn . naive_bayes import GaussianNB from sklearn . ensemble import StackingClassifier from matplotlib import pyplot # get the dataset def get_dataset ( ) : X , y = make_classification ( n_samples = 1000 , n_features = 20 , n_informative = 15 , n_redundant = 5 , random_state = 1 ) return X , y # get a stacking ensemble of models def get_stacking ( ) : # define the base models level0 = list ( ) level0 . append ( ( 'lr' , LogisticRegression ( ) ) ) level0 . append ( ( 'knn' , KNeighborsClassifier ( ) ) ) level0 . append ( ( 'cart' , DecisionTreeClassifier ( ) ) ) level0 . append ( ( 'svm' , SVC ( ) ) ) level0 . append ( ( 'bayes' , GaussianNB ( ) ) ) # define meta learner model level1 = LogisticRegression ( ) # define the stacking ensemble model = StackingClassifier ( estimators = level0 , final_estimator = level1 , cv = 5 ) return model # get a list of models to evaluate def get_models ( ) : models = dict ( ) models [ 'lr' ] = LogisticRegression ( ) models [ 'knn' ] = KNeighborsClassifier ( ) models [ 'cart' ] = DecisionTreeClassifier ( ) models [ 'svm' ] = SVC ( ) models [ 'bayes' ] = GaussianNB ( ) models [ 'stacking' ] = get_stacking ( ) return models # evaluate a give model using cross-validation def evaluate_model ( model ) : cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 ) scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' ) return scores # define dataset X , y = get_dataset ( ) # get the models to evaluate models = get_models ( ) # evaluate the models and store results results , names = list ( ) , list ( ) for name , model in models . items ( ) : scores = evaluate_model ( model ) results . append ( scores ) names . append ( name ) print ( '>%s %.3f (%.3f)' % ( name , mean ( scores ) , std ( scores ) ) ) # plot model performance for comparison pyplot . boxplot ( results , labels = names , showmeans = True ) pyplot . show ( )"
115;115;machinelearningmastery.com;http://machinelearningmastery.com/better-understand-machine-learning-data-weka/;2016-06-29;How to Better Understand Your Machine Learning Data in Weka;"Tweet Share Share

Last Updated on August 22, 2019

It is important to take your time to learn about your data when starting on a new machine learning problem.

There are key things that you can look at to very quickly learn more about your dataset, such as descriptive statistics and data visualizations.

In this post you will discover how you can learn more about your data in the Weka machine learning workbench my reviewing descriptive statistics and visualizations of your data.

After reading this post you will know about:

The distribution of attributes from reviewing statistical summaries.

The distribution of attributes from reviewing univariate plots.

The relationship between attributes from reviewing multivariate plots.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started

Better Understand Your Data With Descriptive Statistics

The Weka explorer will automatically calculate descriptives statistics for numerical attributes.

Open The Weka GUI Chooser. Click “Explorer” to open the Weka Explorer. Load the Pima Indians datasets from data/diabetes.arff

The Pima Indians dataset contains numeric input variables that we can use to demonstrate the calculation of descriptive statistics.

Firstly, note that the dataset summary in the “Current Relation” section. This panel summarizes the following details about the loaded datasets:

Dataset name (relation).

The number of rows (instances).

The number of columns (attributes).

Click on the first attribute in the dataset in the “Attributes” panel.

Take note of the details in the “Selected attribute” panel. It lists a lot of information about the selected attribute, such as:

The name of the attribute.

The number of missing values and the ratio of missing values across the whole dataset.

The number of distinct values.

The data type.

The table below lists a number of descriptive statistics and their values. A useful four number summary is provided for numeric attributes including:

Minimum value.

Maximum value.

Mean value.

Standard deviation.

You can learn a lot from this information. For example:

The presence and ratio of missing data can give you an indication of whether or not you need to remove or impute values.

The mean and standard deviation give you a quantified idea of the spread of data for each attribute.

The number of distinct values can give you an idea of the granularity of the attribute distribution.

Click the class attribute. This attribute has a nominal type. Review the “Selected attribute panel”.

We can now see that for nominal attributes that we are provided with a list of each category and the count of instances that belong to each category. There is also mention of weightings, which we can ignore for now. This is used if we want to assign more or less weight to specific attribute values or instances in the dataset.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Univariate Attribute Distributions

The distribution of each attribute can be plotted to give a visual qualitative understanding of the distribution.

Weka provides these plots automatically when you select an attribute in the “Preprocess” tab.

We can follow on from the previous section where we already have the Pima Indians dataset loaded.

Click on the “preg” attribute in the “Attributes panel” and note the plot below the “Selected attribute” panel. You will see the distribution of preg values between 0 and 17 along the x-axis. The y-axis shows the count or frequency of values with each preg value.

Note the red and blue colors referring to the positive and negative classes respectively. The colors are assigned automatically to each categorical value. If there were three categories for the class value, we would see the breakdown of the preg distribution by three colors rather than two.

This is useful to get a quick idea of whether the problem is easily separable for a given attribute, e.g. all the red and blue are cleanly separated for a single attribute. Clicking through each attribute in the list of Attributes and reviewing the plots, we can see that there is no such easy separation of the classes.

We can quickly get an overview of the distribution of all attributes in the dataset and the breakdown of distributions by class by clicking the “Visualize All” button above the univariate plot.

Looking at these plots we can see a few interesting things about this dataset.

It looks like the plas, pres and mass attributes have a nearly Gaussian distribution.

It looks likes pres, skin, insu and mass have values at 0 that look out of place.

Looking at plots like this and jotting down things that come to mind can give you an idea of furthe"
116;116;machinelearningmastery.com;https://machinelearningmastery.com/probability-for-machine-learning-7-day-mini-course/;2019-10-02;Probability for Machine Learning (7-Day Mini-Course);"Tweet Share Share

Last Updated on January 10, 2020

Probability for Machine Learning Crash Course.

Get on top of the probability used in machine learning in 7 days.

Probability is a field of mathematics that is universally agreed to be the bedrock for machine learning.

Although probability is a large field with many esoteric theories and findings, the nuts and bolts, tools and notations taken from the field are required for machine learning practitioners. With a solid foundation of what probability is, it is possible to focus on just the good or relevant parts.

In this crash course, you will discover how you can get started and confidently understand and implement probabilistic methods used in machine learning with Python in seven days.

This is a big and important post. You might want to bookmark it.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Update Jan/2020: Updated for changes in scikit-learn v0.22 API.

Who Is This Crash-Course For?

Before we get started, let’s make sure you are in the right place.

This course is for developers that may know some applied machine learning. Maybe you know how to work through a predictive modeling problem end-to-end, or at least most of the main steps, with popular tools.

The lessons in this course do assume a few things about you, such as:

You know your way around basic Python for programming.

You may know some basic NumPy for array manipulation.

You want to learn probability to deepen your understanding and application of machine learning.

You do NOT need to be:

A math wiz!

A machine learning expert!

This crash course will take you from a developer that knows a little machine learning to a developer who can navigate the basics of probabilistic methods.

Note: This crash course assumes you have a working Python3 SciPy environment with at least NumPy installed. If you need help with your environment, you can follow the step-by-step tutorial here:

Crash-Course Overview

This crash course is broken down into seven lessons.

You could complete one lesson per day (recommended) or complete all of the lessons in one day (hardcore). It really depends on the time you have available and your level of enthusiasm.

Below is a list of the seven lessons that will get you started and productive with probability for machine learning in Python:

Lesson 01 : Probability and Machine Learning

: Probability and Machine Learning Lesson 02 : Three Types of Probability

: Three Types of Probability Lesson 03 : Probability Distributions

: Probability Distributions Lesson 04 : Naive Bayes Classifier

: Naive Bayes Classifier Lesson 05 : Entropy and Cross-Entropy

: Entropy and Cross-Entropy Lesson 06 : Naive Classifiers

: Naive Classifiers Lesson 07: Probability Scores

Each lesson could take you 60 seconds or up to 30 minutes. Take your time and complete the lessons at your own pace. Ask questions and even post results in the comments below.

The lessons expect you to go off and find out how to do things. I will give you hints, but part of the point of each lesson is to force you to learn where to go to look for help on and about the statistical methods and the NumPy API and the best-of-breed tools in Python. (Hint: I have all of the answers directly on this blog; use the search box.)

Post your results in the comments; I’ll cheer you on!

Hang in there; don’t give up.

Note: This is just a crash course. For a lot more detail and fleshed-out tutorials, see my book on the topic titled “Probability for Machine Learning.”

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Lesson 01: Probability and Machine Learning

In this lesson, you will discover why machine learning practitioners should study probability to improve their skills and capabilities.

Probability is a field of mathematics that quantifies uncertainty.

Machine learning is about developing predictive modeling from uncertain data. Uncertainty means working with imperfect or incomplete information.

Uncertainty is fundamental to the field of machine learning, yet it is one of the aspects that causes the most difficulty for beginners, especially those coming from a developer background.

There are three main sources of uncertainty in machine learning; they are:

Noise in observations , e.g. measurement errors and random noise.

, e.g. measurement errors and random noise. Incomplete coverage of the domain , e.g. you can never observe all data.

, e.g. you can never observe all data. Imperfect model of the problem, e.g. all models have errors, some are useful.

Uncertainty in applied machine learning is managed using probability.

Probability and statistics help us to understand and quantify the expected v"
117;117;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/;2018-08-09;How to Develop a Skillful Machine Learning Time Series Forecasting Model;"Tweet Share Share

Last Updated on August 5, 2019

You are handed data and told to develop a forecast model.

What do you do?

This is a common situation; far more common than most people think.

Perhaps you are sent a CSV file.

Perhaps you are given access to a database.

Perhaps you are starting a competition.

The problem can be reasonably well defined:

You have or can access historical time series data.

You know or can find out what needs to be forecasted.

You know or can find out how what is most important in evaluating a candidate model.

So how do you tackle this problem?

Unless you have been through this trial by fire, you may struggle.

You may struggle because you are new to the fields of machine learning and time series.

You may struggle even if you have machine learning experience because time series data is different.

You may struggle even if you have a background in time series forecasting because machine learning methods may outperform the classical approaches on your data.

In all of these cases, you will benefit from working through the problem carefully and systematically.

In this post, I want to give you a specific and actionable procedure that you can use to work through your time series forecasting problem.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Process Overview

The goal of this process is to get a “good enough” forecast model as fast as possible.

This process may or may not deliver the best possible model, but it will deliver a good model: a model that is better than a baseline prediction, if such a model exists.

Typically, this process will deliver a model that is 80% to 90% of what can be achieved on the problem.

The process is fast. As such, it focuses on automation. Hyperparameters are searched rather than specified based on careful analysis. You are encouraged to test suites of models in parallel, rapidly getting an idea of what works and what doesn’t.

Nevertheless, the process is flexible, allowing you to circle back or go as deep as you like on a given step if you have the time and resources.

This process is divided into four parts; they are:

Define Problem Design Test Harness Test Models Finalize Model

You will notice that the process is different from a classical linear work-through of a predictive modeling problem. This is because it is designed to get a working forecast model fast and then slow down and see if you can get a better model.

What is your process for working through a new time series forecasting problem?

Share it below in the comments.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Use This Process

The biggest mistake is skipping steps.

For example, the mistake that almost all beginners make is going straight to modeling without a strong idea of what problem is being solved or how to robustly evaluate candidate solutions. This almost always results in a lot of wasted time.

Slow down, follow the process, and complete each step.

I recommend having separate code for each experiment that can be re-run at any time.

This is important so that you can circle back when you discover a bug, fix the code, and re-run an experiment. You are running experiments and iterating quickly, but if you are sloppy, then you cannot trust any of your results. This is especially important when it comes to the design of your test harness for evaluating candidate models.

Let’s take a closer look at each step of the process.

1. Define Problem

Define your time series problem.

Some topics to consider and motivating questions within each topic are as follows:

Inputs vs. Outputs What are the inputs and outputs for a forecast? Endogenous vs. Exogenous What are the endogenous and exogenous variables? Unstructured vs. Structured Are the time series variables unstructured or structured? Regression vs. Classification Are you working on a regression or classification predictive modeling problem? What are some alternate ways to frame your time series forecasting problem? Univariate vs. Multivariate Are you working on a univariate or multivariate time series problem? Single-step vs. Multi-step Do you require a single-step or a multi-step forecast? Static vs. Dynamic Do you require a static or a dynamically updated model?

Answer each question even if you have to estimate or guess.

Some useful tools to help get answers include:

Data visualizations (e.g. line plots, etc.).

Statistical analysis (e.g. ACF/PACF plots, etc.).

Domain experts.

Project stakeholders.

Update your answers to these questions as you learn more.

2. Design Test Harness

Design a test harness that you can use to evaluate candidate models.

This includes both the method used to est"
118;118;news.mit.edu;http://news.mit.edu/2020/3-questions-kenda-mutongi-africa-women-power-human-decency-0114;2020-03-19;3 Questions: Professor Kenda Mutongi on Africa, women, power — and human decency;"MIT Professor Kenda Mutongi teaches courses in African history, world history, and gender history, and serves on the MIT Africa Working Group. She is the author of two award-winning books: “Matatu: A History of Popular Transportation in Nairobi” (University of Chicago Press, 2017) and “Worries of the Heart: Widows, Family, and Community in Kenya” (University of Chicago Press, 2007). The latter book explores how widows, a marginalized group in Kenya, weathered the country’s transition to a post-colonial society and found novel ways to address their collective social, economic, and political problems.

Mutongi, born and raised in rural Kenya, recently spoke with SHASS Communications on the interplay of African history and current gender issues within the United States; the importance of multidisciplinary thinking in solving the world's concerns; and the wider implications of her research.

Q: In “Worries of the Heart” you examine how Kenyan widows have navigated the country’s changing power structure. What lessons can be drawn from this history, two years into the #MeToo movement?

A: In that book, I argue that the patriarchal structure of society in rural western Kenya established the expectation that women would be dependent on men. Widows were generally considered weak and helpless, and yet they found ways to get their needs met.

To ensure that men supported them, widows presented their grievances to the men in public so that they could be witnessed by the whole community. By airing their grievances publicly, widows were able to put the men in a vulnerable position. Men who refused to help widows were emasculated in the eyes of others, because strong, respectable men were expected to take good care of women — especially widows.

In essence, the widows turned the very gender norms that were designed to oppress them to their own advantage. While the Kenyan widows did not overturn the patriarchal structure, they managed to refashion it to their advantage and garner the resources they needed to raise their children.

Today, I see something similar taking place in the #MeToo movement, which encourages women who have survived sexual assault and harassment to go public and expose those who have assaulted them. The assumption is that this going public will empower the women, but also send a strong message to the men about the serious consequences of their behavior. By going public, the #MeToo women are doing exactly what widows in Kenya did: They are — at least partly — trying to modify men’s behavior by shaming them, and in the process they are empowering themselves.

Reflecting on this history is important because it underscores a central truth of power dynamics: There are many ways in which populations that are not in power can make gains, even when existing societal structures are heavily stacked against them.



Q: Your latest book, “Matatu: A History of Popular Transportation in Nairobi,” provides a window into African entrepreneurship that serves as a counterpoint to the idea that African businesses need help from outsiders. What other myths do you think most need to be dispelled about Africa’s countries, cultures, and peoples?

A: One myth is that Africans are not capable of mechanical or scientific innovation. Africans are just as innovative as anybody; they just lack the capital to turn their discoveries into something bigger.

This is similar to the idea that Africans lack the entrepreneurial spirit, which can be debunked by examples such as that of Nairobi’s matatu industry. Matatus are minibuses used as shared private taxis. The success of the matatu drivers reflects wholly African entrepreneurship: The industry is thriving without any governmental or outside development assistance.

Unfortunately, Africans are often underestimated. In terms of scientific innovation, I think of my aunt, who was an herbalist when I was growing up in rural western Kenya. She searched the fields and found many herbs that could cure family members of stomach aches, flu, skin rashes, coughs, and colds. Rich drug companies such as Pfizer and Unilever use some of these same herbs — yellow dye root, rosy periwinkle, and Asiatic pennywort — in their pharmaceuticals.

My aunt certainly knew the medical significance of the herbs. What Africans like my aunt too often lack are the resources to turn their specialized knowledge into what is accepted as science in the global context. This is an important topic and one that has thoroughly been addressed in the work of my colleagues [MIT Associate Professor] Clapperton Chakanetsa Mavhunga and [MIT Program in Science, Technology and Science graduate student] Jia-Hui Lee.

We need to start taking all forms of African knowledge seriously, because until we do, we will continue to lose out on the contributions of a large swath of humanity.

Q: MIT President L. Rafael Reif has said that solving the great challenges of our time will require multidisciplinary problem-solving and “bilingual t"
119;119;machinelearningmastery.com;https://machinelearningmastery.com/techniques-to-understand-machine-learning-algorithms-without-the-background-in-mathematics/;2015-08-23;5 Ways To Understand Machine Learning Algorithms (without math);"Tweet Share Share

Last Updated on August 12, 2019

Where does theory fit into a top-down approach to studying machine learning?

In the traditional approach to teaching machine learning, theory comes first requiring an extensive background in mathematics to be able to understand it. In my approach to teaching machine learning, I start with teaching you how to work problems end-to-end and deliver results.

So where does the theory fit?

In this post you will discover what we really mean when we talk about “theory” in machine learning. Hint: it’s all about the algorithms.

You will discover that once you get skilled at working through problems and delivering results, you will develop a compulsion to dive deeper in order to better understanding and results. Nobody will be able to hold you back.

Finally, you will discover 5 techniques that you can use when you are practicing machine learning on standard datasets to incrementally build up your understanding of machine learning algorithms.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Learn Theory Last, Not First

The way machine learning is taught to developers is crap.

It is taught bottom-up. This is crap if you are a developer who is primarily interested in using machine learning as a tool to solve problems rather than being a researcher in the field.

The traditional approach requires that you learn all of the prerequisite mathematics like linear algebra, probability and statistics before learning the theory of algorithms. You’re lucky if you ever go near a working implementation of an algorithm or discuss how to work a problem end-to-end and deliver a working, reliable and accurate predictive model.

I teach a top-down approach to learning machine learning. In this approach we start with 1) learning a systematic process for working through problems end-to-end, 2) map the process onto “best of breed” machine learning tools and platforms then 3) complete targeted practice on test datasets.

You can learn more about my approach to teaching top-down machine learning in the post “Machine Learning for Programmers: Leap from developer to machine learning practitioner“.

So where does theory fit into this process?

If the model is flipped, then theory is taught later. But what theory are we talking about and how exactly do you learn that theory when you are practicing on test datasets?

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

The Theory is Really All About Algorithms

The field of machine learning is theory-dense.

It’s dense because there is a tradition to describe and explain concepts mathematically.

This is useful because mathematical descriptions can be very concise, cutting down on the ambiguity. They also lend themselves to analysis by leveraging the techniques from the context in which they are described (e.g. a probabilistic understanding of a process).

A lot of these tangential mathematical techniques are often bundled in with the description of machine learning algorithms. For someone who just wants to build a superficial understanding of a method to be able to configure and apply it, this feels overwhelming. Frustratingly so.

It is frustrating if you do not have the grounding to be able to parse and understand the description of an algorithm. It’s frustrating because coming from a field like computer science, algorithms are described all the time, but the difference is the descriptions are intended for fast comprehension (e.g. for desk checking) and implementation.

We know that for example when learning what a hash table is and how to use it, that we almost never need to know the specifics of the hashing function in our day-to-day. But we also know what a hashing function is and where to go to learn more about hashing function specifics and how to write your own. Why can’t machine learning work like that?

The bulk of the “theory” one encounters in machine learning is related to machine learning algorithms. If you ask any beginner about why they are frustrated with the theory, you will learn that it is in relation to learning how to understand or use a specific machine learning algorithm.

Here, algorithms is more broad than a process for creating a predictive model. It also refers to algorithms for selecting features, engineering new features, transforming data and estimating the accuracy of a model on unseen data (e.g. cross validation).

So, learning theory last, really means learning about machine learning algorithms.

A Compulsion To Dive Into Theory

I generally advise targeted practice on well known machine learning datasets.

This is because well known machine learning dataset, like those on the UCI Machine Learning"
120;120;machinelearningmastery.com;http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/;2016-07-20;Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras;"# Stacked LSTM for international airline passengers problem with memory

import numpy

import matplotlib . pyplot as plt

from pandas import read_csv

import math

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from sklearn . preprocessing import MinMaxScaler

from sklearn . metrics import mean_squared_error

# convert an array of values into a dataset matrix

def create_dataset ( dataset , look_back = 1 ) :

dataX , dataY = [ ] , [ ]

for i in range ( len ( dataset ) - look_back - 1 ) :

a = dataset [ i : ( i + look_back ) , 0 ]

dataX . append ( a )

dataY . append ( dataset [ i + look_back , 0 ] )

return numpy . array ( dataX ) , numpy . array ( dataY )

# fix random seed for reproducibility

numpy . random . seed ( 7 )

# load the dataset

dataframe = read_csv ( 'airline-passengers.csv' , usecols = [ 1 ] , engine = 'python' )

dataset = dataframe . values

dataset = dataset . astype ( 'float32' )

# normalize the dataset

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

dataset = scaler . fit_transform ( dataset )

# split into train and test sets

train_size = int ( len ( dataset ) * 0.67 )

test_size = len ( dataset ) - train_size

train , test = dataset [ 0 : train_size , : ] , dataset [ train_size : len ( dataset ) , : ]

# reshape into X=t and Y=t+1

look_back = 3

trainX , trainY = create_dataset ( train , look_back )

testX , testY = create_dataset ( test , look_back )

# reshape input to be [samples, time steps, features]

trainX = numpy . reshape ( trainX , ( trainX . shape [ 0 ] , trainX . shape [ 1 ] , 1 ) )

testX = numpy . reshape ( testX , ( testX . shape [ 0 ] , testX . shape [ 1 ] , 1 ) )

# create and fit the LSTM network

batch_size = 1

model = Sequential ( )

model . add ( LSTM ( 4 , batch_input_shape = ( batch_size , look_back , 1 ) , stateful = True , return_sequences = True ) )

model . add ( LSTM ( 4 , batch_input_shape = ( batch_size , look_back , 1 ) , stateful = True ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

for i in range ( 100 ) :

model . fit ( trainX , trainY , epochs = 1 , batch_size = batch_size , verbose = 2 , shuffle = False )

model . reset_states ( )

# make predictions

trainPredict = model . predict ( trainX , batch_size = batch_size )

model . reset_states ( )

testPredict = model . predict ( testX , batch_size = batch_size )

# invert predictions

trainPredict = scaler . inverse_transform ( trainPredict )

trainY = scaler . inverse_transform ( [ trainY ] )

testPredict = scaler . inverse_transform ( testPredict )

testY = scaler . inverse_transform ( [ testY ] )

# calculate root mean squared error

trainScore = math . sqrt ( mean_squared_error ( trainY [ 0 ] , trainPredict [ : , 0 ] ) )

print ( 'Train Score: %.2f RMSE' % ( trainScore ) )

testScore = math . sqrt ( mean_squared_error ( testY [ 0 ] , testPredict [ : , 0 ] ) )

print ( 'Test Score: %.2f RMSE' % ( testScore ) )

# shift train predictions for plotting

trainPredictPlot = numpy . empty_like ( dataset )

trainPredictPlot [ : , : ] = numpy . nan

trainPredictPlot [ look_back : len ( trainPredict ) + look_back , : ] = trainPredict

# shift test predictions for plotting

testPredictPlot = numpy . empty_like ( dataset )

testPredictPlot [ : , : ] = numpy . nan

testPredictPlot [ len ( trainPredict ) + ( look_back* 2 ) + 1 : len ( dataset ) - 1 , : ] = testPredict

# plot baseline and predictions

plt . plot ( scaler . inverse_transform ( dataset ) )

plt . plot ( trainPredictPlot )

plt . plot ( testPredictPlot )"
121;121;machinelearningmastery.com;http://machinelearningmastery.com/what-is-the-weka-machine-learning-workbench/;2014-02-04;What is the Weka Machine Learning Workbench;"Tweet Share Share

Last Updated on August 22, 2019

Machine learning is an iterative process rather than a linear process that requires each step to be revisited as more is learned about the problem under investigation. This iterative process can require using many different tools, programs and scripts for each process.

A machine learning workbench is a platform or environment that supports and facilitates a range of machine learning activities reducing or removing the need for multiple tools.

Some statistical and machine learning work benches like R provide very advanced tools but require a lot of manual configuration in the form of scripts and programming. The tools can also be fragile, written by and for academics rather than written to be robust and used in production environments.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

What is Weka

The Weka machine learning workbench is a modern platform for applied machine learning. Weka is an acronym which stands for Waikato Environment for Knowledge Analysis. It is also the name of a New Zealand bird the Weka.

Five features of Weka that I like to promote are:

Open Source : It is released as open source software under the GNU GPL. It is dual licensed and Pentaho Corporation owns the exclusive license to use the platform for business intelligence in their own product.

: It is released as open source software under the GNU GPL. It is dual licensed and Pentaho Corporation owns the exclusive license to use the platform for business intelligence in their own product. Graphical Interface : It has a Graphical User Interface (GUI). This allows you to complete your machine learning projects without programming.

: It has a Graphical User Interface (GUI). This allows you to complete your machine learning projects without programming. Command Line Interface : All features of the software can used from the command line. This can be very useful for scripting large jobs.

: All features of the software can used from the command line. This can be very useful for scripting large jobs. Java API : It is written in Java and provides a API that is well documented and promotes integration into your own applications. Note that the GNU GPL means that in turn your software would also have to be released as GPL.

: It is written in Java and provides a API that is well documented and promotes integration into your own applications. Note that the GNU GPL means that in turn your software would also have to be released as GPL. Documentation: There books, manuals, wikis and MOOC courses that can train you how to use the platform effectively.

The main reason I promote Weka is because a beginner can go through the process of applied machine learning using the graphical interface without having to do any programming. This is a big deal because getting a handle on the process, handling data and experimenting with algorithms is what a beginner should be learning about, not learning yet another scripting language.

Introduction to the Weka GUI

Now I want to show of the graphical user interface a bit and encourage you to download and have a play with Weka. The workbench provides three main ways to work on your problem: The Explorer for playing around and trying things out, the Experimenter for controlled experiments, and the KnowledgeFlow for graphically designing a pipeline for your problem.

Weka Explorer

The explorer is where you play around with your data and think about what transforms to apply to your data, what algorithms you want to run in experiments.

The Explorer interface is divided into 5 different tabs:

Preprocess : Load a dataset and manipulate the data into a form that you want to work with.

: Load a dataset and manipulate the data into a form that you want to work with. Classify : Select and run classification and regression algorithms to operate on your data.

: Select and run classification and regression algorithms to operate on your data. Cluster : Select and run clustering algorithms on your dataset.

: Select and run clustering algorithms on your dataset. Associate : Run association algorithms to extract insights from your data.

: Run association algorithms to extract insights from your data. Select Attributes : Run attribute selection algorithms on your data to select those attributes that are relevant to the feature you want to predict.

: Run attribute selection algorithms on your data to select those attributes that are relevant to the feature you want to predict. Visualize: Visualize the relationship between attributes.

Weka Experimenter

This interface is for designing experiments with your selection of algorithms and datasets, running experiments and analyzing the results.

The tools for analyzing results are very powerful, allowing you to consider and compare results that are statistically significant over multiple r"
122;122;news.mit.edu;http://news.mit.edu/2020/bluetooth-covid-19-contact-tracing-0409;;Bluetooth signals from your smartphone could automate Covid-19 contact tracing while preserving privacy;"Imagine you’ve been diagnosed as Covid-19 positive. Health officials begin contact tracing to contain infections, asking you to identify people with whom you’ve been in close contact. The obvious people come to mind — your family, your coworkers. But what about the woman ahead of you in line last week at the pharmacy, or the man bagging your groceries? Or any of the other strangers you may have come close to in the past 14 days?

A team led by MIT researchers and including experts from many institutions is developing a system that augments “manual” contact tracing by public health officials, while preserving the privacy of all individuals. The system relies on short-range Bluetooth signals emitted from people’s smartphones. These signals represent random strings of numbers, likened to “chirps” that other nearby smartphones can remember hearing.

If a person tests positive, they can upload the list of chirps their phone has put out in the past 14 days to a database. Other people can then scan the database to see if any of those chirps match the ones picked up by their phones. If there’s a match, a notification will inform that person that they may have been exposed to the virus, and will include information from public health authorities on next steps to take. Vitally, this entire process is done while maintaining the privacy of those who are Covid-19 positive and those wishing to check if they have been in contact with an infected person.

“I keep track of what I’ve broadcasted, and you keep track of what you’ve heard, and this will allow us to tell if someone was in close proximity to an infected person,” says Ron Rivest, MIT Institute Professor and principal investigator of the project. “But for these broadcasts, we’re using cryptographic techniques to generate random, rotating numbers that are not just anonymous, but pseudonymous, constantly changing their ‘ID,’ and that can’t be traced back to an individual.”

This approach to private, automated contact tracing will be available in a number of ways, including through the privacy-first effort launched at MIT in response to Covid-19 called SafePaths. This broad set of mobile apps is under development by a team led by Ramesh Raskar of the MIT Media Lab. The design of the new Bluetooth-based system has benefited from SafePaths’ early work in this area.

Bluetooth exchanges

Smartphones already have the ability to advertise their presence to other devices via Bluetooth. Apple’s “Find My” feature, for example, uses chirps from a lost iPhone or MacBook to catch the attention of other Apple devices, helping the owner of the lost device to eventually find it.

“Find My inspired this system. If my phone is lost, it can start broadcasting a Bluetooth signal that’s just a random number; it’s like being in the middle of the ocean and waving a light. If someone walks by with Bluetooth enabled, their phone doesn’t know anything about me; it will just tell Apple, ‘Hey, I saw this light,’” says Marc Zissman, the associate head of MIT Lincoln Laboratory’s Cyber Security and Information Science Division and co-principal investigator of the project.

With their system, the team is essentially asking a phone to send out this kind of random signal all the time and to keep a log of these signals. At the same time, the phone detects chirps it has picked up from other phones, and only logs chirps that would be medically significant for contact tracing — those emitted from within an approximate 6-foot radius and picked up for a certain duration of time, say 10 minutes.

Phone owners would get involved by downloading an app that enables this system. After a positive diagnosis, a person would receive a QR code from a health official. By scanning the code through that app, that person can upload their log to the cloud. Anyone with the app could then initiate their phones to scan these logs. A notification, if there’s a match, could tell a user how long they were near an infected person and the approximate distance.

Privacy-preserving technology

Some countries most successful at containing the spread of Covid-19 have been using smartphone-based approaches to conduct contact tracing, yet the researchers note these approaches have not always protected individual’s privacy. South Korea, for example, has implemented apps that notify officials if a diagnosed person has left their home, and can tap into people’s GPS data to pinpoint exactly where they’ve been.

“We’re not tracking location, not using GPS, not attaching your personal ID or phone number to any of these random numbers your phone is emitting,” says Daniel Weitzner, a principal research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-principal investigator of this effort. “What we want is to enable everyone to participate in a shared process of seeing if you might have been in contact, without revealing, or forcing anyone to reveal, anything.”

Choice is key. Weitzner sees the sys"
123;123;machinelearningmastery.com;https://machinelearningmastery.com/a-gentle-introduction-to-the-challenge-of-training-deep-learning-neural-network-models/;2019-02-14;A Gentle Introduction to the Challenge of Training Deep Learning Neural Network Models;"Tweet Share Share

Last Updated on August 6, 2019

Deep learning neural networks learn a mapping function from inputs to outputs.

This is achieved by updating the weights of the network in response to the errors the model makes on the training dataset. Updates are made to continually reduce this error until either a good enough model is found or the learning process gets stuck and stops.

The process of training neural networks is the most challenging part of using the technique in general and is by far the most time consuming, both in terms of effort required to configure the process and computational complexity required to execute the process.

In this post, you will discover the challenge of finding model parameters for deep learning neural networks.

After reading this post, you will know:

Neural networks learn a mapping function from inputs to outputs that can be summarized as solving the problem of function approximation.

Unlike other machine learning algorithms, the parameters of a neural network must be found by solving a non-convex optimization problem with many good solutions and many misleadingly good solutions.

The stochastic gradient descent algorithm is used to solve the optimization problem where model parameters are updated each iteration using the backpropagation algorithm.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

Neural Nets Learn a Mapping Function Learning Network Weights Is Hard Navigating the Error Surface Components of the Learning Algorithm

Neural Nets Learn a Mapping Function

Deep learning neural networks learn a mapping function.

Developing a model requires historical data from the domain that is used as training data. This data is comprised of observations or examples from the domain with input elements that describe the conditions and an output element that captures what the observation means.

For example, a problem where the output is a quantity would be described generally as a regression predictive modeling problem. Whereas a problem where the output is a label would be described generally as a classification predictive modeling problem.

A neural network model uses the examples to learn how to map specific sets of input variables to the output variable. It must do this in such a way that this mapping works well for the training dataset, but also works well on new examples not seen by the model during training. This ability to work well on specific examples and new examples is called the ability of the model to generalize.

A multilayer perceptron is just a mathematical function mapping some set of input values to output values.

— Page 5, Deep Learning, 2016.

We can describe the relationship between the input variables and the output variables as a complex mathematical function. For a given model problem, we must believe that a true mapping function exists to best map input variables to output variables and that a neural network model can do a reasonable job at approximating the true unknown underlying mapping function.

A feedforward network defines a mapping and learns the value of the parameters that result in the best function approximation.

— Page 168, Deep Learning, 2016.

As such, we can describe the broader problem that neural networks solve as “function approximation.” They learn to approximate an unknown underlying mapping function given a training dataset. They do this by learning weights and the model parameters, given a specific network structure that we design.

It is best to think of feedforward networks as function approximation machines that are designed to achieve statistical generalization, occasionally drawing some insights from what we know about the brain, rather than as models of brain function.

— Page 169, Deep Learning, 2016.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Learning Network Weights Is Hard

Finding the parameters for neural networks in general is hard.

For many simpler machine learning algorithms, we can calculate an optimal model given the training dataset.

For example, we can use linear algebra to calculate the specific coefficients of a linear regression model and a training dataset that best minimizes the squared error.

Similarly, we can use optimization algorithms that offer convergence guarantees when finding an optimal set of model parameters for nonlinear algorithms such as logistic regression or support vector machines.

Finding parameters for many machine learning algorithms involves solving a convex optimization problem: that is an error surface that is shaped like a bowl with a single best solution.

This is not the case for deep learning neu"
124;124;news.mit.edu;http://news.mit.edu/2020/hacking-challenges-that-are-hard-talk-about-0311;;Hacking the challenges that are hard to talk about;"What happens when 300 people from many industries and walks of life come together to try to address diversity and inclusion challenges? Answer: inventive hacks that can be applied to real-world situations. Last month, Hack for Inclusion — a student-led hackathon to combat bias — partnered with corporations and organizations to pose 14 challenges directly related to problems those groups are currently facing.

“The reason that we have corporations proposing the challenges is to increase the likelihood that the solutions will get resourced and implemented,” says Elaine Harris '78, a director on the MIT Alumni Association Board and one of the roughly dozen alumni who offered support in the planning of the hackathon. “We’re really empowering people to take what they’ve learned here back to their organizations, whether they be corporations, nonprofits, or student groups, and to not be afraid to tackle things that are often challenging to talk about.”

The event, which has been operating in its current format since 2018, was directed by a group of more than 45 planning team members organized by Sloan for Inclusion, an MIT Sloan School of Management student group led by Clare Herceg, Komal Patel, Hannah Phillips, and Udi Rosenhand, and Hacking Discrimination, a group of MIT alumni led by Harris.

Past years’ challenges have addressed bias or lack of inclusion in the workplace, specifically relating to women and underrepresented minorities, those with various physical abilities, and members of the LGBTQ+ community. This year, during the event held Feb. 20–21 at the Microsoft New England Research and Development Center, the challenges spanned topics including changing stigmas around mental illness, gender equity in e-sports, and using technology to address homelessness.

Not only is the outcome of the event focused on finding inclusive solutions, the very nature of the event is structured to be inclusive, says Herceg. She observed many kinds of diversity represented in the turnout, with ages and life stages ranging from high school students to older professionals, as well as people of different races and gender identities. “A lot of people voiced at the end of the event that this is the first time they felt like they could bring their whole self to an event like this — felt like they could be accepted.”

The two-day event, which starts with brainstorming and research and ends in a final pitch, was facilitated by Chris Lloyd, Olivia Seow, and Aaron Stinnett, students from MIT’s Integrated Design and Management program, and employs a design thinking process. Integral to that process’s success is to focus both on the dynamics of the group and on the needs of end users, says Jainaba Seckan, project manager at Harvard University’s Office of Diversity, Inclusion, and Belonging, who led a 2020 challenge sponsored by her office after having participated in the 2019 hackathon.

“The facilitators make sure that you are taking care of your team culture by engaging in exercises where you are checking in and setting norms and setting expectations of each other. By doing that, we created a common language. Taking that time to foster inclusion on the teams really did allow for us to do our best work,” Seckan says.

The challenge Seckan’s office sponsored — Campus Culture: Responding to Traumatic Events — had particular resonance in this university setting. It elicited ideas including an app to organize and prioritize campus-wide concerns to make it easier for all university community members to respond, as well as a communication system that would allow people to send videos and share messages of support following a traumatic event.

“The primary takeaway for me from the two challenge teams was that connection and community are primary solutions to address the isolation, shock, and fear that typically arises following traumatic and triggering events,” says Seckan. “Ultimately, these solutions create opportunities for learning and healing.”

Hack for Inclusion 2020’s overall top prize went to a team called Silver Tsunami, which worked on a Harnessing Wisdom in the Future of Work challenge posed by Steel Partners. With the aim of retaining the knowledge of employees above age 60, the group created a detailed offboarding program. Incorporating mentorship opportunities and community-building activities, the program is designed to support company culture, develop younger workers, and provide a thoughtful and cost-effective way to transition older workers into retirement.

Water Cooler, the second-prize winner, addressed a Deloitte-sponsored challenge focused on workplaces of the future by pitching software that could facilitate informal online and in-person meetings among coworkers from fully remote organizations.

Looking back on the weekend, Herceg says she was inspired not only by all of the teams’ collective output in such a short time frame, but that many of the groups had continued the conversation in the days following the even"
125;125;news.mit.edu;http://news.mit.edu/2018/julie-soriero-receives-ncaa-presidents-pat-summitt-award-0122;;Julie Soriero receives NCAA President's Pat Summitt Award;"On Jan. 18 at the NCAA Convention in Indianapolis, MIT Director of Athletics Julie Soriero was presented with the 2018 NCAA President’s Pat Summitt Award by NCAA President Mark Emmert.

Created in 2017, the award recognizes an individual in the association’s membership who has demonstrated devotion to development of student-athletes and has made a positive impact on their lives.

“I really want to thank President Emmert for recognizing the legacy of Pat Summitt and creating this award,” said Soriero. “I’m certainly humbled, I’m certainly proud and I’m certainly honored to be receiving it today.”

The first Division III administrator and the second overall to receive the honor, Soriero is in her 11th year as the director of athletics at MIT. Under her leadership, the athletic program has transformed into one of the top Division III programs in the nation on an annual basis. MIT has placed in the top 10 in the Learfield Directors’ Cup in four of the past five years and currently sits second nationally after the fall season.

“When I was named to receive this honor, I was really taken aback,” said Soriero. “I coached for over 21 years the sport of women’s basketball. I admired and respected Pat Summitt so much, as I know many of you and the women I know who have been in the basketball career and who have transferred into administration did.”

During her tenure, MIT teams have averaged nearly 10 conference championships, 90 All-American and 13 College Sports Information Directors of America (CoSIDA) Academic All-Americans per year. In the fall of 2017, MIT became the first New England Women’s and Men’s Athletic Conference (NEWMAC) institution to claim all of the women’s fall championships as the Engineers won five league titles in total and had three teams advance to the Sweet 16 of the NCAA Tournament.

Soriero has been active in the college sports governance process, serving on multiple NCAA committees, including chairing the Committee on Women’s Athletics, serving on the Division III Management Council, and working with the Women’s Basketball Rules Committee. Soriero also is the current president of Women Leaders in College Sports.

Award recipients are selected annually by the NCAA president and will receive a $10,000 honorarium to donate to the organization of the honoree’s choice that combats or researches neurological diseases of the brain. Soriero plans to donate the honorarium to Bay Cove Human Services, an organization in Boston that provides care to individuals with developmental disabilities, mental illnesses and substance abuse addictions.

The inaugural recipient of the Pat Summitt Award was Joan Cronan, longtime administrator at Tennessee.

For the latest on MIT Athletics, follow the Engineers via social media on Twitter, Facebook, Instagram and YouTube."
126;126;machinelearningmastery.com;https://machinelearningmastery.com/question-to-understand-any-machine-learning-algorithm/;2016-04-26;6 Questions To Understand Any Machine Learning Algorithm;"Tweet Share Share

Last Updated on August 12, 2019

There are a lot of machine learning algorithms and each algorithm is an island of research.

You have to choose the level of detail that you study machine learning algorithms. There is a sweet spot if you are a developer interested in applied predictive modeling.

This post describes that sweet spot and gives you a template that you can use to quickly understand any machine learning algorithm.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

What You Need To Know About a Machine Learning Algorithm?

What do you need to know about a machine learning algorithm to be able to use it well on a classification or prediction problem?

I won’t argue that the more that you know about how and why a particular algorithm works, the better you can wield it. But I do believe that there is a point of diminishing returns where you can stop, use what you know to be effective and dive deeper into the theory and research on an algorithm if and only if you need to know more in order to get better results.

Let’s take a look at the 6 questions that will reveal how a machine learning algorithms and how to best use it.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

6 Questions To Ask About Any Algorithm

There are 6 questions that you can ask to get to the heart of any machine learning algorithm:

How do you refer to the technique (e.g. what name)? How do you represent a learned model (e.g. what coefficients)? How to you learn a model (e.g. the optimization process from data to the representation)? How do you make predictions from a learned model (e.g. apply the model to new data)? How do you best prepare your data for the modeling with the technique (e.g. assumptions)? How do you get more information on the technique (e.g. where to look)?

You will note that I have phrased all of these questions as How-To. I did this intentionally to separate the practical concerns of how from the more theoretical concerns of why. I think knowing why a technique works is less important than knowing how it works, if you are looking to use it as a tool to get results. More on this in the next section.

Let’s take a closer look at each of these questions in turn.

1. How Do You Refer To The Technique?

This is obvious but important. You need to know the canonical name of the technique.

You need to be able to recognize the classical name or the name of the method from other fields as well and know that it is the same thing. This also includes the acronym for the algorithm, because sometimes they are less than intuitive.

This will help you sort out the base algorithm from extensions and the family tree of where the algorithm fits and relates to similar algorithms.

2. How Do You Represent a Learned Model?

I really like this nuts and bolts question.

This is question often overlooked in textbooks and papers and is perhaps the first question an engineer has when thinking about how a model will actually be used and deployed.

The representation is the numbers and data structure that captures the distinct details learned from data by the learning algorithm to be used by the prediction algorithm. It’s the stuff you save to disk or the database when you finalize your model. It’s the stuff you update when new training data becomes available.

Let’s make this concrete with an example. In the case of linear regression, the representation is the vector of regression coefficients. That’s it. In the case of a decision tree is is the tree itself including the nodes, how they are connected and the variables and cut-off thresholds chosen.

3. How Do You Learn a Model?

Given some training data, the algorithm needs to create the model or fill in the model representation. This question is about exactly how that occurs.

Often learning involves estimating parameters from the training data directly in simpler algorithms.

In most other algorithms it involves using the training data as part of a cost or loss function and an optimization algorithm to minimize the function. Simpler linear techniques may use linear algebra to achieve this result, whereas others may use a numerical optimization.

Often the way a machine learning algorithm learns a model is synonymous with the algorithm itself. This is the challenging and often time consuming part of running a machine learning algorithm.

The learning algorithm may be parameterized and it is often a good idea to list common ranges for parameter values or configuration heuristics that may be used as a starting point.

4. How Do You Make Predictions With A Model?

Once a model is learned, it is intended to be used to make predictions on n"
127;127;machinelearningmastery.com;https://machinelearningmastery.com/how-to-use-an-encoder-decoder-lstm-to-echo-sequences-of-random-integers/;2017-06-11;How to use an Encoder-Decoder LSTM to Echo Sequences of Random Integers;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76

from random import randint from numpy import array from numpy import argmax from pandas import DataFrame from pandas import concat from keras . models import Sequential from keras . layers import LSTM from keras . layers import Dense from keras . layers import TimeDistributed from keras . layers import RepeatVector # generate a sequence of random numbers in [0, 99] def generate_sequence ( length = 25 ) : return [ randint ( 0 , 99 ) for _ in range ( length ) ] # one hot encode sequence def one_hot_encode ( sequence , n_unique = 100 ) : encoding = list ( ) for value in sequence : vector = [ 0 for _ in range ( n_unique ) ] vector [ value ] = 1 encoding . append ( vector ) return array ( encoding ) # decode a one hot encoded string def one_hot_decode ( encoded_seq ) : return [ argmax ( vector ) for vector in encoded_seq ] # convert encoded sequence to supervised learning def to_supervised ( sequence , n_in , n_out ) : # create lag copies of the sequence df = DataFrame ( sequence ) df = concat ( [ df . shift ( n_in - i - 1 ) for i in range ( n_in ) ] , axis = 1 ) # drop rows with missing values df . dropna ( inplace = True ) # specify columns for input and output pairs values = df . values width = sequence . shape [ 1 ] X = values . reshape ( len ( values ) , n_in , width ) y = values [ : , 0 : ( n_out* width ) ] . reshape ( len ( values ) , n_out , width ) return X , y # prepare data for the LSTM def get_data ( n_in , n_out ) : # generate random sequence sequence = generate_sequence ( ) # one hot encode encoded = one_hot_encode ( sequence ) # convert to X,y pairs X , y = to_supervised ( encoded , n_in , n_out ) return X , y # define LSTM n_in = 5 n_out = 2 encoded_length = 100 batch_size = 21 model = Sequential ( ) model . add ( LSTM ( 150 , batch_input_shape = ( batch_size , n_in , encoded_length ) , stateful = True ) ) model . add ( RepeatVector ( n_out ) ) model . add ( LSTM ( 150 , return_sequences = True , stateful = True ) ) model . add ( TimeDistributed ( Dense ( encoded_length , activation = 'softmax' ) ) ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # train LSTM for epoch in range ( 5000 ) : # generate new random sequence X , y = get_data ( n_in , n_out ) # fit model for one epoch on this sequence model . fit ( X , y , epochs = 1 , batch_size = batch_size , verbose = 2 , shuffle = False ) model . reset_states ( ) # evaluate LSTM X , y = get_data ( n_in , n_out ) yhat = model . predict ( X , batch_size = batch_size , verbose = 0 ) # decode all pairs for i in range ( len ( X ) ) : print ( 'Expected:' , one_hot_decode ( y [ i ] ) , 'Predicted' , one_hot_decode ( yhat [ i ] ) )"
128;128;news.mit.edu;http://news.mit.edu/2020/catherine-dignazio-visualizing-covid-19-data-0414;;3 Questions: Catherine D’Ignazio on visualizing Covid-19 data;"The Covid-19 pandemic is generating waves of data points from around the world, recording the number of tests performed, cases confirmed, patients recovered, and people who have died from the virus. As these data are continuously updated, media outlets, government agencies, academics, and data-packaging firms are racing to make sense of the numbers, using novel design and visualization tools to chart and graph the virus many different contexts.

In general, data visualizations can help people quickly distill an otherwise overwhelming flood of numbers. Catherine D’Ignazio, assistant professor of urban science and planning at MIT, says it is critical that data are visualized responsibly in a pandemic.

D’Ignazio is the director of the Data and Feminism Lab, where she uses data and computational techniques to work toward gender and racial equity. MIT News spoke with her about the current boom in Covid-19 data visualizations, and how data visualizers can help us make sense of the pandemic’s uncertain numbers.

Q: How have you seen data visualization of Covid-19 evolve in the last few months, since the virus began its spread?

A: The first thing I'll note is that there has been an explosion of data visualization. Since the information about the virus comes in numbers — case counts, death counts, testing rates — it lends itself easily to data visualization. Maps, bar charts, and line charts of confirmed cases predominated at first, and I would say they are still the most common forms of visualization that we are seeing in media reporting and on social media. As a person in the field, the proliferation is both exciting, because it shows the relevance of visualization, and scary, because there is definitely some irresponsible use of visualization.

Many high-profile organizations are plotting case counts on graduated color maps, which is a big no-no unless you have normalized your numbers. So California, a big and densely populated state, will always appear to be worse off in absolute raw case counts. Conversely, this way of plotting could cause you to miss small states with a high rate of infection since they will be low in relative case numbers and would always show up in lighter colors on the map.

Second, as the crisis has developed, media outlets are mapping things other than simply case counts or death rates. There have been many versions of the “flatten the curve” chart. This one is interesting because it’s not about plotting specific numbers, but about explaining a public health concept to a broad audience with a hypothetical chart. The best visual explanation I’ve seen of the flatten the curve concept is from The Washington Post and comes with simulations and animations that explain virus transmission. There have also been a number of visualizations of how social distancing has changed people’s mobility behavior, shifting traffic patterns , and even a global satellite map where you can see how Covid-19 has reduced urban pollution over the past three months.

Finally, this crisis is posing some difficult visual communication problems: How do you depict exponential growth in an accessible way? How do you visually explain the uncertainty in numbers like case counts, where we (at least in the U.S. context) have not done nearly enough testing to make them a reliable indicator of actual cases?

Journalists and health communicators have responded to these challenges by developing new visual conventions, as well as making heavy use of explanations and disclaimers in the narratives themselves. For example, the chart below, by Lisa Charlotte Rost for DataWrapper, uses a log scale on the y-axis for showing exponential rates of change. But note the dotted reference lines, labeled “deaths double every day” or “...every 2nd day.” These annotations help to highlight the use of the log scale (which otherwise might go unnoticed by readers) as well as to explain how to interpret the different slopes of the lines. Likewise, Rost is explicit about only making charts of death rates, not case counts, because of the variation in availability of tests and vast underreporting in many countries. Whereas actual cases may or may not be detected and counted, deaths are more likely to be counted.

A screenshot of an interactive chart, from Datawrapper, shows cumulative numbers of confirmed deaths due to the Covid-19 disease. Chart: Lisa Charlotte Rost, Datawrapper. Source: Johns Hopkins CSSE. Created with Datawrapper.

Q: What are some things people should keep in mind when digging into available datasets to make their own visualizations?

A: This is such a great question, because there has been a proliferation of visualizations and models that are not only erroneous but also irresponsible in a public health crisis. Usually these are made by folks who do not have expertise in epidemiology but assume that their skills in data science can just be magically ported into a new realm. I’d like to shout out here to Amanda Makulec’s excellent"
129;129;machinelearningmastery.com;http://machinelearningmastery.com/machine-learning-in-r-step-by-step/;2016-02-02;Your First Machine Learning Project in R Step-By-Step;"# a) linear algorithms

set . seed ( 7 )

fit . lda < - train ( Species ~ . , data = dataset , method = ""lda"" , metric = metric , trControl = control )

# b) nonlinear algorithms

# CART

set . seed ( 7 )

fit . cart < - train ( Species ~ . , data = dataset , method = ""rpart"" , metric = metric , trControl = control )

# kNN

set . seed ( 7 )

fit . knn < - train ( Species ~ . , data = dataset , method = ""knn"" , metric = metric , trControl = control )

# c) advanced algorithms

# SVM

set . seed ( 7 )

fit . svm < - train ( Species ~ . , data = dataset , method = ""svmRadial"" , metric = metric , trControl = control )

# Random Forest

set . seed ( 7 )"
130;130;news.mit.edu;http://news.mit.edu/2020/first-majorana-fermion-metal-quantum-computing-0410;;First sighting of mysterious Majorana fermion on a common metal;"Physicists at MIT and elsewhere have observed evidence of Majorana fermions — particles that are theorized to also be their own antiparticle — on the surface of a common metal: gold. This is the first sighting of Majorana fermions on a platform that can potentially be scaled up. The results, published in the Proceedings of the National Academy of Sciences, are a major step toward isolating the particles as stable, error-proof qubits for quantum computing.

In particle physics, fermions are a class of elementary particles that includes electrons, protons, neutrons, and quarks, all of which make up the building blocks of matter. For the most part, these particles are considered Dirac fermions, after the English physicist Paul Dirac, who first predicted that all fermionic fundamental particles should have a counterpart, somewhere in the universe, in the form of an antiparticle — essentially, an identical twin of opposite charge.

In 1937, the Italian theoretical physicist Ettore Majorana extended Dirac’s theory, predicting that among fermions, there should be some particles, since named Majorana fermions, that are indistinguishable from their antiparticles. Mysteriously, the physicist disappeared during a ferry trip off the Italian coast just a year after making his prediction. Scientists have been looking for Majorana’s enigmatic particle ever since. It has been suggested, but not proven, that the neutrino may be a Majorana particle. On the other hand, theorists have predicted that Majorana fermions may also exist in solids under special conditions.

Now the MIT-led team has observed evidence of Majorana fermions in a material system they designed and fabricated, which consists of nanowires of gold grown atop a superconducting material, vanadium, and dotted with small, ferromagnetic “islands” of europium sulfide. When the researchers scanned the surface near the islands, they saw signature signal spikes near zero energy on the very top surface of gold that, according to theory, should only be generated by pairs of Majorana fermions.

“Majorana ferminons are these exotic things, that have long been a dream to see, and we now see them in a very simple material — gold,” says Jagadeesh Moodera, a senior research scientist in MIT’s Department of Physics, and a member of MIT’s Plasma Science and Fusion Center. “We’ve shown they are there, and stable, and easily scalable.”

“The next push will be to take these objects and make them into qubits, which would be huge progress toward practical quantum computing,” adds co-author Patrick Lee, the William and Emma Rogers Professor of Physics at MIT.

Lee and Moodera’s coauthors include former MIT postdoc and first author Sujit Manna (currently on the faculty at the Indian Institute of Technology at Delhi), and former MIT postdoc Peng Wei of University of California at Riverside, along with Yingming Xie and Kam Tuen Law of the Hong Kong University of Science and Technology.

High risk

If they could be harnessed, Majorana fermions would be ideal as qubits, or individual computational units for quantum computers. The idea is that a qubit would be made of combinations of pairs of Majorana fermions, each of which would be separated from its partner. If noise errors affect one member of the pair, the other should remain unaffected, thereby preserving the integrity of the qubit and enabling it to correctly carry out a computation.

Scientists have looked for Majorana fermions in semiconductors, the materials used in conventional, transistor-based computing. In their experiments, researchers have combined semiconductors with superconductors — materials through which electrons can travel without resistance. This combination imparts superconductive properties to conventional semiconductors, which physicists believe should induce particles in the semiconductor to split , forming the pair of Majorana fermions.

“There are several material platforms where people believe they’ve seen Majorana particles,” Lee says. “The evidence is stronger and stronger, but it’s still not 100 percent proven.”

What’s more, the semiconductor-based setups to date have been difficult to scale up to produce the thousands or millions of qubits needed for a practical quantum computer, because they require growing very precise crystals of semiconducting material and it is very challenging to turn these into high-quality superconductors.

About a decade ago, Lee, working with his graduate student Andrew Potter, had an idea: Perhaps physicists might be able to observe Majorana fermions in metal, a material that readily becomes superconductive in proximity with a superconductor. Scientists routinely make metals, including gold, into superconductors. Lee’s idea was to see if gold’s surface state — its very top layer of atoms — could be made to be superconductive. If this could be achieved, then gold could serve as a clean, atomically precise system in which researchers could observe Majorana fermions.

Lee propo"
131;131;machinelearningmastery.com;http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/;2016-06-19;Dropout Regularization in Deep Learning Models With Keras;"# Baseline Model on the Sonar Dataset

from pandas import read_csv

from keras . models import Sequential

from keras . layers import Dense

from keras . wrappers . scikit_learn import KerasClassifier

from keras . optimizers import SGD

from sklearn . model_selection import cross_val_score

from sklearn . preprocessing import LabelEncoder

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import StandardScaler

from sklearn . pipeline import Pipeline

# load dataset

dataframe = read_csv ( ""sonar.csv"" , header = None )

dataset = dataframe . values

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 60 ] . astype ( float )

Y = dataset [ : , 60 ]

# encode class values as integers

encoder = LabelEncoder ( )

encoder . fit ( Y )

encoded_Y = encoder . transform ( Y )

# baseline

def create_baseline ( ) :

# create model

model = Sequential ( )

model . add ( Dense ( 60 , input_dim = 60 , activation = 'relu' ) )

model . add ( Dense ( 30 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# Compile model

sgd = SGD ( lr = 0.01 , momentum = 0.8 )

model . compile ( loss = 'binary_crossentropy' , optimizer = sgd , metrics = [ 'accuracy' ] )

return model

estimators = [ ]

estimators . append ( ( 'standardize' , StandardScaler ( ) ) )

estimators . append ( ( 'mlp' , KerasClassifier ( build_fn = create_baseline , epochs = 300 , batch_size = 16 , verbose = 0 ) ) )

pipeline = Pipeline ( estimators )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True )

results = cross_val_score ( pipeline , X , encoded_Y , cv = kfold )"
132;132;machinelearningmastery.com;https://machinelearningmastery.com/best-advice-for-configuring-backpropagation-for-deep-learning-neural-networks/;2019-02-21;8 Tricks for Configuring Backpropagation to Train Better Neural Networks;"Tweet Share Share

Last Updated on August 6, 2019

Neural network models are trained using stochastic gradient descent and model weights are updated using the backpropagation algorithm.

The optimization solved by training a neural network model is very challenging and although these algorithms are widely used because they perform so well in practice, there are no guarantees that they will converge to a good model in a timely manner.

The challenge of training neural networks really comes down to the challenge of configuring the training algorithms.

In this post, you will discover tips and tricks for getting the most out of the backpropagation algorithm when training neural network models.

After reading this post, you will know:

The challenge of training a neural network is really the balance between learning the training dataset and generalizing to new examples beyond the training dataset.

Eight specific tricks that you can use to train better neural network models, faster.

Second order optimization algorithms that can also be used to train neural networks under certain circumstances.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Post Overview

This tutorial is divided into five parts; they are:

Efficient BackProp Overview Learning and Generalization 8 Practical Tricks for Backpropagation Second Order Optimization Algorithms Discussion and Conclusion

Efficient BackProp Overview

The 1998 book titled “Neural Networks: Tricks of the Trade” provides a collection of chapters by academics and neural network practitioners that describe best practices for configuring and using neural network models.

The book was updated at the cusp of the deep learning renaissance and a second edition was released in 2012 including 13 new chapters.

The first chapter in both editions is titled “Efficient BackProp” written by Yann LeCun, Leon Bottou, (both at Facebook AI), Genevieve Orr, and Klaus-Robert Muller (also co-editors of the book).

The chapter is also available online for free as a pre-print.

The chapter was also summarized in a preface in both editions of the book titled “Speed Learning.”

It is an important chapter and document as it provides a near-exhaustive summary of how to best configure backpropagation under stochastic gradient descent as of 1998, and much of the advice is just as relevant today.

In this post, we will focus on this chapter or paper and attempt to distill the most relevant advice for modern deep learning practitioners.

For reference, the chapter is divided into 10 sections; they are:

1.1: Introduction

1.2: Learning and Generalization

1.3: Standard Backpropagation

1.4: A Few Practical Tricks

1.5: Convergence of Gradient Descent

1.6: Classical Second Order Optimization Methods

1.7: Tricks to Compute the Hessian Information in Multilayer Networks

1.8: Analysis of the Hessian in Multi-layer Networks

1.9: Applying Second Order Methods to Multilayer Networks

1.10: Discussion and Conclusion

We will focus on the tips and tricks for configuring backpropagation and stochastic gradient descent.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Learning and Generalization

The chapter begins with a description of the general problem of the dual challenge of learning and generalization with neural network models.

The authors motivate the article by highlighting that the backpropagation algorithm is the most widely used algorithm to train neural network models because it works and because it is efficient.

Backpropagation is a very popular neural network learning algorithm because it is conceptually simple, computationally efficient, and because it often works. However, getting it to work well, and sometimes to work at all, can seem more of an art than a science.

The authors also remind us that training neural networks with backpropagation is really hard. Although the algorithm is both effective and efficient, it requires the careful configuration of multiple model properties and model hyperparameters, each of which requires deep knowledge of the algorithm and experience to set correctly.

And yet, there are no rules to follow to “best” configure a model and training process.

Designing and training a network using backprop requires making many seemingly arbitrary choices such as the number and types of nodes, layers, learning rates, training and test sets, and so forth. These choices can be critical, yet there is no foolproof recipe for deciding them because they are largely problem and data dependent.

The goal of training a neural network model is most challenging because it requires solving two hard problems at once:

Learning the training dataset in order to best minimize the loss.
"
133;133;machinelearningmastery.com;http://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/;2016-08-21;Data Preparation for Gradient Boosting with XGBoost in Python;"# multiclass classification

import pandas

import xgboost

from sklearn import model_selection

from sklearn . metrics import accuracy_score

from sklearn . preprocessing import LabelEncoder

# load data

data = pandas . read_csv ( 'iris.csv' , header = None )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 4 ]

Y = dataset [ : , 4 ]

# encode string class values as integers

label_encoder = LabelEncoder ( )

label_encoder = label_encoder . fit ( Y )

label_encoded_y = label_encoder . transform ( Y )

seed = 7

test_size = 0.33

X_train , X_test , y_train , y_test = model_selection . train_test_split ( X , label_encoded_y , test_size = test_size , random_state = seed )

# fit model no training data

model = xgboost . XGBClassifier ( )

model . fit ( X_train , y_train )

print ( model )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )"
134;134;machinelearningmastery.com;https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/;2017-08-27;How to Make Predictions with Long Short-Term Memory Models in Keras;"from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from numpy import array

from keras . models import load_model

# return training data

def get_train ( ) :

seq = [ [ 0.0 , 0.1 ] , [ 0.1 , 0.2 ] , [ 0.2 , 0.3 ] , [ 0.3 , 0.4 ] , [ 0.4 , 0.5 ] ]

seq = array ( seq )

X , y = seq [ : , 0 ] , seq [ : , 1 ]

X = X . reshape ( ( len ( X ) , 1 , 1 ) )

return X , y

# define model

model = Sequential ( )

model . add ( LSTM ( 10 , input_shape = ( 1 , 1 ) ) )

model . add ( Dense ( 1 , activation = 'linear' ) )

# compile model

model . compile ( loss = 'mse' , optimizer = 'adam' )

# fit model

X , y = get_train ( )

model . fit ( X , y , epochs = 300 , shuffle = False , verbose = 0 )

# save model to single file

model . save ( 'lstm_model.h5' )

# snip...

# later, perhaps run from another script

# load model from single file

model = load_model ( 'lstm_model.h5' )

# make predictions

yhat = model . predict ( X , verbose = 0 )"
135;135;machinelearningmastery.com;https://machinelearningmastery.com/multi-step-time-series-forecasting-with-machine-learning-models-for-household-electricity-consumption/;2018-10-04;Multi-step Time Series Forecasting with Machine Learning for Electricity Usage;"# direct multi-step forecast by lead time

from math import sqrt

from numpy import split

from numpy import array

from pandas import read_csv

from sklearn . metrics import mean_squared_error

from matplotlib import pyplot

from sklearn . preprocessing import StandardScaler

from sklearn . preprocessing import MinMaxScaler

from sklearn . pipeline import Pipeline

from sklearn . linear_model import LinearRegression

from sklearn . linear_model import Lasso

from sklearn . linear_model import Ridge

from sklearn . linear_model import ElasticNet

from sklearn . linear_model import HuberRegressor

from sklearn . linear_model import Lars

from sklearn . linear_model import LassoLars

from sklearn . linear_model import PassiveAggressiveRegressor

from sklearn . linear_model import RANSACRegressor

from sklearn . linear_model import SGDRegressor

# split a univariate dataset into train/test sets

def split_dataset ( data ) :

# split into standard weeks

train , test = data [ 1 : - 328 ] , data [ - 328 : - 6 ]

# restructure into windows of weekly data

train = array ( split ( train , len ( train ) / 7 ) )

test = array ( split ( test , len ( test ) / 7 ) )

return train , test

# evaluate one or more weekly forecasts against expected values

def evaluate_forecasts ( actual , predicted ) :

scores = list ( )

# calculate an RMSE score for each day

for i in range ( actual . shape [ 1 ] ) :

# calculate mse

mse = mean_squared_error ( actual [ : , i ] , predicted [ : , i ] )

# calculate rmse

rmse = sqrt ( mse )

# store

scores . append ( rmse )

# calculate overall RMSE

s = 0

for row in range ( actual . shape [ 0 ] ) :

for col in range ( actual . shape [ 1 ] ) :

s += ( actual [ row , col ] - predicted [ row , col ] ) * * 2

score = sqrt ( s / ( actual . shape [ 0 ] * actual . shape [ 1 ] ) )

return score , scores

# summarize scores

def summarize_scores ( name , score , scores ) :

s_scores = ', ' . join ( [ '%.1f' % s for s in scores ] )

print ( '%s: [%.3f] %s' % ( name , score , s_scores ) )

# prepare a list of ml models

def get_models ( models = dict ( ) ) :

# linear models

models [ 'lr' ] = LinearRegression ( )

models [ 'lasso' ] = Lasso ( )

models [ 'ridge' ] = Ridge ( )

models [ 'en' ] = ElasticNet ( )

models [ 'huber' ] = HuberRegressor ( )

models [ 'lars' ] = Lars ( )

models [ 'llars' ] = LassoLars ( )

models [ 'pa' ] = PassiveAggressiveRegressor ( max_iter = 1000 , tol = 1e - 3 )

models [ 'ranscac' ] = RANSACRegressor ( )

models [ 'sgd' ] = SGDRegressor ( max_iter = 1000 , tol = 1e - 3 )

print ( 'Defined %d models' % len ( models ) )

return models

# create a feature preparation pipeline for a model

def make_pipeline ( model ) :

steps = list ( )

# standardization

steps . append ( ( 'standardize' , StandardScaler ( ) ) )

# normalization

steps . append ( ( 'normalize' , MinMaxScaler ( ) ) )

# the model

steps . append ( ( 'model' , model ) )

# create pipeline

pipeline = Pipeline ( steps = steps )

return pipeline

# # convert windows of weekly multivariate data into a series of total power

def to_series ( data ) :

# extract just the total power from each week

series = [ week [ : , 0 ] for week in data ]

# flatten into a single series

series = array ( series ) . flatten ( )

return series

# convert history into inputs and outputs

def to_supervised ( history , n_input , output_ix ) :

# convert history to a univariate series

data = to_series ( history )

X , y = list ( ) , list ( )

ix_start = 0

# step over the entire history one time step at a time

for i in range ( len ( data ) ) :

# define the end of the input sequence

ix_end = ix_start + n_input

ix_output = ix_end + output_ix

# ensure we have enough data for this instance

if ix_output < len ( data ) :

X . append ( data [ ix_start : ix_end ] )

y . append ( data [ ix_output ] )

# move along one time step

ix_start += 1

return array ( X ) , array ( y )

# fit a model and make a forecast

def sklearn_predict ( model , history , n_input ) :

yhat_sequence = list ( )

# fit a model for each forecast day

for i in range ( 7 ) :

# prepare data

train_x , train_y = to_supervised ( history , n_input , i )

# make pipeline

pipeline = make_pipeline ( model )

# fit the model

pipeline . fit ( train_x , train_y )

# forecast

x_input = array ( train_x [ - 1 , : ] ) . reshape ( 1 , n_input )

yhat = pipeline . predict ( x_input ) [ 0 ]

# store

yhat_sequence . append ( yhat )

return yhat_sequence

# evaluate a single model

def evaluate_model ( model , train , test , n_input ) :

# history is a list of weekly data

history = [ x for x in train ]

# walk-forward validation over each week

predictions = list ( )

for i in range ( len ( test ) ) :

# predict the week

yhat_sequence = sklearn_predict ( model , history , n_input )

# store the predictions

predictions . append ( yhat_sequence )

# get real observation and add to history for predicting the next week

history . append ( test [ i , : ] )

predictions ="
136;136;machinelearningmastery.com;http://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/;2016-03-17;Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning;"Tweet Share Share

Last Updated on October 25, 2019

Supervised machine learning algorithms can best be understood through the lens of the bias-variance trade-off.

In this post, you will discover the Bias-Variance Trade-Off and how to use it to better understand machine learning algorithms and get better performance on your data.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Update Oct/2019: Removed discussion of parametric/nonparametric models (thanks Alex).

Overview of Bias and Variance

In supervised machine learning an algorithm learns a model from training data.

The goal of any supervised machine learning algorithm is to best estimate the mapping function (f) for the output variable (Y) given the input data (X). The mapping function is often called the target function because it is the function that a given supervised machine learning algorithm aims to approximate.

The prediction error for any machine learning algorithm can be broken down into three parts:

Bias Error

Variance Error

Irreducible Error

The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.

In this post, we will focus on the two parts we can influence with our machine learning algorithms. The bias error and the variance error.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Bias Error

Bias are the simplifying assumptions made by a model to make the target function easier to learn.

Generally, linear algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.

Low Bias : Suggests less assumptions about the form of the target function.

: Suggests less assumptions about the form of the target function. High-Bias: Suggests more assumptions about the form of the target function.

Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.

Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.

Variance Error

Variance is the amount that the estimate of the target function will change if different training data was used.

The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables.

Machine learning algorithms that have a high variance are strongly influenced by the specifics of the training data. This means that the specifics of the training have influences the number and types of parameters used to characterize the mapping function.

Low Variance : Suggests small changes to the estimate of the target function with changes to the training dataset.

: Suggests small changes to the estimate of the target function with changes to the training dataset. High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.

Generally, nonlinear machine learning algorithms that have a lot of flexibility have a high variance. For example, decision trees have a high variance, that is even higher if the trees are not pruned before use.

Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.

Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.

Bias-Variance Trade-Off

The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance.

You can see a general trend in the examples above:

Linear machine learning algorithms often have a high bias but a low variance.

machine learning algorithms often have a high bias but a low variance. Nonlinear machine learning algorithms often have a low bias but a high variance.

The parameterization of machine learning algorithms is often a battle to balance out bias and variance.

Below are two examples of configuring the bias-variance trade-off for specific algorithms:

The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed"
137;137;machinelearningmastery.com;http://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/;2016-08-25;How to Evaluate Gradient Boosting Models with XGBoost in Python;"# train-test split evaluation of xgboost model

from numpy import loadtxt

from xgboost import XGBClassifier

from sklearn . model_selection import train_test_split

from sklearn . metrics import accuracy_score

# load data

dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" )

# split data into X and y

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# split data into train and test sets

X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.33 , random_state = 7 )

# fit model no training data

model = XGBClassifier ( )

model . fit ( X_train , y_train )

# make predictions for test data

y_pred = model . predict ( X_test )

predictions = [ round ( value ) for value in y_pred ]

# evaluate predictions

accuracy = accuracy_score ( y_test , predictions )"
138;138;towardsdatascience.com;https://towardsdatascience.com/how-to-master-python-command-line-arguments-5d5ad4bcf985?source=collection_home---4------0-----------------------;2020-04-19;How to Master Python Command Line Arguments;"I believe most of us have run this command line to execute your python script.

$ python main.py

Can we do a little bit more like defining our own argument in this script? The answer is definitely yes!

$ python main.py arg1 arg2

We are going to use Python argparse module to configure command line arguments and options. The argparse module makes it easy to write user-friendly command-line interfaces. The program defines what arguments it requires, and argparse will figure out how to parse those out of sys.argv. The argparse module also automatically generates help and usage messages and issues errors when users give the program invalid arguments.

Getting Started with Argparse

Installing Argparse

As usual, the first thing that we need to do is install this Python module.

conda install argparse

Defining Positional and Optional Arguments

Create a parser object with ArgumentParser with the description of this script. Positional Arguments and Optional Arguments are defined with add_argument function. A brief description of what the argument does is added with help .

Positional Arguments are arguments that need to be included in the proper position or order.

Optional Arguments are keyword arguments that input with a keyword and equals sign and they are optional.

Let’s try to run this script with help argument -h .

$ python employee.py -h

usage: employee.py [-h] [--address ADDRESS] name title This script is going to create an employee profile. positional arguments:

name Name of Employee

title Job Title of Employee optional arguments:

-h, --help show this help message and exit

--address ADDRESS Address of Employee

-h and --help are defined by default in argparse . It will show the descriptions that we have defined in the script to assist our user when they use the script.

Let’s try to input name and title .

$ python employee.py Alex Manager

Name : Alex

Job Title : Manager

Address : None

Due to the absence of address argument, NoneType is passed to Address in this script. We would have to convert it to string in order to print it.

Let’s try with name only

$ python employee.py Alex

usage: employee.py [-h] [--address ADDRESS] name title

employee.py: error: the following arguments are required: title

Because title is also Positional Argument , it is required in this script.

This time let’s try with name , title and address .

$ python employee.py Alex Manager --address 123 Baker Street

usage: employee.py [-h] [--address ADDRESS] name title

employee.py: error: unrecognized arguments: Baker Street

Because 123 Baker Street contains space, the script will treat Baker Street as other arguments. We would need double quotes at this case.

$ python employee.py Alex Manager --address ""123 Baker Street""

Name : Alex

Job Title : Manager

Address : 123 Baker Street

name and title would need double quotes if the name or title are more than one words.

Defining Boolean Arguments

Let’s add the above codes into the existing script. We are going to define an optional argument with default=True . It means that even we never input anything into this argument, it is equal to True by default.

type=strtobool is used here to ensure the input is converted to boolean data type. Else, it will be string data type when the script passed in the input. We also can define it as type=int if we need an integer argument.

%(default)s) in the help is to retrieve the default value in the argument. This is to make sure the description is not hard coded and flexible with the change of default value.

Let’s try again with name , title and address

$ python employee.py Alex Manager --address ""123 Baker Street""

Name : Alex

Job Title : Manager

Address : 123 Baker Street

Alex is a full time employee.

By default, isFullTime is True , so Alex is a full time employee if we never input anything argument to isFullTime .

Let’s try with a False input.

$ python employee.py Alex Manager --address ""123 Baker Street"" --isFullTime False

Name : Alex

Job Title : Manager

Address : 123 Baker Street

Alex is not a full time employee.

Ops! Alex is not a full time employee now.

Defining Choices in the Arguments

We also can limit the possible values for an input argument with choices argument. This is useful to prevent your user from inputing invalid value. For instances, we can restrict the country value to Singapore, United States and Malaysia only with this choices=[“Singapore”, “United States”, “Malaysia”] .

Let’s see what happen if we input the country value not in the choices.

$ python employee.py Alex Manager --country Japan

usage: employee.py [-h] [--address ADDRESS]

[--country {Singapore,United States,Malaysia}]

[--isFullTime ISFULLTIME]

name title

employee.py: error: argument --country: invalid choice: 'Japan' (choose from 'Singapore', 'United States', 'Malaysia')"
139;139;machinelearningmastery.com;http://machinelearningmastery.com/evaluate-performance-machine-learning-algorithms-python-using-resampling/;2016-05-22;Evaluate the Performance of Machine Learning Algorithms in Python using Resampling;"# Evaluate using a train and a test set

import pandas

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

test_size = 0.33

seed = 7

X_train , X_test , Y_train , Y_test = model_selection . train_test_split ( X , Y , test_size = test_size , random_state = seed )

model = LogisticRegression ( )

model . fit ( X_train , Y_train )

result = model . score ( X_test , Y_test )"
140;140;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-an-intuition-skewed-class-distributions/;2019-12-26;Develop an Intuition for Severely Skewed Class Distributions;"# create and plot synthetic dataset with a given class distribution

from numpy import unique

from numpy import hstack

from numpy import vstack

from numpy import where

from matplotlib import pyplot

from sklearn . datasets import make_blobs

# create a dataset with a given class distribution

def get_dataset ( proportions ) :

# determine the number of classes

n_classes = len ( proportions )

# determine the number of examples to generate for each class

largest = max ( [ v for k , v in proportions . items ( ) ] )

n_samples = largest * n_classes

# create dataset

X , y = make_blobs ( n_samples = n_samples , centers = n_classes , n_features = 2 , random_state = 1 , cluster_std = 3 )

# collect the examples

X_list , y_list = list ( ) , list ( )

for k , v in proportions . items ( ) :

row_ix = where ( y == k ) [ 0 ]

selected = row_ix [ : v ]

X_list . append ( X [ selected , : ] )

y_list . append ( y [ selected ] )

return vstack ( X_list ) , hstack ( y_list )

# scatter plot of dataset, different color for each class

def plot_dataset ( X , y ) :

# create scatter plot for samples from each class

n_classes = len ( unique ( y ) )

for class_value in range ( n_classes ) :

# get row indexes for samples with this class

row_ix = where ( y == class_value ) [ 0 ]

# create scatter of these samples

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( class_value ) )

# show a legend

pyplot . legend ( )

# show the plot

pyplot . show ( )

# define the class distribution

proportions = { 0 : 10000 , 1 : 10 }

# generate dataset

X , y = get_dataset ( proportions )

# plot dataset"
141;141;news.mit.edu;http://news.mit.edu/2020/qa-markus-buehler-setting-coronavirus-and-ai-inspired-proteins-to-music-0402;;Q&A: Markus Buehler on setting coronavirus and AI-inspired proteins to music;"The proteins that make up all living things are alive with music. Just ask Markus Buehler: The musician and MIT professor develops artificial intelligence models to design new proteins, sometimes by translating them into sound. His goal is to create new biological materials for sustainable, non-toxic applications. In a project with the MIT-IBM Watson AI Lab, Buehler is searching for a protein to extend the shelf-life of perishable food. In a new study in Extreme Mechanics Letters, he and his colleagues offer a promising candidate: a silk protein made by honeybees for use in hive building.

In another recent study, in APL Bioengineering, he went a step further and used AI discover an entirely new protein. As both studies went to print, the Covid-19 outbreak was surging in the United States, and Buehler turned his attention to the spike protein of SARS-CoV-2, the appendage that makes the novel coronavirus so contagious. He and his colleagues are trying to unpack its vibrational properties through molecular-based sound spectra, which could hold one key to stopping the virus. Buehler recently sat down to discuss the art and science of his work.

Q: Your work focuses on the alpha helix proteins found in skin and hair. Why makes this protein so intriguing?

A: Proteins are the bricks and mortar that make up our cells, organs, and body. Alpha helix proteins are especially important. Their spring-like structure gives them elasticity and resilience, which is why skin, hair, feathers, hooves, and even cell membranes are so durable. But they’re not just tough mechanically, they have built-in antimicrobial properties. With IBM, we’re trying to harness this biochemical trait to create a protein coating that can slow the spoilage of quick-to-rot foods like strawberries.

Q: How did you enlist AI to produce this silk protein?

A: We trained a deep learning model on the Protein Data Bank, which contains the amino acid sequences and three-dimensional shapes of about 120,000 proteins. We then fed the model a snippet of an amino acid chain for honeybee silk and asked it to predict the protein’s shape, atom-by-atom. We validated our work by synthesizing the protein for the first time in a lab — a first step toward developing a thin antimicrobial, structurally-durable coating that can be applied to food. My colleague, Benedetto Marelli, specializes in this part of the process. We also used the platform to predict the structure of proteins that don’t yet exist in nature. That’s how we designed our entirely new protein in the APL Bioengineering study.

Q: How does your model improve on other protein prediction methods?

A: We use end-to-end prediction. The model builds the protein’s structure directly from its sequence, translating amino acid patterns into three-dimensional geometries. It’s like translating a set of IKEA instructions into a built bookshelf, minus the frustration. Through this approach, the model effectively learns how to build a protein from the protein itself, via the language of its amino acids. Remarkably, our method can accurately predict protein structure without a template. It outperforms other folding methods and is significantly faster than physics-based modeling. Because the Protein Data Bank is limited to proteins found in nature, we needed a way to visualize new structures to make new proteins from scratch.

Q: How could the model be used to design an actual protein?

A: We can build atom-by-atom models for sequences found in nature that haven’t yet been studied, as we did in the APL Bioengineering study using a different method. We can visualize the protein’s structure and use other computational methods to assess its function by analyzing its stablity and the other proteins it binds to in cells. Our model could be used in drug design or to interfere with protein-mediated biochemical pathways in infectious disease.

Q: What’s the benefit of translating proteins into sound?

A: Our brains are great at processing sound! In one sweep, our ears pick up all of its hierarchical features: pitch, timbre, volume, melody, rhythm, and chords. We would need a high-powered microscope to see the equivalent detail in an image, and we could never see it all at once. Sound is such an elegant way to access the information stored in a protein.

Typically, sound is made from vibrating a material, like a guitar string, and music is made by arranging sounds in hierarchical patterns. With AI we can combine these concepts, and use molecular vibrations and neural networks to construct new musical forms. We’ve been working on methods to turn protein structures into audible representations, and translate these representations into new materials.

Q: What can the sonification of SARS-CoV-2's ""spike"" protein tell us?

A: Its protein spike contains three protein chains folded into an intriguing pattern. These structures are too small for the eye to see, but they can be heard. We represented the physical protein structure, with it"
142;142;news.mit.edu;http://news.mit.edu/2014/mit-cheetah-robot-runs-jumps-0915;;Bound for robotic glory;"Speed and agility are hallmarks of the cheetah: The big predator is the fastest land animal on Earth, able to accelerate to 60 mph in just a few seconds. As it ramps up to top speed, a cheetah pumps its legs in tandem, bounding until it reaches a full gallop.

Now MIT researchers have developed an algorithm for bounding that they’ve successfully implemented in a robotic cheetah — a sleek, four-legged assemblage of gears, batteries, and electric motors that weighs about as much as its feline counterpart. The team recently took the robot for a test run on MIT’s Killian Court, where it bounded across the grass at a steady clip.

In experiments on an indoor track, the robot sprinted up to 10 mph, even continuing to run after clearing a hurdle. The MIT researchers estimate that the current version of the robot may eventually reach speeds of up to 30 mph.

The key to the bounding algorithm is in programming each of the robot’s legs to exert a certain amount of force in the split second during which it hits the ground, in order to maintain a given speed: In general, the faster the desired speed, the more force must be applied to propel the robot forward. Sangbae Kim, an associate professor of mechanical engineering at MIT, hypothesizes that this force-control approach to robotic running is similar, in principle, to the way world-class sprinters race.

“Many sprinters, like Usain Bolt, don’t cycle their legs really fast,” Kim says. “They actually increase their stride length by pushing downward harder and increasing their ground force, so they can fly more while keeping the same frequency.”

Kim says that by adapting a force-based approach, the cheetah-bot is able to handle rougher terrain, such as bounding across a grassy field. In treadmill experiments, the team found that the robot handled slight bumps in its path, maintaining its speed even as it ran over a foam obstacle.

“Most robots are sluggish and heavy, and thus they cannot control force in high-speed situations,” Kim says. “That’s what makes the MIT cheetah so special: You can actually control the force profile for a very short period of time, followed by a hefty impact with the ground, which makes it more stable, agile, and dynamic.”

Kim says what makes the robot so dynamic is a custom-designed, high-torque-density electric motor, designed by Jeffrey Lang, the Vitesse Professor of Electrical Engineering at MIT. These motors are controlled by amplifiers designed by David Otten, a principal research engineer in MIT’s Research Laboratory of Electronics. The combination of such special electric motors and custom-designed, bio-inspired legs allow force control on the ground without relying on delicate force sensors on the feet.

Kim and his colleagues — research scientist Hae-Won Park and graduate student Meng Yee Chuah — will present details of the bounding algorithm this month at the IEEE/RSJ International Conference on Intelligent Robots and Systems in Chicago.

Toward the ultimate gait

The act of running can be parsed into a number of biomechanically distinct gaits, from trotting and cantering to more dynamic bounding and galloping. In bounding, an animal’s front legs hit the ground together, followed by its hind legs, similar to the way that rabbits hop — a relatively simple gait that the researchers chose to model first.

“Bounding is like an entry-level high-speed gait, and galloping is the ultimate gait,” Kim says. “Once you get bounding, you can easily split the two legs and get galloping.”

As an animal bounds, its legs touch the ground for a fraction of a second before cycling through the air again. The percentage of time a leg spends on the ground rather than in the air is referred to in biomechanics as a “duty cycle”; the faster an animal runs, the shorter its duty cycle.

Kim and his colleagues developed an algorithm that determines the amount of force a leg should exert in the short period of each cycle that it spends on the ground. That force, they reasoned, should be enough for the robot to push up against the downward force of gravity, in order to maintain forward momentum.

“Once I know how long my leg is on the ground and how long my body is in the air, I know how much force I need to apply to compensate for the gravitational force,” Kim says. “Now we’re able to control bounding at many speeds. And to jump, we can, say, triple the force, and it jumps over obstacles.”

In experiments, the team ran the robot at progressively smaller duty cycles, finding that, following the algorithm’s force prescriptions, the robot was able to run at higher speeds without falling. Kim says the team’s algorithm enables precise control over the forces a robot can exert while running.

By contrast, he says, similar quadruped robots may exert high force, but with poor efficiency. What’s more, such robots run on gasoline and are powered by a gasoline engine, in order to generate high forces.

“As a result, they’re way louder,” Kim says. “Our robot can be sile"
143;143;news.mit.edu;http://news.mit.edu/2020/safe-paths-privacy-first-approach-contact-tracing-0410;;Safe Paths: A privacy-first approach to contact tracing;"The research described in this article has been published on a preprint server but has not yet been peer-reviewed by scientific or medical experts.

Fast containment is key to halting the progression of pandemics, and rapid determination of a diagnosed patient’s locations and contact history is a vital step for communities and cities. This process is labor-intensive, susceptible to human memory errors, and fraught with privacy concerns.

Smartphones can aid in this process, though any type of mass surveillance network and analytics can lead to — or be misused by — a surveillance state.

Early contact-tracing tools deployed in certain countries against the current Covid-19 pandemic have indeed helped slow the spread, but have done so at the expense of the privacy of citizens and businesses, exposing even the most private details about individuals.

To help address this urgent challenge, a team led by MIT Media Lab Associate Professor Ramesh Raskar is designing and developing Safe Paths, a citizen-centric, open source, privacy-first set of digital tools and platforms to help stem the spread of Covid-19.

The Safe Paths project is a multi-faculty, cross-MIT effort, with input and expertise from institutes including Harvard University, Stanford University, and the State University of New York at Buffalo; clinical input from Mayo Clinic and Massachusetts General Hospital; and mentors from the World Health Organization, the U.S. Department of Health and Human Services, and the Graduate Institute of International and Development Studies.

A number of leaders and personnel from the global company EY are volunteering their time across many disciplines, including strategy and inclusion on the core initiative leadership team. Numerous additional companies are also participating in this way, including TripleBlind, Public Consulting Group, and Earned Media Consultants.

Experts from government agencies and academic institutes in Canada, Germany, India, Italy, the United Kingdom, and Vietnam are also helping to guide the platform’s development.

The Safe Paths platform, currently in beta, comprises both a smartphone application, PrivateKit, and a web application, Safe Places. The PrivateKit app will enable users to match the personal diary of location data on their smartphone with anonymized, redacted, and blurred location history of infected patients. The digital contact tracing uses overlapped GPS and Bluetooth trails that allow an individual to check if they have crossed paths with someone who was later diagnosed positive for the virus. The PACT Bluetooth protocol, announced earlier by MIT, will be available through Safe Paths. The design of the PACT system has benefited from Safe Paths early work in this area. Through Safe Places, public health officials are equipped to redact location trails of diagnosed carriers and thus broadcast location information with privacy protection for both diagnosed patients and for local businesses.

The platform takes a fundamentally different approach to app-based epidemic analytics, and in the future will use techniques based on Split Learning, research that Raskar’s Camera Culture group at the Media Lab has been developing for the past several years, and which enables distributed deep learning without the sharing of raw data. Safe Paths uses either on-device calculation or encrypted trail match. The Safe Paths platform provides users information on whether they have experienced a near-contact with a diagnosed individual, while maintaining the privacy of both the user and the diagnosed patient. Users long their location history privately on their own phone and remain in control of their data. Diagnosed patients can opt to provide their location history to health officials (providing similar, yet much more accurate, information to the current healthcare intake interviews).

Safe Places also provides a secure tool for public health officials to make infected patient contact history much more efficient, and enables anonymized and safe sharing of patient location history. In the future, this data will also be encrypted.

In the white paper, ""Apps Gone Rogue: Maintaining Personal Privacy in an Epidemic,"" the research team describes the application of contact tracing to slow the spread of epidemics; outlines the current landscape of interventions; and details challenges and risks to data security and privacy protection. Ongoing and collaborative research designed to further explore critical aspects of contact tracing, and to test increasingly robust privacy protection methodologies. Findings will be continuously shared and published.

“We are dedicated to privacy-first solutions — user location and contact history should never leave a user’s phone without direct consent,” Raskar says. “We strongly believe that all users should be in control of their own data, and that we should never need to sacrifice consent for Covid-19 safety.”

Zelalem Temesgen, an infectious disease specialist at May"
144;144;machinelearningmastery.com;https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/;2018-09-20;How to Develop 1D Convolutional Neural Network Models for Human Activity Recognition;"# multi-headed cnn model

from numpy import mean

from numpy import std

from numpy import dstack

from pandas import read_csv

from matplotlib import pyplot

from keras . utils import to_categorical

from keras . utils . vis_utils import plot_model

from keras . models import Model

from keras . layers import Input

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import Dropout

from keras . layers . convolutional import Conv1D

from keras . layers . convolutional import MaxPooling1D

from keras . layers . merge import concatenate

# load a single file as a numpy array

def load_file ( filepath ) :

dataframe = read_csv ( filepath , header = None , delim_whitespace = True )

return dataframe . values

# load a list of files and return as a 3d numpy array

def load_group ( filenames , prefix = '' ) :

loaded = list ( )

for name in filenames :

data = load_file ( prefix + name )

loaded . append ( data )

# stack group so that features are the 3rd dimension

loaded = dstack ( loaded )

return loaded

# load a dataset group, such as train or test

def load_dataset_group ( group , prefix = '' ) :

filepath = prefix + group + '/Inertial Signals/'

# load all 9 files as a single array

filenames = list ( )

# total acceleration

filenames += [ 'total_acc_x_' + group + '.txt' , 'total_acc_y_' + group + '.txt' , 'total_acc_z_' + group + '.txt' ]

# body acceleration

filenames += [ 'body_acc_x_' + group + '.txt' , 'body_acc_y_' + group + '.txt' , 'body_acc_z_' + group + '.txt' ]

# body gyroscope

filenames += [ 'body_gyro_x_' + group + '.txt' , 'body_gyro_y_' + group + '.txt' , 'body_gyro_z_' + group + '.txt' ]

# load input data

X = load_group ( filenames , filepath )

# load class output

y = load_file ( prefix + group + '/y_' + group + '.txt' )

return X , y

# load the dataset, returns train and test X and y elements

def load_dataset ( prefix = '' ) :

# load all train

trainX , trainy = load_dataset_group ( 'train' , prefix + 'HARDataset/' )

print ( trainX . shape , trainy . shape )

# load all test

testX , testy = load_dataset_group ( 'test' , prefix + 'HARDataset/' )

print ( testX . shape , testy . shape )

# zero-offset class values

trainy = trainy - 1

testy = testy - 1

# one hot encode y

trainy = to_categorical ( trainy )

testy = to_categorical ( testy )

print ( trainX . shape , trainy . shape , testX . shape , testy . shape )

return trainX , trainy , testX , testy

# fit and evaluate a model

def evaluate_model ( trainX , trainy , testX , testy ) :

verbose , epochs , batch_size = 0 , 10 , 32

n_timesteps , n_features , n_outputs = trainX . shape [ 1 ] , trainX . shape [ 2 ] , trainy . shape [ 1 ]

# head 1

inputs1 = Input ( shape = ( n_timesteps , n_features ) )

conv1 = Conv1D ( filters = 64 , kernel_size = 3 , activation = 'relu' ) ( inputs1 )

drop1 = Dropout ( 0.5 ) ( conv1 )

pool1 = MaxPooling1D ( pool_size = 2 ) ( drop1 )

flat1 = Flatten ( ) ( pool1 )

# head 2

inputs2 = Input ( shape = ( n_timesteps , n_features ) )

conv2 = Conv1D ( filters = 64 , kernel_size = 5 , activation = 'relu' ) ( inputs2 )

drop2 = Dropout ( 0.5 ) ( conv2 )

pool2 = MaxPooling1D ( pool_size = 2 ) ( drop2 )

flat2 = Flatten ( ) ( pool2 )

# head 3

inputs3 = Input ( shape = ( n_timesteps , n_features ) )

conv3 = Conv1D ( filters = 64 , kernel_size = 11 , activation = 'relu' ) ( inputs3 )

drop3 = Dropout ( 0.5 ) ( conv3 )

pool3 = MaxPooling1D ( pool_size = 2 ) ( drop3 )

flat3 = Flatten ( ) ( pool3 )

# merge

merged = concatenate ( [ flat1 , flat2 , flat3 ] )

# interpretation

dense1 = Dense ( 100 , activation = 'relu' ) ( merged )

outputs = Dense ( n_outputs , activation = 'softmax' ) ( dense1 )

model = Model ( inputs = [ inputs1 , inputs2 , inputs3 ] , outputs = outputs )

# save a plot of the model

plot_model ( model , show_shapes = True , to_file = 'multichannel.png' )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( [ trainX , trainX , trainX ] , trainy , epochs = epochs , batch_size = batch_size , verbose = verbose )

# evaluate model

_ , accuracy = model . evaluate ( [ testX , testX , testX ] , testy , batch_size = batch_size , verbose = 0 )

return accuracy

# summarize scores

def summarize_results ( scores ) :

print ( scores )

m , s = mean ( scores ) , std ( scores )

print ( 'Accuracy: %.3f%% (+/-%.3f)' % ( m , s ) )

# run an experiment

def run_experiment ( repeats = 10 ) :

# load data

trainX , trainy , testX , testy = load_dataset ( )

# repeat experiment

scores = list ( )

for r in range ( repeats ) :

score = evaluate_model ( trainX , trainy , testX , testy )

score = score * 100.0

print ( '>#%d: %.3f' % ( r + 1 , score ) )

scores . append ( score )

# summarize results

summarize_results ( scores )

# run the experiment"
145;145;machinelearningmastery.com;http://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/;2016-05-24;Metrics To Evaluate Machine Learning Algorithms in Python;"# Cross Validation Classification Accuracy

import pandas

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

seed = 7

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

model = LogisticRegression ( )

scoring = 'accuracy'

results = model_selection . cross_val_score ( model , X , Y , cv = kfold , scoring = scoring )"
146;146;news.mit.edu;http://news.mit.edu/2020/machine-learning-picks-out-hidden-vibrations-earthquake-data-0228;;Machine learning picks out hidden vibrations from earthquake data;"Over the last century, scientists have developed methods to map the structures within the Earth’s crust, in order to identify resources such as oil reserves, geothermal sources, and, more recently, reservoirs where excess carbon dioxide could potentially be sequestered. They do so by tracking seismic waves that are produced naturally by earthquakes or artificially via explosives or underwater air guns. The way these waves bounce and scatter through the Earth can give scientists an idea of the type of structures that lie beneath the surface.

There is a narrow range of seismic waves — those that occur at low frequencies of around 1 hertz — that could give scientists the clearest picture of underground structures spanning wide distances. But these waves are often drowned out by Earth’s noisy seismic hum, and are therefore difficult to pick up with current detectors. Specifically generating low-frequency waves would require pumping in enormous amounts of energy. For these reasons, low-frequency seismic waves have largely gone missing in human-generated seismic data.

Now MIT researchers have come up with a machine learning workaround to fill in this gap.

In a paper appearing in the journal Geophysics, they describe a method in which they trained a neural network on hundreds of different simulated earthquakes. When the researchers presented the trained network with only the high-frequency seismic waves produced from a new simulated earthquake, the neural network was able to imitate the physics of wave propagation and accurately estimate the quake’s missing low-frequency waves.

The new method could allow researchers to artificially synthesize the low-frequency waves that are hidden in seismic data, which can then be used to more accurately map the Earth’s internal structures.

“The ultimate dream is to be able to map the whole subsurface, and be able to say, for instance, ‘this is exactly what it looks like underneath Iceland, so now you know where to explore for geothermal sources,’” says co-author Laurent Demanet, professor of applied mathematics at MIT. “Now we’ve shown that deep learning offers a solution to be able to fill in these missing frequencies.”

Demanet’s co-author is lead author Hongyu Sun, a graduate student in MIT’s Department of Earth, Atmospheric and Planetary Sciences.

Speaking another frequency

A neural network is a set of algorithms modeled loosely after the neural workings of the human brain. The algorithms are designed to recognize patterns in data that are fed into the network, and to cluster these data into categories, or labels. A common example of a neural network involves visual processing; the model is trained to classify an image as either a cat or a dog, based on the patterns it recognizes between thousands of images that are specifically labeled as cats, dogs, and other objects.

Sun and Demanet adapted a neural network for signal processing, specifically, to recognize patterns in seismic data. They reasoned that if a neural network was fed enough examples of earthquakes, and the ways in which the resulting high- and low-frequency seismic waves travel through a particular composition of the Earth, the network should be able to, as they write in their paper, “mine the hidden correlations among different frequency components” and extrapolate any missing frequencies if the network were only given an earthquake’s partial seismic profile.

The researchers looked to train a convolutional neural network, or CNN, a class of deep neural networks that is often used to analyze visual information. A CNN very generally consists of an input and output layer, and multiple hidden layers between, that process inputs to identify correlations between them.

Among their many applications, CNNs have been used as a means of generating visual or auditory “deepfakes” — content that has been extrapolated or manipulated through deep-learning and neural networks, to make it seem, for example, as if a woman were talking with a man’s voice.

“If a network has seen enough examples of how to take a male voice and transform it into a female voice or vice versa, you can create a sophisticated box to do that,” Demanet says. “Whereas here we make the Earth speak another frequency — one that didn’t originally go through it.”

Tracking waves

The researchers trained their neural network with inputs that they generated using the Marmousi model, a complex two-dimensional geophysical model that simulates the way seismic waves travel through geological structures of varying density and composition.

In their study, the team used the model to simulate nine “virtual Earths,” each with a different subsurface composition. For each Earth model, they simulated 30 different earthquakes, all with the same strength, but different starting locations. In total, the researchers generated hundreds of different seismic scenarios. They fed the information from almost all of these simulations into their neural network and let the "
147;147;machinelearningmastery.com;https://machinelearningmastery.com/better-deep-learning-neural-networks-crash-course/;2019-02-17;How to Get Better Deep Learning Results (7-Day Mini-Course);"# example of batch gradient descent

from sklearn . datasets import make_circles

from keras . layers import Dense

from keras . models import Sequential

from keras . optimizers import SGD

from matplotlib import pyplot

# generate dataset

X , y = make_circles ( n_samples = 1000 , noise = 0.1 , random_state = 1 )

# split into train and test

n_train = 500

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# define model

model = Sequential ( )

model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = SGD ( lr = 0.01 , momentum = 0.9 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

# fit model

history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 1000 , batch_size = len ( trainX ) , verbose = 0 )

# evaluate the model

_ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 )

_ , test_acc = model . evaluate ( testX , testy , verbose = 0 )

print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) )

# plot loss learning curves

pyplot . subplot ( 211 )

pyplot . title ( 'Cross-Entropy Loss' , pad = - 40 )

pyplot . plot ( history . history [ 'loss' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' )

pyplot . legend ( )

# plot accuracy learning curves

pyplot . subplot ( 212 )

pyplot . title ( 'Accuracy' , pad = - 40 )

pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' )

pyplot . legend ( )"
148;148;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-baseline-forecasts-for-multi-site-multivariate-air-pollution-time-series-forecasting/;2018-10-14;How to Develop Baseline Forecasts for Multi-Site Multivariate Air Pollution Time Series Forecasting;"# forecast local median per hour of day

from numpy import loadtxt

from numpy import nan

from numpy import isnan

from numpy import unique

from numpy import array

from numpy import nanmedian

from matplotlib import pyplot

# split the dataset by 'chunkID', return a list of chunks

def to_chunks ( values , chunk_ix = 0 ) :

chunks = list ( )

# get the unique chunk ids

chunk_ids = unique ( values [ : , chunk_ix ] )

# group rows by chunk id

for chunk_id in chunk_ids :

selection = values [ : , chunk_ix ] == chunk_id

chunks . append ( values [ selection , : ] )

return chunks

# return a list of relative forecast lead times

def get_lead_times ( ) :

return [ 1 , 2 , 3 , 4 , 5 , 10 , 17 , 24 , 48 , 72 ]

# forecast all lead times for one variable

def forecast_variable ( train_chunks , chunk_train , chunk_test , lead_times , target_ix ) :

forecast = list ( )

# convert target number into column number

col_ix = 3 + target_ix

# enumerate lead times

for i in range ( len ( lead_times ) ) :

# get the hour for this forecast lead time

hour = chunk_test [ i , 2 ]

# check for no test data

if isnan ( hour ) :

forecast . append ( nan )

continue

# select rows in chunk with this hour

selected = chunk_train [ chunk_train [ : , 2 ] == hour ]

# calculate the central tendency for target

value = nanmedian ( selected [ : , col_ix ] )

forecast . append ( value )

return forecast

# forecast for each chunk, returns [chunk][variable][time]

def forecast_chunks ( train_chunks , test_input ) :

lead_times = get_lead_times ( )

predictions = list ( )

# enumerate chunks to forecast

for i in range ( len ( train_chunks ) ) :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 39 ) :

yhat = forecast_variable ( train_chunks , train_chunks [ i ] , test_input [ i ] , lead_times , j )

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# convert the test dataset in chunks to [chunk][variable][time] format

def prepare_test_forecasts ( test_chunks ) :

predictions = list ( )

# enumerate chunks to forecast

for rows in test_chunks :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 3 , rows . shape [ 1 ] ) :

yhat = rows [ : , j ]

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# calculate the error between an actual and predicted value

def calculate_error ( actual , predicted ) :

# give the full actual value if predicted is nan

if isnan ( predicted ) :

return abs ( actual )

# calculate abs difference

return abs ( actual - predicted )

# evaluate a forecast in the format [chunk][variable][time]

def evaluate_forecasts ( predictions , testset ) :

lead_times = get_lead_times ( )

total_mae , times_mae = 0.0 , [ 0.0 for _ in range ( len ( lead_times ) ) ]

total_c , times_c = 0 , [ 0 for _ in range ( len ( lead_times ) ) ]

# enumerate test chunks

for i in range ( len ( test_chunks ) ) :

# convert to forecasts

actual = testset [ i ]

predicted = predictions [ i ]

# enumerate target variables

for j in range ( predicted . shape [ 0 ] ) :

# enumerate lead times

for k in range ( len ( lead_times ) ) :

# skip if actual in nan

if isnan ( actual [ j , k ] ) :

continue

# calculate error

error = calculate_error ( actual [ j , k ] , predicted [ j , k ] )

# update statistics

total_mae += error

times_mae [ k ] += error

total_c += 1

times_c [ k ] += 1

# normalize summed absolute errors

total_mae /= total_c

times_mae = [ times_mae [ i ] / times_c [ i ] for i in range ( len ( times_mae ) ) ]

return total_mae , times_mae

# summarize scores

def summarize_error ( name , total_mae , times_mae ) :

# print summary

lead_times = get_lead_times ( )

formatted = [ '+%d %.3f' % ( lead_times [ i ] , times_mae [ i ] ) for i in range ( len ( lead_times ) ) ]

s_scores = ', ' . join ( formatted )

print ( '%s: [%.3f MAE] %s' % ( name , total_mae , s_scores ) )

# plot summary

pyplot . plot ( [ str ( x ) for x in lead_times ] , times_mae , marker = '.' )

pyplot . show ( )

# load dataset

train = loadtxt ( 'AirQualityPrediction/naive_train.csv' , delimiter = ',' )

test = loadtxt ( 'AirQualityPrediction/naive_test.csv' , delimiter = ',' )

# group data by chunks

train_chunks = to_chunks ( train )

test_chunks = to_chunks ( test )

# forecast

test_input = [ rows [ : , : 3 ] for rows in test_chunks ]

forecast = forecast_chunks ( train_chunks , test_input )

# evaluate forecast

actual = prepare_test_forecasts ( test_chunks )

total_mae , times_mae = evaluate_forecasts ( forecast , actual )

# summarize forecast"
149;149;machinelearningmastery.com;http://machinelearningmastery.com/applied-deep-learning-in-python-mini-course/;2016-07-05;Applied Deep Learning in Python Mini-Course;"from keras . models import Sequential

from keras . layers import Dense

# Load the dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# Define and Compile

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# Fit the model

model . fit ( X , Y , epochs = 150 , batch_size = 10 )

# Evaluate the model

scores = model . evaluate ( X , Y )"
150;150;machinelearningmastery.com;https://machinelearningmastery.com/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python/;2018-10-21;How to Grid Search Triple Exponential Smoothing for Time Series Forecasting in Python;"# grid search ets models for monthly car sales

from math import sqrt

from multiprocessing import cpu_count

from joblib import Parallel

from joblib import delayed

from warnings import catch_warnings

from warnings import filterwarnings

from statsmodels . tsa . holtwinters import ExponentialSmoothing

from sklearn . metrics import mean_squared_error

from pandas import read_csv

from numpy import array

# one-step Holt Winter’s Exponential Smoothing forecast

def exp_smoothing_forecast ( history , config ) :

t , d , s , p , b , r = config

# define model

history = array ( history )

model = ExponentialSmoothing ( history , trend = t , damped = d , seasonal = s , seasonal_periods = p )

# fit model

model_fit = model . fit ( optimized = True , use_boxcox = b , remove_bias = r )

# make one step forecast

yhat = model_fit . predict ( len ( history ) , len ( history ) )

return yhat [ 0 ]

# root mean squared error or rmse

def measure_rmse ( actual , predicted ) :

return sqrt ( mean_squared_error ( actual , predicted ) )

# split a univariate dataset into train/test sets

def train_test_split ( data , n_test ) :

return data [ : - n_test ] , data [ - n_test : ]

# walk-forward validation for univariate data

def walk_forward_validation ( data , n_test , cfg ) :

predictions = list ( )

# split dataset

train , test = train_test_split ( data , n_test )

# seed history with training dataset

history = [ x for x in train ]

# step over each time-step in the test set

for i in range ( len ( test ) ) :

# fit model and make forecast for history

yhat = exp_smoothing_forecast ( history , cfg )

# store forecast in list of predictions

predictions . append ( yhat )

# add actual observation to history for the next loop

history . append ( test [ i ] )

# estimate prediction error

error = measure_rmse ( test , predictions )

return error

# score a model, return None on failure

def score_model ( data , n_test , cfg , debug = False ) :

result = None

# convert config to a key

key = str ( cfg )

# show all warnings and fail on exception if debugging

if debug :

result = walk_forward_validation ( data , n_test , cfg )

else :

# one failure during model validation suggests an unstable config

try :

# never show warnings when grid searching, too noisy

with catch_warnings ( ) :

filterwarnings ( ""ignore"" )

result = walk_forward_validation ( data , n_test , cfg )

except :

error = None

# check for an interesting result

if result is not None :

print ( ' > Model[%s] %.3f' % ( key , result ) )

return ( key , result )

# grid search configs

def grid_search ( data , cfg_list , n_test , parallel = True ) :

scores = None

if parallel :

# execute configs in parallel

executor = Parallel ( n_jobs = cpu_count ( ) , backend = 'multiprocessing' )

tasks = ( delayed ( score_model ) ( data , n_test , cfg ) for cfg in cfg_list )

scores = executor ( tasks )

else :

scores = [ score_model ( data , n_test , cfg ) for cfg in cfg_list ]

# remove empty results

scores = [ r for r in scores if r [ 1 ] != None ]

# sort configs by error, asc

scores . sort ( key = lambda tup : tup [ 1 ] )

return scores

# create a set of exponential smoothing configs to try

def exp_smoothing_configs ( seasonal = [ None ] ) :

models = list ( )

# define config lists

t_params = [ 'add' , 'mul' , None ]

d_params = [ True , False ]

s_params = [ 'add' , 'mul' , None ]

p_params = seasonal

b_params = [ True , False ]

r_params = [ True , False ]

# create config instances

for t in t_params :

for d in d_params :

for s in s_params :

for p in p_params :

for b in b_params :

for r in r_params :

cfg = [ t , d , s , p , b , r ]

models . append ( cfg )

return models

if __name__ == '__main__' :

# load dataset

series = read_csv ( 'monthly-car-sales.csv' , header = 0 , index_col = 0 )

data = series . values

# data split

n_test = 12

# model configs

cfg_list = exp_smoothing_configs ( seasonal = [ 0 , 6 , 12 ] )

# grid search

scores = grid_search ( data [ : , 0 ] , cfg_list , n_test )

print ( 'done' )

# list top 3 configs

for cfg , error in scores [ : 3 ] :"
151;151;machinelearningmastery.com;http://machinelearningmastery.com/how-to-identify-outliers-in-your-data/;2013-12-30;How to Identify Outliers in your Data;"Tweet Share Share

Last Updated on May 30, 2019

Bojan Miletic asked a question about outlier detection in datasets when working with machine learning algorithms. This post is in answer to his question.

If you have a question about machine learning, sign-up to the newsletter and reply to an email or use the contact form and ask, I will answer your question and may even turn it into a blog post.

Outliers

Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately poorer results.

Even before predictive models are prepared on training data, outliers can result in misleading representations and in turn misleading interpretations of collected data. Outliers can skew the summary distribution of attribute values in descriptive statistics like mean and standard deviation and in plots such as histograms and scatterplots, compressing the body of the data.

Finally, outliers can represent examples of data instances that are relevant to the problem such as anomalies in the case of fraud detection and computer security.

Outlier Modeling

Outliers are extreme values that fall a long way outside of the other observations. For example, in a normal distribution, outliers may be values on the tails of the distribution.

The process of identifying outliers has many names in data mining and machine learning such as outlier mining, outlier modeling and novelty detection and anomaly detection.

In his book Outlier Analysis (affiliate link), Aggarwal provides a useful taxonomy of outlier detection methods, as follows:

Extreme Value Analysis : Determine the statistical tails of the underlying distribution of the data. For example, statistical methods like the z-scores on univariate data.

: Determine the statistical tails of the underlying distribution of the data. For example, statistical methods like the z-scores on univariate data. Probabilistic and Statistical Models : Determine unlikely instances from a probabilistic model of the data. For example, gaussian mixture models optimized using expectation-maximization.

: Determine unlikely instances from a probabilistic model of the data. For example, gaussian mixture models optimized using expectation-maximization. Linear Models : Projection methods that model the data into lower dimensions using linear correlations. For example, principle component analysis and data with large residual errors may be outliers.

: Projection methods that model the data into lower dimensions using linear correlations. For example, principle component analysis and data with large residual errors may be outliers. Proximity-based Models : Data instances that are isolated from the mass of the data as determined by cluster, density or nearest neighbor analysis.

: Data instances that are isolated from the mass of the data as determined by cluster, density or nearest neighbor analysis. Information Theoretic Models : Outliers are detected as data instances that increase the complexity (minimum code length) of the dataset.

: Outliers are detected as data instances that increase the complexity (minimum code length) of the dataset. High-Dimensional Outlier Detection: Methods that search subspaces for outliers give the breakdown of distance based measures in higher dimensions (curse of dimensionality).

Aggarwal comments that the interpretability of an outlier model is critically important. Context or rationale is required around decisions why a specific data instance is or is not an outlier.

In his contributing chapter to Data Mining and Knowledge Discovery Handbook (affiliate link), Irad Ben-Gal proposes a taxonomy of outlier models as univariate or multivariate and parametric and nonparametric. This is a useful way to structure methods based on what is known about the data. For example:

Are you considered with outliers in one or more than one attributes (univariate or multivariate methods)?

Can you assume a statistical distribution from which the observations were sampled or not (parametric or nonparametric)?

Get Started

There are many methods and much research put into outlier detection. Start by making some assumptions and design experiments where you can clearly observe the effects of the those assumptions against some performance or accuracy measure.

I recommend working through a stepped process from extreme value analysis, proximity methods and projection methods.

Extreme Value Analysis

You do not need to know advanced statistical methods to look for, analyze and filter out outliers from your data. Start out simple with extreme value analysis.

Focus on univariate methods

Visualize the data using scatterplots, histograms and box and whisker plots and look for extreme values

Assume a distribution (Gaussian) and look for values more than 2 or 3 standard deviations f"
152;152;machinelearningmastery.com;https://machinelearningmastery.com/relationship-between-applied-statistics-and-machine-learning/;2018-06-28;The Close Relationship Between Applied Statistics and Machine Learning;"Tweet Share Share

Last Updated on August 8, 2019

The machine learning practitioner has a tradition of algorithms and a pragmatic focus on results and model skill above other concerns such as model interpretability.

Statisticians work on much the same type of modeling problems under the names of applied statistics and statistical learning. Coming from a mathematical background, they have more of a focus on the behavior of models and explainability of predictions.

The very close relationship between the two approaches to the same problem means that both fields have a lot to learn from each other. The statisticians need to consider algorithmic methods was called out in the classic “two cultures” paper. Machine learning practitioners must also take heed, keep an open mind, and learn both the terminology and relevant methods from applied statistics.

In this post, you will discover that machine learning and statistical learning are two closely related but different perspectives on the same problem.

After reading this post, you will know:

“Machine learning” and “predictive modeling” are a computer science perspective on modeling data with a focus on algorithmic methods and model skill.

“Statistics” and “statistical learning” are a mathematical perspective on modeling data with a focus on data models and on goodness of fit.

Machine learning practitioners must keep an open mind and leverage methods and understand the terminology from the closely related fields of applied statistics and statistical learning.

Discover statistical hypothesis testing, resampling methods, estimation statistics and nonparametric methods in my new book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Machine Learning

Machine learning is a subfield of artificial intelligence and is related to the broader field of computer science.

When it comes to developing machine learning models in order to make predictions, there is a heavy focus on algorithms, code, and results.

Machine learning is a lot broader than developing models in order to make predictions, as can be seen by the definition in the classic 1997 textbook by Tom Mitchell.

The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience.

— Page xv, Machine Learning, 1997.

Here, we can see that from a research perspective, machine learning is really the study of learning with computer programs. It just so happens that some of these learning programs are useful for predictive modeling problems, and some in fact have been borrowed from other fields, such as statistics.

Linear regression is a perfect example. It is a more-than-a-century-old method from the (at the time: nascent) field of statistics that is used for fitting a line or plane to real-valued data. From a machine learning perspective, we look at it as a system for learning weights (coefficients) in response to examples from a domain.

Many methods have been developed in the field of artificial intelligence and machine learning, sometimes by statisticians, that prove very useful for the task of predictive modeling. A good example is classification and regression trees that bears no resemblance to classical methods in statistics.

Need help with Statistics for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Predictive Modeling

The useful part of machine learning for the practitioner may be called predictive modeling.

This explicitly ignores distinctions between statistics and machine learning. It also shucks off the broader objectives of statistics (understanding data) and machine learning (understanding learning in software) and only concerns itself, as its name suggests, with developing models that make predictions.

The term predictive modeling may stir associations such as machine learning, pattern recognition, and data mining. Indeed, these associations are appropriate and the methods implied by these terms are an integral piece of the predictive modeling process. But predictive modeling encompasses much more than the tools and techniques for uncovering patterns within data. The practice of predictive modeling defines the process of developing a model in a way that we can understand and quantify the model’s prediction accuracy on future, yet-to-be-seen data.

— Page vii, Applied Predictive Modeling, 2013

Predictive modeling provides a laser-focus on developing models with the objective of getting the best possible results with regard to some measure of model skill. This pragmatic approach often means that results in the form of maximum skill or minimum error are sought at the expense of almost everything else.

It doesn’t really matter what we call the process, machine learning or predictive modeling. In some sense it is marketing and group identification. Getting result"
153;153;news.mit.edu;http://news.mit.edu/2019/mit-briefing-bold-action-needed-make-technology-work-society;;MIT Briefing: Bold action needed to make technology work for society;"CAMBRIDGE, Mass. -- On Tuesday, September 10, the MIT Task Force on the Work of the Future will detail their findings on how automation, robotics and artificial intelligence are likely to impact the American workforce, and describe public and private action they say is needed to harness new technologies for shared prosperity. The briefing will be held at the National Press Club in Washington, D.C. at 8:30 A.M., and will feature remarks from MIT President L. Rafael Reif and MIT researchers, as well as a panel discussion and opportunity for questions.

The MIT Task Force on the Work of the Future – which is led by David Autor, the Ford Professor of Economics; David Mindell, the Frances and David Dibner Professor of the History of Engineering and Manufacturing, and a professor of aeronautics and astronautics; and Elisabeth Reynolds, executive director of the Task Force on the Work of the Future and a lecturer in the Department of Urban Studies and Planning – consists of dozens of MIT scholars, as well as an advisory board of business executives and policy leaders. The Task Force report draws on new data, the expert knowledge of many technology sectors, and a close analysis of both technology-centered firms and economic data spanning the postwar era.

In recent decades, technology has contributed to the polarization of employment, helping high-skilled professionals while reducing opportunities for many other workers. A critical challenge is not necessarily a lack of jobs, but the low quality of jobs and the resulting lack of viable careers for many people. With this in mind, the MIT Task Force on the Work of the Future finds that the future of work can be shaped beneficially by new policies, renewed support for labor, and reformed institutions, not just new technologies.

Eduardo Porter, an economics reporter for The New York Times, will moderate the panel discussion. Participants include: John E. Kelly III, executive vice president of IBM; Juan Salgado, chancellor of the City Colleges of Chicago; and Liz Shuler, secretary-treasurer of the American Federation of Labor and Congress of Industrial Organizations (AFL-CIO).

WHAT:

MIT Task Force on the Work of the Future Briefing

WHO:

MIT speakers:

L. Rafael Reif , MIT President;

, MIT President; David Autor , Ford Professor of Economics

, Ford Professor of Economics David Mindell , Frances and David Dibner Professor of the History of Engineering and Manufacturing, and a professor of aeronautics and astronautics;

, Frances and David Dibner Professor of the History of Engineering and Manufacturing, and a professor of aeronautics and astronautics; Elisabeth Reynolds, executive director of the Task Force on the Work of the Future and the MIT Industrial Performance Center (IPC), and a lecturer in the Department of Urban Studies and Planning

Guest speakers and participants:

Eduardo Porter , economics reporter, The New York Times;

, economics reporter, The New York Times; John E. Kelly III , executive vice President, IBM;

, executive vice President, IBM; Juan Salgado , chancellor, City Colleges of Chicago;

, chancellor, City Colleges of Chicago; Liz Shuler, secretary-treasurer, AFL-CIO

WHERE:

National Press Club

First Amendment Lounge

529 14th Street, NW, 13th Floor

Washington, D.C. 20045

WHEN:

Tuesday, September 10, 2019

8:00 A.M.: Coffee and breakfast

8:30 A.M. – 9:45 A.M.: Remarks and panel discussion

Media RSVP:

Reporters interested in attending should email Abby Abazorius at abbya@mit.edu or expertrequests@mit.edu to RSVP and for more information.

###"
154;154;machinelearningmastery.com;https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/;2017-07-16;Gentle Introduction to Models for Sequence Prediction with RNNs;"Tweet Share Share

Last Updated on August 25, 2019

Sequence prediction is a problem that involves using historical sequence information to predict the next value or values in the sequence.

The sequence may be symbols like letters in a sentence or real values like those in a time series of prices. Sequence prediction may be easiest to understand in the context of time series forecasting as the problem is already generally understood.

In this post, you will discover the standard sequence prediction models that you can use to frame your own sequence prediction problems.

After reading this post, you will know:

How sequence prediction problems are modeled with recurrent neural networks.

The 4 standard sequence prediction models used by recurrent neural networks.

The 2 most common misunderstandings made by beginners when applying sequence prediction models.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Tutorial Overview

This tutorial is divided into 4 parts; they are:

Sequence Prediction with Recurrent Neural Networks Models for Sequence Prediction Cardinality from Timesteps not Features Two Common Misunderstandings by Practitioners

Sequence Prediction with Recurrent Neural Networks

Recurrent Neural Networks, like Long Short-Term Memory (LSTM) networks, are designed for sequence prediction problems.

In fact, at the time of writing, LSTMs achieve state-of-the-art results in challenging sequence prediction problems like neural machine translation (translating English to French).

LSTMs work by learning a function (f(…)) that maps input sequence values (X) onto output sequence values (y).

y(t) = f(X(t)) 1 y(t) = f(X(t))

The learned mapping function is static and may be thought of as a program that takes input variables and uses internal variables. Internal variables are represented by an internal state maintained by the network and built up or accumulated over each value in the input sequence.

… RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables.

— Andrej Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks, 2015

The static mapping function may be defined with a different number of inputs or outputs, as we will review in the next section.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Models for Sequence Prediction

In this section, will review the 4 primary models for sequence prediction.

We will use the following terminology:

X: The input sequence value, may be delimited by a time step, e.g. X(1).

u: The hidden state value, may be delimited by a time step, e.g. u(1).

y: The output sequence value, may be delimited by a time step, e.g. y(1).

One-to-One Model

A one-to-one model produces one output value for each input value.

The internal state for the first time step is zero; from that point onward, the internal state is accumulated over the prior time steps.

In the case of a sequence prediction, this model would produce one time step forecast for each observed time step received as input.

This is a poor use for RNNs as the model has no chance to learn over input or output time steps (e.g. BPTT). If you find implementing this model for sequence prediction, you may intend to be using a many-to-one model instead.

One-to-Many Model

A one-to-many model produces multiple output values for one input value.

The internal state is accumulated as each value in the output sequence is produced.

This model can be used for image captioning where one image is provided as input and a sequence of words are generated as output.

Many-to-One Model

A many-to-one model produces one output value after receiving multiple input values.

The internal state is accumulated with each input value before a final output value is produced.

In the case of time series, this model would use a sequence of recent observations to forecast the next time step. This architecture would represent the classical autoregressive time series model.

Many-to-Many Model

A many-to-many model produces multiple outputs after receiving multiple input values.

As with the many-to-one case, state is accumulated until the first output is created, but in this case multiple time steps are output.

Importantly, the number of input time steps do not have to match the number of output time steps. Think of the input and output time steps operating at different rates.

In the case of time series forecasting, this model would use a sequence of recent observations to make a multi-step forecast.

In a sense, it combi"
155;155;machinelearningmastery.com;http://machinelearningmastery.com/persistence-time-series-forecasting-with-python/;2016-12-25;How to Make Baseline Predictions for Time Series Forecasting with Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41

from pandas import read_csv from pandas import datetime from pandas import DataFrame from pandas import concat from matplotlib import pyplot from sklearn . metrics import mean_squared_error def parser ( x ) : return datetime . strptime ( '190' + x , '%Y-%m' ) series = read_csv ( 'shampoo-sales.csv' , header = 0 , parse_dates = [ 0 ] , index_col = 0 , squeeze = True , date_parser = parser ) # Create lagged dataset values = DataFrame ( series . values ) dataframe = concat ( [ values . shift ( 1 ) , values ] , axis = 1 ) dataframe . columns = [ 't-1' , 't+1' ] print ( dataframe . head ( 5 ) ) # split into train and test sets X = dataframe . values train_size = int ( len ( X ) * 0.66 ) train , test = X [ 1 : train_size ] , X [ train_size : ] train_X , train_y = train [ : , 0 ] , train [ : , 1 ] test_X , test_y = test [ : , 0 ] , test [ : , 1 ] # persistence model def model_persistence ( x ) : return x # walk-forward validation predictions = list ( ) for x in test_X : yhat = model_persistence ( x ) predictions . append ( yhat ) test_score = mean_squared_error ( test_y , predictions ) print ( 'Test MSE: %.3f' % test_score ) # plot predictions and expected results pyplot . plot ( train_y ) pyplot . plot ( [ None for i in train_y ] + [ x for x in test_y ] ) pyplot . plot ( [ None for i in train_y ] + [ x for x in predictions ] ) pyplot . show ( )"
156;156;machinelearningmastery.com;https://machinelearningmastery.com/indoor-movement-time-series-classification-with-machine-learning-algorithms/;2018-09-09;Indoor Movement Time Series Classification with Machine Learning Algorithms;"# spot check for ES2

from pandas import read_csv

from matplotlib import pyplot

from sklearn . metrics import accuracy_score

from sklearn . linear_model import LogisticRegression

from sklearn . neighbors import KNeighborsClassifier

from sklearn . tree import DecisionTreeClassifier

from sklearn . svm import SVC

from sklearn . ensemble import RandomForestClassifier

from sklearn . ensemble import GradientBoostingClassifier

from sklearn . pipeline import Pipeline

from sklearn . preprocessing import StandardScaler

# load dataset

train = read_csv ( 'es2_train.csv' , header = None )

test = read_csv ( 'es2_test.csv' , header = None )

# split into inputs and outputs

trainX , trainy = train . values [ : , : - 1 ] , train . values [ : , - 1 ]

testX , testy = test . values [ : , : - 1 ] , test . values [ : , - 1 ]

# create a list of models to evaluate

models , names = list ( ) , list ( )

# logistic

models . append ( LogisticRegression ( ) )

names . append ( 'LR' )

# knn

models . append ( KNeighborsClassifier ( ) )

names . append ( 'KNN' )

# knn

models . append ( KNeighborsClassifier ( n_neighbors = 7 ) )

names . append ( 'KNN-7' )

# cart

models . append ( DecisionTreeClassifier ( ) )

names . append ( 'CART' )

# svm

models . append ( SVC ( ) )

names . append ( 'SVM' )

# random forest

models . append ( RandomForestClassifier ( ) )

names . append ( 'RF' )

# gbm

models . append ( GradientBoostingClassifier ( ) )

names . append ( 'GBM' )

# evaluate models

all_scores = list ( )

for i in range ( len ( models ) ) :

# create a pipeline for the model

scaler = StandardScaler ( )

model = Pipeline ( steps = [ ( 's' , scaler ) , ( 'm' , models [ i ] ) ] )

# fit

# model = models[i]

model . fit ( trainX , trainy )

# predict

yhat = model . predict ( testX )

# evaluate

score = accuracy_score ( testy , yhat ) * 100

all_scores . append ( score )

# summarize

print ( '%s %.3f%%' % ( names [ i ] , score ) )

# plot

pyplot . bar ( names , all_scores )"
157;157;machinelearningmastery.com;https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/;2018-12-20;How to Develop an Ensemble of Deep Learning Models in Keras;"Tweet Share Share

Last Updated on January 10, 2020

Deep learning neural network models are highly flexible nonlinear algorithms capable of learning a near infinite number of mapping functions.

A frustration with this flexibility is the high variance in a final model. The same neural network model trained on the same dataset may find one of many different possible “good enough” solutions each time it is run.

Model averaging is an ensemble learning technique that reduces the variance in a final neural network model, sacrificing spread in the performance of the model for a confidence in what performance to expect from the model.

In this tutorial, you will discover how to develop a model averaging ensemble in Keras to reduce the variance in a final model.

After completing this tutorial, you will know:

Model averaging is an ensemble learning technique that can be used to reduce the expected variance of deep learning neural network models.

How to implement model averaging in Keras for classification and regression predictive modeling problems.

How to work through a multi-class classification problem and use model averaging to reduce the variance of the final model.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Updated Oct/2019 : Updated for Keras 2.3 and TensorFlow 2.0.

: Updated for Keras 2.3 and TensorFlow 2.0. Updated Jan/2020: Updated for changes in scikit-learn v0.22 API.

Tutorial Overview

This tutorial is divided into six parts; they are:

Model Averaging How to Average Models in Keras Multi-Class Classification Problem MLP Model for Multi-Class Classification High Variance of MLP Model Model Averaging Ensemble

Model Averaging

Deep learning neural network models are nonlinear methods that learn via a stochastic training algorithm.

This means that they are highly flexible, capable of learning complex relationships between variables and approximating any mapping function, given enough resources. A downside of this flexibility is that the models suffer high variance.

This means that the models are highly dependent on the specific training data used to train the model and on the initial conditions (random initial weights) and serendipity during the training process. The result is a final model that makes different predictions each time the same model configuration is trained on the same dataset.

This can be frustrating when training a final model for use in making predictions on new data, such as operationally or in a machine learning competition.

The high variance of the approach can be addressed by training multiple models for the problem and combining their predictions. This approach is called model averaging and belongs to a family of techniques called ensemble learning.

Want Better Results with Deep Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Average Models in Keras

The simplest way to develop a model averaging ensemble in Keras is to train multiple models on the same dataset then combine the predictions from each of the trained models.

Train Multiple Models

Training multiple models may be resource intensive, depending on the size of the model and the size of the training data.

You may have to train the models sequentially on the same hardware. For very large models, it may be worth training the models in parallel using cloud infrastructure such as Amazon Web Services.

The number of models required for the ensemble may vary based on the complexity of the problem and model. A benefit of the approach is that you can continue to create models, add them to the ensemble, and evaluate their impact on the performance by making predictions on a holdout test set.

For small models, you can train the models sequentially and keep them in memory for use in your experiment. For example:

... # train models and keep them in memory n_members = 10 models = list() for _ in range(n_members): # define and fit model model = ... # store model in memory as ensemble member models.add(models) ... 1 2 3 4 5 6 7 8 9 10 . . . # train models and keep them in memory n_members = 10 models = list ( ) for _ in range ( n_members ) : # define and fit model model = . . . # store model in memory as ensemble member models . add ( models ) . . .

For large models, perhaps trained on different hardware, you can save each model to file.

... # train models and keep them to file n_members = 10 for i in range(n_members): # define and fit model model = ... # save model to file filename = 'model_' + str(i + 1) + '.h5' model.save(filename) print('Saved: %s' % filename) ... 1 2 3 4 5 6 7 8 9 10 11 . . . # train models and keep them to file n_members = 10 for i in range ( n_members ) : # define and fit model model = . . . # save model to file file"
158;158;machinelearningmastery.com;https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/;2017-04-06;Time Series Forecasting with the Long Short-Term Memory Network in Python;"from pandas import DataFrame

from pandas import Series

from pandas import concat

from pandas import read_csv

from pandas import datetime

from sklearn . metrics import mean_squared_error

from sklearn . preprocessing import MinMaxScaler

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from math import sqrt

from matplotlib import pyplot

import numpy

# date-time parsing function for loading the dataset

def parser ( x ) :

return datetime . strptime ( '190' + x , '%Y-%m' )

# frame a sequence as a supervised learning problem

def timeseries_to_supervised ( data , lag = 1 ) :

df = DataFrame ( data )

columns = [ df . shift ( i ) for i in range ( 1 , lag + 1 ) ]

columns . append ( df )

df = concat ( columns , axis = 1 )

df . fillna ( 0 , inplace = True )

return df

# create a differenced series

def difference ( dataset , interval = 1 ) :

diff = list ( )

for i in range ( interval , len ( dataset ) ) :

value = dataset [ i ] - dataset [ i - interval ]

diff . append ( value )

return Series ( diff )

# invert differenced value

def inverse_difference ( history , yhat , interval = 1 ) :

return yhat + history [ - interval ]

# scale train and test data to [-1, 1]

def scale ( train , test ) :

# fit scaler

scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) )

scaler = scaler . fit ( train )

# transform train

train = train . reshape ( train . shape [ 0 ] , train . shape [ 1 ] )

train_scaled = scaler . transform ( train )

# transform test

test = test . reshape ( test . shape [ 0 ] , test . shape [ 1 ] )

test_scaled = scaler . transform ( test )

return scaler , train_scaled , test_scaled

# inverse scaling for a forecasted value

def invert_scale ( scaler , X , value ) :

new_row = [ x for x in X ] + [ value ]

array = numpy . array ( new_row )

array = array . reshape ( 1 , len ( array ) )

inverted = scaler . inverse_transform ( array )

return inverted [ 0 , - 1 ]

# fit an LSTM network to training data

def fit_lstm ( train , batch_size , nb_epoch , neurons ) :

X , y = train [ : , 0 : - 1 ] , train [ : , - 1 ]

X = X . reshape ( X . shape [ 0 ] , 1 , X . shape [ 1 ] )

model = Sequential ( )

model . add ( LSTM ( neurons , batch_input_shape = ( batch_size , X . shape [ 1 ] , X . shape [ 2 ] ) , stateful = True ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mean_squared_error' , optimizer = 'adam' )

for i in range ( nb_epoch ) :

model . fit ( X , y , epochs = 1 , batch_size = batch_size , verbose = 0 , shuffle = False )

model . reset_states ( )

return model

# make a one-step forecast

def forecast_lstm ( model , batch_size , X ) :

X = X . reshape ( 1 , 1 , len ( X ) )

yhat = model . predict ( X , batch_size = batch_size )

return yhat [ 0 , 0 ]

# load dataset

series = read_csv ( 'shampoo-sales.csv' , header = 0 , parse_dates = [ 0 ] , index_col = 0 , squeeze = True , date_parser = parser )

# transform data to be stationary

raw_values = series . values

diff_values = difference ( raw_values , 1 )

# transform data to be supervised learning

supervised = timeseries_to_supervised ( diff_values , 1 )

supervised_values = supervised . values

# split data into train and test-sets

train , test = supervised_values [ 0 : - 12 ] , supervised_values [ - 12 : ]

# transform the scale of the data

scaler , train_scaled , test_scaled = scale ( train , test )

# repeat experiment

repeats = 30

error_scores = list ( )

for r in range ( repeats ) :

# fit the model

lstm_model = fit_lstm ( train_scaled , 1 , 3000 , 4 )

# forecast the entire training dataset to build up state for forecasting

train_reshaped = train_scaled [ : , 0 ] . reshape ( len ( train_scaled ) , 1 , 1 )

lstm_model . predict ( train_reshaped , batch_size = 1 )

# walk-forward validation on the test data

predictions = list ( )

for i in range ( len ( test_scaled ) ) :

# make one-step forecast

X , y = test_scaled [ i , 0 : - 1 ] , test_scaled [ i , - 1 ]

yhat = forecast_lstm ( lstm_model , 1 , X )

# invert scaling

yhat = invert_scale ( scaler , X , yhat )

# invert differencing

yhat = inverse_difference ( raw_values , yhat , len ( test_scaled ) + 1 - i )

# store forecast

predictions . append ( yhat )

# report performance

rmse = sqrt ( mean_squared_error ( raw_values [ - 12 : ] , predictions ) )

print ( '%d) Test RMSE: %.3f' % ( r + 1 , rmse ) )

error_scores . append ( rmse )

# summarize results

results = DataFrame ( )

results [ 'rmse' ] = error_scores

print ( results . describe ( ) )

results . boxplot ( )"
159;159;machinelearningmastery.com;http://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/;2016-04-05;Linear Discriminant Analysis for Machine Learning;"Tweet Share Share

Last Updated on February 4, 2020

Logistic regression is a classification algorithm traditionally limited to only two-class classification problems.

If you have more than two classes then Linear Discriminant Analysis is the preferred linear classification technique.

In this post you will discover the Linear Discriminant Analysis (LDA) algorithm for classification predictive modeling problems. After reading this post you will know:

The limitations of logistic regression and the need for linear discriminant analysis.

The representation of the model that is learned from data and can be saved to file.

How the model is estimated from your data.

How to make predictions from a learned LDA model.

How to prepare your data to get the most from the LDA model.

This post is intended for developers interested in applied machine learning, how the models work and how to use them well. As such no background in statistics or linear algebra is required, although it does help if you know about the mean and variance of a distribution.

LDA is a simple model in both preparation and application. There is some interesting statistics behind how the model is setup and how the prediction equation is derived, but is not covered in this post.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Limitations of Logistic Regression

Logistic regression is a simple and powerful linear classification algorithm. It also has limitations that suggest at the need for alternate linear classification algorithms.

Two-Class Problems . Logistic regression is intended for two-class or binary classification problems. It can be extended for multi-class classification, but is rarely used for this purpose.

. Logistic regression is intended for two-class or binary classification problems. It can be extended for multi-class classification, but is rarely used for this purpose. Unstable With Well Separated Classes . Logistic regression can become unstable when the classes are well separated.

. Logistic regression can become unstable when the classes are well separated. Unstable With Few Examples. Logistic regression can become unstable when there are few examples from which to estimate the parameters.

Linear Discriminant Analysis does address each of these points and is the go-to linear method for multi-class classification problems. Even with binary-classification problems, it is a good idea to try both logistic regression and linear discriminant analysis.

Representation of LDA Models

The representation of LDA is straight forward.

It consists of statistical properties of your data, calculated for each class. For a single input variable (x) this is the mean and the variance of the variable for each class. For multiple variables, this is the same properties calculated over the multivariate Gaussian, namely the means and the covariance matrix.

These statistical properties are estimated from your data and plug into the LDA equation to make predictions. These are the model values that you would save to file for your model.

Let’s look at how these parameters are estimated.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Learning LDA Models

LDA makes some simplifying assumptions about your data:

That your data is Gaussian, that each variable is is shaped like a bell curve when plotted. That each attribute has the same variance, that values of each variable vary around the mean by the same amount on average.

With these assumptions, the LDA model estimates the mean and variance from your data for each class. It is easy to think about this in the univariate (single input variable) case with two classes.

The mean (mu) value of each input (x) for each class (k) can be estimated in the normal way by dividing the sum of values by the total number of values.

muk = 1/nk * sum(x)

Where muk is the mean value of x for the class k, nk is the number of instances with class k. The variance is calculated across all classes as the average squared difference of each value from the mean.

sigma^2 = 1 / (n-K) * sum((x – mu)^2)

Where sigma^2 is the variance across all inputs (x), n is the number of instances, K is the number of classes and mu is the mean for input x.

Making Predictions with LDA

LDA makes predictions by estimating the probability that a new set of inputs belongs to each class. The class that gets the highest probability is the output class and a prediction is made.

The model uses Bayes Theorem to estimate the probabilities. Briefly Bayes’ Theorem can be used to estimate the probability of the output class (k) given the input (x) using the probability of each class and the probability of the da"
160;160;news.mit.edu;http://news.mit.edu/2020/neural-networks-optimize-materials-search-0326;;Neural networks facilitate optimization in the search for new materials;"When searching through theoretical lists of possible new materials for particular applications, such as batteries or other energy-related devices, there are often millions of potential materials that could be considered, and multiple criteria that need to be met and optimized at once. Now, researchers at MIT have found a way to dramatically streamline the discovery process, using a machine learning system.

As a demonstration, the team arrived at a set of the eight most promising materials, out of nearly 3 million candidates, for an energy storage system called a flow battery. This culling process would have taken 50 years by conventional analytical methods, they say, but they accomplished it in five weeks.

The findings are reported in the journal ACS Central Science, in a paper by MIT professor of chemical engineering Heather Kulik, Jon Paul Janet PhD ’19, Sahasrajit Ramesh, and graduate student Chenru Duan.

The study looked at a set of materials called transition metal complexes. These can exist in a vast number of different forms, and Kulik says they “are really fascinating, functional materials that are unlike a lot of other material phases. The only way to understand why they work the way they do is to study them using quantum mechanics.”

To predict the properties of any one of millions of these materials would require either time-consuming and resource-intensive spectroscopy and other lab work, or time-consuming, highly complex physics-based computer modeling for each possible candidate material or combination of materials. Each such study could consume hours to days of work.

Instead, Kulik and her team took a small number of different possible materials and used them to teach an advanced machine-learning neural network about the relationship between the materials’ chemical compositions and their physical properties. That knowledge was then applied to generate suggestions for the next generation of possible materials to be used for the next round of training of the neural network. Through four successive iterations of this process, the neural network improved significantly each time, until reaching a point where it was clear that further iterations would not yield any further improvements.

This iterative optimization system greatly streamlined the process of arriving at potential solutions that satisfied the two conflicting criteria being sought. This kind of process of finding the best solutions in situations, where improving one factor tends to worsen the other, is known as a Pareto front, representing a graph of the points such that any further improvement of one factor would make the other worse. In other words, the graph represents the best possible compromise points, depending on the relative importance assigned to each factor.

Training typical neural networks requires very large data sets, ranging from thousands to millions of examples, but Kulik and her team were able to use this iterative process, based on the Pareto front model, to streamline the process and provide reliable results using only the few hundred samples.

In the case of screening for the flow battery materials, the desired characteristics were in conflict, as is often the case: The optimum material would have high solubility and a high energy density (the ability to store energy for a given weight). But increasing solubility tends to decrease the energy density, and vice versa.

Not only was the neural network able to rapidly come up with promising candidates, it also was able to assign levels of confidence to its different predictions through each iteration, which helped to allow the refinement of the sample selection at each step. “We developed a better than best-in-class uncertainty quantification technique for really knowing when these models were going to fail,” Kulik says.

The challenge they chose for the proof-of-concept trial was materials for use in redox flow batteries, a type of battery that holds promise for large, grid-scale batteries that could play a significant role in enabling clean, renewable energy. Transition metal complexes are the preferred category of materials for such batteries, Kulik says, but there are too many possibilities to evaluate by conventional means. They started out with a list of 3 million such complexes before ultimately whittling that down to the eight good candidates, along with a set of design rules that should enable experimentalists to explore the potential of these candidates and their variations.

“Through that process, the neural net both gets increasingly smarter about the [design] space, but also increasingly pessimistic that anything beyond what we’ve already characterized can further improve on what we already know,” she says.

Apart from the specific transition metal complexes suggested for further investigation using this system, she says, the method itself could have much broader applications. “We do view it as the framework that can be applied to any materials desig"
161;161;machinelearningmastery.com;http://machinelearningmastery.com/how-to-use-r-for-machine-learning/;2016-01-17;How To Use R For Machine Learning;"Tweet Share Share

Last Updated on August 22, 2019

There are a ton of packages for R. Which ones are best to use for your machine learning project?

In this post you will discover the exact R functions and packages recommended for each sub task in a machine learning journey.

This is useful. Bookmark this page. I’m sure you will be checking back time and again.

If you’re an R user and know a better way, share it in the comments and I will update the list.

Discover how to prepare data, fit machine learning models and evaluate their predictions in R with my new book, including 14 step-by-step tutorials, 3 projects, and full source code.

Let’s get started.

What R Packages Should You Use?

There are more than 6,000 third party packages for R. The vast number of packages available is one of the benefits of the R platform. It is also the frustration.

Which packages should you use?

There are specific tasks that you need to perform as part of a machine learning project. Tasks like loading data, evaluating algorithms and improving accuracy. You can use multiple techniques for each task and multiple packages may provide those techniques.

Given that there are so many different ways to complete a given subtask, you need to discover those functions and packages that best meet your needs.

Map Best-Of-Breed Packages Onto Project Tasks

The way to solve this problem is to create a mapping of all of the sub-tasks you are likely to work on during a machine learning project and find the best-of-breed packages and functions that you can use.

You start by listing all of the sub-tasks in a machine learning project. You can take a close look at the process of applied machine learning and the machine learning project checklist.

Given that R is a statistical language, it provides a lot of tools that you can use for data analysis as well as predictive models that you can train and use to generate predictions.

Using your favorite search engine, you can locate all of the packages and functions in packages that you can use to complete each task. This can be exhaustive and you can end up with many different candidate solutions.

You need to reduce each list of options down to the one preferred way of completing a task. You could experiment with each and see what works for you. You could also carefully review you search results and tease out the most popular functions used by practitioners.

Next up is a mapping from R packages and functions to the tasks of a machine learning project that you can use to get started today using R for machine learning.

Need more Help with R for Machine Learning? Take my free 14-day email course and discover how to use R on your project (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

How To Use R For Machine Learning Projects

This section lists many of the main sub-tasks of a generic machine learning project. Each task lists the specific function and parent package that you can use in R to complete the task.

Some properties of the chosen functions are:

Minimum : the list is a bare minimum of both the machine learning tasks in a project and only the function and package name that you can use. More homework is required to actually use each of the functions listed.

: the list is a bare minimum of both the machine learning tasks in a project and only the function and package name that you can use. More homework is required to actually use each of the functions listed. Simple : functions were chosen for simplicity in delivering a direct result for the task. One function was preferred over multiple function calls.

: functions were chosen for simplicity in delivering a direct result for the task. One function was preferred over multiple function calls. Preference: functions were chosen based on my preferences and best estimation. Other practitioners may have different alternatives (share in the comments!).

Tasks are organized into three broad groups:

Data preparation tasks for getting data ready for modeling. Evaluating algorithm tasks for racing and evaluating predictive modeling algorithms. Improve results tasks for getting more out of well performing algorithms.

1. Data Preparation Tasks

Data Loading

Load a dataset from your file.

CSV: read.csv function from the utils package

Data Cleaning

Clean up a dataset to ensure that the data is reasonable and consistent ready for analysis and modeling.

Imputing: impute from the Hmisc package.

Outliers: various functions from the outliers package.

Rebalance: SMOTE function from the DMwR package.

Data Summary

Summarize a dataset using descriptive statistics.

Summarize Distributions: summary function from base package.

Summarize Correlations: cor function from the stats package

Data Visualization

Summarize a dataset visually.

Scatterplot Matrix: pairs function from the graphics package.

Histogram: hist function from the graphics package.

Density Plot: densitypl"
162;162;news.mit.edu;http://news.mit.edu/2019/g-anthony-grant-named-director-athletics-daper-department-head-1209;;G. Anthony Grant named director of athletics, DAPER department head;"G. Anthony Grant has been named MIT’s new athletic director and head of the Department of Athletics, Physical Education, and Recreation (DAPER).

“This role is a big one, and I am excited for Dr. Grant as he begins an exciting new phase of his career, and for the MIT community as we welcome a new leader to our team,” says Suzy M. Nelson, vice president and dean for student life.

Grant joins MIT from Metropolitan State University (MSU Denver) in Colorado, where he has been director of athletics. In more than four years at MSU Denver, Grant worked very closely with student-athletes and university leadership to align the athletics department’s values with those of the university overall. Grant also collaborated with internal and external constituents to develop a three-year strategic plan that set the department’s future direction.

“It is an honor to be selected as the director of athletics and department head of DAPER at MIT,” says Grant. “I would like to thank Suzy Nelson, Judy Robinson, and the members of the search committee for this tremendous opportunity. There is important work being done in DAPER and the Division of Student Life. I am eager to collaborate with faculty, staff, students, and supporters to continue to build on the academic and athletic success that has been established within the department, as well as to provide a service to the broader community as it relates to promoting an environment of health and wellness throughout campus.”

In competition, teams in Grant’s program were competitive and successful in the Rocky Mountain Athletic Conference (RMAC), as well as on the regional and national level within NCAA Division II. The program also boasted an excellent academic record, with an average of 78 student-athletes on the athletic director’s honor roll each semester. These academic achievements rank among Grants’ proudest achievements during his MSU Denver tenure.

As a thought leader among collegiate athletic directors, Grant committed himself to supporting underrepresented minority student-athletes and administrators by serving on the NCAA Minority Opportunities and Interests Committee, and the RMAC Diversity and Inclusion Committee. Before MSU Denver, Grant held positions of increasing responsibility in the athletic departments at Millersville University, Pennsylvania, and the University of Iowa, where he completed his PhD. He also holds a bachelor’s degree from Penn State University and a master’s from Temple University."
163;163;machinelearningmastery.com;https://machinelearningmastery.com/dont-start-with-open-source-code-when-implementing-machine-learning-algorithms/;2014-10-30;Don’t Start with Open-Source Code When Implementing Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

Edward Raff is the author of the Java Machine Learning library called JSAT (which is an acronym for Java Statistical Analysis Tool).

Edward has implemented many algorithms in creating this library and I recently reached out to him and asked what advice he could give to beginners implementing machine learning algorithms from scratch.

In this post we take a look at tips on implementing machine learning algorithms based on Edwards advice.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Don’t Read Other Peoples Source Code

At least, not initially.

What drew me to ask Edward questions about his advice on implementing machine learning algorithms from scratch was his comment on a Reddit question, titled appropriately “Implementing Machine Learning Algorithms“.

In his comment, Edward suggested that beginners avoid looking at source code of other open source implementations as much as possible. He knew this was counter to most advice (even my own) and it really caught my attention.

Edward start’s out suggesting that there are two quite different tasks when implementing machine learning algorithms:

Implementing Well Known Algorithms. These are well described in many papers, books, websites lecture notes and so on. You have many sources, they algorithms are relatively straight forward and they are good case studies for self education. Implementing Algorithms From Papers. These are algorithms that have limited and sparse descriptions in literature and require significant work and prior knowledge to understand and implement.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Implementing Well Known Algorithms

Edward suggests reading code is a bad idea if you are interested in implementing well known algorithms.

I distilled at least 3 key reasons for why this is the case from his comments:

Code Optimization : Code in open source libraries is very likely highly optimized for execution speed, memory and accuracy. This means that it implements all kinds of mathematical and programming tricks. As a result, the code will be very difficult to follow. You will spend the majority of your time figuring out the tricks rather than figuring out the algorithm, which was your goal in the first place.

: Code in open source libraries is very likely highly optimized for execution speed, memory and accuracy. This means that it implements all kinds of mathematical and programming tricks. As a result, the code will be very difficult to follow. You will spend the majority of your time figuring out the tricks rather than figuring out the algorithm, which was your goal in the first place. Project Centric : The code will not be a generic implementation ready for you to run in isolation, it will be carefully designed to work within the project’s framework. It is also very likely that details will be abstracted and hidden from you “conveniently” by the framework. You will spend your time learning that framework and it’s design in order to understand the algorithm implementation.

: The code will not be a generic implementation ready for you to run in isolation, it will be carefully designed to work within the project’s framework. It is also very likely that details will be abstracted and hidden from you “conveniently” by the framework. You will spend your time learning that framework and it’s design in order to understand the algorithm implementation. Limited Understanding: Studying an implementation of an algorithm does not help you in your ambition to understand the algorithm, it can teach you tricks of efficient algorithm implementation. In the beginning, the most critical time, other peoples code will confuse you.

I think there is deep wisdom here.

I would point out that open source implementations can sometimes help in the understanding of a specific technical detail, such as an update rule or other modular piece of mathematics that may be poorly described, but realized in a single function in code. I have experienced this myself many times, but it is a heuristic, not a rule.

Edward suggests algorithms like k-means and stochastic gradient descent as good examples to start with.

Implementing Algorithms From Papers

Edward suggests that implementing machine learning algorithms from papers is a big jump if you have not first implemented well known algorithms, as described above.

From Edwards comments you can sketch out a process for learning machine learning algorithms by implementing them from scratch. My interpretation of that process looks something like the following:

Implement the algorithm yourself from scratch. Compare performance to off-the-shelf implement"
164;164;machinelearningmastery.com;https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-computer-vision-7-day-mini-course/;2019-04-08;How to Get Started With Deep Learning for Computer Vision (7-Day Mini-Course);"# fit a cnn on the fashion mnist dataset

from keras . datasets import fashion_mnist

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Conv2D

from keras . layers import MaxPooling2D

from keras . layers import Dense

from keras . layers import Flatten

# load dataset

( trainX , trainY ) , ( testX , testY ) = fashion_mnist . load_data ( )

# reshape dataset to have a single channel

trainX = trainX . reshape ( ( trainX . shape [ 0 ] , 28 , 28 , 1 ) )

testX = testX . reshape ( ( testX . shape [ 0 ] , 28 , 28 , 1 ) )

# convert from integers to floats

trainX , testX = trainX . astype ( 'float32' ) , testX . astype ( 'float32' )

# normalize to range 0-1

trainX , testX = trainX / 255.0 , testX / 255.0

# one hot encode target values

trainY , testY = to_categorical ( trainY ) , to_categorical ( testY )

# define model

model = Sequential ( )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , kernel_initializer = 'he_uniform' , input_shape = ( 28 , 28 , 1 ) ) )

model . add ( MaxPooling2D ( ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 100 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 10 , activation = 'softmax' ) )

model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( trainX , trainY , epochs = 10 , batch_size = 32 , verbose = 2 )

# evaluate model

loss , acc = model . evaluate ( testX , testY , verbose = 0 )"
165;165;machinelearningmastery.com;https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/;2016-10-25;How To Implement Simple Linear Regression From Scratch With Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98

# Simple Linear Regression on the Swedish Insurance Dataset from random import seed from random import randrange from csv import reader from math import sqrt # Load a CSV file def load_csv ( filename ) : dataset = list ( ) with open ( filename , 'r' ) as file : csv_reader = reader ( file ) for row in csv_reader : if not row : continue dataset . append ( row ) return dataset # Convert string column to float def str_column_to_float ( dataset , column ) : for row in dataset : row [ column ] = float ( row [ column ] . strip ( ) ) # Split a dataset into a train and test set def train_test_split ( dataset , split ) : train = list ( ) train_size = split * len ( dataset ) dataset_copy = list ( dataset ) while len ( train ) < train_size : index = randrange ( len ( dataset_copy ) ) train . append ( dataset_copy . pop ( index ) ) return train , dataset_copy # Calculate root mean squared error def rmse_metric ( actual , predicted ) : sum_error = 0.0 for i in range ( len ( actual ) ) : prediction_error = predicted [ i ] - actual [ i ] sum_error += ( prediction_error * * 2 ) mean_error = sum_error / float ( len ( actual ) ) return sqrt ( mean_error ) # Evaluate an algorithm using a train/test split def evaluate_algorithm ( dataset , algorithm , split , * args ) : train , test = train_test_split ( dataset , split ) test_set = list ( ) for row in test : row_copy = list ( row ) row_copy [ - 1 ] = None test_set . append ( row_copy ) predicted = algorithm ( train , test_set , * args ) actual = [ row [ - 1 ] for row in test ] rmse = rmse_metric ( actual , predicted ) return rmse # Calculate the mean value of a list of numbers def mean ( values ) : return sum ( values ) / float ( len ( values ) ) # Calculate covariance between x and y def covariance ( x , mean_x , y , mean_y ) : covar = 0.0 for i in range ( len ( x ) ) : covar += ( x [ i ] - mean_x ) * ( y [ i ] - mean_y ) return covar # Calculate the variance of a list of numbers def variance ( values , mean ) : return sum ( [ ( x - mean ) * * 2 for x in values ] ) # Calculate coefficients def coefficients ( dataset ) : x = [ row [ 0 ] for row in dataset ] y = [ row [ 1 ] for row in dataset ] x_mean , y_mean = mean ( x ) , mean ( y ) b1 = covariance ( x , x_mean , y , y_mean ) / variance ( x , x_mean ) b0 = y_mean - b1 * x_mean return [ b0 , b1 ] # Simple linear regression algorithm def simple_linear_regression ( train , test ) : predictions = list ( ) b0 , b1 = coefficients ( train ) for row in test : yhat = b0 + b1 * row [ 0 ] predictions . append ( yhat ) return predictions # Simple linear regression on insurance dataset seed ( 1 ) # load and prepare data filename = 'insurance.csv' dataset = load_csv ( filename ) for i in range ( len ( dataset [ 0 ] ) ) : str_column_to_float ( dataset , i ) # evaluate algorithm split = 0.6 rmse = evaluate_algorithm ( dataset , simple_linear_regression , split ) print ( 'RMSE: %.3f' % ( rmse ) )"
166;166;news.mit.edu;http://news.mit.edu/press/filming-guidelines;;Filming Guidelines;"The MIT News Office media relations team is responsible for approving requests for non-commercial filming on the MIT campus. If you are not a member of the media, please send your request to Peter Bebergal in the MIT Technology Licensing Office.

When the details of a given request have been agreed upon, a location agreement must be signed and returned to the MIT News Office prior to the crew's arrival. Once a location agreement is approved, video crews are welcome to film on campus property, as long as the reporting activities do not disrupt Institute activities, interfere with the privacy of students, faculty or staff, or jeopardize the safety of Institute personnel, visitors or facilities.

Guidelines

Filming is prohibited along the Infinite Corridor, in Lobby 7, Lobby 10 and in residence halls. In the Stata Center, filming is only permitted on the first floor (or ""student street"") and in individual offices and labs, with permission from occupants.

When filming students, we ask that the media respect the right of students not to be interviewed, if they so decline. Media may not take or use pictures of students or film students without first getting their permission to be filmed. Students must also be given complete details about what is being filmed and how their photograph might be used. All other locations/subjects must be approved prior to filming.

Submitting Requests

Please e-mail requests to film on campus to Sarah McDonnell with at least 72 business hours (three business days) of advanced notice, and include the following information:

Name and description of the organization or individual making the request.

Name, address, and phone number of the contact person.

Project description and the intended use of the resulting material.

Date(s) and time(s) requested.

Location of photo shoot.

Number of people, and amount and type of equipment involved.

Proof of adequate insurance coverage and indemnity.

Contact

Sarah McDonnell

Media Relations Manager

s_mcd@mit.edu

617-253-8923"
167;167;machinelearningmastery.com;http://machinelearningmastery.com/perform-feature-selection-machine-learning-data-weka/;2016-07-12;How to Perform Feature Selection With Machine Learning Data in Weka;"Tweet Share Share

Last Updated on December 13, 2019

Raw machine learning data contains a mixture of attributes, some of which are relevant to making predictions.

How do you know which features to use and which to remove? The process of selecting features in your data to model your problem is called feature selection.

In this post you will discover how to perform feature selection with your machine learning data in Weka.

After reading this post you will know:

About the importance of feature selection when working through a machine learning problem.

How feature selection is supported on the Weka platform.

How to use various different feature selection techniques in Weka on your dataset.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Update March/2018: Added alternate link to download the dataset as the original appears to have been taken down.

Predict the Onset of Diabetes

The dataset used for this example is the Pima Indians onset of diabetes dataset.

It is a classification problem where each instance represents medical details for one patient and the task is to predict whether the patient will have an onset of diabetes within the next five years.

You can learn more about the dataset here:

You can also access this dataset in your Weka installation, under the data/ directory in the file called diabetes.arff.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Feature Selection in Weka

Many feature selection techniques are supported in Weka.

A good place to get started exploring feature selection in Weka is in the Weka Explorer.

Open the Weka GUI Chooser. Click the “Explorer” button to launch the Explorer. Open the Pima Indians dataset. Click the “Select attributes” tab to access the feature selection methods.

Feature selection is divided into two parts:

Attribute Evaluator

Search Method.

Each section has multiple techniques from which to choose.

The attribute evaluator is the technique by which each attribute in your dataset (also called a column or feature) is evaluated in the context of the output variable (e.g. the class). The search method is the technique by which to try or navigate different combinations of attributes in the dataset in order to arrive on a short list of chosen features.

Some Attribute Evaluator techniques require the use of specific Search Methods. For example, the CorrelationAttributeEval technique used in the next section can only be used with a Ranker Search Method, that evaluates each attribute and lists the results in a rank order. When selecting different Attribute Evaluators, the interface may ask you to change the Search Method to something compatible with the chosen technique.

Both the Attribute Evaluator and Search Method techniques can be configured. Once chosen, click on the name of the technique to get access to its configuration details.

Click the “More” button to get more documentation on the feature selection technique and configuration parameters. Hover your mouse cursor over a configuration parameter to get a tooltip containing more details.

Now that we know how to access feature selection techniques in Weka, let’s take a look at how to use some popular methods on our chosen standard dataset.

Correlation Based Feature Selection

A popular technique for selecting the most relevant attributes in your dataset is to use correlation.

Correlation is more formally referred to as Pearson’s correlation coefficient in statistics.

You can calculate the correlation between each attribute and the output variable and select only those attributes that have a moderate-to-high positive or negative correlation (close to -1 or 1) and drop those attributes with a low correlation (value close to zero).

Weka supports correlation based feature selection with the CorrelationAttributeEval technique that requires use of a Ranker search method.

Running this on our Pima Indians dataset suggests that one attribute (plas) has the highest correlation with the output class. It also suggests a host of attributes with some modest correlation (mass, age, preg). If we use 0.2 as our cut-off for relevant attributes, then the remaining attributes could possibly be removed (pedi, insu, skin and pres).

Information Gain Based Feature Selection

Another popular feature selection technique is to calculate the information gain.

You can calculate the information gain (also called entropy) for each attribute for the output variable. Entry values vary from 0 (no information) to 1 (maximum information). Those attributes that contribute more information will have a higher information gain value and can be selected, whereas those that do not add much "
168;168;machinelearningmastery.com;https://machinelearningmastery.com/how-to-scale-data-for-long-short-term-memory-networks-in-python/;2017-07-06;How to Scale Data for Long Short-Term Memory Networks in Python;"from pandas import Series

from sklearn . preprocessing import MinMaxScaler

# define contrived series

data = [ 10.0 , 20.0 , 30.0 , 40.0 , 50.0 , 60.0 , 70.0 , 80.0 , 90.0 , 100.0 ]

series = Series ( data )

print ( series )

# prepare data for normalization

values = series . values

values = values . reshape ( ( len ( values ) , 1 ) )

# train the normalization

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

scaler = scaler . fit ( values )

print ( 'Min: %f, Max: %f' % ( scaler . data_min_ , scaler . data_max_ ) )

# normalize the dataset and print

normalized = scaler . transform ( values )

print ( normalized )

# inverse transform and print

inversed = scaler . inverse_transform ( normalized )"
169;169;news.mit.edu;http://news.mit.edu/2020/how-covid-19-tests-work-why-they-are-in-short-supply-0410;;3 Questions: How Covid-19 tests work and why they’re in short supply;"One key to stopping the spread of Covid-19 is knowing who has it. A delay in reliable tests and Covid-19 diagnostics in the United States has painted an unreliable picture of just how many people are infected and how the epidemic is evolving. But new testing options are now becoming available and the information from these diagnostics will help guide decisions and actions important for public health.

McGovern Institute research scientists Omar Abuddayeh and Jonathan Gootenberg have been developing CRISPR technologies to rapidly diagnose Covid-19 and other infectious diseases. They recently described the current state of Covid-19 testing.

Q: How do Covid-19 tests work?

A: There are three main types of tests. The first uses the detection of nucleic acid. These tests directly test for the RNA genome of the virus in a variety of sample types, such as nasopharyngeal swabs or sputum. These tests are most commonly performed using polymerase chain reaction (PCR), which can amplify a small part of the virus RNA sequence billions-of-fold higher to allow detection with a fluorescence measuring instrument. These types of tests are highly sensitive, allowing for early detection of the virus days after infection. PCR tests require complex instrumentation and are usually performed by skilled personnel in an advanced laboratory setting. An alternative method is SHERLOCK, a nucleic acid-based test developed here at MIT stemming from the CRISPR gene editing tool that does not need complex instrumentation and can be read out using a paper strip akin to a pregnancy test, without any loss of sensitivity or specificity. The test is also low-cost and can be performed in less than an hour. Because of these features, we are hoping to gain FDA approval that allows deployment at the point of care or at home testing with our Covid-19 SHERLOCK test kit.

The second type of Covid-19 test detects viral proteins. Some tests use a paper strip that have antibodies against Covid-19 proteins. These allow for easy detection of the virus in less than an hour but are at least a million-fold less sensitive than nucleic acid-based tests because there is no amplification step. This makes them less ideal for screening purposes, as many patients will not have enough viral load in sputum or swabs and will receive false negative results.

The third category is serology tests that detect antibodies against the virus. These tests can also be used as a paper strip with antibodies that detect other antibodies that develop in someone’s blood in response to Covid-19 infection. Antibodies do not show up in blood until one to two weeks after symptoms present, so these tests are not great for catching infection at early stages. Serology tests are more useful for determining if someone has had the infection, recovered, and developed immunity. They may serve a purpose for finding immune people and deciding whether they can go back to work, or for developing antibody-based therapies.

Q: Why aren’t there more Covid-19 tests available?

A: The difficulties in getting nucleic acid detection tests stem from a confluence of multiple factors, including limited supplies of tests, limited supplies of other consumables needed for testing (such as nasal swabs or RNA purification kits), insufficient testing bandwidth at sites that can perform tests (often due to bottlenecks in labor or instruments), and complications behind the logistics of assigning tests or reporting back results. As a result, just producing more testing material would not solve the issue outright, and either more instrumentation and labor is required, or newer, more rapid tests need to be developed that can be performed in a more distributed manner with reduced dependence on equipment, centralized labs, or RNA purification kits.

Q: What kind of Covid-19 test are you developing now?

A: We are working on a nucleic acid-based test that does not require complex instrumentation, rapidly returns results (with a goal of under one hour), and can be performed at a point-of-care location without trained professionals. We hope to accomplish this using a combination of techniques. First, we are incorporating isothermal amplification technologies, which, unlike current PCR-based tests, do not require intricate heating and cooling to operate. We are combining this with our CRISPR-based diagnostics, allowing for sensitive detection and readout in a simple visual format, akin to a pregnancy test. We hope that this test will significantly lower the barrier for accurate diagnosis and provide another approach for Covid-19 surveillance."
170;170;machinelearningmastery.com;https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/;2017-08-13;Multivariate Time Series Forecasting with LSTMs in Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95

from math import sqrt from numpy import concatenate from matplotlib import pyplot from pandas import read_csv from pandas import DataFrame from pandas import concat from sklearn . preprocessing import MinMaxScaler from sklearn . preprocessing import LabelEncoder from sklearn . metrics import mean_squared_error from keras . models import Sequential from keras . layers import Dense from keras . layers import LSTM # convert series to supervised learning def series_to_supervised ( data , n_in = 1 , n_out = 1 , dropnan = True ) : n_vars = 1 if type ( data ) is list else data . shape [ 1 ] df = DataFrame ( data ) cols , names = list ( ) , list ( ) # input sequence (t-n, ... t-1) for i in range ( n_in , 0 , - 1 ) : cols . append ( df . shift ( i ) ) names += [ ( 'var%d(t-%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ] # forecast sequence (t, t+1, ... t+n) for i in range ( 0 , n_out ) : cols . append ( df . shift ( - i ) ) if i == 0 : names += [ ( 'var%d(t)' % ( j + 1 ) ) for j in range ( n_vars ) ] else : names += [ ( 'var%d(t+%d)' % ( j + 1 , i ) ) for j in range ( n_vars ) ] # put it all together agg = concat ( cols , axis = 1 ) agg . columns = names # drop rows with NaN values if dropnan : agg . dropna ( inplace = True ) return agg # load dataset dataset = read_csv ( 'pollution.csv' , header = 0 , index_col = 0 ) values = dataset . values # integer encode direction encoder = LabelEncoder ( ) values [ : , 4 ] = encoder . fit_transform ( values [ : , 4 ] ) # ensure all data is float values = values . astype ( 'float32' ) # normalize features scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) ) scaled = scaler . fit_transform ( values ) # frame as supervised learning reframed = series_to_supervised ( scaled , 1 , 1 ) # drop columns we don't want to predict reframed . drop ( reframed . columns [ [ 9 , 10 , 11 , 12 , 13 , 14 , 15 ] ] , axis = 1 , inplace = True ) print ( reframed . head ( ) ) # split into train and test sets values = reframed . values n_train_hours = 365 * 24 train = values [ : n_train_hours , : ] test = values [ n_train_hours : , : ] # split into input and outputs train_X , train_y = train [ : , : - 1 ] , train [ : , - 1 ] test_X , test_y = test [ : , : - 1 ] , test [ : , - 1 ] # reshape input to be 3D [samples, timesteps, features] train_X = train_X . reshape ( ( train_X . shape [ 0 ] , 1 , train_X . shape [ 1 ] ) ) test_X = test_X . reshape ( ( test_X . shape [ 0 ] , 1 , test_X . shape [ 1 ] ) ) print ( train_X . shape , train_y . shape , test_X . shape , test_y . shape ) # design network model = Sequential ( ) model . add ( LSTM ( 50 , input_shape = ( train_X . shape [ 1 ] , train_X . shape [ 2 ] ) ) ) model . add ( Dense ( 1 ) ) model . compile ( loss = 'mae' , optimizer = 'adam' ) # fit network history = model . fit ( train_X , train_y , epochs = 50 , batch_size = 72 , validation_data = ( test_X , test_y ) , verbose = 2 , shuffle = False ) # plot history pyplot . plot ( history . history [ 'loss' ] , label = 'train' ) pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' ) pyplot . legend ( ) pyplot . show ( ) # make a prediction yhat = model . predict ( test_X ) test_X = test_X . reshape ( ( test_X . shape [ 0 ] , test_X . shape [ 2 ] ) ) # invert scaling for forecast inv_yhat = concatenate ( ( yhat , test_X [ : , 1 : ] ) , axis = 1 ) inv_yhat = scaler . inverse_transform ( inv_yhat ) inv_yhat = inv_yhat [ : , 0 ] # invert scaling for actual test_y = test_y . reshape ( ( len ( test_y ) , 1 ) ) inv_y = concatenate ( ( test_y , test_X [ : , 1 : ] ) , axis = 1 ) inv_y = scaler . inverse_transform ( inv_y ) inv_y = inv_y [ : , 0 ] # calculate RMSE rmse = sqrt ( mean_squared_error ( inv_y , inv_yhat ) ) print ( 'Test RMSE: %.3f' % rmse )"
171;171;machinelearningmastery.com;https://machinelearningmastery.com/how-to-implement-pix2pix-gan-models-from-scratch-with-keras/;2019-07-30;How to Implement Pix2Pix GAN Models From Scratch With Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144

# example of defining a composite model for training the generator model from keras . optimizers import Adam from keras . initializers import RandomNormal from keras . models import Model from keras . models import Input from keras . layers import Conv2D from keras . layers import Conv2DTranspose from keras . layers import LeakyReLU from keras . layers import Activation from keras . layers import Concatenate from keras . layers import Dropout from keras . layers import BatchNormalization from keras . layers import LeakyReLU from keras . utils . vis_utils import plot_model # define the discriminator model def define_discriminator ( image_shape ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # source image input in_src_image = Input ( shape = image_shape ) # target image input in_target_image = Input ( shape = image_shape ) # concatenate images channel-wise merged = Concatenate ( ) ( [ in_src_image , in_target_image ] ) # C64 d = Conv2D ( 64 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( merged ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C128 d = Conv2D ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C256 d = Conv2D ( 256 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C512 d = Conv2D ( 512 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # second last output layer d = Conv2D ( 512 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = BatchNormalization ( ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # patch output d = Conv2D ( 1 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) patch_out = Activation ( 'sigmoid' ) ( d ) # define model model = Model ( [ in_src_image , in_target_image ] , patch_out ) # compile model opt = Adam ( lr = 0.0002 , beta_1 = 0.5 ) model . compile ( loss = 'binary_crossentropy' , optimizer = opt , loss_weights = [ 0.5 ] ) return model # define an encoder block def define_encoder_block ( layer_in , n_filters , batchnorm = True ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # add downsampling layer g = Conv2D ( n_filters , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( layer_in ) # conditionally add batch normalization if batchnorm : g = BatchNormalization ( ) ( g , training = True ) # leaky relu activation g = LeakyReLU ( alpha = 0.2 ) ( g ) return g # define a decoder block def decoder_block ( layer_in , skip_in , n_filters , dropout = True ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # add upsampling layer g = Conv2DTranspose ( n_filters , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( layer_in ) # add batch normalization g = BatchNormalization ( ) ( g , training = True ) # conditionally add dropout if dropout : g = Dropout ( 0.5 ) ( g , training = True ) # merge with skip connection g = Concatenate ( ) ( [ g , skip_in ] ) # relu activation g = Activation ( 'relu' ) ( g ) return g # define the standalone generator model def define_generator ( image_shape = ( 256 , 256 , 3 ) ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # image input in_image = Input ( shape = image_shape ) # encoder model: C64-C128-C256-C512-C512-C512-C512-C512 e1 = define_encoder_block ( in_image , 64 , batchnorm = False ) e2 = define_encoder_block ( e1 , 128 ) e3 = define_encoder_block ( e2 , 256 ) e4 = define_encoder_block ( e3 , 512 ) e5 = define_encoder_block ( e4 , 512 ) e6 = define_encoder_block ( e5 , 512 ) e7 = define_encoder_block ( e6 , 512 ) # bottleneck, no batch norm and relu b = Conv2D ( 512 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( e7 ) b = Activation ( 'relu' ) ( b ) # decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128 d1 = decoder_block ( b , e7 , 512 ) d2 = decoder_block ( d1 , e6 , 512 ) d3 = decoder_block ( d2 , e5 , 512 ) d4 = decoder_block ( d3 , e4 , 512 , dropout = False ) d5 = decoder_block ( d4 , e3 , 256 , dropout = False ) d6 = decoder_block ( d5 , e2 , 128 , dropout = False ) d7 = decoder_block ( d6 , e1 , 64 , dropout = False ) # output g = Conv2DTranspose ( 3 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initial"
172;172;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/;2018-11-13;How to Develop LSTM Models for Time Series Forecasting;"Tweet Share Share

Last Updated on January 6, 2020

Long Short-Term Memory networks, or LSTMs for short, can be applied to time series forecasting.

There are many types of LSTM models that can be used for each specific type of time series forecasting problem.

In this tutorial, you will discover how to develop a suite of LSTM models for a range of standard time series forecasting problems.

The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem.

After completing this tutorial, you will know:

How to develop LSTM models for univariate time series forecasting.

How to develop LSTM models for multivariate time series forecasting.

How to develop LSTM models for multi-step time series forecasting.

This is a large and important post; you may want to bookmark it for future reference.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

In this tutorial, we will explore how to develop a suite of different types of LSTM models for time series forecasting.

The models are demonstrated on small contrived time series problems intended to give the flavor of the type of time series problem being addressed. The chosen configuration of the models is arbitrary and not optimized for each problem; that was not the goal.

This tutorial is divided into four parts; they are:

Univariate LSTM Models Data Preparation Vanilla LSTM Stacked LSTM Bidirectional LSTM CNN LSTM ConvLSTM Multivariate LSTM Models Multiple Input Series. Multiple Parallel Series. Multi-Step LSTM Models Data Preparation Vector Output Model Encoder-Decoder Model Multivariate Multi-Step LSTM Models Multiple Input Multi-Step Output. Multiple Parallel Input and Multi-Step Output.

Univariate LSTM Models

LSTMs can be used to model univariate time series forecasting problems.

These are problems comprised of a single series of observations and a model is required to learn from the series of past observations to predict the next value in the sequence.

We will demonstrate a number of variations of the LSTM model for univariate time series forecasting.

This section is divided into six parts; they are:

Data Preparation Vanilla LSTM Stacked LSTM Bidirectional LSTM CNN LSTM ConvLSTM

Each of these models are demonstrated for one-step univariate time series forecasting, but can easily be adapted and used as the input part of a model for other types of time series forecasting problems.

Data Preparation

Before a univariate series can be modeled, it must be prepared.

The LSTM model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the LSTM can learn.

Consider a given univariate sequence:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.

X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ... 1 2 3 4 5 X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ...

The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.

# split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on our small contrived dataset above.

The complete example is listed below.

# univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # ga"
173;173;machinelearningmastery.com;http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/;2016-08-03;Text Generation With LSTM Recurrent Neural Networks in Python with Keras;"# Load Larger LSTM network and generate text

import sys

import numpy

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Dropout

from keras . layers import LSTM

from keras . callbacks import ModelCheckpoint

from keras . utils import np_utils

# load ascii text and covert to lowercase

filename = ""wonderland.txt""

raw_text = open ( filename , 'r' , encoding = 'utf-8' ) . read ( )

raw_text = raw_text . lower ( )

# create mapping of unique chars to integers, and a reverse mapping

chars = sorted ( list ( set ( raw_text ) ) )

char_to_int = dict ( ( c , i ) for i , c in enumerate ( chars ) )

int_to_char = dict ( ( i , c ) for i , c in enumerate ( chars ) )

# summarize the loaded data

n_chars = len ( raw_text )

n_vocab = len ( chars )

print ""Total Characters: "" , n_chars

print ""Total Vocab: "" , n_vocab

# prepare the dataset of input to output pairs encoded as integers

seq_length = 100

dataX = [ ]

dataY = [ ]

for i in range ( 0 , n_chars - seq_length , 1 ) :

seq_in = raw_text [ i : i + seq_length ]

seq_out = raw_text [ i + seq_length ]

dataX . append ( [ char_to_int [ char ] for char in seq_in ] )

dataY . append ( char_to_int [ seq_out ] )

n_patterns = len ( dataX )

print ""Total Patterns: "" , n_patterns

# reshape X to be [samples, time steps, features]

X = numpy . reshape ( dataX , ( n_patterns , seq_length , 1 ) )

# normalize

X = X / float ( n_vocab )

# one hot encode the output variable

y = np_utils . to_categorical ( dataY )

# define the LSTM model

model = Sequential ( )

model . add ( LSTM ( 256 , input_shape = ( X . shape [ 1 ] , X . shape [ 2 ] ) , return_sequences = True ) )

model . add ( Dropout ( 0.2 ) )

model . add ( LSTM ( 256 ) )

model . add ( Dropout ( 0.2 ) )

model . add ( Dense ( y . shape [ 1 ] , activation = 'softmax' ) )

# load the network weights

filename = ""weights-improvement-47-1.2219-bigger.hdf5""

model . load_weights ( filename )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' )

# pick a random seed

start = numpy . random . randint ( 0 , len ( dataX ) - 1 )

pattern = dataX [ start ]

print ""Seed:""

print ""\"""" , '' . join ( [ int_to_char [ value ] for value in pattern ] ) , ""\""""

# generate characters

for i in range ( 1000 ) :

x = numpy . reshape ( pattern , ( 1 , len ( pattern ) , 1 ) )

x = x / float ( n_vocab )

prediction = model . predict ( x , verbose = 0 )

index = numpy . argmax ( prediction )

result = int_to_char [ index ]

seq_in = [ int_to_char [ value ] for value in pattern ]

sys . stdout . write ( result )

pattern . append ( index )

pattern = pattern [ 1 : len ( pattern ) ]"
174;174;machinelearningmastery.com;https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/;2019-06-13;18 Impressive Applications of Generative Adversarial Networks (GANs);"Tweet Share Share

Last Updated on July 12, 2019

A Generative Adversarial Network, or GAN, is a type of neural network architecture for generative modeling.

Generative modeling involves using a model to generate new examples that plausibly come from an existing distribution of samples, such as generating new photographs that are similar but specifically different from a dataset of existing photographs.

A GAN is a generative model that is trained using two neural network models. One model is called the “generator” or “generative network” model that learns to generate new plausible samples. The other model is called the “discriminator” or “discriminative network” and learns to differentiate generated examples from real examples.

The two models are set up in a contest or a game (in a game theory sense) where the generator model seeks to fool the discriminator model, and the discriminator is provided with both examples of real and generated samples.

After training, the generative model can then be used to create new plausible samples on demand.

GANs have very specific use cases and it can be difficult to understand these use cases when getting started.

In this post, we will review a large number of interesting applications of GANs to help you develop an intuition for the types of problems where GANs can be used and useful. It’s not an exhaustive list, but it does contain many example uses of GANs that have been in the media.

We will divide these applications into the following areas:

Generate Examples for Image Datasets

Generate Photographs of Human Faces

Generate Realistic Photographs

Generate Cartoon Characters

Image-to-Image Translation

Text-to-Image Translation

Semantic-Image-to-Photo Translation

Face Frontal View Generation

Generate New Human Poses

Photos to Emojis

Photograph Editing

Face Aging

Photo Blending

Super Resolution

Photo Inpainting

Clothing Translation

Video Prediction

3D Object Generation

Did I miss an interesting application of GANs or great paper on a specific GAN application?

Please let me know in the comments.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Generate Examples for Image Datasets

Generating new plausible samples was the application described in the original paper by Ian Goodfellow, et al. in the 2014 paper “Generative Adversarial Networks” where GANs were used to generate new plausible examples for the MNIST handwritten digit dataset, the CIFAR-10 small object photograph dataset, and the Toronto Face Database.

This was also the demonstration used in the important 2015 paper titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” by Alec Radford, et al. called DCGAN that demonstrated how to train stable GANs at scale. They demonstrated models for generating new examples of bedrooms.

Importantly, in this paper, they also demonstrated the ability to perform vector arithmetic with the input to the GANs (in the latent space) both with generated bedrooms and with generated faces.

Generate Photographs of Human Faces

Tero Karras, et al. in their 2017 paper titled “Progressive Growing of GANs for Improved Quality, Stability, and Variation” demonstrate the generation of plausible realistic photographs of human faces. They are so real looking, in fact, that it is fair to call the result remarkable. As such, the results received a lot of media attention. The face generations were trained on celebrity examples, meaning that there are elements of existing celebrities in the generated faces, making them seem familiar, but not quite.

Their methods were also used to demonstrate the generation of objects and scenes.

Examples from this paper were used in a 2018 report titled “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation” to demonstrate the rapid progress of GANs from 2014 to 2017 (found via this tweet by Ian Goodfellow).

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Generate Realistic Photographs

Andrew Brock, et al. in their 2018 paper titled “Large Scale GAN Training for High Fidelity Natural Image Synthesis” demonstrate the generation of synthetic photographs with their technique BigGAN that are practically indistinguishable from real photographs.

Generate Cartoon Characters

Yanghua Jin, et al. in their 2017 paper titled “Towards the Automatic Anime Characters Creation with Generative Adversarial Networks” demonstrate the training and use of a GAN for generating faces of anime characters (i.e. Japanese comic book characters).

Inspired by the anime examples, a number of people have tried to generate Pokemon characters, such as the pokeGAN project and the Generate Pokemon with DCGAN project, with limite"
175;175;machinelearningmastery.com;http://machinelearningmastery.com/spot-check-regression-machine-learning-algorithms-python-scikit-learn/;2016-05-29;Spot-Check Regression Machine Learning Algorithms in Python with scikit-learn;"# Linear Regression

import pandas

from sklearn import model_selection

from sklearn . linear_model import LinearRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data""

names = [ 'CRIM' , 'ZN' , 'INDUS' , 'CHAS' , 'NOX' , 'RM' , 'AGE' , 'DIS' , 'RAD' , 'TAX' , 'PTRATIO' , 'B' , 'LSTAT' , 'MEDV' ]

dataframe = pandas . read_csv ( url , delim_whitespace = True , names = names )

array = dataframe . values

X = array [ : , 0 : 13 ]

Y = array [ : , 13 ]

seed = 7

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

model = LinearRegression ( )

scoring = 'neg_mean_squared_error'

results = model_selection . cross_val_score ( model , X , Y , cv = kfold , scoring = scoring )"
176;176;machinelearningmastery.com;http://machinelearningmastery.com/how-to-improve-machine-learning-results/;2013-12-29;How to Improve Machine Learning Results;"Tweet Share Share

Last Updated on May 14, 2019

Having one or two algorithms that perform reasonably well on a problem is a good start, but sometimes you may be incentivised to get the best result you can given the time and resources you have available.

In this post, you will review methods you can use to squeeze out extra performance and improve the results you are getting from machine learning algorithms.

When tuning algorithms you must have a high confidence in the results given by your test harness. This means that you should be using techniques that reduce the variance of the performance measure you are using to assess algorithm runs. I suggest cross validation with a reasonably high number of folds (the exact number of which depends on your dataset).

The three strategies you will learn about in this post are:

Algorithm Tuning

Ensembles

Extreme Feature Engineering

Algorithm Tuning

The place to start is to get better results from algorithms that you already know perform well on your problem. You can do this by exploring and fine tuning the configuration for those algorithms.

Machine learning algorithms are parameterized and modification of those parameters can influence the outcome of the learning process. Think of each algorithm parameter as a dimension on a graph with the values of a given parameter as a point along the axis. Three parameters would be a cube of possible configurations for the algorithm, and n-parameters would be an n-dimensional hypercube of possible configurations for the algorithm.

The objective of algorithm tuning is to find the best point or points in that hypercube for your problem. You will be optimizing against your test harness, so again you cannot underestimate the importance of spending the time to build a trusted test harness.

You can approach this search problem by using automated methods that impose a grid on the possibility space and sample where good algorithm configuration might be. You can then use those points in an optimization algorithm to zoom in on the best performance.

You can repeat this process with a number of well performing methods and explore the best you can achieve with each. I strongly advise that the process is automated and reasonably coarse grained as you can quickly reach points of diminishing returns (fractional percentage performance increases) that may not translate to the production system.

The more tuned the parameters of an algorithm, the more biased the algorithm will be to the training data and test harness. This strategy can be effective, but it can also lead to more fragile models that overfit your test harness and don’t perform as well in practice.

Ensembles

Ensemble methods are concerned with combining the results of multiple methods in order to get improved results. Ensemble methods work well when you have multiple “good enough” models that specialize in different parts of the problem.

This may be achieved through many ways. Three ensemble strategies you can explore are:

Bagging : Known more formally as Bootstrapped Aggregation is where the same algorithm has different perspectives on the problem by being trained on different subsets of the training data.

: Known more formally as Bootstrapped Aggregation is where the same algorithm has different perspectives on the problem by being trained on different subsets of the training data. Boosting : Different algorithms are trained on the same training data.

: Different algorithms are trained on the same training data. Blending: Known more formally as Stacked Generalization or Stacking is where a variety of models whose predictions are taken as input to a new model that learns how to combine the predictions into an overall prediction.

It is a good idea to get into ensemble methods after you have exhausted more traditional methods. There are two good reasons for this, they are generally more complex than traditional methods and the traditional methods give you a good base level from which you can improve and draw from to create your ensembles.

Extreme Feature Engineering

The previous two strategies have looked at getting more from machine learning algorithms. This strategy is about exposing more structure in the problem for the algorithms to learn. In data preparation learned about feature decomposition and aggregation in order to better normalize the data for machine learning algorithms. In this strategy, we push that idea to the limits. I call this strategy extreme feature engineering, when really the term “feature engineering” would suffice.

Think of your data as having complex multi-dimensional structures embedded in it that machine learning algorithms know how to find and exploit to make decisions. You want to best expose those structures to algorithms so that the algorithms can do their best work. A difficulty is that some of those structures may be too dense or too complex for the algorithms to find without help. You may also have some knowledge of such structure"
177;177;machinelearningmastery.com;http://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/;;Time Series Forecasting With Python;"Introduction to Time Series Forecasting With Python

Discover How to Prepare Data and Develop Models to Predict the Future

$37 USD Time series forecasting is different from other machine learning problems. The key difference is the fixed sequence of observations and the constraints and additional structure this provides. In this mega Ebook written in the friendly Machine Learning Mastery style that you’re used to, finally cut through the math and specialized methods for time series forecasting. Using clear explanations, standard Python libraries and step-by-step tutorials you will discover how to load and prepare data, evaluate model skill, and implement forecasting models for time series data. Technical Details About the Book PDF format Ebook.

8 parts, 34 chapters, 367 pages.

28 step-by-step tutorial lessons.

3 end-to-end projects.

181 Python (.py) files. Clear and Complete Examples.

No Math. Nothing Hidden. Convinced?

Click to jump straight to the packages. Nothing comes close to the level of detail and practicality of these masterpieces. Nader Nazemi Bioinformatician

Time Series Problems are Important

Time series forecasting is an important area of machine learning that is often neglected.

It is important because there are so many prediction problems that involve a time component. These problems are neglected because it is this time component that makes time series problems more difficult to handle.

You can’t just fire a machine learning algorithm at a time series dataset.

Time series data must be transformed into a supervised learning problem.

Time series data has temporal structure like trends and seasonality that must be handled.

Time series data has a forecast horizon.

There are a few conceptual steps you must make before you can start developing forecasting models.

There are also specialized terminology and algorithms to consider and use when working with time series data.

It can feel overwhelming for a beginner and standard machine learning libraries like scikit-learn do not make it easy to get started.

Introducing: “ Time Series Forecasting With Python “

This is the book I wish I had when I was getting started with univariate time series forecasting.

It is designed for the practical and hands-on way you prefer to learn.

The goal of this book is to:

Show you how to get results on univariate time series forecasting problems using the Python ecosystem.

It is a cookbook designed for immediate use.

This book was developed using five principles.

They are:

Application : The focus is on the application of forecasting rather than the theory.

: The focus is on the application of forecasting rather than the theory. Lessons : The book is broken down into short lessons, each focused on a specific topic.

: The book is broken down into short lessons, each focused on a specific topic. Value : Lessons focus on the most used and most useful aspects of a forecasting project.

: Lessons focus on the most used and most useful aspects of a forecasting project. Results : Each lesson provides a path to a usable and reproducible result.

: Each lesson provides a path to a usable and reproducible result. Speed: Each lesson is designed to provide the shortest path to a result.

These principles shape the structure and organization of the book.

What You Will Know and Be Able to Do (Reading Outcomes)

If you choose to work through all of the lessons and projects of this book, you can set some reasonable expectations on your new found capabilities.

They are:

Time Series Foundations : You will be able to identify time series forecasting problems as distinct from other predictive modeling problems and how time series can be framed as supervised learning.

: You will be able to identify time series forecasting problems as distinct from other predictive modeling problems and how time series can be framed as supervised learning. Transform Data For Modeling : You will be able to transform, rescale, smooth and engineer features from time series data in order to best expose the underlying inherent properties of the problem (the signal) to learning algorithms for forecasting.

: You will be able to transform, rescale, smooth and engineer features from time series data in order to best expose the underlying inherent properties of the problem (the signal) to learning algorithms for forecasting. Harness Temporal Structure : You will be able to analyze time series data and understand the temporal structure inherent in it such as trends and seasonality and how these structures may be addressed, removed and harnessed when forecasting.

: You will be able to analyze time series data and understand the temporal structure inherent in it such as trends and seasonality and how these structures may be addressed, removed and harnessed when forecasting. Evaluate Models : You will be able to devise a model test harness for a univariate forecasting problem and estimate the baseline skill and expected model performance o"
178;178;machinelearningmastery.com;http://machinelearningmastery.com/machine-learning-in-python-step-by-step/;2019-02-09;Your First Machine Learning Project in Python Step-By-Step;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42

# compare algorithms from pandas import read_csv from matplotlib import pyplot from sklearn . model_selection import train_test_split from sklearn . model_selection import cross_val_score from sklearn . model_selection import StratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . tree import DecisionTreeClassifier from sklearn . neighbors import KNeighborsClassifier from sklearn . discriminant_analysis import LinearDiscriminantAnalysis from sklearn . naive_bayes import GaussianNB from sklearn . svm import SVC # Load dataset url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"" names = [ 'sepal-length' , 'sepal-width' , 'petal-length' , 'petal-width' , 'class' ] dataset = read_csv ( url , names = names ) # Split-out validation dataset array = dataset . values X = array [ : , 0 : 4 ] y = array [ : , 4 ] X_train , X_validation , Y_train , Y_validation = train_test_split ( X , y , test_size = 0.20 , random_state = 1 , shuffle = True ) # Spot Check Algorithms models = [ ] models . append ( ( 'LR' , LogisticRegression ( solver = 'liblinear' , multi_class = 'ovr' ) ) ) models . append ( ( 'LDA' , LinearDiscriminantAnalysis ( ) ) ) models . append ( ( 'KNN' , KNeighborsClassifier ( ) ) ) models . append ( ( 'CART' , DecisionTreeClassifier ( ) ) ) models . append ( ( 'NB' , GaussianNB ( ) ) ) models . append ( ( 'SVM' , SVC ( gamma = 'auto' ) ) ) # evaluate each model in turn results = [ ] names = [ ] for name , model in models : kfold = StratifiedKFold ( n_splits = 10 , random_state = 1 ) cv_results = cross_val_score ( model , X_train , Y_train , cv = kfold , scoring = 'accuracy' ) results . append ( cv_results ) names . append ( name ) print ( '%s: %f (%f)' % ( name , cv_results . mean ( ) , cv_results . std ( ) ) ) # Compare Algorithms pyplot . boxplot ( results , labels = names ) pyplot . title ( 'Algorithm Comparison' ) pyplot . show ( )"
179;179;machinelearningmastery.com;https://machinelearningmastery.com/examples-of-linear-algebra-in-machine-learning/;2018-03-08;10 Examples of Linear Algebra in Machine Learning;"Tweet Share Share

Last Updated on August 9, 2019

Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms.

It is a key foundation to the field of machine learning, from notations used to describe the operation of algorithms to the implementation of algorithms in code.

Although linear algebra is integral to the field of machine learning, the tight relationship is often left unexplained or explained using abstract concepts such as vector spaces or specific matrix operations.

In this post, you will discover 10 common examples of machine learning that you may be familiar with that use, require and are really best understood using linear algebra.

After reading this post, you will know:

The use of linear algebra structures when working with data, such as tabular datasets and images.

Linear algebra concepts when working with data preparation, such as one hot encoding and dimensionality reduction.

The ingrained use of linear algebra notation and methods in sub-fields such as deep learning, natural language processing, and recommender systems.

Discover vectors, matrices, tensors, matrix types, matrix factorization, PCA, SVD and much more in my new book, with 19 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we will review 10 obvious and concrete examples of linear algebra in machine learning.

I tried to pick examples that you may be familiar with or have even worked with before. They are:

Dataset and Data Files Images and Photographs One-Hot Encoding Linear Regression Regularization Principal Component Analysis Singular-Value Decomposition Latent Semantic Analysis Recommender Systems Deep Learning

Do you have your own favorite example of linear algebra in machine learning?

Let me know in the comments below.

Need help with Linear Algebra for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Dataset and Data Files

In machine learning, you fit a model on a dataset.

This is the table-like set of numbers where each row represents an observation and each column represents a feature of the observation.

For example, below is a snippet of the Iris flowers dataset:

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 1 2 3 4 5 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa

This data is in fact a matrix: a key data structure in linear algebra.

Further, when you split the data into inputs and outputs to fit a supervised machine learning model, such as the measurements and the flower species, you have a matrix (X) and a vector (y). The vector is another key data structure in linear algebra.

Each row has the same length, i.e. the same number of columns, therefore we can say that the data is vectorized where rows can be provided to a model one at a time or in a batch and the model can be pre-configured to expect rows of a fixed width.

For help loading data files as NumPy arrays, see the tutorial:

2. Images and Photographs

Perhaps you are more used to working with images or photographs in computer vision applications.

Each image that you work with is itself a table structure with a width and height and one pixel value in each cell for black and white images or 3 pixel values in each cell for a color image.

A photo is yet another example of a matrix from linear algebra.

Operations on the image, such as cropping, scaling, shearing, and so on are all described using the notation and operations of linear algebra.

For help loading images as NumPy arrays, see the tutorial:

3. One Hot Encoding

Sometimes you work with categorical data in machine learning.

Perhaps the class labels for classification problems, or perhaps categorical input variables.

It is common to encode categorical variables to make them easier to work with and learn by some techniques. A popular encoding for categorical variables is the one hot encoding.

A one hot encoding is where a table is created to represent the variable with one column for each category and a row for each example in the dataset. A check, or one-value, is added in the column for the categorical value for a given row, and a zero-value is added to all other columns.

For example, the color variable with the 3 rows:

red green blue ... 1 2 3 4 red green blue ...

Might be encoded as:

red, green, blue 1, 0, 0 0, 1, 0 0, 0, 1 ... 1 2 3 4 5 red, green, blue 1, 0, 0 0, 1, 0 0, 0, 1 ...

Each row is encoded as a binary vector, a vector with zero or one values and this is an example of a sparse representation, a whole sub-field of linear algebra.

For more on one hot encoding, see the tutorial:

4. Linear Regression

Linear regression is an old method from statistics for describi"
180;180;machinelearningmastery.com;https://machinelearningmastery.com/how-to-model-human-activity-from-smartphone-data/;2018-09-16;How to Model Human Activity From Smartphone Data;"# plot durations of each activity by subject

from numpy import array

from numpy import dstack

from numpy import unique

from pandas import read_csv

from matplotlib import pyplot

# load a single file as a numpy array

def load_file ( filepath ) :

dataframe = read_csv ( filepath , header = None , delim_whitespace = True )

return dataframe . values

# load a list of files, such as x, y, z data for a given variable

def load_group ( filenames , prefix = '' ) :

loaded = list ( )

for name in filenames :

data = load_file ( prefix + name )

loaded . append ( data )

# stack group so that features are the 3rd dimension

loaded = dstack ( loaded )

return loaded

# load a dataset group, such as train or test

def load_dataset ( group , prefix = '' ) :

filepath = prefix + group + '/Inertial Signals/'

# load all 9 files as a single array

filenames = list ( )

# total acceleration

filenames += [ 'total_acc_x_' + group + '.txt' , 'total_acc_y_' + group + '.txt' , 'total_acc_z_' + group + '.txt' ]

# body acceleration

filenames += [ 'body_acc_x_' + group + '.txt' , 'body_acc_y_' + group + '.txt' , 'body_acc_z_' + group + '.txt' ]

# body gyroscope

filenames += [ 'body_gyro_x_' + group + '.txt' , 'body_gyro_y_' + group + '.txt' , 'body_gyro_z_' + group + '.txt' ]

# load input data

X = load_group ( filenames , filepath )

# load class output

y = load_file ( prefix + group + '/y_' + group + '.txt' )

return X , y

# get all data for one subject

def data_for_subject ( X , y , sub_map , sub_id ) :

# get row indexes for the subject id

ix = [ i for i in range ( len ( sub_map ) ) if sub_map [ i ] == sub_id ]

# return the selected samples

return X [ ix , : , : ] , y [ ix ]

# convert a series of windows to a 1D list

def to_series ( windows ) :

series = list ( )

for window in windows :

# remove the overlap from the window

half = int ( len ( window ) / 2 ) - 1

for value in window [ - half : ] :

series . append ( value )

return series

# group data by activity

def data_by_activity ( X , y , activities ) :

# group windows by activity

return { a : X [ y [ : , 0 ] == a , : , : ] for a in activities }

# plot activity durations by subject

def plot_activity_durations_by_subject ( X , y , sub_map ) :

# get unique subjects and activities

subject_ids = unique ( sub_map [ : , 0 ] )

activity_ids = unique ( y [ : , 0 ] )

# enumerate subjects

activity_windows = { a : list ( ) for a in activity_ids }

for sub_id in subject_ids :

# get data for one subject

_ , subj_y = data_for_subject ( X , y , sub_map , sub_id )

# count windows by activity

for a in activity_ids :

activity_windows [ a ] . append ( len ( subj_y [ subj_y [ : , 0 ] == a ] ) )

# organize durations into a list of lists

durations = [ activity_windows [ a ] for a in activity_ids ]

pyplot . boxplot ( durations , labels = activity_ids )

pyplot . show ( )

# load training dataset

X , y = load_dataset ( 'train' , 'HARDataset/' )

# load mapping of rows to subjects

sub_map = load_file ( 'HARDataset/train/subject_train.txt' )

# plot durations"
181;181;news.mit.edu;http://news.mit.edu/2018/blind-cheetah-robot-climb-stairs-obstacles-disaster-zones-0705;;“Blind” Cheetah 3 robot can climb stairs littered with obstacles;"MIT’s Cheetah 3 robot can now leap and gallop across rough terrain, climb a staircase littered with debris, and quickly recover its balance when suddenly yanked or shoved, all while essentially blind.

The 90-pound mechanical beast — about the size of a full-grown Labrador — is intentionally designed to do all this without relying on cameras or any external environmental sensors. Instead, it nimbly “feels” its way through its surroundings in a way that engineers describe as “blind locomotion,” much like making one’s way across a pitch-black room.

“There are many unexpected behaviors the robot should be able to handle without relying too much on vision,” says the robot’s designer, Sangbae Kim, associate professor of mechanical engineering at MIT. “Vision can be noisy, slightly inaccurate, and sometimes not available, and if you rely too much on vision, your robot has to be very accurate in position and eventually will be slow. So we want the robot to rely more on tactile information. That way, it can handle unexpected obstacles while moving fast.”

Researchers will present the robot’s vision-free capabilities in October at the International Conference on Intelligent Robots, in Madrid. In addition to blind locomotion, the team will demonstrate the robot’s improved hardware, including an expanded range of motion compared to its predecessor Cheetah 2, that allows the robot to stretch backwards and forwards, and twist from side to side, much like a cat limbering up to pounce.

Within the next few years, Kim envisions the robot carrying out tasks that would otherwise be too dangerous or inaccessible for humans to take on.

“Cheetah 3 is designed to do versatile tasks such as power plant inspection, which involves various terrain conditions including stairs, curbs, and obstacles on the ground,” Kim says. ""I think there are countless occasions where we [would] want to send robots to do simple tasks instead of humans. Dangerous, dirty, and difficult work can be done much more safely through remotely controlled robots.”

Making a commitment

The Cheetah 3 can blindly make its way up staircases and through unstructured terrain, and can quickly recover its balance in the face of unexpected forces, thanks to two new algorithms developed by Kim’s team: a contact detection algorithm, and a model-predictive control algorithm.

The contact detection algorithm helps the robot determine the best time for a given leg to switch from swinging in the air to stepping on the ground. For example, if the robot steps on a light twig versus a hard, heavy rock, how it reacts — and whether it continues to carry through with a step, or pulls back and swings its leg instead — can make or break its balance.

“When it comes to switching from the air to the ground, the switching has to be very well-done,” Kim says. “This algorithm is really about, ‘When is a safe time to commit my footstep?’”

The contact detection algorithm helps the robot determine the best time to transition a leg between swing and step, by constantly calculating for each leg three probabilities: the probability of a leg making contact with the ground, the probability of the force generated once the leg hits the ground, and the probability that the leg will be in midswing. The algorithm calculates these probabilities based on data from gyroscopes, accelerometers, and joint positions of the legs, which record the leg’s angle and height with respect to the ground.

If, for example, the robot unexpectedly steps on a wooden block, its body will suddenly tilt, shifting the angle and height of the robot. That data will immediately feed into calculating the three probabilities for each leg, which the algorithm will combine to estimate whether each leg should commit to pushing down on the ground, or lift up and swing away in order to keep its balance — all while the robot is virtually blind.

“If humans close our eyes and make a step, we have a mental model for where the ground might be, and can prepare for it. But we also rely on the feel of touch of the ground,” Kim says. “We are sort of doing the same thing by combining multiple [sources of] information to determine the transition time.”

The researchers tested the algorithm in experiments with the Cheetah 3 trotting on a laboratory treadmill and climbing on a staircase. Both surfaces were littered with random objects such as wooden blocks and rolls of tape.

“It doesn’t know the height of each step, and doesn’t know there are obstacles on the stairs, but it just plows through without losing its balance,” Kim says. “Without that algorithm, the robot was very unstable and fell easily.”

Future force

The robot’s blind locomotion was also partly due to the model-predictive control algorithm, which predicts how much force a given leg should apply once it has committed to a step.

“The contact detection algorithm will tell you, ‘this is the time to apply forces on the ground,’” Kim says. “But once you’re on the ground, now you ne"
182;182;machinelearningmastery.com;http://machinelearningmastery.com/normalize-standardize-machine-learning-data-weka/;2016-07-04;How to Normalize and Standardize Your Machine Learning Data in Weka;"Tweet Share Share

Last Updated on December 11, 2019

Machine learning algorithms make assumptions about the dataset you are modeling.

Often, raw data is comprised of attributes with varying scales. For example, one attribute may be in kilograms and another may be a count. Although not required, you can often get a boost in performance by carefully choosing methods to rescale your data.

In this post you will discover how you can rescale your data so that all of the data has the same scale.

After reading this post you will know:

How to normalize your numeric attributes between the range of 0 and 1.

How to standardize your numeric attributes to have a 0 mean and unit variance.

When to choose normalization or standardization.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Update March/2018: Added alternate link to download the dataset as the original appears to have been taken down.

Predict the Onset of Diabetes

The dataset used for this example is the Pima Indians onset of diabetes dataset.

It is a classification problem where each instance represents medical details for one patient and the task is to predict whether the patient will have an onset of diabetes within the next five years.

This is a good dataset to practice scaling as the 8 input variables all have varying scales, such as the count of the number of times the patient was pregnant (preg) and the calculation of the patients body mass index (mass).

Download the dataset and place it in your current working directory.

You can also access this dataset in your Weka installation, under the data/ directory in the file called diabetes.arff.

About Data Filters in Weka

Weka provides filters for transforming your dataset. The best way to see what filters are supported and to play with them on your dataset is to use the Weka Explorer.

The “Filter” pane allows you to choose a filter.

Filters are divided into two types:

Supervised Filters : That can be applied but require user control in some way. Such as rebalancing instances for a class.

: That can be applied but require user control in some way. Such as rebalancing instances for a class. Unsupervised Filters: That can be applied in an undirected manner. For example, rescale all values to the range 0-to-1.

Personally, I think the distinction between these two types of filters is a little arbitrary and confusing. Nevertheless, that is how they are laid out.

Within these two groups, filters are further divided into filters for Attributes and Instances:

Attribute Filters : Apply an operation on attributes or one attribute at a time.

: Apply an operation on attributes or one attribute at a time. Instance Filters: Apply an operation on instance or one instance at a time.

This distinction makes a lot more sense.

After you have selected a filter, its name will appear in the box next to the “Choose” button.

You can configure a filter by clicking its name which will open the configuration window. You can change the parameters of the filter and even save or load the configuration of the filter itself. This is great for reproducibility.

You can learn more about each configuration option by hovering over it and reading the tooltip.

You can also read all of the details about the filter including the configuration, papers and books for further reading and more information about the filter works by clicking the “More” button.

You can close the help and apply the configuration by clicking the “OK” button.

You can apply a filter to your loaded dataset by clicking the “Apply” button next to the filter name.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Normalize Your Numeric Attributes

Data normalization is the process of rescaling one or more attributes to the range of 0 to 1. This means that the largest value for each attribute is 1 and the smallest value is 0.

Normalization is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve).

You can normalize all of the attributes in your dataset with Weka by choosing the Normalize filter and applying it to your dataset.

You can use the following recipe to normalize your dataset:

1. Open the Weka Explorer.

2. Load your dataset.

3. Click the “Choose” button to select a Filter and select unsupervised.attribute.Normalize.

4. Click the “Apply” button to normalize your dataset.

5. Click the “Save” button and type a filename to save the normalized copy of your dataset.

Reviewing the details of each attribute in the “Selected attribute” window will give you confidence that the filter was successful and that each attribute was re"
183;183;machinelearningmastery.com;https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/;2017-11-05;How to Develop a Character-Based Neural Language Model in Keras;"from numpy import array

from pickle import dump

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

# load doc into memory

def load_doc ( filename ) :

# open the file as read only

file = open ( filename , 'r' )

# read all text

text = file . read ( )

# close the file

file . close ( )

return text

# load

in_filename = 'char_sequences.txt'

raw_text = load_doc ( in_filename )

lines = raw_text . split ( '

' )

# integer encode sequences of characters

chars = sorted ( list ( set ( raw_text ) ) )

mapping = dict ( ( c , i ) for i , c in enumerate ( chars ) )

sequences = list ( )

for line in lines :

# integer encode line

encoded_seq = [ mapping [ char ] for char in line ]

# store

sequences . append ( encoded_seq )

# vocabulary size

vocab_size = len ( mapping )

print ( 'Vocabulary Size: %d' % vocab_size )

# separate into input and output

sequences = array ( sequences )

X , y = sequences [ : , : - 1 ] , sequences [ : , - 1 ]

sequences = [ to_categorical ( x , num_classes = vocab_size ) for x in X ]

X = array ( sequences )

y = to_categorical ( y , num_classes = vocab_size )

# define model

model = Sequential ( )

model . add ( LSTM ( 75 , input_shape = ( X . shape [ 1 ] , X . shape [ 2 ] ) ) )

model . add ( Dense ( vocab_size , activation = 'softmax' ) )

print ( model . summary ( ) )

# compile model

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( X , y , epochs = 100 , verbose = 2 )

# save the model to file

model . save ( 'model.h5' )

# save the mapping"
184;184;machinelearningmastery.com;http://machinelearningmastery.com/spot-check-classification-machine-learning-algorithms-python-scikit-learn/;2016-05-26;Spot-Check Classification Machine Learning Algorithms in Python with scikit-learn;"# Logistic Regression Classification

import pandas

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

seed = 7

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

model = LogisticRegression ( )

results = model_selection . cross_val_score ( model , X , Y , cv = kfold )"
185;185;machinelearningmastery.com;https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/;2017-11-06;How to Get Started with Deep Learning for Natural Language Processing;"from gensim . models import Word2Vec

from sklearn . decomposition import PCA

from matplotlib import pyplot

# define training data

sentences = [ [ 'this' , 'is' , 'the' , 'first' , 'sentence' , 'for' , 'word2vec' ] ,

[ 'this' , 'is' , 'the' , 'second' , 'sentence' ] ,

[ 'yet' , 'another' , 'sentence' ] ,

[ 'one' , 'more' , 'sentence' ] ,

[ 'and' , 'the' , 'final' , 'sentence' ] ]

# train model

model = Word2Vec ( sentences , min_count = 1 )

# fit a 2D PCA model to the vectors

X = model [ model . wv . vocab ]

pca = PCA ( n_components = 2 )

result = pca . fit_transform ( X )

# create a scatter plot of the projection

pyplot . scatter ( result [ : , 0 ] , result [ : , 1 ] )

words = list ( model . wv . vocab )

for i , word in enumerate ( words ) :

pyplot . annotate ( word , xy = ( result [ i , 0 ] , result [ i , 1 ] ) )"
186;186;machinelearningmastery.com;https://machinelearningmastery.com/how-to-load-and-manipulate-images-for-deep-learning-in-python-with-pil-pillow/;2019-03-21;How to Load and Manipulate Images for Deep Learning in Python With PIL/Pillow;"Tweet Share Share

Last Updated on September 12, 2019

Before you can develop predictive models for image data, you must learn how to load and manipulate images and photographs.

The most popular and de facto standard library in Python for loading and working with image data is Pillow. Pillow is an updated version of the Python Image Library, or PIL, and supports a range of simple and sophisticated image manipulation functionality. It is also the basis for simple image support in other Python libraries such as SciPy and Matplotlib.

In this tutorial, you will discover how to load and manipulate image data using the Pillow Python library.

After completing this tutorial, you will know:

How to install the Pillow library and confirm it is working correctly.

How to load images from file, convert loaded images to NumPy arrays, and save images in new formats.

How to perform basic transforms to image data such as resize, flips, rotations, and cropping.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Updated Sep/2019: Updated to reflect minor changes to Pillow API.

Tutorial Overview

This tutorial is divided into six parts; they are:

How to Install Pillow How to Load and Display Images How to Convert Images to NumPy Arrays and Back How to Save Images to File How to Resize Images How to Flip, Rotate, and Crop Images

How to Install Pillow

The Python Imaging Library, or PIL for short, is an open source library for loading and manipulating images.

It was developed and made available more than 25 years ago and has become a de facto standard API for working with images in Python. The library is now defunct and no longer updated and does not support Python 3.

Pillow is a PIL library that supports Python 3 and is the preferred modern library for image manipulation in Python. It is even required for simple image loading and saving in other Python scientific libraries such as SciPy and Matplotlib.

The Pillow library is installed as a part of most SciPy installations; for example, if you are using Anaconda.

For help setting up your SciPy environment, see the step-by-step tutorial:

If you manage the installation of Python software packages yourself for your workstation, you can easily install Pillow using pip; for example:

sudo pip install Pillow 1 sudo pip install Pillow

For more help installing Pillow manually, see:

Pillow is built on top of the older PIL and you can confirm that the library was installed correctly by printing the version number; for example:

# check Pillow version number import PIL print('Pillow Version:', PIL.__version__) 1 2 3 # check Pillow version number import PIL print ( 'Pillow Version:' , PIL . __version__ )

Running the example will print the version number for Pillow; your version number should be the same or higher.

Pillow Version: 6.1.0 1 Pillow Version: 6.1.0

Now that your environment is set up, let’s look at how to load an image.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Load and Display Images

We need a test image to demonstrate some important features of using the Pillow library.

In this tutorial, we will use a photograph of the Sydney Opera House, taken by Ed Dunens and made available on Flickr under a creative commons license, some rights reserved.

Download the photograph and save it in your current working directory with the file name “opera_house.jpg“.

Images are typically in PNG or JPEG format and can be loaded directly using the open() function on Image class. This returns an Image object that contains the pixel data for the image as well as details about the image. The Image class is the main workhorse for the Pillow library and provides a ton of properties about the image as well as functions that allow you to manipulate the pixels and format of the image.

The ‘format‘ property on the image will report the image format (e.g. JPEG), the ‘mode‘ will report the pixel channel format (e.g. RGB or CMYK), and the ‘size‘ will report the dimensions of the image in pixels (e.g. 640×480).

The show() function will display the image using your operating systems default application.

The example below demonstrates how to load and show an image using the Image class in the Pillow library.

# load and show an image with Pillow from PIL import Image # load the image image = Image.open('opera_house.jpg') # summarize some details about the image print(image.format) print(image.mode) print(image.size) # show the image image.show() 1 2 3 4 5 6 7 8 9 10 # load and show an image with Pillow from PIL import Image # load the image image = Image . open ( 'opera_house.jpg' ) # summarize some details about the image print ( image . format ) print ( im"
187;187;machinelearningmastery.com;https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/;2017-12-07;Encoder-Decoder Models for Text Summarization in Keras;"vocab_size = . . .

src_txt_length = . . .

sum_txt_length = . . .

# encoder input model

inputs = Input ( shape = ( src_txt_length , ) )

encoder1 = Embedding ( vocab_size , 128 ) ( inputs )

encoder2 = LSTM ( 128 ) ( encoder1 )

encoder3 = RepeatVector ( sum_txt_length ) ( encoder2 )

# decoder output model

decoder1 = LSTM ( 128 , return_sequences = True ) ( encoder3 )

outputs = TimeDistributed ( Dense ( vocab_size , activation = 'softmax' ) ) ( decoder1 )

# tie it together

model = Model ( inputs = inputs , outputs = outputs )"
188;188;machinelearningmastery.com;https://machinelearningmastery.com/machine-learning-in-python-step-by-step/;2019-02-09;Your First Machine Learning Project in Python Step-By-Step;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42

# compare algorithms from pandas import read_csv from matplotlib import pyplot from sklearn . model_selection import train_test_split from sklearn . model_selection import cross_val_score from sklearn . model_selection import StratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . tree import DecisionTreeClassifier from sklearn . neighbors import KNeighborsClassifier from sklearn . discriminant_analysis import LinearDiscriminantAnalysis from sklearn . naive_bayes import GaussianNB from sklearn . svm import SVC # Load dataset url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"" names = [ 'sepal-length' , 'sepal-width' , 'petal-length' , 'petal-width' , 'class' ] dataset = read_csv ( url , names = names ) # Split-out validation dataset array = dataset . values X = array [ : , 0 : 4 ] y = array [ : , 4 ] X_train , X_validation , Y_train , Y_validation = train_test_split ( X , y , test_size = 0.20 , random_state = 1 , shuffle = True ) # Spot Check Algorithms models = [ ] models . append ( ( 'LR' , LogisticRegression ( solver = 'liblinear' , multi_class = 'ovr' ) ) ) models . append ( ( 'LDA' , LinearDiscriminantAnalysis ( ) ) ) models . append ( ( 'KNN' , KNeighborsClassifier ( ) ) ) models . append ( ( 'CART' , DecisionTreeClassifier ( ) ) ) models . append ( ( 'NB' , GaussianNB ( ) ) ) models . append ( ( 'SVM' , SVC ( gamma = 'auto' ) ) ) # evaluate each model in turn results = [ ] names = [ ] for name , model in models : kfold = StratifiedKFold ( n_splits = 10 , random_state = 1 ) cv_results = cross_val_score ( model , X_train , Y_train , cv = kfold , scoring = 'accuracy' ) results . append ( cv_results ) names . append ( name ) print ( '%s: %f (%f)' % ( name , cv_results . mean ( ) , cv_results . std ( ) ) ) # Compare Algorithms pyplot . boxplot ( results , labels = names ) pyplot . title ( 'Algorithm Comparison' ) pyplot . show ( )"
189;189;news.mit.edu;http://news.mit.edu/2020/mit-scientist-jill-crittenden-helps-build-covid-19-resource-addressing-face-mask-shortage-0403;;MIT scientist helps build Covid-19 resource to address shortage of face masks;"When the Covid-19 crisis hit the United States this March, MIT neuroscientist Jill Crittenden wanted to help. One of her greatest concerns was the shortage of face masks, which are a key weapon for health care providers, frontline service workers, and the public to protect against respiratory transmission of Covid-19. For those caring for Covid-19 patients, face masks that provide a near-100 percent seal are essential. These critical pieces of equipment, called N95 masks, are now scarce, and health-care workers are now faced with reusing potentially contaminated masks.

To address this, Crittenden joined a team of 60 scientists and engineers, students, and clinicians drawn from universities and the private sector to synthesize the scientific literature about mask decontamination and create a set of best practices for bad times. The group has now unveiled a website, N95decon.org, which provides a summary of this critical information.

“I first heard about the group from Larissa Little, a Harvard graduate student with John Doyle,” explains Crittenden, who is a research scientist in Ann Graybiel's lab at the McGovern Institute for Brain Research at MIT. “The three of us began communicating because we are all also members of the Boston-based MGB Covid-19 Innovation Center, and we agreed that helping to assess the flood of information on N95 decontamination would be an important contribution.”

The team members who came together over several weeks scoured hundreds of peer-reviewed publications and held continuous online meetings to review studies of decontamination methods that had been used to inactivate previous viral and bacterial pathogens, and to then assess the potential for these methods to neutralize the novel SARS-CoV-2 virus that causes Covid-19.

“This group is absolutely amazing,” says Crittenden. “The Zoom meetings are very productive because it is all data- and solutions-driven. Everyone throws out ideas, what they know and what the literature source is, with the only goal being to get to a data-based consensus efficiently.”

Reliable resource

The goal of the consortium was to provide overwhelmed health officials, who don’t have the time to study the literature for themselves, reliable, pre-digested scientific information about the pros and cons of three decontamination methods that offer the best options should local shortages force a choice between decontamination and reuse, or going unmasked.

The three methods involve (1) heat and humidity, (2) a specific wavelength of light called ultraviolet C (UVC), and (3) treatment with hydrogen peroxide vapors (HPV). The scientists did not endorse any one method, but instead sought to describe the circumstances under which each could inactivate the virus provided rigorous procedures were followed. Devices that rely on heat, for instance, could be used under specific temperature, humidity, and time parameters. With UVC devices — which emit a particular wavelength and energy level of light — considerations involve making sure masks are properly oriented to the light so the entire surface is bathed in sufficient energy. The HPV method has the potential advantage of decontaminating masks in volume, as the U.S. Food and Drug Administration, acting in this emergency, has certified certain vendors to offer hydrogen peroxide vapor treatments on a large scale. In addition to giving health officials the scientific information to assess the methods best suited to their circumstances, N95decon.org points decision-makers to sources of reliable and detailed how-to information provided by other organizations, institutions, and commercial services.

“While there is no perfect method for decontamination of N95 masks, it is crucial that decision-makers and users have as much information as possible about the strengths and weaknesses of various approaches,” says Manu Prakash, an associate professor of bioengineering at Stanford University, who helped coordinate this ad hoc, volunteer undertaking. “Manufacturers currently do not recommend N95 mask reuse. We aim to provide information and evidence in this critical time to help those on the front lines of this crisis make risk-management decisions given the specific conditions and limitations they face.”

The researchers stressed that decontamination does not solve the N95 shortage, and expressed the hope that new masks should be made available in large numbers as soon as possible so that health-care workers and first providers could be issued fresh protective gear whenever needed as specified by the non-emergency guidelines set by the U.S. Centers for Disease Control and Prevention.

Forward thinking

Meanwhile, these ad hoc volunteers have pledged to continue working together to update the N95decon.org website as new information becomes available, and to coordinate their efforts to do research to plug the gaps in current knowledge to avoid duplication of effort.

“We are, at heart, a group of people that want to help bette"
190;190;machinelearningmastery.com;https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/;2017-06-29;Attention in Long Short-Term Memory Recurrent Neural Networks;"Tweet Share Share

Last Updated on August 14, 2019

The Encoder-Decoder architecture is popular because it has demonstrated state-of-the-art results across a range of domains.

A limitation of the architecture is that it encodes the input sequence to a fixed length internal representation. This imposes limits on the length of input sequences that can be reasonably learned and results in worse performance for very long input sequences.

In this post, you will discover the attention mechanism for recurrent neural networks that seeks to overcome this limitation.

After reading this post, you will know:

The limitation of the encode-decoder architecture and the fixed-length internal representation.

The attention mechanism to overcome the limitation that allows the network to learn where to pay attention in the input sequence for each item in the output sequence.

5 applications of the attention mechanism with recurrent neural networks in domains such as text translation, speech recognition, and more.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Problem With Long Sequences

The encoder-decoder recurrent neural network is an architecture where one set of LSTMs learn to encode input sequences into a fixed-length internal representation, and second set of LSTMs read the internal representation and decode it into an output sequence.

This architecture has shown state-of-the-art results on difficult sequence prediction problems like text translation and quickly became the dominant approach.

For example, see:

The encoder-decoder architecture still achieves excellent results on a wide range of problems. Nevertheless, it suffers from the constraint that all input sequences are forced to be encoded to a fixed length internal vector.

This is believed to limit the performance of these networks, especially when considering long input sequences, such as very long sentences in text translation problems.

A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.

— Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Attention within Sequences

Attention is the idea of freeing the encoder-decoder architecture from the fixed-length internal representation.

This is achieved by keeping the intermediate outputs from the encoder LSTM from each step of the input sequence and training the model to learn to pay selective attention to these inputs and relate them to items in the output sequence.

Put another way, each item in the output sequence is conditional on selective items in the input sequence.

Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words. … it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector.

— Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015

This increases the computational burden of the model, but results in a more targeted and better-performing model.

In addition, the model is also able to show how attention is paid to the input sequence when predicting the output sequence. This can help in understanding and diagnosing exactly what the model is considering and to what degree for specific input-output pairs.

The proposed approach provides an intuitive way to inspect the (soft-)alignment between the words in a generated translation and those in a source sentence. This is done by visualizing the annotation weights… Each row of a matrix in each plot indicates the weights associated with the annotations. From this we see which positions in the source sentence were considered more important when generating the target word.

— Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015

Problem with Large Images

Convolutional neural networks applied to computer vision problems also suffer from similar limitat"
191;191;web.mit.edu;http://web.mit.edu/notice-proposed-settlement-class-action-lawsuit;;MIT - Massachusetts Institute of Technology;"NOTICE OF PROPOSED SETTLEMENT OF CLASS ACTION LAWSUIT

ATTENTION: ALL PEOPLE WHO ARE DEAF OR HARD OF HEARING WHO WANT CAPTIONING OF MIT’S ONLINE CONTENT

If you are deaf or hard of hearing and have tried to access or would like to access online video content of the Massachusetts Institute of Technology (“MIT”) with captions or to access MIT’s online audio content with a transcript, you may be a member of the proposed Settlement Class affected by this lawsuit. The Settlement Class in this case does not include students of MIT. This is a court-authorized notice.

PLEASE READ THIS NOTICE CAREFULLY. YOUR RIGHTS MAY BE AFFECTED BY LEGAL PROCEEDINGS IN THIS CASE.

NOTICE OF CLASS ACTION

The purpose of this notice is to inform you of a proposed settlement in a pending class action lawsuit brought by the National Association of the Deaf (“NAD”) and three Deaf plaintiffs on behalf of deaf and hard of hearing individuals against MIT. The case is titled National Association of the Deaf v. Massachusetts Institute of Technology, No. 3:15-cv-30024-KAR, and is pending in the United States District Court for the District of Massachusetts. The proposed class action settlement (“Settlement”) is set forth in a proposed Consent Decree, which must be approved by the United States District Court.

BACKGROUND

This lawsuit alleges that MIT violated the Americans with Disabilities Act and the Rehabilitation Act by failing to provide captioning for its publicly available online content. Plaintiffs and other deaf and hard of hearing individuals alleged that they attempted to access MIT’s publicly available online content but were unable to do so because it did not have captions or had inaccurate captions.

This is a class action. In a class action, one or more people or organizations, called Class Representatives (in this case the National Association of the Deaf, C. Wayne Dore, Christy Smith, and Lee Nettles (“Plaintiffs”)), sue on behalf of people who have similar legal claims. All of these people are a Class or Class Members. One court resolves the issues for all Class Members. United States Magistrate Judge Katherine A. Robertson is in charge of this class action.

The Court did not decide in favor of either Plaintiffs or MIT in this case. Instead, both sides agreed to a settlement. That way, they avoid the cost, delay, and uncertainty of a trial. The settlement provides benefits that go to the Class Members. The Class Representatives and Class Counsel (the attorneys appointed by the Court to represent the Class) think the proposed settlement is in the best interests of the Class Members, taking into account the benefits of the settlement, the risks of continued litigation, and the delay in obtaining relief for the Class if the litigation continues.

THE SETTLEMENT CLASS

The Settlement Class includes all persons (other than students of MIT) who, at any time between February 11, 2012 and the date of preliminary approval of this settlement, have claimed or could have claimed to assert a right under Title III of the ADA, Section 504 of the Rehabilitation Act, and/or other federal, state or local statutes or regulations that set forth standards or obligations coterminous with or equivalent to Title III of the Americans with Disabilities Act or any of the rules or regulations promulgated thereunder, alleging that they are deaf or hard of hearing and that MIT has failed to make accessible to persons who are deaf or hard of hearing online content posted and available for the general public that is produced, created, hosted, linked to, or embedded by MIT.

SUMMARY OF THE PROPOSED SETTLEMENT

The following is a summary of certain provisions of the Settlement. The complete Settlement, set forth in the proposed Consent Decree, is available as set forth below.

The Settlement requires MIT to caption content on Covered MIT Webpages as follows:

Content posted by faculty or employees acting within the scope of their employment, or any MIT Sponsored Student Group (as defined by the Association of Student Activities), on or after the date 60 days after the Effective Date will include captioning when posted.

Content posted by faculty or employees acting within the scope of their employment, or any MIT Sponsored Student Group (as defined by the Association of Student Activities), prior to January 1, 2019 will be captioned upon request within seven business days.

Content posted by faculty or employees acting within in the scope of their employment, or any MIT Sponsored Student Group (as defined by the Association of Student Activities), after January 1, 2019 but before the date 60 days after the Effective Date will be captioned as soon as practicable but no later than one year from the Effective Date, or upon request within seven days.

“Covered MIT Webpages” means public webpages within the MIT.edu domain and corresponding public platforms such as YouTube, Vimeo, and Soundcloud channels operated by MIT, with the exception of certain specific"
192;192;machinelearningmastery.com;http://machinelearningmastery.com/prepare-data-machine-learning-python-scikit-learn/;2016-05-17;How To Prepare Your Data For Machine Learning in Python with Scikit-Learn;"# Rescale data (between 0 and 1)

import pandas

import scipy

import numpy

from sklearn . preprocessing import MinMaxScaler

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

# separate array into input and output components

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

scaler = MinMaxScaler ( feature_range = ( 0 , 1 ) )

rescaledX = scaler . fit_transform ( X )

# summarize transformed data

numpy . set_printoptions ( precision = 3 )"
193;193;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-1-dimensional-function-from-scratch-in-keras/;2019-06-25;How to Develop a 1D Generative Adversarial Network From Scratch in Keras;"# train a generative adversarial network on a one-dimensional function

from numpy import hstack

from numpy import zeros

from numpy import ones

from numpy . random import rand

from numpy . random import randn

from keras . models import Sequential

from keras . layers import Dense

from matplotlib import pyplot

# define the standalone discriminator model

def define_discriminator ( n_inputs = 2 ) :

model = Sequential ( )

model . add ( Dense ( 25 , activation = 'relu' , kernel_initializer = 'he_uniform' , input_dim = n_inputs ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

return model

# define the standalone generator model

def define_generator ( latent_dim , n_outputs = 2 ) :

model = Sequential ( )

model . add ( Dense ( 15 , activation = 'relu' , kernel_initializer = 'he_uniform' , input_dim = latent_dim ) )

model . add ( Dense ( n_outputs , activation = 'linear' ) )

return model

# define the combined generator and discriminator model, for updating the generator

def define_gan ( generator , discriminator ) :

# make weights in the discriminator not trainable

discriminator . trainable = False

# connect them

model = Sequential ( )

# add generator

model . add ( generator )

# add the discriminator

model . add ( discriminator )

# compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' )

return model

# generate n real samples with class labels

def generate_real_samples ( n ) :

# generate inputs in [-0.5, 0.5]

X1 = rand ( n ) - 0.5

# generate outputs X^2

X2 = X1 * X1

# stack arrays

X1 = X1 . reshape ( n , 1 )

X2 = X2 . reshape ( n , 1 )

X = hstack ( ( X1 , X2 ) )

# generate class labels

y = ones ( ( n , 1 ) )

return X , y

# generate points in latent space as input for the generator

def generate_latent_points ( latent_dim , n ) :

# generate points in the latent space

x_input = randn ( latent_dim * n )

# reshape into a batch of inputs for the network

x_input = x_input . reshape ( n , latent_dim )

return x_input

# use the generator to generate n fake examples, with class labels

def generate_fake_samples ( generator , latent_dim , n ) :

# generate points in latent space

x_input = generate_latent_points ( latent_dim , n )

# predict outputs

X = generator . predict ( x_input )

# create class labels

y = zeros ( ( n , 1 ) )

return X , y

# evaluate the discriminator and plot real and fake points

def summarize_performance ( epoch , generator , discriminator , latent_dim , n = 100 ) :

# prepare real samples

x_real , y_real = generate_real_samples ( n )

# evaluate discriminator on real examples

_ , acc_real = discriminator . evaluate ( x_real , y_real , verbose = 0 )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( generator , latent_dim , n )

# evaluate discriminator on fake examples

_ , acc_fake = discriminator . evaluate ( x_fake , y_fake , verbose = 0 )

# summarize discriminator performance

print ( epoch , acc_real , acc_fake )

# scatter plot real and fake data points

pyplot . scatter ( x_real [ : , 0 ] , x_real [ : , 1 ] , color = 'red' )

pyplot . scatter ( x_fake [ : , 0 ] , x_fake [ : , 1 ] , color = 'blue' )

pyplot . show ( )

# train the generator and discriminator

def train ( g_model , d_model , gan_model , latent_dim , n_epochs = 10000 , n_batch = 128 , n_eval = 2000 ) :

# determine half the size of one batch, for updating the discriminator

half_batch = int ( n_batch / 2 )

# manually enumerate epochs

for i in range ( n_epochs ) :

# prepare real samples

x_real , y_real = generate_real_samples ( half_batch )

# prepare fake examples

x_fake , y_fake = generate_fake_samples ( g_model , latent_dim , half_batch )

# update discriminator

d_model . train_on_batch ( x_real , y_real )

d_model . train_on_batch ( x_fake , y_fake )

# prepare points in latent space as input for the generator

x_gan = generate_latent_points ( latent_dim , n_batch )

# create inverted labels for the fake samples

y_gan = ones ( ( n_batch , 1 ) )

# update the generator via the discriminator's error

gan_model . train_on_batch ( x_gan , y_gan )

# evaluate the model every n_eval epochs

if ( i + 1 ) % n_eval == 0 :

summarize_performance ( i , g_model , d_model , latent_dim )

# size of the latent space

latent_dim = 5

# create the discriminator

discriminator = define_discriminator ( )

# create the generator

generator = define_generator ( latent_dim )

# create the gan

gan_model = define_gan ( generator , discriminator )

# train model"
194;194;machinelearningmastery.com;https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/;2019-06-26;How to Develop a Deep Learning Photo Caption Generator from Scratch;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179

from numpy import array from pickle import load from keras . preprocessing . text import Tokenizer from keras . preprocessing . sequence import pad_sequences from keras . utils import to_categorical from keras . utils import plot_model from keras . models import Model from keras . layers import Input from keras . layers import Dense from keras . layers import LSTM from keras . layers import Embedding from keras . layers import Dropout from keras . layers . merge import add from keras . callbacks import ModelCheckpoint # load doc into memory def load_doc ( filename ) : # open the file as read only file = open ( filename , 'r' ) # read all text text = file . read ( ) # close the file file . close ( ) return text # load a pre-defined list of photo identifiers def load_set ( filename ) : doc = load_doc ( filename ) dataset = list ( ) # process line by line for line in doc . split ( '

' ) : # skip empty lines if len ( line ) < 1 : continue # get the image identifier identifier = line . split ( '.' ) [ 0 ] dataset . append ( identifier ) return set ( dataset ) # load clean descriptions into memory def load_clean_descriptions ( filename , dataset ) : # load document doc = load_doc ( filename ) descriptions = dict ( ) for line in doc . split ( '

' ) : # split line by white space tokens = line . split ( ) # split id from description image_id , image_desc = tokens [ 0 ] , tokens [ 1 : ] # skip images not in the set if image_id in dataset : # create list if image_id not in descriptions : descriptions [ image_id ] = list ( ) # wrap description in tokens desc = 'startseq ' + ' ' . join ( image_desc ) + ' endseq' # store descriptions [ image_id ] . append ( desc ) return descriptions # load photo features def load_photo_features ( filename , dataset ) : # load all features all_features = load ( open ( filename , 'rb' ) ) # filter features features = { k : all_features [ k ] for k in dataset } return features # covert a dictionary of clean descriptions to a list of descriptions def to_lines ( descriptions ) : all_desc = list ( ) for key in descriptions . keys ( ) : [ all_desc . append ( d ) for d in descriptions [ key ] ] return all_desc # fit a tokenizer given caption descriptions def create_tokenizer ( descriptions ) : lines = to_lines ( descriptions ) tokenizer = Tokenizer ( ) tokenizer . fit_on_texts ( lines ) return tokenizer # calculate the length of the description with the most words def max_length ( descriptions ) : lines = to_lines ( descriptions ) return max ( len ( d . split ( ) ) for d in lines ) # create sequences of images, input sequences and output words for an image def create_sequences ( tokenizer , max_length , descriptions , photos , vocab_size ) : X1 , X2 , y = list ( ) , list ( ) , list ( ) # walk through each image identifier for key , desc_list in descriptions . items ( ) : # walk through each description for the image for desc in desc_list : # encode the sequence seq = tokenizer . texts_to_sequences ( [ desc ] ) [ 0 ] # split one sequence into multiple X,y pairs for i in range ( 1 , len ( seq ) ) : # split into input and output pair in_seq , out_seq = seq [ : i ] , seq [ i ] # pad input sequence in_seq = pad_sequences ( [ in_seq ] , maxlen = max_length ) [ 0 ] # encode output sequence out_seq = to_categorical ( [ out_seq ] , num_classes = vocab_size ) [ 0 ] # store X1 . append ( photos [ key ] [ 0 ] ) X2 . append ( in_seq ) y . append ( out_seq ) return array ( X1 ) , array ( X2 ) , array ( y ) # define the captioning model def define_model ( vocab_size , max_length ) : # feature extractor model inputs1 = Input ( shape = ( 4096 , ) ) fe1 = Dropout ( 0.5 ) ( inputs1 ) fe2 = Dense ( 256 , activation = 'relu' ) ( fe1 ) # sequence model inputs2 = Input ( shape = ( max_length , ) ) se1 = Embedding ( vocab_size , 256 , mask_zero = True ) ( inputs2 ) se2 = Dropout ( 0.5 ) ( se1 ) se3 = LSTM ( 256 ) ( se2 ) # decoder model decoder1 = add ( [ fe2 , se3 ] ) decoder2 = Dense ( 256 , activation = 'relu' ) ( decoder1 ) outputs = Dense ( vocab_size , activation = 'softmax' ) ( decoder2 ) # tie it together [image, seq] [word] model = Model ( inputs = [ inputs1 , inputs2 ] , outputs = outputs ) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' ) # summarize model print ( model . summary ( ) ) plot_model ( model , to_file = 'model.png' , show_shapes = True ) return model # train dataset # l"
195;195;news.mit.edu;http://news.mit.edu/2020/reducing-delays-wireless-networks-csail-0410;;Reducing delays in wireless networks;"MIT researchers have designed a congestion-control scheme for wireless networks that could help reduce lag times and increase quality in video streaming, video chat, mobile gaming, and other web services.

To keep web services running smoothly, congestion-control schemes infer information about a network’s bandwidth capacity and congestion based on feedback from the network routers, which is encoded in data packets. That information determines how fast data packets are sent through the network.

Deciding a good sending rate can be a tough balancing act. Senders don’t want to be overly conservative: If a network’s capacity constantly varies from, say, 2 megabytes per second to 500 kilobytes per second, the sender could always send traffic at the lowest rate. But then your Netflix video, for example, will be unnecessarily low-quality. On the other hand, if the sender constantly maintains a high rate, even when network capacity dips, it could overwhelm the network, creating a massive queue of data packets waiting to be delivered. Queued packets can increase the network’s delay, causing, say, your Skype call to freeze.

Things get even more complicated in wireless networks, which have “time-varying links,” with rapid, unpredictable capacity shifts. Depending on various factors, such as the number of network users, cell tower locations, and even surrounding buildings, capacities can double or drop to zero within fractions of a second. In a paper at the USENIX Symposium on Networked Systems Design and Implementation, the researchers presented “Accel-Brake Control” (ABC), a simple scheme that achieves about 50 percent higher throughput, and about half the network delays, on time-varying links.

The scheme relies on a novel algorithm that enables the routers to explicitly communicate how many data packets should flow through a network to avoid congestion but fully utilize the network. It provides that detailed information from bottlenecks — such as packets queued between cell towers and senders — by repurposing a single bit already available in internet packets. The researchers are already in talks with mobile network operators to test the scheme.

“In cellular networks, your fraction of data capacity changes rapidly, causing lags in your service. Traditional schemes are too slow to adapt to those shifts,” says first author Prateesh Goyal, a graduate student in CSAIL. “ABC provides detailed feedback about those shifts, whether it’s gone up or down, using a single data bit.”

Joining Goyal on the paper are Anup Agarwal, now a graduate student at Carnegie Melon University; Ravi Netravali, now an assistant professor of computer science at the University of California at Los Angeles; Mohammad Alizadeh, an associate professor in MIT’s Department of Electrical Engineering (EECS) and CSAIL; and Hari Balakrishnan, the Fujitsu Professor in EECS. The authors have all been members of the Networks and Mobile Systems group at CSAIL.

Achieving explicit control

Traditional congestion-control schemes rely on either packet losses or information from a single “congestion” bit in internet packets to infer congestion and slow down. A router, such as a base station, will mark the bit to alert a sender — say, a video server — that its sent data packets are in a long queue, signaling congestion. In response, the sender will then reduce its rate by sending fewer packets. The sender also reduces its rate if it detects a pattern of packets being dropped before reaching the receiver.

In attempts to provide greater information about bottlenecked links on a network path, researchers have proposed “explicit” schemes that include multiple bits in packets that specify current rates. But this approach would mean completely changing the way the internet sends data, and it has proved impossible to deploy.

“It’s a tall task,” Alizadeh says. “You’d have to make invasive changes to the standard Internet Protocol (IP) for sending data packets. You’d have to convince all Internet parties, mobile network operators, ISPs, and cell towers to change the way they send and receive data packets. That’s not going to happen.”

With ABC, the researchers still use the available single bit in each data packet, but they do so in such a way that the bits, aggregated across multiple data packets, can provide the needed real-time rate information to senders. The scheme tracks each data packet in a round-trip loop, from sender to base station to receiver. The base station marks the bit in each packet with “accelerate” or “brake,” based on the current network bandwidth. When the packet is received, the marked bit tells the sender to increase or decrease the “in-flight” packets — packets sent but not received — that can be in the network.

If it receives an accelerate command, it means the packet made good time and the network has spare capacity. The sender then sends two packets: one to replace the packet that was received and another to utilize the spare capacity. W"
196;196;machinelearningmastery.com;http://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/;2013-12-24;How to Prepare Data For Machine Learning;"Tweet Share Share

Last Updated on June 7, 2016

Machine learning algorithms learn from data. It is critical that you feed them the right data for the problem you want to solve. Even if you have good data, you need to make sure that it is in a useful scale, format and even that meaningful features are included.

In this post you will learn how to prepare data for a machine learning algorithm. This is a big topic and you will cover the essentials.

Data Preparation Process

The more disciplined you are in your handling of data, the more consistent and better results you are like likely to achieve. The process for getting data ready for a machine learning algorithm can be summarized in three steps:

Step 1 : Select Data

: Select Data Step 2 : Preprocess Data

: Preprocess Data Step 3: Transform Data

You can follow this process in a linear manner, but it is very likely to be iterative with many loops.

Step 1: Select Data

This step is concerned with selecting the subset of all available data that you will be working with. There is always a strong desire for including all data that is available, that the maxim “more is better” will hold. This may or may not be true.

You need to consider what data you actually need to address the question or problem you are working on. Make some assumptions about the data you require and be careful to record those assumptions so that you can test them later if needed.

Below are some questions to help you think through this process:

What is the extent of the data you have available? For example through time, database tables, connected systems. Ensure you have a clear picture of everything that you can use.

What data is not available that you wish you had available? For example data that is not recorded or cannot be recorded. You may be able to derive or simulate this data.

What data don’t you need to address the problem? Excluding data is almost always easier than including data. Note down which data you excluded and why.

It is only in small problems, like competition or toy datasets where the data has already been selected for you.

Step 2: Preprocess Data

After you have selected the data, you need to consider how you are going to use the data. This preprocessing step is about getting the selected data into a form that you can work.

Three common data preprocessing steps are formatting, cleaning and sampling:

Formatting : The data you have selected may not be in a format that is suitable for you to work with. The data may be in a relational database and you would like it in a flat file, or the data may be in a proprietary file format and you would like it in a relational database or a text file.

: The data you have selected may not be in a format that is suitable for you to work with. The data may be in a relational database and you would like it in a flat file, or the data may be in a proprietary file format and you would like it in a relational database or a text file. Cleaning : Cleaning data is the removal or fixing of missing data. There may be data instances that are incomplete and do not carry the data you believe you need to address the problem. These instances may need to be removed. Additionally, there may be sensitive information in some of the attributes and these attributes may need to be anonymized or removed from the data entirely.

: Cleaning data is the removal or fixing of missing data. There may be data instances that are incomplete and do not carry the data you believe you need to address the problem. These instances may need to be removed. Additionally, there may be sensitive information in some of the attributes and these attributes may need to be anonymized or removed from the data entirely. Sampling: There may be far more selected data available than you need to work with. More data can result in much longer running times for algorithms and larger computational and memory requirements. You can take a smaller representative sample of the selected data that may be much faster for exploring and prototyping solutions before considering the whole dataset.

It is very likely that the machine learning tools you use on the data will influence the preprocessing you will be required to perform. You will likely revisit this step.

Step 3: Transform Data

The final step is to transform the process data. The specific algorithm you are working with and the knowledge of the problem domain will influence this step and you will very likely have to revisit different transformations of your preprocessed data as you work on your problem.

Three common data transformations are scaling, attribute decompositions and attribute aggregations. This step is also referred to as feature engineering.

Scaling : The preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. Many machine learning methods like data attributes to have the same scale such as between 0 and 1 for the smallest and largest "
197;197;machinelearningmastery.com;https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/;2019-06-16;A Gentle Introduction to Generative Adversarial Networks (GANs);"Tweet Share Share

Last Updated on July 19, 2019

Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.

Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.

GANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated). The two models are trained together in a zero-sum game, adversarial, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples.

GANs are an exciting and rapidly changing field, delivering on the promise of generative models in their ability to generate realistic examples across a range of problem domains, most notably in image-to-image translation tasks such as translating photos of summer to winter or day to night, and in generating photorealistic photos of objects, scenes, and people that even humans cannot tell are fake.

In this post, you will discover a gentle introduction to Generative Adversarial Networks, or GANs.

After reading this post, you will know:

Context for GANs, including supervised vs. unsupervised learning and discriminative vs. generative modeling.

GANs are an architecture for automatically training a generative model by treating the unsupervised problem as supervised and using both a generative and a discriminative model.

GANs provide a path to sophisticated domain-specific data augmentation and a solution to problems that require a generative solution, such as image-to-image translation.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

What Are Generative Models? What Are Generative Adversarial Networks? Why Generative Adversarial Networks?

What Are Generative Models?

In this section, we will review the idea of generative models, stepping over the supervised vs. unsupervised learning paradigms and discriminative vs. generative modeling.

Supervised vs. Unsupervised Learning

A typical machine learning problem involves using a model to make a prediction, e.g. predictive modeling.

This requires a training dataset that is used to train a model, comprised of multiple examples, called samples, each with input variables (X) and output class labels (y). A model is trained by showing examples of inputs, having it predict outputs, and correcting the model to make the outputs more like the expected outputs.

In the predictive or supervised learning approach, the goal is to learn a mapping from inputs x to outputs y, given a labeled set of input-output pairs …

— Page 2, Machine Learning: A Probabilistic Perspective, 2012.

This correction of the model is generally referred to as a supervised form of learning, or supervised learning.

Examples of supervised learning problems include classification and regression, and examples of supervised learning algorithms include logistic regression and random forest.

There is another paradigm of learning where the model is only given the input variables (X) and the problem does not have any output variables (y).

A model is constructed by extracting or summarizing the patterns in the input data. There is no correction of the model, as the model is not predicting anything.

The second main type of machine learning is the descriptive or unsupervised learning approach. Here we are only given inputs, and the goal is to find “interesting patterns” in the data. […] This is a much less well-defined problem, since we are not told what kinds of patterns to look for, and there is no obvious error metric to use (unlike supervised learning, where we can compare our prediction of y for a given x to the observed value).

— Page 2, Machine Learning: A Probabilistic Perspective, 2012.

This lack of correction is generally referred to as an unsupervised form of learning, or unsupervised learning.

Examples of unsupervised learning problems include clustering and generative modeling, and examples of unsupervised learning algorithms are K-means and Generative Adversarial Networks.

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Discriminative vs. Generative Modeling

In supervised learning, we may be interested in developing a model to predict a clas"
198;198;machinelearningmastery.com;http://machinelearningmastery.com/time-series-datasets-for-machine-learning/;2016-11-29;7 Time Series Datasets for Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

Machine learning can be applied to time series datasets.

These are problems where a numeric or categorical value must be predicted, but the rows of data are ordered by time.

A problem when getting started in time series forecasting with machine learning is finding good quality standard datasets on which to practice.

In this post, you will discover 8 standard time series datasets that you can use to get started and practice time series forecasting with machine learning.

After reading this post, you will know:

4 univariate time series datasets.

3 multivariate time series datasets.

Websites that you can use to search and download more datasets.

Discover how to prepare and visualize time series data and develop autoregressive forecasting models in my new book, with 28 step-by-step tutorials, and full python code.

Let’s get started.

Updated Apr/2019: Updated the links to the datasets.

Univariate Time Series Datasets

Time series datasets that only have one variable are called univariate datasets.

These datasets are a great place to get started because:

They are so simple and easy to understand.

You can plot them easily in excel or your favorite plotting tool.

You can easily plot the predictions compared to the expected results.

You can quickly try and evaluate a suite of traditional and newer methods.

There are many sources of time series dataset, such as the “Time Series Data Library” created by Rob Hyndman, Professor of Statistics at Monash University, Australia

Below are 4 univariate time series datasets that you can download from a range of fields such as Sales, Meteorology, Physics and Demography.

Stop learning Time Series Forecasting the slow way! Take my free 7-day email course and discover how to get started (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Shampoo Sales Dataset

This dataset describes the monthly number of sales of shampoo over a 3 year period.

The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright and Hyndman (1998).

Below is a sample of the first 5 rows of data including the header row.

""Month"",""Sales of shampoo over a three year period"" ""1-01"",266.0 ""1-02"",145.9 ""1-03"",183.1 ""1-04"",119.3 ""1-05"",180.3 1 2 3 4 5 6 ""Month"",""Sales of shampoo over a three year period"" ""1-01"",266.0 ""1-02"",145.9 ""1-03"",183.1 ""1-04"",119.3 ""1-05"",180.3

Below is a plot of the entire dataset.

The dataset shows an increasing trend and possibly some seasonal component.

Minimum Daily Temperatures Dataset

This dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.

The units are in degrees Celsius and there are 3650 observations. The source of the data is credited as the Australian Bureau of Meteorology.

Below is a sample of the first 5 rows of data including the header row.

""Date"",""Daily minimum temperatures in Melbourne, Australia, 1981-1990"" ""1981-01-01"",20.7 ""1981-01-02"",17.9 ""1981-01-03"",18.8 ""1981-01-04"",14.6 ""1981-01-05"",15.8 1 2 3 4 5 6 ""Date"",""Daily minimum temperatures in Melbourne, Australia, 1981-1990"" ""1981-01-01"",20.7 ""1981-01-02"",17.9 ""1981-01-03"",18.8 ""1981-01-04"",14.6 ""1981-01-05"",15.8

Below is a plot of the entire dataset.

The dataset shows a strong seasonality component and has a nice fine grained detail to work with.

Monthly Sunspot Dataset

This dataset describes a monthly count of the number of observed sunspots for just over 230 years (1749-1983).

The units are a count and there are 2,820 observations. The source of the dataset is credited to Andrews & Herzberg (1985).

Below is a sample of the first 5 rows of data including the header row.

""Month"",""Zuerich monthly sunspot numbers 1749-1983"" ""1749-01"",58.0 ""1749-02"",62.6 ""1749-03"",70.0 ""1749-04"",55.7 ""1749-05"",85.0 1 2 3 4 5 6 ""Month"",""Zuerich monthly sunspot numbers 1749-1983"" ""1749-01"",58.0 ""1749-02"",62.6 ""1749-03"",70.0 ""1749-04"",55.7 ""1749-05"",85.0

Below is a plot of the entire dataset.

The dataset shows seasonality with large differences between seasons.

Daily Female Births Dataset

This dataset describes the number of daily female births in California in 1959.

The units are a count and there are 365 observations. The source of the dataset is credited to Newton (1988).

Below is a sample of the first 5 rows of data including the header row.

""Date"",""Daily total female births in California, 1959"" ""1959-01-01"",35 ""1959-01-02"",32 ""1959-01-03"",30 ""1959-01-04"",31 ""1959-01-05"",44 1 2 3 4 5 6 ""Date"",""Daily total female births in California, 1959"" ""1959-01-01"",35 ""1959-01-02"",32 ""1959-01-03"",30 ""1959-01-04"",31 ""1959-01-05"",44

Below is a plot of the entire dataset.

Multivariate Time Series Datasets

Multivariate datasets are generally more challenging and are the sweet spot for machine learning methods.

A great source of multivariate time series data is the UCI Machin"
199;199;machinelearningmastery.com;https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/;2019-10-06;How to Develop a Naive Bayes Classifier from Scratch in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41

# example of preparing and making a prediction with a naive bayes model from sklearn . datasets import make_blobs from scipy . stats import norm from numpy import mean from numpy import std # fit a probability distribution to a univariate data sample def fit_distribution ( data ) : # estimate parameters mu = mean ( data ) sigma = std ( data ) print ( mu , sigma ) # fit distribution dist = norm ( mu , sigma ) return dist # calculate the independent conditional probability def probability ( X , prior , dist1 , dist2 ) : return prior * dist1 . pdf ( X [ 0 ] ) * dist2 . pdf ( X [ 1 ] ) # generate 2d classification dataset X , y = make_blobs ( n_samples = 100 , centers = 2 , n_features = 2 , random_state = 1 ) # sort data into classes Xy0 = X [ y == 0 ] Xy1 = X [ y == 1 ] # calculate priors priory0 = len ( Xy0 ) / len ( X ) priory1 = len ( Xy1 ) / len ( X ) # create PDFs for y==0 distX1y0 = fit_distribution ( Xy0 [ : , 0 ] ) distX2y0 = fit_distribution ( Xy0 [ : , 1 ] ) # create PDFs for y==1 distX1y1 = fit_distribution ( Xy1 [ : , 0 ] ) distX2y1 = fit_distribution ( Xy1 [ : , 1 ] ) # classify one example Xsample , ysample = X [ 0 ] , y [ 0 ] py0 = probability ( Xsample , priory0 , distX1y0 , distX2y0 ) py1 = probability ( Xsample , priory1 , distX1y1 , distX2y1 ) print ( 'P(y=0 | %s) = %.3f' % ( Xsample , py0* 100 ) ) print ( 'P(y=1 | %s) = %.3f' % ( Xsample , py1* 100 ) ) print ( 'Truth: y=%d' % ysample )"
200;200;machinelearningmastery.com;https://machinelearningmastery.com/imbalanced-classification-with-python-7-day-mini-course/;2020-01-15;Imbalanced Classification With Python (7-Day Mini-Course);"# plot imbalanced classification problem

from collections import Counter

from sklearn . datasets import make_classification

from matplotlib import pyplot

from numpy import where

# define dataset

X , y = make_classification ( n_samples = 1000 , n_features = 2 , n_redundant = 0 , n_clusters_per_class = 1 , weights = [ 0.99 , 0.01 ] , flip_y = 0 )

# summarize class distribution

counter = Counter ( y )

print ( counter )

# scatter plot of examples by class label

for label , _ in counter . items ( ) :

row_ix = where ( y == label ) [ 0 ]

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( label ) )

pyplot . legend ( )"
201;201;machinelearningmastery.com;https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/;2019-12-31;Failure of Classification Accuracy for Imbalanced Class Distributions;"# create a dataset with a given class distribution

def get_dataset ( proportions ) :

# determine the number of classes

n_classes = len ( proportions )

# determine the number of examples to generate for each class

largest = max ( [ v for k , v in proportions . items ( ) ] )

n_samples = largest * n_classes

# create dataset

X , y = make_blobs ( n_samples = n_samples , centers = n_classes , n_features = 2 , random_state = 1 , cluster_std = 3 )

# collect the examples

X_list , y_list = list ( ) , list ( )

for k , v in proportions . items ( ) :

row_ix = where ( y == k ) [ 0 ]

selected = row_ix [ : v ]

X_list . append ( X [ selected , : ] )

y_list . append ( y [ selected ] )"
202;202;news.mit.edu;http://news.mit.edu/2020/charlotte-minsky-lyndie-mitchell-zollinger-named-gates-cambridge-scholars-0218;;Charlotte Minsky and Lyndie Mitchell Zollinger named 2020 Gates Cambridge Scholars;"MIT seniors Charlotte Minsky and Lyndie Mitchell Zollinger have won the prestigious Gates Cambridge Scholarship, which offers students an opportunity to pursue graduate study in the field of their choice at Cambridge University in England.

Minsky, from Greenfield, Massachusetts, is completing her bachelor’s degree in earth, atmospheric, and planetary sciences, as well as history. She had always thought her dual interest in science and the humanities were disparate until she joined the MIT and Slavery project, which illuminated for her the ways that science and technology can be tools for the structures of oppression. Minsky then realized that she could combine both science and history, and that the combined studies would allow her to struggle with the historical legacies of science. At Cambridge, she plans to read for an MPhil in history and philosophy of science before returning to the United States to earn a PhD in planetary science.

Regarding Minsky's work on the MIT and Slavery project, Professor Anne McCants notes, ""I was awed by the sophistication of the public presentation she gave of her research on the relationship between MIT and the economy of the post-Civil War reconstruction South, an event that was attended by all members of the MIT upper administration, as well as live-streamed for a global public audience. For someone so young, her clarity of thinking, personal confidence, and historical humility about the remaining questions were all quite extraordinary. I can honestly say that I had never seen anything like it before from a student, let alone one not even halfway through college.”

Minsky has proven herself equally adept at scientific research, and is currently working with Professor Ray Jayawardhana's group at Cornell University on testing a new method to characterize exoplanet atmospheres. She previously studied with Professor Julien de Wit (assistant professor in earth, atmospheric, and planetary sciences) to investigate the propagation of biases in exoplanet atmosphere models, as well as with Professor Benjamin Weiss and Research Scientist Mary Knapp, searching for the theorized Planet 9 by using archival radio data. She plans to continue similar work during her doctoral research.

Minsky is the current vice president for the Undergraduate Association (UA), and former chief of staff to the UA. She is the former president and co-founder of the Prison Education Initiative, and regularly teaches astronomy to inmates, as well as educating the MIT community about mass incarceration. She also served as the president of Queer West, a LGBTQ+ community and advocacy organization at MIT.

Mitchell Zollinger, from Sandy, Utah, was raised with a passion for learning, teaching, building, and medicine. After conducting research at the University of Utah’s Chemistry Department, she decided to come to MIT to study engineering. Mitchell Zollinger will graduate from MIT with a bachelor’s degree in mechanical engineering, and then pursue a doctorate in engineering at the University of Cambridge. During an unfortunate accident when a giant hamster wheel fell on top of her in one of her mechanical engineering classes, she realized the importance of a mechanical perspective on medical challenges. At Cambridge, she will develop mechanical models of the progression of traumatic brain injuries. This will provide clinicians with a range of patient-specific predicted outcomes to assist them in choosing the best treatment options, and will improve patients’ lives by saving vital time and reducing the risk of further brain damage.

Mitchell Zollinger’s work at Cambridge will build upon her summer research at the University of Auckland, where she worked to develop implantable sensors for the brain. Previously, she worked with Steven Gillmer of MIT Lincoln Laboratory, investigating the complexity of motions and required forces to open doors for people in wheelchairs. Her end goal was to create robotic assistive devices for people in wheelchairs who struggle with things like this on a day-to-day basis. “The most important thing about Lyndie’s research,” says Gillmer, “is she is doing it for the well-being of others.” She was also selected as one of only seven juniors to be a Pappalardo Apprentice.

Mitchell Zollinger has always been committed to encouraging women in STEM, as she herself was encouraged in the field by a female neighbor who had a doctorate in science. AT MIT, she has served as a residential tutor for the Women’s Technology Program in the Department of Mechanical Engineering, where she worked with high school girls to introduce and encourage them to pursue STEM fields. Mitchell Zollinger plans to continue similar initiatives through her future career as an academic in engineering.

Mitchell Zollinger led the effort to create the Addir Interfaith Engagement Association to expand the group’s efforts beyond conversation about diversity to promoting greater mutual respect and understanding across the Inst"
203;203;news.mit.edu;http://news.mit.edu/2020/gamma-radiation-found-ineffective-in-sterilizing-n95-masks-0410;;Gamma radiation found ineffective in sterilizing N95 masks;"The research described in this article has been published on a preprint server but has not yet been peer-reviewed by scientific or medical experts.

In mid-March, members of the Department of Nuclear Science and Engineering (NSE) joined forces with colleagues in Boston’s medical community to answer a question of critical importance during the Covid-19 pandemic: Can gamma irradiation sterilize disposable N95 masks without diminishing the masks’ effectiveness?

This type of personal protective equipment (PPE), which offers protection against infectious particles like coronavirus-laden aerosols, is in desperately short supply worldwide, and medical professionals in Covid-19 hotspots are already rationing the masks. Gamma radiation is commonly used to sterilize hospital foods and equipment surfaces, as well as much of the public’s food supply, and there has been significant interest in determining if it could allow N95 masks to be reused and address the expanding scarcity.

In a study uploaded on March 28 to medRχiv, the preprint server for health sciences, researchers announced their results: N95 masks subjected to cobalt-60 gamma irradiation for sterilization pass a qualitative fit test but lose a significant degree of filtration efficiency. This form of sterilization compromises the masks’ ability to protect medical providers from Covid-19.

The study, NSE’s first research effort related to the pandemic, also drew on the expertise of MIT’s Office of Environment, Health, and Safety.

“One of our students thought gamma irradiation might be a cool solution to a big problem, and I really wanted it to work,” says Michael Short, the Class of ’42 Associate Professor of Nuclear Science and Engineering, one of the study’s coauthors. “But we quickly recognized that the data went against the hypothesis.”

Team members believe these negative results nevertheless contribute to the larger effort to combat the pandemic. “There has never been a time when negative results are more significant,” notes study lead and co-author Avilash Cramer SM ’18, a fifth-year doctoral candidate in the Harvard-MIT Program in Health Sciences and Technology studying radiation physics. “Publishing as quickly as we can means that others working on the same problem can direct their energies in different directions.”

Fast-track research

While they may not have produced the desired outcome, the researchers nevertheless pulled off a study remarkable for its speed and multidisciplinary cooperation — a process inspired and shaped by the immediate threat of the Covid-19 pandemic. “The study took nine days from start to finish,” says Short. “It was the fastest I’ve ever done anything, by orders of magnitude.”

The dire reality of an N95 shortage in the United States sparked widespread concerns early in March. “It had already hit New York, and was on its way to Massachusetts, and President [L. Rafel] Reif wanted to know if we could do something to masks to permit their reuse,” recounts Short. “We looked into different methods, and noticed the idea of using gamma radiation was popping up in a lot of places.”

Cramer was losing sleep worrying about his classmates, medical residents at Boston-area hospitals already in the thick of treating Covid-19 patients. “After reading the literature, it was clear there wasn't a lot of good research out there regarding reusing masks,” he says. “The sky was falling in hospitals with equipment shortages everywhere, and while others had shown gamma rays could inactivate viruses, I wanted to demonstrate one way or the other if they damage the masks themselves.”

N95 masks are manufactured through a variety of proprietary processes using wool, glass particles, and plastics, with 1-2 percent copper and/or zinc. Viewed under a scanning electron microscope, these masks reveal a matrix of fibers with openings of approximately 1 micron. Because the filtering occurs through an electrostatic, rather than mechanical, process, a mask can repel or trap smaller incoming particles. This includes at least 95 percent of airborne particles 0.3 microns or larger in size, such as the airborne droplets that can convey the Covid-19 virus.

A call for multidisciplinary action

On March 11, Cramer emailed several contacts in the radiation physics community in search of a gamma irradiation source. Among the group was Short, who has some experience, among many things, in irradiating plastics. Cramer had worked with Short on previous research ventures, and was familiar with NSE from his time serving as a teaching assistant for an NSE class, Radiation Biophysics (22.055), taught by his PhD advisor, Rajiv Gupta, a physician at Massachusetts General Hospital and an associate professor of radiology at Harvard Medical School.

Short instantly responded to Cramer, offering the campus Cobalt-60 irradiation facility, a source of gamma radiation. “I had an exemption to work on campus and thought, let’s just do it: irradiate and sterilize the masks, then "
204;204;machinelearningmastery.com;https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/;2019-01-10;How to Fix the Vanishing Gradients Problem Using the ReLU;"# deeper mlp with relu for the two circles classification problem with callback

from sklearn . datasets import make_circles

from sklearn . preprocessing import MinMaxScaler

from keras . layers import Dense

from keras . models import Sequential

from keras . optimizers import SGD

from keras . callbacks import TensorBoard

# generate 2d classification dataset

X , y = make_circles ( n_samples = 1000 , noise = 0.1 , random_state = 1 )

scaler = MinMaxScaler ( feature_range = ( - 1 , 1 ) )

X = scaler . fit_transform ( X )

# split into train and test

n_train = 500

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# define model

model = Sequential ( )

model . add ( Dense ( 5 , input_dim = 2 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 5 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = SGD ( lr = 0.01 , momentum = 0.9 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

# prepare callback

tb = TensorBoard ( histogram_freq = 1 , write_grads = True )

# fit model"
205;205;machinelearningmastery.com;https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/;2017-10-03;How to Use Word Embedding Layers for Deep Learning with Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64

from numpy import array from numpy import asarray from numpy import zeros from keras . preprocessing . text import Tokenizer from keras . preprocessing . sequence import pad_sequences from keras . models import Sequential from keras . layers import Dense from keras . layers import Flatten from keras . layers import Embedding # define documents docs = [ 'Well done!' , 'Good work' , 'Great effort' , 'nice work' , 'Excellent!' , 'Weak' , 'Poor effort!' , 'not good' , 'poor work' , 'Could have done better.' ] # define class labels labels = array ( [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] ) # prepare tokenizer t = Tokenizer ( ) t . fit_on_texts ( docs ) vocab_size = len ( t . word_index ) + 1 # integer encode the documents encoded_docs = t . texts_to_sequences ( docs ) print ( encoded_docs ) # pad documents to a max length of 4 words max_length = 4 padded_docs = pad_sequences ( encoded_docs , maxlen = max_length , padding = 'post' ) print ( padded_docs ) # load the whole embedding into memory embeddings_index = dict ( ) f = open ( '../glove_data/glove.6B/glove.6B.100d.txt' ) for line in f : values = line . split ( ) word = values [ 0 ] coefs = asarray ( values [ 1 : ] , dtype = 'float32' ) embeddings_index [ word ] = coefs f . close ( ) print ( 'Loaded %s word vectors.' % len ( embeddings_index ) ) # create a weight matrix for words in training docs embedding_matrix = zeros ( ( vocab_size , 100 ) ) for word , i in t . word_index . items ( ) : embedding_vector = embeddings_index . get ( word ) if embedding_vector is not None : embedding_matrix [ i ] = embedding_vector # define model model = Sequential ( ) e = Embedding ( vocab_size , 100 , weights = [ embedding_matrix ] , input_length = 4 , trainable = False ) model . add ( e ) model . add ( Flatten ( ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) # compile the model model . compile ( optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = [ 'accuracy' ] ) # summarize the model print ( model . summary ( ) ) # fit the model model . fit ( padded_docs , labels , epochs = 50 , verbose = 0 ) # evaluate the model loss , accuracy = model . evaluate ( padded_docs , labels , verbose = 0 ) print ( 'Accuracy: %f' % ( accuracy* 100 ) )"
206;206;towardsdatascience.com;https://towardsdatascience.com/tools-for-sharing-jupyter-notebooks-online-28c8d4ff821c?source=collection_home---4------1-----------------------;2020-04-17;Tools for Sharing Jupyter Notebooks Online;"Photo by Marvin Meyer on Unsplash

Tools for Sharing Jupyter Notebooks Online

Three tools for sharing python notebooks with non-programmers

I recently started helping out on a new data project set up by a friend of mine. The data side of the project is an analysis piece applying some natural language processing to text-based responses from a survey. I created a Github repository for the project and completed the analysis in a Jupyter Notebook.

As part of this project, I want to be able to share the notebooks with non-programmers who won’t necessarily be set up with or familiar with Github. This is a common problem that most data scientists face.

Jupyter Notebooks are a great tool for exploratory data analysis but it is pretty common to need to share this analysis with non-programmer stakeholders in a project. Fortunately, there are a number of tools available to host notebooks online for non-Github users.

In the following article, I am going to walk through how to use three of these tools and discuss the pros and cons of each."
207;207;machinelearningmastery.com;http://machinelearningmastery.com/estimate-performance-machine-learning-algorithms-weka/;2016-07-17;How To Estimate The Performance of Machine Learning Algorithms in Weka;"Tweet Share Share

Last Updated on August 22, 2019

The problem of predictive modeling is to create models that have good performance making predictions on new unseen data.

Therefore it is critically important to use robust techniques to train and evaluate your models on your available training data. The more reliable the estimate of the performance on your model, the further you can push the performance and be confident it will translate to the operational use of your model.

In this post you will discover the various different ways that you can estimate the performance of your machine learning models in Weka.

After reading this post you will know:

How to evaluate your model using the training dataset.

How to evaluate your model using a random train and test split.

How to evaluate your model using k-fold cross validation.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

Model Evaluation Techniques

There are a number of model evaluation techniques that you can choose from, and the Weka machine learning workbench offers four of them, as follows:

Training Dataset

Prepare your model on the entire training dataset, then evaluate the model on the same dataset. This is generally problematic not least because a perfect algorithm could game this evaluation technique by simply memorizing (storing) all training patterns and achieve a perfect score, which would be misleading.

Supplied Test Set

Split your dataset manually using another program. Prepare your model on the entire training dataset and use the separate test set to evaluate the performance of the model. This is a good approach if you have a large dataset (many tens of thousands of instances).

Percentage Split

Randomly split your dataset into a training and a testing partitions each time you evaluate a model. This can give you a very quick estimate of performance and like using a supplied test set, is preferable only when you have a large dataset.

Cross Validation

Split the dataset into k-partitions or folds. Train a model on all of the partitions except one that is held out as the test set, then repeat this process creating k-different models and give each fold a chance of being held out as the test set. Then calculate the average performance of all k models. This is the gold standard for evaluating model performance, but has the cost of creating many more models.

You can see these techniques in the Weka Explorer on the “Classify” tab after you have loaded a dataset.

Which Test Option to Use

Given that there are four different test options to choose from, which one should you use?

Each test option has a time and place, summarized as follows:

Training Dataset : Only to be used when you have all of the data and you are interested in creating a descriptive rather than a predictive model. Because you have all of the data, you do not need to make new predictions. You are interested in creating a model to better understand the problem.

: Only to be used when you have all of the data and you are interested in creating a descriptive rather than a predictive model. Because you have all of the data, you do not need to make new predictions. You are interested in creating a model to better understand the problem. Supplied Test Set : When the data is very large, e.g. millions of records and you do not need all of it to train a model. Also useful when the test set has been defined by a third party.

: When the data is very large, e.g. millions of records and you do not need all of it to train a model. Also useful when the test set has been defined by a third party. Percentage Split : Excellent to use to get a quick idea of the performance of a model. Not to be used to make decisions, unless you have a very large dataset and are confident (e.g. you have tested) that the splits sufficiently describe the problem. A common split value is 66% to 34% for train and test sets respectively.

: Excellent to use to get a quick idea of the performance of a model. Not to be used to make decisions, unless you have a very large dataset and are confident (e.g. you have tested) that the splits sufficiently describe the problem. A common split value is 66% to 34% for train and test sets respectively. Cross Validation: The default. To be used when you are unsure. Generally provides a more accurate estimate of the performance than the other techniques. Not to be used when you have a very large data. Common values for k are 5 and 10, depending on the size of the dataset.

If in doubt, use k-fold cross validation where k is set to 10.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

What About The Final Model

Test options are concerned "
208;208;news.mit.edu;http://news.mit.edu/2020/events-postponed-canceled-covid-19-0309;;Events postponed or canceled as MIT responds to COVID-19;"MIT schools, departments, labs, centers, and offices have acted swiftly to postpone or cancel large events through May 15 in the wake of the Institute’s announcement last week of new policies regarding gatherings likely to attract 150 or more people.

To safeguard against COVID-19, and the spread of the 2019 novel coronavirus, many other MIT events have been modified both on campus and elsewhere, with increased opportunities offered for livestreaming.

The guidelines put forth last week have also now been expanded to include some large classes: The Institute will move classes with more than 150 students online, starting this week.

Impacts on classes and student travel

Following consultation with senior academic leadership and experts within MIT Medical, the Institute has suspended in-person meetings of classes with more than 150 students, effective tomorrow, Tuesday, March 10. The approximately 20 classes impacted by the decision will continue to be offered in virtual form.

“We are being guided by our medical professionals who are in close contact with state and national public health officials,” Ian Waitz, vice chancellor for undergraduate and graduate education, wrote today in a letter to deans and department heads. “They have advised us that while the risk to the community is low and there are no cases on campus as of now, we need to move quickly to help prevent the potential transmission of the disease and to be ready if and when it impacts our campus.”

“Our approach is to be aggressive, but to move forward in stages,” Waitz added, “while keeping in mind that some individual faculty and departments may be moving faster than others, that the level of comfort with remote teaching varies, and that some classes may translate better than others to alternative formats.”

As of now, midterm examinations will proceed as scheduled, but the plan for large courses is to run midterms in several rooms simultaneously so the number of students in each room remains well below 150. The Registrar’s Office is working on room scheduling strategies to best accommodate that approach.

The Institute has also decided that all MIT-sponsored student domestic travel of more than 100 miles will have to go through the Institute’s high-risk travel waiver process.

Impacts on undergraduate and graduate admissions

As shared in President L. Rafael Reif’s letter of last Thursday, MIT’s new policy on events will apply to Campus Preview Weekend, ordinarily an on-campus gathering for students admitted to the incoming first-year undergraduate class. In the coming weeks, the Admissions Office will be connecting with admitted students, current students, and campus partners to discuss what to do instead of a conventional CPW. For more information, please see: https://mitadmissions.org/blogs/entry/mits-covid-19-precautions-and-its-impact-on-admissions/

The Admissions Office will not host any programming for K-12 students, including admitted students and their families, between now and May 15, regardless of the size of the event. All scheduled admissions sessions and tours have been canceled between now and May 15, and MIT Admissions is canceling all scheduled admissions officer travel to domestic and international events in that time window.

Additionally, all graduate admissions visit days have been canceled, effective immediately. “Based upon reducing risk, we ask all departments to cancel all remaining graduate open houses and visit days, and to move to virtual formats,” Waitz says. “Many departments have already done this.”

Despite the cancellation of these formal events, the MIT campus currently remains open for visits by prospective students. However, in keeping with suggested best practices for public health, visitors from countries that the U.S. Centers for Disease Control and Prevention (CDC) finds have “widespread sustained (ongoing) transmission” of COVID-19 cannot visit campus until they have successfully completed 14 days of self-quarantine.

Impacts on major campus events

The MIT Excellence Awards and Collier Medal celebration, scheduled for this Thursday, March 12, has been postponed; a rescheduled date will be announced as soon as it is confirmed. The Excellence Awards and Collier Medal recognize the work of service, support, administrative, and sponsored research staff. The Excellence Awards acknowledge the extraordinary efforts made by members of the MIT community toward fulfilling the goals, values, and mission of the Institute. The Collier Medal is awarded to an individual or group exhibiting qualities such as a commitment to community service, kindness, selflessness, and generosity; it honors the memory of MIT Police Officer Sean Collier, who lost his life while protecting the MIT campus. A full list of this year’s honorees is available.

Career Advising and Professional Development is working on plans to change the format of the Spring Career Fair, previously scheduled for April 2, to a virtual career fair for a d"
209;209;machinelearningmastery.com;http://machinelearningmastery.com/support-vector-machines-for-machine-learning/;2016-04-19;Support Vector Machines for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.

They were extremely popular around the time they were developed in the 1990s and continue to be the go-to method for a high-performing algorithm with little tuning.

In this post you will discover the Support Vector Machine (SVM) machine learning algorithm. After reading this post you will know:

How to disentangle the many names used to refer to support vector machines.

The representation used by SVM when the model is actually stored on disk.

How a learned SVM model representation can be used to make predictions for new data.

How to learn an SVM model from training data.

How to best prepare your data for the SVM algorithm.

Where you might look to get more information on SVM.

SVM is an exciting algorithm and the concepts are relatively simple. This post was written for developers with little or no background in statistics and linear algebra.

As such we will stay high-level in this description and focus on the specific implementation concerns. The question around why specific equations are used or how they were derived are not covered and you may want to dive deeper in the further reading section.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Maximal-Margin Classifier

The Maximal-Margin Classifier is a hypothetical classifier that best explains how SVM works in practice.

The numeric input variables (x) in your data (the columns) form an n-dimensional space. For example, if you had two input variables, this would form a two-dimensional space.

A hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions you can visualize this as a line and let’s assume that all of our input points can be completely separated by this line. For example:

B0 + (B1 * X1) + (B2 * X2) = 0

Where the coefficients (B1 and B2) that determine the slope of the line and the intercept (B0) are found by the learning algorithm, and X1 and X2 are the two input variables.

You can make classifications using this line. By plugging in input values into the line equation, you can calculate whether a new point is above or below the line.

Above the line, the equation returns a value greater than 0 and the point belongs to the first class (class 0).

Below the line, the equation returns a value less than 0 and the point belongs to the second class (class 1).

A value close to the line returns a value close to zero and the point may be difficult to classify.

If the magnitude of the value is large, the model may have more confidence in the prediction.

The distance between the line and the closest data points is referred to as the margin. The best or optimal line that can separate the two classes is the line that as the largest margin. This is called the Maximal-Margin hyperplane.

The margin is calculated as the perpendicular distance from the line to only the closest points. Only these points are relevant in defining the line and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane.

The hyperplane is learned from training data using an optimization procedure that maximizes the margin.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Soft Margin Classifier

In practice, real data is messy and cannot be separated perfectly with a hyperplane.

The constraint of maximizing the margin of the line that separates the classes must be relaxed. This is often called the soft margin classifier. This change allows some points in the training data to violate the separating line.

An additional set of coefficients are introduced that give the margin wiggle room in each dimension. These coefficients are sometimes called slack variables. This increases the complexity of the model as there are more parameters for the model to fit to the data to provide this complexity.

A tuning parameter is introduced called simply C that defines the magnitude of the wiggle allowed across all dimensions. The C parameters defines the amount of violation of the margin allowed. A C=0 is no violation and we are back to the inflexible Maximal-Margin Classifier described above. The larger the value of C the more violations of the hyperplane are permitted.

During the learning of the hyperplane from data, all training instances that lie within the distance of the margin will affect the placement of the hyperplane and are referred to as support vectors. And as C affects "
210;210;machinelearningmastery.com;https://machinelearningmastery.com/how-to-handle-big-p-little-n-p-n-in-machine-learning/;2020-04-14;How to Handle Big-p, Little-n (p >> n) in Machine Learning;"Tweet Share Share

What if I have more Columns than Rows in my dataset?

Machine learning datasets are often structured or tabular data comprised of rows and columns.

The columns that are fed as input to a model are called predictors or “p” and the rows are samples “n“. Most machine learning algorithms assume that there are many more samples than there are predictors, denoted as p << n.

Sometimes, this is not the case, and there are many more predictors than samples in the dataset, referred to as “big-p, little-n” and denoted as p >> n. These problems often require specialized data preparation and modeling algorithms to address them correctly.

In this tutorial, you will discover the challenge of big-p, little n or p >> n machine learning problems.

After completing this tutorial, you will know:

Most machine learning problems have many more samples than predictors and most machine learning algorithms make this assumption during the training process.

Some modeling problems have many more predictors than samples, referred to as p >> n.

Algorithms to explore when modeling machine learning datasets with more predictors than samples.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Predictors (p) and Samples (n)

Machine Learning Assumes p << n

How to Handle p >> n

Predictors (p) and Samples (n)

Consider a predictive modeling problem, such as classification or regression.

The dataset is structured data or tabular data, like what you might see in an Excel spreadsheet.

There are columns and rows. Most of the columns would be used as inputs to a model and one column would represent the output or variable to be predicted.

The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable—in this case, sales—is often called the response or dependent variable, and is typically denoted using the symbol Y.

— Page 15, An Introduction to Statistical Learning with Applications in R, 2017.

Each column represents a variable or one aspect of a sample. The columns that represent the inputs to the model are called predictors.

Each row represents one sample with values across each of the columns or features.

Predictors : Input columns of a dataset, also called input variables or features.

: Input columns of a dataset, also called input variables or features. Samples: Rows of a dataset, also called an observation, example, or instance.

It is common to describe a training dataset in machine learning in terms of the predictors and samples.

The number of predictors in a dataset is described using the term “p” and the number of samples in a dataset is described using the term “n” or sometimes “N“.

p : The number of predictors in a dataset.

: The number of predictors in a dataset. n: The number of samples in a dataset.

To make this concrete, let’s take a look at the iris flowers classification problem.

Below is a sample of the first five rows of this dataset.

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ... 1 2 3 4 5 6 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa ...

This dataset has five columns and 150 rows.

The first four columns are inputs and the fifth column is the output, meaning that there are four predictors.

We would describe the iris flowers dataset as:

p=4, n=150.

Machine Learning Assumes p << n

It is almost always the case that the number of predictors (p) will be smaller than the number of samples (n).

Often much smaller.

We can summarize this expectation as p << n, where “<<” is a mathematical inequality that means “much less than.”

p << n: Typically we have fewer predictors than samples.

To demonstrate this, let’s look at a few more standard machine learning datasets:

Most machine learning algorithms operate based on the assumption that there are many more samples than predictors.

One way to think about predictors and samples is to take a geometrical perspective.

Consider a hypercube where the number of predictors (p) defines the number of dimensions of the hypercube. The volume of this hypercube is the scope of possible samples that could be drawn from the domain. The number of samples (n) are the actual samples drawn from the domain that you must use to model your predictive modeling problem.

This is a rationale for the axiom “get as much data as possible” in applied machine learning. It is a desire to gather a sufficiently representative sample of the p-dimensional problem domain.

As the number of dimensions (p) increases, the volume of the domain increases exponentially. This, in turn, requires more samples (n) from the domain to provide effective coverage of the domain for a learning algorithm. We don’t need full coverage of the domain, just what is likely to be observable.

Th"
211;211;machinelearningmastery.com;http://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/;2016-09-15;Tune Learning Rate for Gradient Boosting with XGBoost in Python;"# XGBoost on Otto dataset, Tune learning_rate

from pandas import read_csv

from xgboost import XGBClassifier

from sklearn . model_selection import GridSearchCV

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import LabelEncoder

import matplotlib

matplotlib . use ( 'Agg' )

from matplotlib import pyplot

# load data

data = read_csv ( 'train.csv' )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 94 ]

y = dataset [ : , 94 ]

# encode string class values as integers

label_encoded_y = LabelEncoder ( ) . fit_transform ( y )

# grid search

model = XGBClassifier ( )

learning_rate = [ 0.0001 , 0.001 , 0.01 , 0.1 , 0.2 , 0.3 ]

param_grid = dict ( learning_rate = learning_rate )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 )

grid_search = GridSearchCV ( model , param_grid , scoring = ""neg_log_loss"" , n_jobs = - 1 , cv = kfold )

grid_result = grid_search . fit ( X , label_encoded_y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :

print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )

# plot

pyplot . errorbar ( learning_rate , means , yerr = stds )

pyplot . title ( ""XGBoost learning_rate vs Log Loss"" )

pyplot . xlabel ( 'learning_rate' )

pyplot . ylabel ( 'Log Loss' )"
212;212;machinelearningmastery.com;https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/;2018-03-01;How to Calculate Principal Component Analysis (PCA) from Scratch in Python;"from numpy import array

from numpy import mean

from numpy import cov

from numpy . linalg import eig

# define a matrix

A = array ( [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] ] )

print ( A )

# calculate the mean of each column

M = mean ( A . T , axis = 1 )

print ( M )

# center columns by subtracting column means

C = A - M

print ( C )

# calculate covariance matrix of centered matrix

V = cov ( C . T )

print ( V )

# eigendecomposition of covariance matrix

values , vectors = eig ( V )

print ( vectors )

print ( values )

# project data

P = vectors . T . dot ( C . T )"
213;213;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-autoregressive-forecasting-models-for-multi-step-air-pollution-time-series-forecasting/;2018-10-16;How to Develop Multi-Step Time Series Forecasting Models for Air Pollution;"# autoregression forecast with global impute strategy

from numpy import loadtxt

from numpy import nan

from numpy import isnan

from numpy import count_nonzero

from numpy import unique

from numpy import array

from numpy import nanmedian

from statsmodels . tsa . arima_model import ARIMA

from matplotlib import pyplot

from warnings import catch_warnings

from warnings import filterwarnings

# split the dataset by 'chunkID', return a list of chunks

def to_chunks ( values , chunk_ix = 0 ) :

chunks = list ( )

# get the unique chunk ids

chunk_ids = unique ( values [ : , chunk_ix ] )

# group rows by chunk id

for chunk_id in chunk_ids :

selection = values [ : , chunk_ix ] == chunk_id

chunks . append ( values [ selection , : ] )

return chunks

# return a list of relative forecast lead times

def get_lead_times ( ) :

return [ 1 , 2 , 3 , 4 , 5 , 10 , 17 , 24 , 48 , 72 ]

# interpolate series of hours (in place) in 24 hour time

def interpolate_hours ( hours ) :

# find the first hour

ix = - 1

for i in range ( len ( hours ) ) :

if not isnan ( hours [ i ] ) :

ix = i

break

# fill-forward

hour = hours [ ix ]

for i in range ( ix + 1 , len ( hours ) ) :

# increment hour

hour += 1

# check for a fill

if isnan ( hours [ i ] ) :

hours [ i ] = hour % 24

# fill-backward

hour = hours [ ix ]

for i in range ( ix - 1 , - 1 , - 1 ) :

# decrement hour

hour -= 1

# check for a fill

if isnan ( hours [ i ] ) :

hours [ i ] = hour % 24

# return true if the array has any non-nan values

def has_data ( data ) :

return count_nonzero ( isnan ( data ) ) < len ( data )

# impute missing data

def impute_missing ( train_chunks , rows , hours , series , col_ix ) :

# impute missing using the median value for hour in all series

imputed = list ( )

for i in range ( len ( series ) ) :

if isnan ( series [ i ] ) :

# collect all rows across all chunks for the hour

all_rows = list ( )

for rows in train_chunks :

[ all_rows . append ( row ) for row in rows [ rows [ : , 2 ] == hours [ i ] ] ]

# calculate the central tendency for target

all_rows = array ( all_rows )

# fill with median value

value = nanmedian ( all_rows [ : , col_ix ] )

if isnan ( value ) :

value = 0.0

imputed . append ( value )

else :

imputed . append ( series [ i ] )

return imputed

# layout a variable with breaks in the data for missing positions

def variable_to_series ( chunk_train , col_ix , n_steps = 5 * 24 ) :

# lay out whole series

data = [ nan for _ in range ( n_steps ) ]

# mark all available data

for i in range ( len ( chunk_train ) ) :

# get position in chunk

position = int ( chunk_train [ i , 1 ] - 1 )

# store data

data [ position ] = chunk_train [ i , col_ix ]

return data

# fit AR model and generate a forecast

def fit_and_forecast ( series ) :

# define the model

model = ARIMA ( series , order = ( 2 , 0 , 0 ) )

# return a nan forecast in case of exception

try :

# ignore statsmodels warnings

with catch_warnings ( ) :

filterwarnings ( ""ignore"" )

# fit the model

model_fit = model . fit ( disp = False )

# forecast 72 hours

yhat = model_fit . predict ( len ( series ) , len ( series ) + 72 )

# extract lead times

lead_times = array ( get_lead_times ( ) )

indices = lead_times - 1

return yhat [ indices ]

except :

return [ nan for _ in range ( len ( get_lead_times ( ) ) ) ]

# forecast all lead times for one variable

def forecast_variable ( hours , train_chunks , chunk_train , chunk_test , lead_times , target_ix ) :

# convert target number into column number

col_ix = 3 + target_ix

# check for no data

if not has_data ( chunk_train [ : , col_ix ] ) :

forecast = [ nan for _ in range ( len ( lead_times ) ) ]

return forecast

# get series

series = variable_to_series ( chunk_train , col_ix )

# impute

imputed = impute_missing ( train_chunks , chunk_train , hours , series , col_ix )

# fit AR model and forecast

forecast = fit_and_forecast ( imputed )

return forecast

# forecast for each chunk, returns [chunk][variable][time]

def forecast_chunks ( train_chunks , test_input ) :

lead_times = get_lead_times ( )

predictions = list ( )

# enumerate chunks to forecast

for i in range ( len ( train_chunks ) ) :

# prepare sequence of hours for the chunk

hours = variable_to_series ( train_chunks [ i ] , 2 )

# interpolate hours

interpolate_hours ( hours )

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 39 ) :

yhat = forecast_variable ( hours , train_chunks , train_chunks [ i ] , test_input [ i ] , lead_times , j )

chunk_predictions . append ( yhat )

chunk_predictions = array ( chunk_predictions )

predictions . append ( chunk_predictions )

return array ( predictions )

# convert the test dataset in chunks to [chunk][variable][time] format

def prepare_test_forecasts ( test_chunks ) :

predictions = list ( )

# enumerate chunks to forecast

for rows in test_chunks :

# enumerate targets for chunk

chunk_predictions = list ( )

for j in range ( 3 , rows . shape [ 1 ] ) :

yh"
214;214;www.statnews.com;https://www.statnews.com/2020/04/16/blood-clots-coronavirus-tpa/;2020-04-16;Blood clots leave clinicians with clues about Covid-19 — but no proven treatments;"Blood clots in severe Covid-19 patients leave clinicians with clues about the illness — but no proven treatments

Doctors treating the sickest Covid-19 patients have zeroed in on a new phenomenon: Some people have developed widespread blood clots, their lungs peppered with tiny blockages that prevent oxygen from pumping into the bloodstream and body.

A number of doctors are now trying to blast those clots with tPA, or tissue plasminogen activator, an antithrombotic drug typically reserved for treating strokes and heart attacks. Other doctors are eyeing the blood thinner heparin as a potential way to prevent clotting before it starts.

Without a rigorous study, though, it’s impossible to know the potential risks or benefits of tPA, blood thinners, or other drugs — or what makes a difference. Until more robust research gets underway, the body of evidence now is a handful of case reports and anecdotal observations on the use of drugs to combat clots.

advertisement

“I can’t stress enough that it is important to have a controlled study to demonstrate that people who get this either do or don’t do better,” said Christopher Barrett, a senior surgical resident at Beth Israel Deaconess Medical Center, a research fellow at MIT and co-author of case reports recently published on blood clots in Covid-19 patients.

As with so much else about the Covid-19 response, health experts are learning about the symptom on the fly. Blood clots are common in patients who are immobilized, but they seem to be smaller and cause far more severe damage in some Covid-19 patients. Doctors have said they see patients with blood clots forming not only in their lungs, but also in blood vessels. Autopsies have also revealed blood clots in kidneys and other organs, which some experts say suggests an overwhelming immune system response to the virus that inflicts harm on the body.

advertisement

Physicians from the U.S., the Netherlands, and China have published a number of case reports in scientific journals about Covid-19 patients with a multitude of small blood clots. In one report, researchers in China said 7 out of 10 patients who died of Covid-19 had small blood clots throughout the bloodstream, compared to fewer than 1 in 100 people who survived. Some of the patients in those case reports received blood thinners or tPA, sometimes when there seemed to be nothing else to try. Some survived, some did not.

“This is a real-time learning experience,” said Clyde Yancy, chief of cardiology at Northwestern University Feinberg School of Medicine.

“I don’t think any of us can declare anything definitively, but we know from the best available data that about one-third of patients who have Covid-19 infections do in fact have evidence of thrombotic disease,” he added. Yancy said there is early-stage, preliminary evidence to suggest that a regimen of anti-coagulants used as a preventive tool could reduce the number of clotting episodes a patient experiences.

It still isn’t clear why the virus leads to these blood clots forming, or why patients’ bodies can’t break them up. It also isn’t clear how significant a role they play in a patient’s illness. Those questions will take time to answer, Barrett said.

But there remains a need for treatments that can buy time to help people fight the virus.

“It’s not necessarily the virus killing people, it’s the organ failure that happens as a result of the viral infection,” Barrett said. “If you can support people through their organ failure, … the immune system will eventually clear out the virus.”

The three patients in Barrett’s case reports, all of whom were on ventilators to help them breathe, initially did better when they were given tPA in what’s known as off-label use in salvage therapy. One of them died, one of them improved briefly, and one of them had a durable response, he said.

Barrett is part of a group awaiting approval from the Food and Drug Administration to move forward with a randomized clinical trial to determine what if any role tPA might play. The trial they hope to conduct at three hospitals in Colorado, one in Massachusetts, and one in New York will give people the drug when they are not as sick as the people in the case reports, who had exhausted all other treatments. Patients will be randomly assigned to receive the drug or a placebo; the trial will also test different dosing.

“We really need the data to prove or disprove that it’s working.” Hunter Moore, transplant surgery fellow at University of Colorado, Denver

“Until then, we’re kind of handicapped,” said Hunter Moore, a transplant surgery fellow at the University of Colorado, Denver, and a researcher working on the trial with Barrett. Now, he said, “it’s all based on off-label use and it’s kind of hearsay in terms of how it’s done. So we really need the data to prove or disprove that it’s working.”

Doctors around the country are already giving patients heparin or tPA. Many reached out to Moore and Barrett after reading thei"
215;215;machinelearningmastery.com;http://machinelearningmastery.com/stochastic-gradient-boosting-xgboost-scikit-learn-python/;2016-09-18;Stochastic Gradient Boosting with XGBoost and scikit-learn in Python;"# XGBoost on Otto dataset, tune subsample

from pandas import read_csv

from xgboost import XGBClassifier

from sklearn . model_selection import GridSearchCV

from sklearn . model_selection import StratifiedKFold

from sklearn . preprocessing import LabelEncoder

import matplotlib

matplotlib . use ( 'Agg' )

from matplotlib import pyplot

# load data

data = read_csv ( 'train.csv' )

dataset = data . values

# split data into X and y

X = dataset [ : , 0 : 94 ]

y = dataset [ : , 94 ]

# encode string class values as integers

label_encoded_y = LabelEncoder ( ) . fit_transform ( y )

# grid search

model = XGBClassifier ( )

subsample = [ 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 1.0 ]

param_grid = dict ( subsample = subsample )

kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 )

grid_search = GridSearchCV ( model , param_grid , scoring = ""neg_log_loss"" , n_jobs = - 1 , cv = kfold )

grid_result = grid_search . fit ( X , label_encoded_y )

# summarize results

print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) )

means = grid_result . cv_results_ [ 'mean_test_score' ]

stds = grid_result . cv_results_ [ 'std_test_score' ]

params = grid_result . cv_results_ [ 'params' ]

for mean , stdev , param in zip ( means , stds , params ) :

print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )

# plot

pyplot . errorbar ( subsample , means , yerr = stds )

pyplot . title ( ""XGBoost subsample vs Log Loss"" )

pyplot . xlabel ( 'subsample' )

pyplot . ylabel ( 'Log Loss' )"
216;216;machinelearningmastery.com;http://machinelearningmastery.com/best-tune-multithreading-support-xgboost-python/;2016-09-04;How to Best Tune Multithreading Support for XGBoost in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35

# Otto, parallel cross validation from pandas import read_csv from xgboost import XGBClassifier from sklearn . model_selection import StratifiedKFold from sklearn . model_selection import cross_val_score from sklearn . preprocessing import LabelEncoder import time # load data data = read_csv ( 'train.csv' ) dataset = data . values # split data into X and y X = dataset [ : , 0 : 94 ] y = dataset [ : , 94 ] # encode string class values as integers label_encoded_y = LabelEncoder ( ) . fit_transform ( y ) # prepare cross validation kfold = StratifiedKFold ( n_splits = 10 , shuffle = True , random_state = 7 ) # Single Thread XGBoost, Parallel Thread CV start = time . time ( ) model = XGBClassifier ( nthread = 1 ) results = cross_val_score ( model , X , label_encoded_y , cv = kfold , scoring = 'neg_log_loss' , n_jobs = - 1 ) elapsed = time . time ( ) - start print ( ""Single Thread XGBoost, Parallel Thread CV: %f"" % ( elapsed ) ) # Parallel Thread XGBoost, Single Thread CV start = time . time ( ) model = XGBClassifier ( nthread = - 1 ) results = cross_val_score ( model , X , label_encoded_y , cv = kfold , scoring = 'neg_log_loss' , n_jobs = 1 ) elapsed = time . time ( ) - start print ( ""Parallel Thread XGBoost, Single Thread CV: %f"" % ( elapsed ) ) # Parallel Thread XGBoost and CV start = time . time ( ) model = XGBClassifier ( nthread = - 1 ) results = cross_val_score ( model , X , label_encoded_y , cv = kfold , scoring = 'neg_log_loss' , n_jobs = - 1 ) elapsed = time . time ( ) - start print ( ""Parallel Thread XGBoost and CV: %f"" % ( elapsed ) )"
217;217;machinelearningmastery.com;http://machinelearningmastery.com/best-programming-language-for-machine-learning/;2014-05-09;Best Programming Language for Machine Learning;"Tweet Share Share

Last Updated on September 27, 2016

A question I get asked a lot is:

What is the best programming language for machine learning?

I’ve replied to this question many times now it’s about time to explore this further in a blog post.

Ultimately, the programming language you use for machine learning should consider your own requirements and predilections. No one can meaningfully address those concerns for you.

No one can meaningfully address those concerns for you.

What Languages Are Being Used

Before I give you my opinion, it is good to have a look around to see what languages and platforms are popular in self-selected communities of data analysis and machine learning professionals.

KDnuggets has had language polls forever. A recent poll is titled “What programming/statistics languages you used for an analytics / data mining / data science work in 2013“. The trends are almost identical to the previous year. The results suggest heavy use of R and Python and SQL for data access. SAS and MATLAB rank higher than I would have expected. I’d expect SAS accounts for larger corporate (Fortune 500) data analysis and MATLAB for engineering, research and student use.

Kaggle offer machine learning competitions and have polled their user base as to the tools and programming languages used by participants in competitions. They posted results in 2011 titled Kagglers’ Favorite Tools (also see the forum discussion). The results suggested the abundant use of R. The results also show good use of MATLAB and SAS with much lower Python representation. I can attest that I prefer R over Python for competition work. It just feels though it has more on offer in terms of data analysis and algorithm selection.

Ben Hamner, Kaggle Admin and author of the blog post above on the Kaggle blog goes into more detail on the options when it comes to programming languages for machine learning in a forum post titled “What tools do people generally use to solve problems“.

Ben comments that MATLAB/Octave is a good language for matrix operations and can be good when working with a well defined feature matrix. Python is fragmented by comprehensive and can be very slow unless you drop into C. He prefers Python when not working with a well defined feature matrix and uses Pandas and NLTK. Ben comments that “As a general rule, if it’s found to be interesting for statisticians, it’s been implemented in R” (well said). He also complains about the language itself being ugly and painful to work with. Finally, Ben comments on Julia that doesn’t have much to offer in the way of libraries but is his new favorite language. He comments that it has the conciseness of languages like MATLAB and Python with the speed of C.

Anthony Goldbloom, the CEO of Kaggle gave a presentation to the Bay Area R user group in 2011 on the popularity of R in Kaggle competitions titled Predictive modeling competitions: making data science a sport (see the powerpoint slides). The presentation slides give more detail on the use of programming languages and suggest an Other category that is as close to as large as large as the usage of R. It would be nice to have the raw data that was collected (why didn’t they release it to their own data community, seriously!?).

John Langford on his blog Hunch has an excellent article on the properties of a programming language to consider when working with machine learning algorithms titled “Programming Languages for Machine Learning Implementations“. He divides the properties into concerns of speed and the concerns of programability (programming ease). He points to powerful industry standard implementations of algorithms, all in C and comments that he has not used R or MATLAB (the post was written 8 years ago). Take some time and read some of the comments by academics and industry specialists alike. This is a deep and nuanced problem that really comes down to the specifics of the problem you are solving and the environment in which you are solving it.

Machine Learning Languages

I think of programming languages in the context of the machine learning activities I want to perform.

MATLAB/Octave

I think MATLAB is excellent for representing and working with matrices. As such, I think it’s an excellent language or platform to use when climbing into the linear algebra of a given method. I think it’s suited to learning about algorithms both superficially the first time around and deeply when you are trying to figure something out or go deep into the method. For example, it’s popular in university courses for beginners, like Andrew Ng’s Coursera Machine Learning course.

R

R is a workhorse for statistical analysis and by extension machine learning. Much talk is given to the learning curve, I didn’t really see the problem. It is the platform to use to understand and explore your data using statistical methods and graphs. It has an enormous number of machine learning algorithms, and advanced implementations too written by the deve"
218;218;machinelearningmastery.com;https://machinelearningmastery.com/understand-machine-learning-algorithms-by-implementing-them-from-scratch/;2015-08-27;Understand Machine Learning Algorithms By Implementing Them From Scratch;"Tweet Share Share

Last Updated on August 13, 2019

Implementing machine learning algorithms from scratch seems like a great way for a programmer to understand machine learning.

And maybe it is.

But there some downsides to this approach too.

In this post you will discover some great resources that you can use to implement machine learning algorithms from scratch.

You will also discover some of the limitations of this seemingly perfect approach.

Discover how to code ML algorithms from scratch including kNN, decision trees, neural nets, ensembles and much more in my new book, with full Python code and no fancy libraries.

Have you implemented a machine learning algorithm from scratch in an effort to learn about it Leave a comment, I’d love to hear about your experience.

Benefits of Implementing Machine Learning Algorithms From Scratch

I promote the idea of implementing machine learning algorithms from scratch.

I think you can learn a lot about how algorithms work. I also think that as a developer, it provides a bridge into learning the mathematical notations, descriptions and intuitions used in machine learning.

I’ve discussed the benefits of implementing algorithms from scratch before in the post “Benefits of Implementing Machine Learning Algorithms From Scratch“.

In the post I listed the benefits as:

the understanding you gain the starting point it provides the ownership of the algorithm and code it forces

Also in that post I comment how you can short-cut the process by leveraging existing tutorials and books. There is a wealth of good resources for getting started, but there are also stumbling blocks to watch out for.

In the next section I point out three books that you can follow to implement machine learning algorithms from scratch.

I’ve helped a lot of programmers get started in machine learning over the last few years. From my experience, I list 5 of the most common stumbling blocks that I see tripping up programmers and the tactics that you can use to over come them.

Finally, you will discover 3 quick tips to getting the most from code tutorials and going from a copy-paste programmer (if you happen to be one) to truly diving down the rabbit hole of machine learning algorithms.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Great Books You Can Use To Implement Algorithms

I have implemented a lot of algorithms from scratch, directly from research papers. It can be very difficult.

It is a much gentler start to follow someone else’s tutorial.

There are many excellent resources that you can use to get started implementing machine learning algorithms from scratch.

Perhaps the most authoritative are books that guide you through tutorials.

There are many benefits to starting with a book. For example:

Someone else has figured out the algorithm and how to turn it into code.

You can use it as a known working starting point for tinkering and experimentation.

Some great books that guide you through implementing machine learning algorithms step-by-step are:

Data Science from Scratch: First Principles with Python by Joel Grus

This truly is from scratch, working through visualization, stats, probability, working with data and then 12 or so different machine learning algorithms.

This is one of my favorite beginner machine learning books from this year.

Machine Learning: An Algorithmic Perspective by Stephen Marsland

This is the long awaited second edition to this popular book. This covers a large number of diverse machine learning algorithms with implementations.

I like that it gives a mix of mathematical description, pseudo code as well as working source code.

Machine Learning in Action by Peter Harrington

This book works through the 10 most popular machine learning algorithms providing case study problems and worked code examples in Python.

I like that there is a good effort to tie the code to the descriptions using numbering and arrows.

Did I miss a good book that provides programming tutorials for implementing machine learning algorithms from scratch?

Let me know in the comments.

5 Stumbling Blocks When Implementing Algorithms From Scratch (and how to overcome them)

Implementing machine learning algorithms from scratch using tutorials is a lot of fun.

But there can be stumbling blocks, and if you’re not careful, they may trip you up and kill your motivation.

In this section I want to point out the 5 most common stumbling blocks that I see and how to roll with them and not let them hold you up. I want you to get unstuck and plow on (or move on to another tutorial).

Some good general advice for avoiding the stumbling blocks below is to carefully check the reviews of books (or the comments on blog posts) before diving into a tutorial. You want to be sure that the code works and that you’re"
219;219;machinelearningmastery.com;http://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/;2016-04-07;Classification And Regression Trees for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Decision Trees are an important type of algorithm for predictive modeling machine learning.

The classical decision tree algorithms have been around for decades and modern variations like random forest are among the most powerful techniques available.

In this post you will discover the humble decision tree algorithm known by it’s more modern name CART which stands for Classification And Regression Trees. After reading this post, you will know:

The many names used to describe the CART algorithm for machine learning.

The representation used by learned CART models that is actually stored on disk.

How a CART model can be learned from training data.

How a learned CART model can be used to make predictions on unseen data.

Additional resources that you can use to learn more about CART and related algorithms.

If you have taken an algorithms and data structures course, it might be hard to hold you back from implementing this simple and powerful algorithm. And from there, you’re a small step away from your own implementation of Random Forests.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Update Aug 2017: Fixed a typo that indicated that Gini is the count of instances for a class, should have been the proportion of instances. Also updated to show Gini weighting for evaluating the split in addition to calculating purity for child nodes.

Decision Trees

Classification and Regression Trees or CART for short is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.

Classically, this algorithm is referred to as “decision trees”, but on some platforms like R they are referred to by the more modern term CART.

The CART algorithm provides a foundation for important algorithms like bagged decision trees, random forest and boosted decision trees.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

CART Model Representation

The representation for the CART model is a binary tree.

This is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).

The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.

Given a dataset with two inputs (x) of height in centimeters and weight in kilograms the output of sex as male or female, below is a crude example of a binary decision tree (completely fictitious for demonstration purposes only).

The tree can be stored to file as a graph or a set of rules. For example, below is the above decision tree as a set of rules.

If Height > 180 cm Then Male If Height <= 180 cm AND Weight > 80 kg Then Male If Height <= 180 cm AND Weight <= 80 kg Then Female Make Predictions With CART Models 1 2 3 4 If Height > 180 cm Then Male If Height <= 180 cm AND Weight > 80 kg Then Male If Height <= 180 cm AND Weight <= 80 kg Then Female Make Predictions With CART Models

With the binary tree representation of the CART model described above, making predictions is relatively straightforward.

Given a new input, the tree is traversed by evaluating the specific input started at the root node of the tree.

A learned binary tree is actually a partitioning of the input space. You can think of each input variable as a dimension on a p-dimensional space. The decision tree split this up into rectangles (when p=2 input variables) or some kind of hyper-rectangles with more inputs.

New data is filtered through the tree and lands in one of the rectangles and the output value for that rectangle is the prediction made by the model. This gives you some feeling for the type of decisions that a CART model is capable of making, e.g. boxy decision boundaries.

For example, given the input of [height = 160 cm, weight = 65 kg], we would traverse the above tree as follows:

Height > 180 cm: No Weight > 80 kg: No Therefore: Female 1 2 3 Height > 180 cm: No Weight > 80 kg: No Therefore: Female

Learn a CART Model From Data

Creating a CART model involves selecting input variables and split points on those variables until a suitable tree is constructed.

The selection of which input variable to use and the specific split or cut-point is chosen using a greedy algorithm to minimize a cost function. Tree construction ends using a predefined stopping criterion, such as a minimum number of training instances assigned to each leaf node of the tree.

Greedy Splitting

Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is used to divid"
220;220;news.mit.edu;http://news.mit.edu/2020/instrument-may-enable-mail-testing-detect-heavy-metals-water-0225;;Instrument may enable mail-in testing to detect heavy metals in water;"Lead, arsenic, and other heavy metals are increasingly present in water systems around the world due to human activities, such as pesticide use and, more recently, the inadequate disposal of electronic waste. Chronic exposure to even trace levels of these contaminants, at concentrations of parts per billion, can cause debilitating health conditions in pregnant women, children, and other vulnerable populations.

Monitoring water for heavy metals is a formidable task, however, particularly for resource-constrained regions where workers must collect many liters of water and chemically preserve samples before transporting them to distant laboratories for analysis.

To simplify the monitoring process, MIT researchers have developed an approach called SEPSTAT, for solid-phase extraction, preservation, storage, transportation, and analysis of trace contaminants. The method is based on a small, user-friendly device the team developed, which absorbs trace contaminants in water and preserves them in a dry state so the samples can be easily dropped in the mail and shipped to a laboratory for further analysis.



A whisk-like device lined with small pockets filled with gold polymer beads, fits inside a typical sampling bottle, and can be twirled to pick up any metal contaminants in water.

The device resembles a small, flexible propeller, or whisk, which fits inside a typical sampling bottle. When twirled inside the bottle for several minutes, the instrument can absorb most of the trace contaminants in the water sample. A user can either air-dry the device or blot it with a piece of paper, then flatten it and mail it in an envelope to a laboratory, where scientists can dip it in a solution of acid to remove the contaminants and collect them for further analysis in the lab.

“We initially designed this for use in India, but it’s taught me a lot about our own water issues and trace contaminants in the United States,” says device designer Emily Hanhauser, a graduate student in MIT’s Department of Mechanical Engineering. “For instance, someone who has heard about the water crisis in Flint, Michigan, who now wants to know what’s in their water, might one day order something like this online, do the test themselves, and send it to a lab.”

Hanhauser and her colleagues recently published their results in the journal Environmental Science and Technology. Her MIT co-authors are Chintan Vaishnav of the Tata Center for Technology and Design and the MIT Sloan School of Management; John Hart, associate professor of mechanical engineering; and Rohit Karnik, professor of mechanical engineering and associate department head for education, along with Michael Bono of Boston University.

From teabags to whisks

The team originally set out to understand the water monitoring infrastructure in India. Millions of water samples are collected by workers at local laboratories all around the country, which are equipped to perform basic water quality analysis. However, to analyze trace contaminants, workers at these local labs need to chemically preserve large numbers of water samples and transport the vessels, often over hundreds of kilometers, to state capitals, where centralized labs have facilities to properly analyze trace contaminants.

“If you’re collecting a lot of these samples and trying to bring them to a lab, it’s pretty onerous work, and there is a significant transportation barrier,” Hanhauser says.



After the device is pulled out and dried, it can preserve any metal contaminants that it has picked up, for long periods of time. The device can be flattened and mailed to a lab, where the contaminants can be further analyzed.

In looking to streamline the logistics of water monitoring, she and her colleagues wondered whether they could bypass the need to transport the water, and instead transport the contaminants by themselves, in a dry state.

They eventually found inspiration in dry blood spotting, a simple technique that involves pricking a person’s finger and collecting a drop of blood on a card of cellulose. When dried, the chemicals in the blood are stable and preserved, and the cards can be mailed off for further analysis, avoiding the need to preserve and ship large volumes of blood.

The team started thinking of a similar collection system for heavy metals, and looked through the literature for materials that could both absorb trace contaminants from water and keep them stable when dry.

They eventually settled on ion-exchange resins, a class of material that comes in the form of small polymer beads, several hundreds of microns wide. These beads contain groups of molecules bound to a hydrogen ion. When dipped in water, the hydrogen comes off and can be exchanged with another ion, such as a heavy metal cation, that takes hydrogen’s place on the bead. In this way, the beads can absorb heavy metals and other trace contaminants from water.

The researchers then looked for ways to immerse the beads in water, and first considered"
221;221;machinelearningmastery.com;http://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/;2016-04-24;Boosting and AdaBoost for Machine Learning;"Tweet Share Share

Last Updated on August 12, 2019

Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers.

In this post you will discover the AdaBoost Ensemble method for machine learning. After reading this post, you will know:

What the boosting ensemble method is and generally how it works.

How to learn to boost decision trees using the AdaBoost algorithm.

How to make predictions using the learned AdaBoost model.

How to best prepare your data for use with the AdaBoost algorithm

This post was written for developers and assumes no background in statistics or mathematics. The post focuses on how the algorithm works and how to use it for predictive modeling problems. If you have any questions, leave a comment and I will do my best to answer.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Boosting Ensemble Method

Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.

This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.

AdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting.

Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Learning An AdaBoost Model From Data

AdaBoost is best used to boost the performance of decision trees on binary classification problems.

AdaBoost was originally called AdaBoost.M1 by the authors of the technique Freund and Schapire. More recently it may be referred to as discrete AdaBoost because it is used for classification rather than regression.

AdaBoost can be used to boost the performance of any machine learning algorithm. It is best used with weak learners. These are models that achieve accuracy just above random chance on a classification problem.

The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps.

Each instance in the training dataset is weighted. The initial weight is set to:

weight(xi) = 1/n

Where xi is the i’th training instance and n is the number of training instances.

How To Train One Model

A weak classifier (decision stump) is prepared on the training data using the weighted samples. Only binary (two-class) classification problems are supported, so each decision stump makes one decision on one input variable and outputs a +1.0 or -1.0 value for the first or second class value.

The misclassification rate is calculated for the trained model. Traditionally, this is calculated as:

error = (correct – N) / N

Where error is the misclassification rate, correct are the number of training instance predicted correctly by the model and N is the total number of training instances. For example, if the model predicted 78 of 100 training instances correctly the error or misclassification rate would be (78-100)/100 or 0.22.

This is modified to use the weighting of the training instances:

error = sum(w(i) * terror(i)) / sum(w)

Which is the weighted sum of the misclassification rate, where w is the weight for training instance i and terror is the prediction error for training instance i which is 1 if misclassified and 0 if correctly classified.

For example, if we had 3 training instances with the weights 0.01, 0.5 and 0.2. The predicted values were -1, -1 and -1, and the actual output variables in the instances were -1, 1 and -1, then the terrors would be 0, 1, and 0. The misclassification rate would be calculated as:

error = (0.01*0 + 0.5*1 + 0.2*0) / (0.01 + 0.5 + 0.2)

or

error = 0.704

A stage value is calculated for the trained model which provides a weighting for any predictions that the model makes. The stage value for a trained model is calculated as follows:

stage = ln((1-error) / error)

Where stage is the stage value used to weight predictions from the model, ln() is the natural logarithm and error is the misclassification error for the model. The effect of the stage weight is that more accurate models have more weight or contribution to the final prediction.

The training weights are updated giving more weight to incorrectly predicted instances, and less weight to correctly predicted instances.

For example, the weight of one training instance (w) is updated "
222;222;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-deep-learning-models-for-univariate-time-series-forecasting/;2018-10-28;How to Develop Deep Learning Models for Univariate Time Series Forecasting;"# evaluate convlstm

from math import sqrt

from numpy import array

from numpy import mean

from numpy import std

from pandas import DataFrame

from pandas import concat

from pandas import read_csv

from sklearn . metrics import mean_squared_error

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Flatten

from keras . layers import ConvLSTM2D

from matplotlib import pyplot

# split a univariate dataset into train/test sets

def train_test_split ( data , n_test ) :

return data [ : - n_test ] , data [ - n_test : ]

# transform list into supervised learning format

def series_to_supervised ( data , n_in = 1 , n_out = 1 ) :

df = DataFrame ( data )

cols = list ( )

# input sequence (t-n, ... t-1)

for i in range ( n_in , 0 , - 1 ) :

cols . append ( df . shift ( i ) )

# forecast sequence (t, t+1, ... t+n)

for i in range ( 0 , n_out ) :

cols . append ( df . shift ( - i ) )

# put it all together

agg = concat ( cols , axis = 1 )

# drop rows with NaN values

agg . dropna ( inplace = True )

return agg . values

# root mean squared error or rmse

def measure_rmse ( actual , predicted ) :

return sqrt ( mean_squared_error ( actual , predicted ) )

# difference dataset

def difference ( data , interval ) :

return [ data [ i ] - data [ i - interval ] for i in range ( interval , len ( data ) ) ]

# fit a model

def model_fit ( train , config ) :

# unpack config

n_seq , n_steps , n_filters , n_kernel , n_nodes , n_epochs , n_batch = config

n_input = n_seq * n_steps

# prepare data

data = series_to_supervised ( train , n_in = n_input )

train_x , train_y = data [ : , : - 1 ] , data [ : , - 1 ]

train_x = train_x . reshape ( ( train_x . shape [ 0 ] , n_seq , 1 , n_steps , 1 ) )

# define model

model = Sequential ( )

model . add ( ConvLSTM2D ( filters = n_filters , kernel_size = ( 1 , n_kernel ) , activation = 'relu' , input_shape = ( n_seq , 1 , n_steps , 1 ) ) )

model . add ( Flatten ( ) )

model . add ( Dense ( n_nodes , activation = 'relu' ) )

model . add ( Dense ( 1 ) )

model . compile ( loss = 'mse' , optimizer = 'adam' )

# fit

model . fit ( train_x , train_y , epochs = n_epochs , batch_size = n_batch , verbose = 0 )

return model

# forecast with a pre-fit model

def model_predict ( model , history , config ) :

# unpack config

n_seq , n_steps , _ , _ , _ , _ , _ = config

n_input = n_seq * n_steps

# prepare data

x_input = array ( history [ - n_input : ] ) . reshape ( ( 1 , n_seq , 1 , n_steps , 1 ) )

# forecast

yhat = model . predict ( x_input , verbose = 0 )

return yhat [ 0 ]

# walk-forward validation for univariate data

def walk_forward_validation ( data , n_test , cfg ) :

predictions = list ( )

# split dataset

train , test = train_test_split ( data , n_test )

# fit model

model = model_fit ( train , cfg )

# seed history with training dataset

history = [ x for x in train ]

# step over each time-step in the test set

for i in range ( len ( test ) ) :

# fit model and make forecast for history

yhat = model_predict ( model , history , cfg )

# store forecast in list of predictions

predictions . append ( yhat )

# add actual observation to history for the next loop

history . append ( test [ i ] )

# estimate prediction error

error = measure_rmse ( test , predictions )

print ( ' > %.3f' % error )

return error

# repeat evaluation of a config

def repeat_evaluate ( data , config , n_test , n_repeats = 30 ) :

# fit and evaluate the model n times

scores = [ walk_forward_validation ( data , n_test , config ) for _ in range ( n_repeats ) ]

return scores

# summarize model performance

def summarize_scores ( name , scores ) :

# print a summary

scores_m , score_std = mean ( scores ) , std ( scores )

print ( '%s: %.3f RMSE (+/- %.3f)' % ( name , scores_m , score_std ) )

# box and whisker plot

pyplot . boxplot ( scores )

pyplot . show ( )

series = read_csv ( 'monthly-car-sales.csv' , header = 0 , index_col = 0 )

data = series . values

# data split

n_test = 12

# define config

config = [ 3 , 12 , 256 , 3 , 200 , 200 , 100 ]

# grid search

scores = repeat_evaluate ( data , config , n_test )

# summarize scores"
223;223;machinelearningmastery.com;http://machinelearningmastery.com/how-to-run-your-first-classifier-in-weka/;2014-02-16;How to Run Your First Classifier in Weka;"Tweet Share Share

Last Updated on August 22, 2019

Weka makes learning applied machine learning easy, efficient, and fun. It is a GUI tool that allows you to load datasets, run algorithms and design and run experiments with results statistically robust enough to publish.

I recommend Weka to beginners in machine learning because it lets them focus on learning the process of applied machine learning rather than getting bogged down by the mathematics and the programming — those can come later.

In this post, I want to show you how easy it is to load a dataset, run an advanced classification algorithm and review the results.

If you follow along, you will have machine learning results in under 5 minutes, and the knowledge and confidence to go ahead and try more datasets and more algorithms.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

1. Download Weka and Install

Visit the Weka Download page and locate a version of Weka suitable for your computer (Windows, Mac, or Linux).

Weka requires Java. You may already have Java installed and if not, there are versions of Weka listed on the download page (for Windows) that include Java and will install it for you. I’m on a Mac myself, and like everything else on Mac, Weka just works out of the box.

If you are interested in machine learning, then I know you can figure out how to download and install software into your own computer. If you need help installing Weka, see the following post that provides step-by-step instructions:

2. Start Weka

Start Weka. This may involve finding it in program launcher or double clicking on the weka.jar file. This will start the Weka GUI Chooser.

The Weka GUI Chooser lets you choose one of the Explorer, Experimenter, KnowledgeExplorer and the Simple CLI (command line interface).

Click the “Explorer” button to launch the Weka Explorer.

This GUI lets you load datasets and run classification algorithms. It also provides other features, like data filtering, clustering, association rule extraction, and visualization, but we won’t be using these features right now.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

3. Open the data/iris.arff Dataset

Click the “Open file…” button to open a data set and double click on the “data” directory.

Weka provides a number of small common machine learning datasets that you can use to practice on.

Select the “iris.arff” file to load the Iris dataset.

The Iris Flower dataset is a famous dataset from statistics and is heavily borrowed by researchers in machine learning. It contains 150 instances (rows) and 4 attributes (columns) and a class attribute for the species of iris flower (one of setosa, versicolor, and virginica). You can read more about Iris flower dataset on Wikipedia.

4. Select and Run an Algorithm

Now that you have loaded a dataset, it’s time to choose a machine learning algorithm to model the problem and make predictions.

Click the “Classify” tab. This is the area for running algorithms against a loaded dataset in Weka.

You will note that the “ZeroR” algorithm is selected by default.

Click the “Start” button to run this algorithm.

The ZeroR algorithm selects the majority class in the dataset (all three species of iris are equally present in the data, so it picks the first one: setosa) and uses that to make all predictions. This is the baseline for the dataset and the measure by which all algorithms can be compared. The result is 33%, as expected (3 classes, each equally represented, assigning one of the three to each prediction results in 33% classification accuracy).

You will also note that the test options selects Cross Validation by default with 10 folds. This means that the dataset is split into 10 parts: the first 9 are used to train the algorithm, and the 10th is used to assess the algorithm. This process is repeated, allowing each of the 10 parts of the split dataset a chance to be the held-out test set. You can read more about cross validation here.

The ZeroR algorithm is important, but boring.

Click the “Choose” button in the “Classifier” section and click on “trees” and click on the “J48” algorithm.

This is an implementation of the C4.8 algorithm in Java (“J” for Java, 48 for C4.8, hence the J48 name) and is a minor extension to the famous C4.5 algorithm. You can read more about the C4.5 algorithm here.

Click the “Start” button to run the algorithm.

5. Review Results

After running the J48 algorithm, you can note the results in the “Classifier output” section.

The algorithm was run with 10-fold cross-validation: this means it was given an opportunity to make a prediction for each instance of the dataset (with different training folds) and the "
224;224;machinelearningmastery.com;http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/;2016-05-31;How To Compare Machine Learning Algorithms in Python with scikit-learn;"# Compare Algorithms

import pandas

import matplotlib . pyplot as plt

from sklearn import model_selection

from sklearn . linear_model import LogisticRegression

from sklearn . tree import DecisionTreeClassifier

from sklearn . neighbors import KNeighborsClassifier

from sklearn . discriminant_analysis import LinearDiscriminantAnalysis

from sklearn . naive_bayes import GaussianNB

from sklearn . svm import SVC

# load dataset

url = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv""

names = [ 'preg' , 'plas' , 'pres' , 'skin' , 'test' , 'mass' , 'pedi' , 'age' , 'class' ]

dataframe = pandas . read_csv ( url , names = names )

array = dataframe . values

X = array [ : , 0 : 8 ]

Y = array [ : , 8 ]

# prepare configuration for cross validation test harness

seed = 7

# prepare models

models = [ ]

models . append ( ( 'LR' , LogisticRegression ( ) ) )

models . append ( ( 'LDA' , LinearDiscriminantAnalysis ( ) ) )

models . append ( ( 'KNN' , KNeighborsClassifier ( ) ) )

models . append ( ( 'CART' , DecisionTreeClassifier ( ) ) )

models . append ( ( 'NB' , GaussianNB ( ) ) )

models . append ( ( 'SVM' , SVC ( ) ) )

# evaluate each model in turn

results = [ ]

names = [ ]

scoring = 'accuracy'

for name , model in models :

kfold = model_selection . KFold ( n_splits = 10 , random_state = seed )

cv_results = model_selection . cross_val_score ( model , X , Y , cv = kfold , scoring = scoring )

results . append ( cv_results )

names . append ( name )

msg = ""%s: %f (%f)"" % ( name , cv_results . mean ( ) , cv_results . std ( ) )

print ( msg )

# boxplot algorithm comparison

fig = plt . figure ( )

fig . suptitle ( 'Algorithm Comparison' )

ax = fig . add_subplot ( 111 )

plt . boxplot ( results )

ax . set_xticklabels ( names )"
225;225;machinelearningmastery.com;https://machinelearningmastery.com/how-to-calculate-joint-marginal-and-conditional-probability/;2019-09-29;How to Develop an Intuition for Joint, Marginal, and Conditional Probability;"Tweet Share Share

Last Updated on December 6, 2019

Probability for a single random variable is straight forward, although it can become complicated when considering two or more variables.

With just two variables, we may be interested in the probability of two simultaneous events, called joint probability: the probability of one event given the occurrence of another event called the conditional probability, or just the probability of an event regardless of other variables, called the marginal probability.

These types of probability are easy to define but the intuition behind their meaning can take some time to sink in, requiring some worked examples that can be tinkered with.

In this tutorial, you will discover the intuitions behind calculating the joint, marginal, and conditional probability.

After completing this tutorial, you will know:

How to calculate joint, marginal, and conditional probability for independent random variables.

How to collect observations from joint random variables and construct a joint probability table.

How to calculate joint, marginal, and conditional probability from a joint probability table.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Update Oct/2019 : Fixed some minor typos, thanks Minel and jmy.

: Fixed some minor typos, thanks Minel and jmy. Update Dec/2019: Fixed typo in the description of the joint probability table.

Tutorial Overview

This tutorial is divided into three parts; they are:

Joint, Marginal, and Conditional Probabilities Probabilities of Rolling Two Dice Probabilities of Weather in Two Cities

Joint, Marginal, and Conditional Probabilities

Calculating probability is relatively straight forward when working with a single random variable.

It gets more interesting when considering two or more random variables, as we often do in many real world circumstances.

There are three main types of probabilities that we may be interested in calculating when working with two (or more) random variables.

Briefly, they are:

Joint Probability . The probability of simultaneous events.

. The probability of simultaneous events. Marginal Probability . The probability of an event irrespective of the other variables.

. The probability of an event irrespective of the other variables. Conditional Probability. The probability of events given the presence of other events.

The meaning and calculation of these different types of probabilities vary depending on whether the two random variables are independent (simpler) or dependent (more complicated).

We will explore how to calculate and interpret these three types of probability with worked examples.

In the next section, we will look at the independent rolls of two dice, and in the following section, we will look at the occurrence of weather events of two geographically close cities.

Probabilities of Rolling Two Dice

A good starting point for exploring joint and marginal probabilities is to consider independent random variables as the calculations are very simple.

The roll of a fair die gives a one in six (1/6) or 0.166 (16.666%) probability of a number 1 to 6 coming up.

P(dice1=1) = 1/6

P(dice1=2) = 1/6

P(dice1=3) = 1/6

P(dice1=4) = 1/6

P(dice1=5) = 1/6

P(dice1=6) = 1/6

If we roll a second die, we get the same probability of each value on that die. Each event for a die has an equal probability and the rolls of dice1 and dice2 do not affect each other.

P(dice1={1,2,3,4,5,6}) = 1.0

P(dice2={1,2,3,4,5,6}) = 1.0

First, we can calculate the probability of rolling an even number for dice1 as the sum of the probabilities of rolling a 2, 4, or 6, for example:

P(dice1={2, 4, 6}) = P(dice1=2) + P(dice1=4) + P(dice1=6)

P(dice1={2, 4, 6}) = 1/6 + 1/6 + 1/6

This is 0.5 or 50% as we might intuitively expect.

Now, we might consider the joint probability of rolling an even number with both dice simultaneously. The joint probability for independent random variables is calculated as follows:

P(A and B) = P(A) * P(B)

This is calculated as the probability of rolling an even number for dice1 multiplied by the probability of rolling an even number for dice2. The probability of the first event constrains the probability of the second event.

P(dice1={2, 4, 6} and dice2={2, 4, 6}) = P(dice1={2, 4, 6}) * P(dice2={2, 4, 6})

We know that the probability of rolling an even number of each die is 0.5, therefore the probability of rolling two even numbers is 3/6 or 0.5. Plugging that in, we get: 0.5 * 0.5 (0.25) or 25%.

Another way to look at this is to consider that rolling one die gives 6 combinations. Rolling two dice together gives 6 combinations for dice2 for each of the 6 combinations of dice1 or (6×6) 36 combinations. A total of 3 of the 6 combinations of dice1 will be even, and 3 of the 6 combinations of those will be even. That gives (3×3) 9 out of the 36 combina"
226;226;towardsdatascience.com;https://towardsdatascience.com/work-smarter-not-harder-when-building-neural-networks-6f4aa7c5ee61?source=collection_home---4------0-----------------------;2020-04-17;Work smarter, not harder when building Neural Networks;"Work smarter, not harder when building Neural Networks

Using a simple example to illuminate design principles for neural networks

A fundamental technique in applied mathematics is to find a change of coordinates that converts a difficult or impossible problem into a simpler one. Perhaps my favorite example of this are the equations for a circle. If we write down the equation of the unit circle in Cartesian coordinates we can express the geometric concept of a circle as an implicit function:

This gets even worse when we try to get an explicit expression by solving for y.

Now we have to piece this simple function together from two branches of the square root (top and bottom half of the circle). Beyond aesthetics using this representation of a circle can prevent us from seeing the geometrical simplicity inherent in a perfect circle. By contrast, if we represent this same mathematical object using polar coordinates, then the unit circle becomes very simple to work with.

Polar coordinates for a point.

In polar coordinates our unit circle takes the simple form r=1, θ ∈ [0,2π). So in polar coordinates a circle takes the form of a rectangle (height=1, width=2π). Cartesian coordinates are the natural coordinates for rectangles and polar coordinates are the natural coordinates for circles. In this article I will apply this line of thinking to constructing simple neural networks, and build a illustrative example of the importance of choosing the correct coordinates.

Artificial Neural Networks for Curve Fitting

If you are reading this article then you have probably heard of Artificial Neural Networks (ANN). We will focus on the simplest type of ANN’s called a feed-forward network in this discussion. Briefly, ANN’s are just networks(functions) built by chaining together simple nonlinear functions (layers).

Feed-forward neural network

By chaining (many) of these simple layers together, the universal approximation theorem tells us we can represent any (nice) function as a ANN with finite depth and width. In the starting polar coordinates example this is like saying that we can write every point in the (x,y) plane in polar coordinates (no holes). This is great and a necessary property to ensure we don’t miss anything when we change coordinates, but it doesn’t tell us two things:

How to find the actual coordinates.

If that is actually a good way of representing the information.

For ANNs the “coordinates” are the parameters of the neural network (the weights and biases for each layer in the neural network). Instead of having a quick mathematical formula to find those coordinates, for neural networks we find them using optimization to minimize a loss function. The loss function measures the distance between our specified function and the training data.

Let’s look at a simple example of this to build an approximation to a sine function using a neural network.

Fitting a sin function using a Neural Network

Let’s apply this powerful function approximation approach to build a neural network version of the simple function sin(x). I will be using the fantastic neural network library Flux written in Julia. This package provides a very simple and powerful framework for building neural networks. It also adds very little extra syntax to build neural networks and lets us focus on the fundamental building blocks. I’m just going to include code snippets in the article. For the full code check out this github repo.

The below code constructs a simple network with two hidden layers using a standard nonlinear function tanh.



ann = Chain(Dense(1,20,tanh),Dense(20,20,tanh),Dense(20,1));



We can visualize this neural network as:

A simple feed-forward neural network for learning the sin function

The above diagram shows we have a single input and output value with two layers in-between. These layers are called hidden because they are invisible if you are only tracking the input and outputs. Recall that this diagram is used to represent a certain family of functions, were a particular member is specified by fixing the parameters.

The functional form of the above neural network. The parameters are given by the two weight matrices W and the bias vectors b. The C parameter gives the intercept of the linear output layer.

Alright, so we are hoping that we can approximately represent the function sin(x) by some member of this family of functions. To try and find the member that is closest to this we should minimize a loss function using some training data.

function loss(x, y)

pred=ann(x)

loss=Flux.mse(ann(x), y)

#loss+=0.1*sum(l1,params(ann)) #l1 reg

return loss

end @epochs 3000 Flux.train!(loss,params(ann), data, ADAM())

After fitting our neural network we can see how well it does for the training data, and a set of test data.

A feed-forward neural network with a tanh non-linearity fit for 3000 epochs to the training data shown as blue circles. The test data is shown as green crosses.

Clearly this neural network does a "
227;227;machinelearningmastery.com;https://machinelearningmastery.com/how-to-use-test-time-augmentation-to-improve-model-performance-for-image-classification/;2019-04-14;How to Use Test-Time Augmentation to Make Better Predictions;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104

# cnn model for the cifar10 problem with test-time augmentation import numpy from numpy import argmax from numpy import mean from numpy import std from numpy import expand_dims from sklearn . metrics import accuracy_score from keras . datasets . cifar10 import load_data from keras . utils import to_categorical from keras . preprocessing . image import ImageDataGenerator from keras . models import Sequential from keras . layers import Conv2D from keras . layers import MaxPooling2D from keras . layers import Dense from keras . layers import Flatten from keras . layers import BatchNormalization # load and return the cifar10 dataset ready for modeling def load_dataset ( ) : # load dataset ( trainX , trainY ) , ( testX , testY ) = load_data ( ) # normalize pixel values trainX = trainX . astype ( 'float32' ) / 255 testX = testX . astype ( 'float32' ) / 255 # one hot encode target values trainY = to_categorical ( trainY ) testY = to_categorical ( testY ) return trainX , trainY , testX , testY # define the cnn model for the cifar10 dataset def define_model ( ) : # define model model = Sequential ( ) model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' , kernel_initializer = 'he_uniform' , input_shape = ( 32 , 32 , 3 ) ) ) model . add ( BatchNormalization ( ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' , padding = 'same' , kernel_initializer = 'he_uniform' ) ) model . add ( BatchNormalization ( ) ) model . add ( MaxPooling2D ( ( 2 , 2 ) ) ) model . add ( Flatten ( ) ) model . add ( Dense ( 128 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( BatchNormalization ( ) ) model . add ( Dense ( 10 , activation = 'softmax' ) ) # compile model model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] ) return model # make a prediction using test-time augmentation def tta_prediction ( datagen , model , image , n_examples ) : # convert image into dataset samples = expand_dims ( image , 0 ) # prepare iterator it = datagen . flow ( samples , batch_size = n_examples ) # make predictions for each augmented image yhats = model . predict_generator ( it , steps = n_examples , verbose = 0 ) # sum across predictions summed = numpy . sum ( yhats , axis = 0 ) # argmax across classes return argmax ( summed ) # evaluate a model on a dataset using test-time augmentation def tta_evaluate_model ( model , testX , testY ) : # configure image data augmentation datagen = ImageDataGenerator ( horizontal_flip = True ) # define the number of augmented images to generate per test set image n_examples_per_image = 7 yhats = list ( ) for i in range ( len ( testX ) ) : # make augmented prediction yhat = tta_prediction ( datagen , model , testX [ i ] , n_examples_per_image ) # store for evaluation yhats . append ( yhat ) # calculate accuracy testY_labels = argmax ( testY , axis = 1 ) acc = accuracy_score ( testY_labels , yhats ) return acc # fit and evaluate a defined model def evaluate_model ( model , trainX , trainY , testX , testY ) : # fit model model . fit ( trainX , trainY , epochs = 3 , batch_size = 128 , verbose = 0 ) # evaluate model using tta acc = tta_evaluate_model ( model , testX , testY ) return acc # repeatedly evaluate model, return distribution of scores def repeated_evaluation ( trainX , trainY , testX , testY , repeats = 10 ) : scores = list ( ) for _ in range ( repeats ) : # define model model = define_model ( ) # fit and evaluate model accuracy = evaluate_model ( model , trainX , trainY , testX , testY ) # store score scores . append ( accuracy ) print ( '> %.3f' % accuracy ) return scores # load dataset trainX , trainY , testX , testY = load_dataset ( ) # evaluate model scores = repeated_evaluation ( trainX , trainY , testX , testY ) # summarize result print ( 'Accuracy: %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) )"
228;228;machinelearningmastery.com;http://machinelearningmastery.com/5-step-life-cycle-neural-network-models-keras/;2016-08-10;5 Step Life-Cycle for Neural Network Models in Keras;"# Sample Multilayer Perceptron Neural Network in Keras

from keras . models import Sequential

from keras . layers import Dense

import numpy

# load and prepare the dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# 1. define the network

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# 2. compile the network

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# 3. fit the network

history = model . fit ( X , Y , epochs = 100 , batch_size = 10 )

# 4. evaluate the network

loss , accuracy = model . evaluate ( X , Y )

print ( ""

Loss: %.2f, Accuracy: %.2f%%"" % ( loss , accuracy* 100 ) )

# 5. make predictions

probabilities = model . predict ( X )

predictions = [ float ( round ( x ) ) for x in probabilities ]

accuracy = numpy . mean ( predictions == Y )"
229;229;machinelearningmastery.com;https://machinelearningmastery.com/why-learn-linear-algebra-for-machine-learning/;2018-01-28;5 Reasons to Learn Linear Algebra for Machine Learning;"Tweet Share Share

Last Updated on August 9, 2019

Why Learn Linear Algebra for Machine Learning?

Linear algebra is a field of mathematics that could be called the mathematics of data.

It is undeniably a pillar of the field of machine learning, and many recommend it as a prerequisite subject to study prior to getting started in machine learning. This is misleading advice, as linear algebra makes more sense to a practitioner once they have a context of the applied machine learning process in which to interpret it.

In this post, you will discover why machine learning practitioners should study linear algebra to improve their skills and capabilities as practitioners.

After reading this post, you will know:

Not everyone should learn linear algebra, that it depends where you are in your process of learning machine learning.

5 Reasons why a deeper understanding of linear algebra is required for intermediate machine learning practitioners.

Where to get started once you are motivated to begin your journey into the field of linear algebra.

Discover vectors, matrices, tensors, matrix types, matrix factorization, PCA, SVD and much more in my new book, with 19 step-by-step tutorials and full source code.

Let’s get started.

Reasons to NOT Learn Linear Algebra

Before we go through the reasons that you should learn linear algebra, let’s start off by taking a small look at the reason why you should not.

I think you should not study linear algebra if you are just getting started with applied machine learning.

It’s not required . Having an appreciation for the abstract operations that underly some machine learning algorithms is not required in order to use machine learning as a tool to solve problems.

. Having an appreciation for the abstract operations that underly some machine learning algorithms is not required in order to use machine learning as a tool to solve problems. It’s slow . Taking months to years to study an entire related field before machine learning will delay you achieving your goals of being able to work through predictive modeling problems.

. Taking months to years to study an entire related field before machine learning will delay you achieving your goals of being able to work through predictive modeling problems. It’s a huge field. Not all of linear algebra is relevant to theoretical machine learning, let alone applied machine learning.

I recommend a breadth-first approach to getting started in applied machine learning.

I call this approach a results-first approach. It is where you start by learning and practicing the steps for working through a predictive modeling problem end-to-end (e.g. how to get results) with a tool (such as scikit-learn and Pandas in Python).

This process then provides the skeleton and context for progressively deepening your knowledge, such as how algorithms work and eventually the math that underlies them.

After you know how to work through a predictive modeling problem, let’s look at why you should deepen your understanding of linear algebra.

Need help with Linear Algebra for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. You Need to Learn Linear Algebra Notation

You need to be able to read and write vector and matrix notation.

Algorithms are described in books, papers and on websites using vector and matrix notation.

Linear algebra is the mathematics of data and the notation allows you to describe operations on data precisely with specific operators.

You need to be able to read and write this notation. This skill will allow you to:

Read descriptions of existing algorithms in textbooks.

Interpret and implement descriptions of new methods in research papers.

Concisely describe your own methods to other practitioners.

Further, programming languages such as Python offer efficient ways of implementing linear algebra notation directly.

An understanding of the notation and how it is realized in your language or library will allow for shorter and perhaps more efficient implementations of machine learning algorithms.

2. You Need to Learn Linear Algebra Arithmetic

In partnership with the notation of linear algebra are the arithmetic operations performed.

You need to know how to add, subtract, and multiply scalars, vectors, and matrices.

A challenge for newcomers to the field of linear algebra are operations such as matrix multiplication and tensor multiplication that are not implemented as the direct multiplication of the elements of these structures, and at first glance appear nonintuitive.

Again, most if not all of these operations are implemented efficiently and provided via API calls in modern linear algebra libraries.

An understanding of how vector and matrix operations are implemented is required as a part of being able to effectively read and write matrix notation.

3. You Need to Learn Linear Algebra for Stat"
230;230;machinelearningmastery.com;https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/;2019-09-26;A Gentle Introduction to Joint, Marginal, and Conditional Probability;"Tweet Share Share

Last Updated on November 22, 2019

Probability quantifies the uncertainty of the outcomes of a random variable.

It is relatively easy to understand and compute the probability for a single variable. Nevertheless, in machine learning, we often have many random variables that interact in often complex and unknown ways.

There are specific techniques that can be used to quantify the probability for multiple random variables, such as the joint, marginal, and conditional probability. These techniques provide the basis for a probabilistic understanding of fitting a predictive model to data.

In this post, you will discover a gentle introduction to joint, marginal, and conditional probability for multiple random variables.

After reading this post, you will know:

Joint probability is the probability of two events occurring simultaneously.

Marginal probability is the probability of an event irrespective of the outcome of another variable.

Conditional probability is the probability of one event occurring in the presence of a second event.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Update Oct/2019 : Fixed minor typo, thanks Anna.

: Fixed minor typo, thanks Anna. Update Nov/2019: Described the symmetrical calculation of joint probability.

Overview

This tutorial is divided into three parts; they are:

Probability of One Random Variable Probability of Multiple Random Variables Probability of Independence and Exclusivity

Probability of One Random Variable

Probability quantifies the likelihood of an event.

Specifically, it quantifies how likely a specific outcome is for a random variable, such as the flip of a coin, the roll of a dice, or drawing a playing card from a deck.

Probability gives a measure of how likely it is for something to happen.

— Page 57, Probability: For the Enthusiastic Beginner, 2016.

For a random variable x, P(x) is a function that assigns a probability to all values of x.

Probability Density of x = P(x)

The probability of a specific event A for a random variable x is denoted as P(x=A), or simply as P(A).

Probability of Event A = P(A)

Probability is calculated as the number of desired outcomes divided by the total possible outcomes, in the case where all outcomes are equally likely.

Probability = (number of desired outcomes) / (total number of possible outcomes)

This is intuitive if we think about a discrete random variable such as the roll of a die. For example, the probability of a die rolling a 5 is calculated as one outcome of rolling a 5 (1) divided by the total number of discrete outcomes (6) or 1/6 or about 0.1666 or about 16.666%.

The sum of the probabilities of all outcomes must equal one. If not, we do not have valid probabilities.

Sum of the Probabilities for All Outcomes = 1.0.

The probability of an impossible outcome is zero. For example, it is impossible to roll a 7 with a standard six-sided die.

Probability of Impossible Outcome = 0.0

The probability of a certain outcome is one. For example, it is certain that a value between 1 and 6 will occur when rolling a six-sided die.

Probability of Certain Outcome = 1.0

The probability of an event not occurring, called the complement.

This can be calculated by one minus the probability of the event, or 1 – P(A). For example, the probability of not rolling a 5 would be 1 – P(5) or 1 – 0.166 or about 0.833 or about 83.333%.

Probability of Not Event A = 1 – P(A)

Now that we are familiar with the probability of one random variable, let’s consider probability for multiple random variables.

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Probability of Multiple Random Variables

In machine learning, we are likely to work with many random variables.

For example, given a table of data, such as in excel, each row represents a separate observation or event, and each column represents a separate random variable.

Variables may be either discrete, meaning that they take on a finite set of values, or continuous, meaning they take on a real or numerical value.

As such, we are interested in the probability across two or more random variables.

This is complicated as there are many ways that random variables can interact, which, in turn, impacts their probabilities.

This can be simplified by reducing the discussion to just two random variables (X, Y), although the principles generalize to multiple variables.

And further, to discuss the probability of just two events, one for each variable (X=A, Y=B), although we could just as easily be discussing groups of events for each variable.

Therefore, we will introduce the probability of multiple random variables as the probability of event A and ev"
231;231;news.mit.edu;http://news.mit.edu/2020/algorithm-eye-visibility-helps-pilots-alaska-0106;;An algorithm with an eye for visibility helps pilots in Alaska;"More than three-quarters of Alaskan communities have no access to highways or roads. In these remote regions, small aircraft are a town's bus, ambulance, and food delivery — the only means of getting people and things in and out.

As routine as daily flight may be, it can be dangerous. These small (or general aviation) aircraft are typically flown visually, by a pilot looking out the cockpit windows. If sudden storms or fog appears, a pilot might not be able to see a runway, nearby aircraft, or rising terrain. In 2018, the Federal Aviation Administration (FAA) reported 95 aviation accidents in Alaska, including several fatal crashes that occurred in remote regions where poor visibility may have played a role.

""General aviation pilots in Alaska need to be aware of the forecasted conditions during pre-flight planning, but also of any rapidly changing conditions during flight,"" says Michael Matthews, a meteorologist at MIT Lincoln Laboratory. ""There are certain rules, like you can't fly with less than three miles of visibility. If it is worse, pilots need to fly on instruments, but they need to be certified for that.""

Pilots check current or forecasted weather conditions before they fly, but a lack of automated weather observation stations throughout the Alaskan bush makes it hard to know exactly what to expect. To help, the FAA recently installed 221 web cameras near runways and mountain passes. Pilots can look at the image feeds online to plan their route. Still, it's difficult to go through what could be hundreds of images and estimate just how far one can see.

So, Matthews has been working with the FAA to turn these web cameras into visibility sensors. He has developed an algorithm, called Visibility Estimation through Image Analytics (VEIA), that uses a camera's image feed to automatically determine the area's visibility. These estimates can then be shared among forecasters and with pilots online in real-time.

Trained eyes

In concept, the VEIA algorithm determines visibility the same way humans do. It looks for stationary ""edges."" For human observers, these edges are landmarks of known distances from an airfield, such as a tower or mountain top. They're trained to interpret how well they can see each marker compared to on a clear, sunny day.

Likewise, the algorithm is first taught what edges look like in clear conditions. The system looks at the past 10 days' worth of imagery, an optimal timeframe because any shorter timeframe could be skewed by bad weather and any longer could be affected by seasonal changes, according to Matthews. Using these 10-day images, the system creates a composite ""clear"" image. This image becomes the reference to which a current image is compared.

To run a comparison, an edge-detection algorithm (called a Sobel filter) is applied to both the reference and current image. This algorithm identifies edges that are persistent — the horizon, buildings, mountain sides — and removes fleeting edges like cars and clouds. Then, the system compares the overall edge strengths and generates a ratio. The ratio is converted into visibility in miles.

Developing an algorithm that works well across images from any web camera was challenging, Matthews says. Based on where they are placed, some cameras might have a view of 100 miles and others just 100 feet. Other problems stemmed from permanent objects that were very close to the camera and dominated the view, such as a large antenna. The algorithm had to be designed to look past these near objects.

""If you're an observer on Mount Washington, you have a trained eye to look for very specific things to get a visibility estimate. Say, the ski lifts on Attitash Mountain, and so on. We didn't want to make an algorithm that is trained so specifically; we wanted this same algorithm to apply anywhere and across all types of edges,"" Matthews says.

To validate its estimates, the VEIA algorithm was tested against data from Automated Surface Observing Stations (ASOS). These stations, of which there are close to 50 in Alaska, are outfitted with sensors that can estimate visibility each hour. The VEIA algorithm, which provides estimates every 10 minutes, was more than 90 percent accurate in detecting low-visibility conditions when compared to co-located ASOS data.

Informed pilots

The FAA plans to test the VEIA algorithm in summer 2020 on an experimental website. During the test period, pilots can visit the experimental website to see real-time visibility estimates alongside the camera imagery itself.

""Furthermore, the VEIA estimates can be ingested into weather prediction models to improve the forecasts,"" says Jenny Colavito, who is the ceiling and visibility research project lead at the FAA. ""All of this leads to keeping pilots better informed of weather conditions so that they can avoid flying into hazards.""

The FAA is looking into using weather cameras in other regions, starting in Hawaii. ""Like Alaska, Hawaii has extreme terrain and weather co"
232;232;machinelearningmastery.com;https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/#comments;2020-04-09;Stacking Ensemble Machine Learning With Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65

# compare ensemble to each baseline classifier from numpy import mean from numpy import std from sklearn . datasets import make_classification from sklearn . model_selection import cross_val_score from sklearn . model_selection import RepeatedStratifiedKFold from sklearn . linear_model import LogisticRegression from sklearn . neighbors import KNeighborsClassifier from sklearn . tree import DecisionTreeClassifier from sklearn . svm import SVC from sklearn . naive_bayes import GaussianNB from sklearn . ensemble import StackingClassifier from matplotlib import pyplot # get the dataset def get_dataset ( ) : X , y = make_classification ( n_samples = 1000 , n_features = 20 , n_informative = 15 , n_redundant = 5 , random_state = 1 ) return X , y # get a stacking ensemble of models def get_stacking ( ) : # define the base models level0 = list ( ) level0 . append ( ( 'lr' , LogisticRegression ( ) ) ) level0 . append ( ( 'knn' , KNeighborsClassifier ( ) ) ) level0 . append ( ( 'cart' , DecisionTreeClassifier ( ) ) ) level0 . append ( ( 'svm' , SVC ( ) ) ) level0 . append ( ( 'bayes' , GaussianNB ( ) ) ) # define meta learner model level1 = LogisticRegression ( ) # define the stacking ensemble model = StackingClassifier ( estimators = level0 , final_estimator = level1 , cv = 5 ) return model # get a list of models to evaluate def get_models ( ) : models = dict ( ) models [ 'lr' ] = LogisticRegression ( ) models [ 'knn' ] = KNeighborsClassifier ( ) models [ 'cart' ] = DecisionTreeClassifier ( ) models [ 'svm' ] = SVC ( ) models [ 'bayes' ] = GaussianNB ( ) models [ 'stacking' ] = get_stacking ( ) return models # evaluate a give model using cross-validation def evaluate_model ( model ) : cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 ) scores = cross_val_score ( model , X , y , scoring = 'accuracy' , cv = cv , n_jobs = - 1 , error_score = 'raise' ) return scores # define dataset X , y = get_dataset ( ) # get the models to evaluate models = get_models ( ) # evaluate the models and store results results , names = list ( ) , list ( ) for name , model in models . items ( ) : scores = evaluate_model ( model ) results . append ( scores ) names . append ( name ) print ( '>%s %.3f (%.3f)' % ( name , mean ( scores ) , std ( scores ) ) ) # plot model performance for comparison pyplot . boxplot ( results , labels = names , showmeans = True ) pyplot . show ( )"
233;233;news.mit.edu;http://news.mit.edu/2019/first-laser-ultrasound-images-humans-1219;;Researchers produce first laser ultrasound images of humans;"For most people, getting an ultrasound is a relatively easy procedure: As a technician gently presses a probe against a patient’s skin, sound waves generated by the probe travel through the skin, bouncing off muscle, fat, and other soft tissues before reflecting back to the probe, which detects and translates the waves into an image of what lies beneath.

Conventional ultrasound doesn’t expose patients to harmful radiation as X-ray and CT scanners do, and it’s generally noninvasive. But it does require contact with a patient’s body, and as such, may be limiting in situations where clinicians might want to image patients who don’t tolerate the probe well, such as babies, burn victims, or other patients with sensitive skin. Furthermore, ultrasound probe contact induces significant image variability, which is a major challenge in modern ultrasound imaging.

Now, MIT engineers have come up with an alternative to conventional ultrasound that doesn’t require contact with the body to see inside a patient. The new laser ultrasound technique leverages an eye- and skin-safe laser system to remotely image the inside of a person. When trained on a patient’s skin, one laser remotely generates sound waves that bounce through the body. A second laser remotely detects the reflected waves, which researchers then translate into an image similar to conventional ultrasound.

In a paper published today by Nature in the journal Light: Science and Applications, the team reports generating the first laser ultrasound images in humans. The researchers scanned the forearms of several volunteers and observed common tissue features such as muscle, fat, and bone, down to about 6 centimeters below the skin. These images, comparable to conventional ultrasound, were produced using remote lasers focused on a volunteer from half a meter away.

“We’re at the beginning of what we could do with laser ultrasound,” says Brian W. Anthony, a principal research scientist in MIT’s Department of Mechanical Engineering and Institute for Medical Engineering and Science (IMES), a senior author on the paper. “Imagine we get to a point where we can do everything ultrasound can do now, but at a distance. This gives you a whole new way of seeing organs inside the body and determining properties of deep tissue, without making contact with the patient.”

Early concepts for noncontact laser ultrasound for medical imaging originated from a Lincoln Laboratory program established by Rob Haupt of the Active Optical Systems Group and Chuck Wynn of the Advanced Capabilities and Technologies Group, who are co-authors on the new paper along with Matthew Johnson. From there, the research grew via collaboration with Anthony and his students, Xiang (Shawn) Zhang, who is now an MIT postdoc and is the paper’s first author, and recent doctoral graduate Jonathan Fincke, who is also a co-author. The project combined the Lincoln Laboratory researchers’ expertise in laser and optical systems with the Anthony group's experience with advanced ultrasound systems and medical image reconstruction.

Yelling into a canyon — with a flashlight

In recent years, researchers have explored laser-based methods in ultrasound excitation in a field known as photoacoustics. Instead of directly sending sound waves into the body, the idea is to send in light, in the form of a pulsed laser tuned at a particular wavelength, that penetrates the skin and is absorbed by blood vessels.

The blood vessels rapidly expand and relax — instantly heated by a laser pulse then rapidly cooled by the body back to their original size — only to be struck again by another light pulse. The resulting mechanical vibrations generate sound waves that travel back up, where they can be detected by transducers placed on the skin and translated into a photoacoustic image.

While photoacoustics uses lasers to remotely probe internal structures, the technique still requires a detector in direct contact with the body in order to pick up the sound waves. What’s more, light can only travel a short distance into the skin before fading away. As a result, other researchers have used photoacoustics to image blood vessels just beneath the skin, but not much deeper.

Since sound waves travel further into the body than light, Zhang, Anthony, and their colleagues looked for a way to convert a laser beam’s light into sound waves at the surface of the skin, in order to image deeper in the body.

Based on their research, the team selected 1,550-nanometer lasers, a wavelength which is highly absorbed by water (and is eye- and skin-safe with a large safety margin). As skin is essentially composed of water, the team reasoned that it should efficiently absorb this light, and heat up and expand in response. As it oscillates back to its normal state, the skin itself should produce sound waves that propagate through the body.

The researchers tested this idea with a laser setup, using one pulsed laser set at 1,550 nanometers to generate sound wav"
234;234;towardsdatascience.com;https://towardsdatascience.com/how-we-have-beaten-the-crypto-market-using-machine-learning-a45e8a7dbdcd?source=collection_home---4------1-----------------------;2020-04-18;How We Managed to Beat the Crypto Market Using Machine Learning;"We lived in 5-star hotels, worked on trading bots and gambled in local casinos for a break. It was a surreal experience that completely changed my career. Upon leaving, I was confident that I’d take on beating the market myself, but years have passed, and I haven’t got into it.

While that experience was inspiring, it was also quite demotivating. The only person I knew who managed to beat the market was clearly out of my league, both intellectually and psychologically. He had a brilliant mind and an outstanding ability to handle stress. I had severe doubts about whether I was good enough.

Later on, I started building a company, and while hiring software developers and data scientists, I always had this potential project in mind. At some point, we seemingly got the right people, but it was too hard to justify the risk. Imagine that you’re a senior data scientist at Google making half a million a year. What are the chances you can quit your job and beat the market significantly enough to make it worth it? I would not take this risk. Similarly, we were doing some generously compensated client work and the idea of getting our top talent busy with an at least year-long experiment that would most likely fail… was simply scary.

What changed my mind was one of our projects, a liquidity provider. It was growing from primitive markets, got more and more clients among crypto derivative exchanges and was being used under increasingly competitive requirements. We hit the limits of our ability to keep the profit above zero — as our old approach was about not doing anything stupid rather than doing something particularly smart. We decided to move the goalpost from not losing money to beating the market as a way to push ourselves to the next level. Even if we failed (which was highly probable), the insights we would get during this process would most likely improve our liquidity provider.

We started building a team. We were lucky enough to have access to a mature trading infrastructure, so we could focus on data science. The core of the team were our two most experienced data scientists in time series (predictions) and clustering (market regimes) and a data engineer who helped with data processing, backtesting and tooling. Also, I was responsible for research and management. Overall, we put together a pretty well-balanced team, and it was time to get our hands dirty.

It was intense. We treated this project as a kind of intellectual war. We did not discuss or plan it; it just naturally worked out this way. It felt like this attitude was the only way to overcome the enormous gap that was separating us from the people who can consistently beat the market. We worked at full speed; the drive was going through the roof. It was R&D in its pure form: exhausting in the most exciting way possible.

The first two months were fun. We got improvement after improvement. It seemed like we were getting so close to profitability... But then we hit a plateau, and over the next two months we made next to zero progress, and there was a growing feeling of discouragement. Psychologically it was a very tough period. Can we do this? Aren’t we wasting our time?

There were several reasons behind that plateau. First, the initial progress was mostly driven by the publicly available information, and we didn’t notice how fast we got into the territory where you’re mostly on your own. Second, the further you go, the more complex and time-consuming ideas/experiments get. Third, due to overconfidence caused by the initial progress, we spread our resources too thin by working on too many things.

It was clear we needed to shake things up, so we added more structure, switched from creative chaos to a more mature and sustainable process with more granular tasks that gave us doping in the form of regular progress. Even if sometimes it was a little artificial, the motivation coming out of it made it worth it.

And we definitely got lucky. I recall at least three occasions when we got a breakthrough out of nowhere. For example, at some point, we struggled with detecting trends under specific market conditions. Among other things, we were playing with an idea that was mentioned between the lines in a hardly relevant white paper. It turned out to be a game changer. We haven’t seen that idea anywhere else, so the chances of finding it were slim to none. We would probably have eventually discovered it ourselves, but it would have taken longer, and we may have needed to cancel the project by that time. When margins are that thin, luck becomes crucial.

After about six and a half months of hard work, we saw the first consistently profitable version. Last week, almost three months later, it has reached $100k in profit. It felt like we finished the first milestone, so I wanted to sit down and put all the thoughts together.

Below, I will cover some more technical topics in detail.

Market making

On the surface, market making is a “get rich fast” strategy. Let’s "
235;235;machinelearningmastery.com;https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/;2017-10-31;Gentle Introduction to Statistical Language Modeling and Neural Language Models;"Tweet Share Share

Last Updated on August 7, 2019

Language modeling is central to many important natural language processing tasks.

Recently, neural-network-based language models have demonstrated better performance than classical methods both standalone and as part of more challenging natural language processing tasks.

In this post, you will discover language modeling for natural language processing.

After reading this post, you will know:

Why language modeling is critical to addressing tasks in natural language processing.

What a language model is and some examples of where they are used.

How neural networks can be used for language modeling.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Updated Jun/2019: Added links to step-by-step language model tutorials.

Overview

This post is divided into 3 parts; they are:

Problem of Modeling Language Statistical Language Modeling Neural Language Models

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Problem of Modeling Language

Formal languages, like programming languages, can be fully specified.

All the reserved words can be defined and the valid ways that they can be used can be precisely defined.

We cannot do this with natural language. Natural languages are not designed; they emerge, and therefore there is no formal specification.

There may be formal rules for parts of the language, and heuristics, but natural language that does not confirm is often used. Natural languages involve vast numbers of terms that can be used in ways that introduce all kinds of ambiguities, yet can still be understood by other humans.

Further, languages change, word usages change: it is a moving target.

Nevertheless, linguists try to specify the language with formal grammars and structures. It can be done, but it is very difficult and the results can be fragile.

An alternative approach to specifying the model of the language is to learn it from examples.

2. Statistical Language Modeling

Statistical Language Modeling, or Language Modeling and LM for short, is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede it.

Language modeling is the task of assigning a probability to sentences in a language. […] Besides assigning a probability to each sequence of words, the language models also assigns a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words

— Page 105, Neural Network Methods in Natural Language Processing, 2017.

A language model learns the probability of word occurrence based on examples of text. Simpler models may look at a context of a short sequence of words, whereas larger models may work at the level of sentences or paragraphs. Most commonly, language models operate at the level of words.

The notion of a language model is inherently probabilistic. A language model is a function that puts a probability measure over strings drawn from some vocabulary.

— Page 238, An Introduction to Information Retrieval, 2008.

A language model can be developed and used standalone, such as to generate new sequences of text that appear to have come from the corpus.

Language modeling is a root problem for a large range of natural language processing tasks. More practically, language models are used on the front-end or back-end of a more sophisticated model for a task that requires language understanding.

… language modeling is a crucial component in real-world applications such as machine-translation and automatic speech recognition, […] For these reasons, language modeling plays a central role in natural-language processing, AI, and machine-learning research.

— Page 105, Neural Network Methods in Natural Language Processing, 2017.

A good example is speech recognition, where audio data is used as an input to the model and the output requires a language model that interprets the input signal and recognizes each new word within the context of the words already recognized.

Speech recognition is principally concerned with the problem of transcribing the speech signal as a sequence of words. […] From this point of view, speech is assumed to be a generated by a language model which provides estimates of Pr(w) for all word strings w independently of the observed signal […] THe goal of speech recognition is to find the most likely word sequence given the observed acoustic signal.

— Pages 205-206, The Oxford Handbook of Computational Linguistics, 2005.

Similarly, language models are used to generate text in many similar natural language processing tasks, for example:

Optical Character Recognition

Handwriting Recognition.

Machine Translation.

Spe"
236;236;news.mit.edu;http://news.mit.edu/2020/mit-entrepreneur-innovation-covid-19-0402;;MIT’s entrepreneurial ecosystem steps up to the challenge of Covid-19;"Innovation and entrepreneurship aren’t easy. New companies are forced to make due with minimal resources. Decisions must be made in the face of great uncertainty. Conditions change rapidly.

Perhaps unsurprisingly then, MIT’s I&E community has stepped up to the unforeseen challenges of the Covid-19 pandemic. Groups from many corners of the Institute are adapting to the myriad disruptions brought on by the emergency and spearheading efforts to help the people most affected.

At a time when most students would be on spring break, many were collaborating on projects and participating in hacking workshops to respond to Covid-19. And as faculty and staff develop new curricula and support structures, they’re focusing on the needs of their students with the same devotion entrepreneurs must focus on their customers.

Above all, members of the MIT community have treated the challenges presented by Covid-19 as opportunities to help. Perhaps nowhere is that more apparent than the Covid-19 Rapid Innovation Dashboard, which was just a rough idea as recently as March 16, but is now a bustling hub of MIT’s Covid-19-related activities. Projects on the dashboard include an initiative to help low-income K-12 students with school shutdowns, an effort leveraging behavioral science to reduce the spread of misinformation about the virus, and multiple projects aimed at improving access to ventilators.

People following those projects would hardly suspect the participants have been uprooted from their lives and forced to radically change the way they work.

“We never would’ve wished this on anybody, but I feel like we’re ready for it,” says Bill Aulet, the managing director of the Martin Trust Center for MIT Entrepreneurship and a professor of the practice at MIT’s Sloan School of Management. “Working in an environment of great change, if you’re a great entrepreneur, is playing to your strengths. I think the students will rise to the occasion, and that’s what we’re seeing now.”

The Rapid Innovation Dashboard

In the second week of March, as the global consequences of Covid-19’s spread were becoming apparent, members of the MIT Innovation Initiative began getting contacted by members of the MIT community looking for ways to help.

Most people wanted information on the various grassroots projects that had sprouted up around campus to address disruptions related to the spread of the virus. Some people were looking for ways to promote their projects and get support.

MITii’s team began brainstorming ways to help fill in those gaps, officially beginning work on the dashboard the week of March 16 — the same time staff members began working remotely.

“From ideation to whiteboarding, to concept, to iteration, to launch, we did it all in real time, and we went from idea to standing the dashboard up in four days,” MITii executive director Gene Keselman says. “It was beautiful for all of us innovation nerds.”

The site launched on March 19 with six projects. Today there are 50 live projects on the site and counting. Some of them deal with mechanical or scientific problems, like the aforementioned efforts to improve access to ventilators, while others are more data-focused, like an initiative to track the spread of the virus at the county level. Still others are oriented toward wellness, like a collection of MIT-related coloring pages for destressing.

“A lot of the things we’re seeing are data-driven, creative-driven projects to get people involved and get them feeling like they’re making an impact,” Keselman says.

The current dashboard is version 1.0 of an ongoing project that will continue to evolve based on the community’s needs. Down the line, the MITii team is considering ways to better connect the MIT community with investors looking to fund projects related to the virus.

“This is going to be a long term problem, and even when we go back to the office, issues will persist, we’ll be dealing with things that are the runoff from Covid-19,” Keselman says. “There will be an opportunity to keep this thing going to solve all kinds of second- or third-order problems.”

Overcoming adversity

The dashboard is just one example of how different entrepreneurial organizations on campus are stepping up to the challenges of Covid-19. The Trust Center is encouraging students to leverage its Orbit app, to get help from entrepreneurs in residence, engage with other members of MIT’s entrepreneurial community, and navigate MIT’s myriad entrepreneurial resources. And in response to Covid-19, the Trust Center launched the Antifragile Entrepreneurship Speaker Series to provide thought leadership to students.

“We’ve revitalized our speaker series,” Aulet says. “We used to fly people in, but now we can have anyone. They’re sitting at home, they’re bored, and we can have more interaction than we did before. We try to create antifragile humans, and antifragile humans excel in times like this.”

MIT D-Lab, where hands-on learning and common makerspaces are c"
237;237;news.mit.edu;http://news.mit.edu/2020/stimulus-jonathan-parker-economic-recovery-0330;;3 Questions: Jonathan Parker on building an economic recovery;"The Covid-19 pandemic is a public health crisis with enormous economic implications: As much of the U.S. reduces daily activity in spring 2020, unemployment is already surging and experts are forecasting major drops in GDP during the second quarter of the year. U.S. Congress has also just passed a $2 trillion aid package for individuals and businesses.

To assess the current state of the economy, MIT News contacted Jonathan Parker, the Robert C. Merton Professor of Finance at the MIT Sloan School of Management. Among his other areas of research, Parker is a leading expert in understanding how U.S. citizens use stimulus payments from the government, and how big an impact such efforts make on GDP and the macroeconomy.

Q: What are the particular effects of the Covid-19 pandemic on the economy, and how should economic policy be used to respond?

A: Unlike in the typical recession, the main responsibility of our government today is not directly economic policy. First and foremost, we have to focus on winning the medical war against the virus. This not only saves lives, but is also the best way to help the economy. However, the war hasn’t gone well at this point, and for good public health reasons we have shut down large parts of our economy. People are not going to work, producing goods, and earning income, and people are avoiding the types of consumption that would put them in crowded places. So, there is going to be a huge collapse in GDP and national income.

Q: The U.S. Congress just passed a $2 trillion aid package to help compensate for the drastic economic slowdown. To what extent can such policy measures maintain incomes?

A: There is no way for us to make up the lost income, because we have lost it by not producing the goods and services that earn it. That said, we can transfer money to people so that the most vulnerable people don’t lose access completely to the goods and services that we do have. And that is part of what House and Senate leaders have just done in passing the recent relief package. The bill includes what are now being called “stimulus payments” to send around $1,200 out to American households. [The package also includes enhanced unemployment insurance for many people, as well as other aid for people adversely affected by the shutdown.]

While this is called stimulus, it is better thought of as disaster insurance for now. We don’t want the economy stimulated. People should be staying home. But the hardest hit need to be able to pay bills and eat. Ideally, we would freeze time for the period when we are isolating, to limit the spread of the virus and allow the government to catch up with the production of virus-wartime medical supplies like ventilators and masks and test kits, so that we can move from isolating all of us to isolating only the sick. And then having frozen time, we would restart the economy where we were before. Sending out checks to people allows those at the bottom of the income and wealth distribution to survive this freeze, and is part of restarting where we left off.

Q: Don’t we need to give significant funds to businesses for the same reason?

A: No, and yes. Starting with “no,” we don’t have to give funds to large firms, or even make them favorable loans. In the American economic system, when large companies that are profitable in the long run go bankrupt, they continue to operate and employ Americans, and emerge from bankruptcy sometimes stronger than before. This happened for General Motors in the financial crisis, and American Airlines operated for years in bankruptcy. For large companies, bankruptcy is only about the division of profits between stockholders and bondholders, not about whether the company continues to operate, so loans and transfers to large corporations almost exclusively benefit the stockholders.

U.S. stocks are owned by the very wealthiest people all over the world, and I think it is a mistake for the stimulus program to be transferring money from taxpayers to the world’s wealthiest people right now (or any time). The parts of the $2 trillion bill that are for supporting large firms are incorrectly fighting the last war. In 2008, the government supported banks because they were all threatened and, like Lehman Brothers, they cannot survive bankruptcy. So, this aspect of the current legislation is a mistake.

But there is an important answer of “yes,” also. First, in crisis times, there is a large increase in the demand for money and safe money-like assets so that financial markets can function. The Federal Reserve is tasked with providing the money and money-like assets that are appropriate with the demands of businesses, and it is doing this nicely. This type of support makes the taxpayer money, so it’s a win-win situation, not a bailout. Of course, this legislation also has the Treasury involved and is supporting private bond markets, and while this can also help, we have to look more closely at what is and is not a subsidy from taxpayers "
238;238;news.mit.edu;http://news.mit.edu/2020/refrigerator-super-cools-molecules-nanokelvin-temperatures-0408;;New “refrigerator” super-cools molecules to nanokelvin temperatures;"For years, scientists have looked for ways to cool molecules down to ultracold temperatures, at which point the molecules should slow to a crawl, allowing scientists to precisely control their quantum behavior. This could enable researchers to use molecules as complex bits for quantum computing, tuning individual molecules like tiny knobs to carry out multiple streams of calculations at a time.

While scientists have super-cooled atoms, doing the same for molecules, which are more complex in their behavior and structure, has proven to be a much bigger challenge.

Now MIT physicists have found a way to cool molecules of sodium lithium down to 200 billionths of a Kelvin, just a hair above absolute zero. They did so by applying a technique called collisional cooling, in which they immersed molecules of cold sodium lithium in a cloud of even colder sodium atoms. The ultracold atoms acted as a refrigerant to cool the molecules even further.

Collisional cooling is a standard technique used to cool down atoms using other, colder atoms. And for more than a decade, researchers have attempted to supercool a number of different molecules using collisional cooling, only to find that when molecules collided with atoms, they exchanged energy in such a way that the molecules were heated or destroyed in the process, called “bad” collisions.

In their own experiments, the MIT researchers found that if sodium lithium molecules and sodium atoms were made to spin in the same way, they could avoid self-destructing, and instead engaged in “good” collisions, where the atoms took away the molecules’ energy, in the form of heat. The team used precise control of magnetic fields and an intricate system of lasers to choreograph the spin and the rotational motion of the molecules. As result, the atom-molecule mixture had a high ratio of good-to-bad collisions and was cooled down from 2 microkelvins to 220 nanokelvins.

“Collisional cooling has been the workhorse for cooling atoms,” adds Nobel Prize laureate Wolfgang Ketterle, the John D. Arthur professor of physics at MIT. “I wasn’t convinced that our scheme would work, but since we didn’t know for sure, we had to try it. We know now that it works for cooling sodium lithium molecules. Whether it will work for other classes of molecules remains to be seen.”

Their findings, published today in the journal Nature, mark the first time researchers have successfully used collisional cooling to cool molecules down to nanokelvin temperatures.

Ketterle’s coauthors on the paper are lead author Hyungmok Son, a graduate student in Harvard University’s Department of Physics, along with MIT physics graduate student Juliana Park, and Alan Jamison, a professor of physics and member of the Institute for Quantum Computing at the University of Waterloo and visiting scientist in MIT’s Research Laboratory of Electronics.

Reaching ultralow temperatures

In the past, scientists found that when they tried to cool molecules down to ultracold temperatures by surrounding them with even colder atoms, the particles collided such that the atoms imparted extra energy or rotation to the molecules, sending them flying out of the trap, or self-destructing all together by chemical reactions.

The MIT researchers wondered whether molecules and atoms, having the same spin, could avoid this effect, and remain ultracold and stable as a result. They looked to test their idea with sodium lithium, a “diatomic” molecule that Ketterle’s group experiments with regularly, consisting of one lithium and one sodium atom.

“Sodium lithium molecules are quite different from other molecules people have tried,” Jamison says. “Many folks expected those differences would make cooling even less likely to work. However, we had a feeling these differences could be an advantage instead of a detriment.”

The researchers fine-tuned a system of more than 20 laser beams and various magnetic fields to trap and cool atoms of sodium and lithium in a vacuum chamber, down to about 2 microkelvins — a temperature Son says is optimal for the atoms to bond together as sodium lithium molecules.

Once the researchers were able to produce enough molecules, they shone laser beams of specific frequencies and polarizations to control the quantum state of the molecules and carefully tuned microwave fields to make atoms spin in the same way as the molecules. “Then we make the refrigerator colder and colder,” says Son, referring to the sodium atoms that surround the cloud of the newly formed molecules. “We lower the power of the trapping laser, making the optical trap looser and looser, which brings the temperature of sodium atoms down, and further cools the molecules, to 200 billionths of a kelvin.”

The group observed that the molecules were able to remain at these ultracold temperatures for up to one second. “In our world, a second is very long,” Ketterle says. “What you want to do with these molecules is quantum computation and exploring new materials, whic"
239;239;news.mit.edu;http://news.mit.edu/2019/hello-world-hello-mit;;Hello, World. Hello, MIT.;"MIT will celebrate the launch of the new MIT Stephen A. Schwarzman College of Computing with three days of event programming that will explore the future of computing education and research, and offer insights from leading experts in computer science, artificial intelligence (AI), education, ethics, and more. Events take place Tuesday, Feb. 26 – Thursday, Feb. 28, with most activities in the Kresge Auditorium. Reporters are invited to attend.

The celebration opens with a range of student-centered experiences across campus, including a panel discussion on Tuesday with pioneering women working in the field of computing. It continues on Wednesday with a full-day academic symposium followed by an evening fireside chat with MIT’s recipients of the Turing Award, often described as the “Nobel Prize of computing.”

Those activities build up to a Thursday program designed to engage a broad audience. Massachusetts Governor Charlie Baker will kick off Thursday’s morning program, featuring TED-style talks from MIT faculty and remarks by Alphabet’s Eric Schmidt, as well as a panel of entrepreneurs and investors on computing for the marketplace. The afternoon features a Q&A session with MIT President L. Rafael Reif and Stephen A. Schwarzman, the chairman, CEO, and co-founder of Blackstone; a discussion led by New York Times columnist and author Tom Friedman with former U.S. Secretary of State Henry Kissinger; and a panel discussion on “Computing for the People” chaired by Prof. Melissa Nobles, dean of MIT’s School of Humanities, Arts, and Social Sciences, and moderated by Friedman.

Announced in October 2018, the MIT Stephen A. Schwarzman College of Computing was established as part of MIT’s $1 billion commitment to advancing the study of computing and AI and its ethical and societal implications, while also integrating computer science education with every academic discipline at MIT. The new college is slated to open in September 2019 and will serve as an interdisciplinary hub for work in computer science, AI, data science, and related fields.

Additional details about the MIT Stephen A. Schwarzman College of Computing launch celebration are available at: https://helloworld.mit.edu. The celebration theme – “Hello, World. Hello, MIT.” – is a nod to a tradition in computer science of using “Hello, World” when testing a new program.

MEDIA RSVP:

Journalists interested in attending are encouraged to email Abby Abazorius at abbya@mit.edu or expertrequests@mit.edu for more information or to RSVP."
240;240;news.mit.edu;http://news.mit.edu/2020/titans-missing-river-deltas-and-earthly-climate-connection-samuel-birch-0408;;Titan’s missing river deltas and an Earthly climate connection;"“I’ll never forget the moment when I first saw new Cassini data come down from Titan’s surface,” says Samuel Birch. “I was in awe at witnessing this brand new, never-seen-before bit of our solar system.”

Birch explores and models the evolution of the surfaces of planets, moons, and small bodies in the outer solar system, including Saturn’s largest moon, Titan, and the Comet 67P/Churyumov-Gerasimenko — two very different, icy worlds investigated by the spacecraft Cassini and Rosetta. He joins MIT this summer as one of eight recipients of the 2020 Heising-Simons Foundation 51 Pegasi b Fellows bridging planetary science and astronomy, accelerating our understanding of planetary system formation and evolution, and advance new technologies for detecting Earthlike worlds.

Over the years, the Heising-Simons Foundation has generously supported a growing cohort of exoplanet researchers at MIT, including Jason Dittmann, Ian Wong, Ben Rackham, Clara Sousa-Silva, and now Samuel Birch, a research associate from Cornell University. In the coming three years, with support networks, mentorship from MIT Department of Earth, Atmospheric and Planetary Science (EAPS) members like Professor Taylor Perron and Research Scientist Jason Soderblom, and a grant of up to $375,000, Birch will have the space and time to fully explore ideas, deciphering what the surfaces of those objects tell us about their climatological past and potential habitability. He’ll also develop and operate related spacecraft missions and mission concepts that seek to study edges of our solar system.

“I like to think of myself as an explorer of the outer solar system, trying to figure what is shaping the weird landscapes on these icy worlds,” Birch says.

Not quite familiar territory

As scientists learn more about the geophysics of Saturn’s moon Titan, their findings motivate newer and bigger questions that extend to Earth and other planetary bodies, highlighting the need for its continued study. “Titan’s surface is perhaps the most intriguing in our solar system, as there are rivers and seas of liquid methane and sand dunes made of organic plastics — all the result of a dense, nitrogen-dominated atmosphere,” says Birch. With a salty liquid water ocean beneath the surface, and an icy exterior sculpted by rivers, seas, and waves, Titan’s hydrologic cycle is similar to Earth’s. However, when its coastal rivers meet the lakes and sea, they seem to be missing deltas at their ends, Birch says. This may be because deltas like those on Earth do not form (or rarely form) because of differences in materials, dynamics, and coastal conditions. Alternately, their characteristics and representation in Cassini datasets may make them difficult to identify.

To solve this mystery, Birch and MIT researchers will investigate deltaic and river dynamics, using a combination of theoretical, experimental, and numerical modeling, atmospheric simulations, and a re-evaluation of Cassini data for evidence of the resulting landforms. This suite of studies will help them understand what a delta “looks” like and map their distribution, which may unveil a record of Titan’s climate history and reveal how liquid methane has molded its landscapes.

“If we can understand the reasons for the stark differences between Earth and Titan — and with it, the fate of all the mass eroded by Titan’s rivers,” Birch says, “we have the chance to really advance our knowledge of the history of erosion, sea-level, and climate change on Titan.”

Life extensions

This work inherently informs the study of fundamental Earthlike surface processes related to climate and the search for life beyond Earth. Since Titan lacks the complex interplay of diverse physical and chemical processes of Earth’s biosphere — like active tectonics, variable bedrock lithologies, diverse climate zones, vegetation, and (as far as we know) organisms — the moon serves as a natural laboratory for studying the effects of sea-level change on shoreline, river, and delta evolution. Additionally, scientists target deltas because of their high astrobiological potential for harboring life, like those on Mars. Analogous, active environments like Titan’s offer promise for the upcoming Dragonfly mission — when a nuclear-powered, dual-quadcopter will explore the moon, and perhaps these valuable spots.

In the long run, Birch would like to parlay the skills he cultivates here to develop his own research group and continue to participate in missions that address key questions regarding the evolution of planetary surfaces. “I am extremely honored by this opportunity and that the community and the Heising-Simons Foundation value my work … I am fortunate that the mentors I will have at MIT are some of the best in the field,” Birch says, acknowledging the support of his collaborators and advisor, and welcoming the challenge and rewards that the future research will bring. “It is a fantastic opportunity and can’t wait to see what we can all discover on "
241;241;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-cyclegan-models-from-scratch-with-keras/;2019-08-06;How to Implement CycleGAN Models From Scratch With Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141

# example of defining composite models for training cyclegan generators from keras . optimizers import Adam from keras . models import Model from keras . models import Sequential from keras . models import Input from keras . layers import Conv2D from keras . layers import Conv2DTranspose from keras . layers import Activation from keras . layers import LeakyReLU from keras . initializers import RandomNormal from keras . layers import Concatenate from keras_contrib . layers . normalization . instancenormalization import InstanceNormalization from keras . utils . vis_utils import plot_model # define the discriminator model def define_discriminator ( image_shape ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # source image input in_image = Input ( shape = image_shape ) # C64 d = Conv2D ( 64 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( in_image ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C128 d = Conv2D ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C256 d = Conv2D ( 256 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # C512 d = Conv2D ( 512 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # second last output layer d = Conv2D ( 512 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) d = InstanceNormalization ( axis = - 1 ) ( d ) d = LeakyReLU ( alpha = 0.2 ) ( d ) # patch output patch_out = Conv2D ( 1 , ( 4 , 4 ) , padding = 'same' , kernel_initializer = init ) ( d ) # define model model = Model ( in_image , patch_out ) # compile model model . compile ( loss = 'mse' , optimizer = Adam ( lr = 0.0002 , beta_1 = 0.5 ) , loss_weights = [ 0.5 ] ) return model # generator a resnet block def resnet_block ( n_filters , input_layer ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # first layer convolutional layer g = Conv2D ( n_filters , ( 3 , 3 ) , padding = 'same' , kernel_initializer = init ) ( input_layer ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # second convolutional layer g = Conv2D ( n_filters , ( 3 , 3 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) # concatenate merge channel-wise with input layer g = Concatenate ( ) ( [ g , input_layer ] ) return g # define the standalone generator model def define_generator ( image_shape , n_resnet = 9 ) : # weight initialization init = RandomNormal ( stddev = 0.02 ) # image input in_image = Input ( shape = image_shape ) # c7s1-64 g = Conv2D ( 64 , ( 7 , 7 ) , padding = 'same' , kernel_initializer = init ) ( in_image ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # d128 g = Conv2D ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # d256 g = Conv2D ( 256 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # R256 for _ in range ( n_resnet ) : g = resnet_block ( 256 , g ) # u128 g = Conv2DTranspose ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # u64 g = Conv2DTranspose ( 64 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) g = Activation ( 'relu' ) ( g ) # c7s1-3 g = Conv2D ( 3 , ( 7 , 7 ) , padding = 'same' , kernel_initializer = init ) ( g ) g = InstanceNormalization ( axis = - 1 ) ( g ) out_image = Activation ( 'tanh' ) ( g ) # define model model = Model ( in_image , out_image ) return model # define a composite model for updating generators by adversarial and cycle loss def define_composite_model ( g_model_1 , d_model , g_model_2 , image_shape ) : # ensure the model we're updating is trainable g_model_1 . trainable = True # mark discriminator as not trainable d_model . trainable = False # mark other generator model as not trainable g_model_2 . trainable = False # discriminator element input_gen "
242;242;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-an-intuition-for-probability-with-worked-examples/;2019-10-01;How to Develop an Intuition for Probability With Worked Examples;"Tweet Share Share

Last Updated on November 1, 2019

Probability calculations are frustratingly unintuitive.

Our brains are too eager to take shortcuts and get the wrong answer, instead of thinking through a problem and calculating the probability correctly.

To make this issue obvious and aid in developing intuition, it can be useful to work through classical problems from applied probability. These problems, such as the birthday problem, boy or girl problem, and the Monty Hall problem trick us with the incorrect intuitive answer and require a careful application of the rules of marginal, conditional, and joint probability in order to arrive at the correct solution.

In this post, you will discover how to develop an intuition for probability by working through classical thought-provoking problems.

After reading this post, you will know:

How to solve the birthday problem by multiplying probabilities together.

How to solve the boy or girl problem using conditional probability.

How to solve the Monty Hall problem using joint probability.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Birthday Problem Boy or Girl Problem Monty Hall Problem

Birthday Problem

A classic example of applied probability involves calculating the probability of two people having the same birthday.

It is a classic example because the result does not match our intuition. As such, it is sometimes called the birthday paradox.

The problem can be generally stated as:

Problem: How many people are required so that any two people in the group have the same birthday with at least a 50-50 chance?

There are no tricks to this problem; it involves simply calculating the marginal probability.

It is assumed that the probability of a randomly selected person having a birthday on any given day of the year (excluding leap years) is uniformly distributed across the days of the year, e.g. 1/365 or about 0.273%.

Our intuition might leap to an answer and assume that we might need at least as many people as there are days in the year, e.g. 365. Our intuition likely fails because we are thinking about ourselves and other people matching our own birthday. That is, we are thinking about how many people are needed for another person born on the same day as you. That is a different question.

Instead, to calculate the solution, we can think about comparing pairs of people within a group and the probability of a given pair being born on the same day. This unlocks the calculation required.

The number of pairwise comparisons within a group (excluding comparing each person with themselves) is calculated as follows:

comparisons = n * (n – 1) / 2

For example, if we have a group of five people, we would be doing 10 pairwise comparisons among the group to check if they have the same birthday, which is more opportunity for a hit than we might expect. Importantly, the number of comparisons within the group increases exponentially with the size of the group.

One more step is required. It is easier to calculate the inverse of the problem. That is, the probability that two people in a group do not have the same birthday. We can then invert the final result to give the desired probability, for example:

p(2 in n people have the same birthday) = 1 – p(2 in n people do not have the same birthday)

We can see why calculating the probability of non-matching birthdays is easy with an example with a small group, in this case, three people.

People can be added to the group one-by-one. Each time a person is added to the group, it decreases the number of available days where there is no birthday in the year, decreasing the number of available days by one. For example 365 days, 364 days, etc.

Additionally, the probability of a non-match for a given additional person added to the group must be combined with the prior calculated probabilities before it. For example P(n=2) * P(n=3), etc.

This gives the following, calculating the probability of no matching birthdays with a group size of three:

P(n=3) = 365/365 * 364/365 * 363/365

P(n=3) = 99.18%

Inverting this gives about 0.820% of a matching birthday among a group of three people.

Stepping through this, the first person has a birthday, which reduces the number of candidate days for the rest of the group from 365 to 364 unused days (i.e. days without a birthday). For the second person, we calculate the probability of a conflicting birthday as 364 safe days from 365 days in the year or about a (364/365) 99.72% probability of not having the same birthday. We now subtract the second person’s birthday from the number of available days to give 363. The probability of the third person of not having a matching birthday is then given as 363/365 multiplied by the prior probability to give about 99.18%

This ca"
243;243;machinelearningmastery.com;http://machinelearningmastery.com/python-growing-platform-applied-machine-learning/;2016-10-31;Python is the Growing Platform for Applied Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

You should pick the right tool for the job.

The specific predictive modeling problem that you are working on should dictate the specific programming language, libraries and even machine learning algorithms to use.

But, what if you are just getting started and looking for a platform to learn and practice machine learning?

In this post, you will discover that Python is the growing platform for applied machine learning, likely to outpace and topple R in terms of adoption and perhaps capability.

After reading this post you will know:

That search volume for Python machine learning is growing fast and has already outpaced R.

That the percentage of Python machine learning jobs is growing and has already outpaced R.

That Python is used by nearly 50% of polled practitioners and growing.

Discover how to prepare data with pandas, fit and evaluate models with scikit-learn, and more in my new book, with 16 step-by-step tutorials, 3 projects, and full python code.

Let’s get started.

Python for Machine Learning is Growing

Let’s look at 3 areas where we can see Python for machine learning growing:

Search Volume. Job Ads. Professional Tool Usage.

Python Machine Learning Search Volume is Growing

Search volume is probably indicative of students, engineers and other practitioners searching for information to get started or go deeper into the topic.

Google provides a tool called Google Trends that gives insight into the search volume of keywords over time.

We can investigate the growth of “Python machine learning” from 2004 to 2016 (the last 12 years). Below is a graph of the change in search volume for this period:

We can see that the trend upward started in Perhaps 2012 with a steeper rise starting in 2015, likely boosted by Python Deep Learning tools like TensorFlow.

We can also contrast this to the search volume for R machine learning and we can see that from about the middle of 2015, Python machine learning has been beating out R.

Blue denotes “Python Machine Learning” and red denotes “R Machine Learning”.

Python Machine Learning Jobs are Growing

Indeed is a job search website and like Google trends, they show the volume of job ads that match keywords.

We can investigate the demand for “python machine learning jobs” for the last 4 years.

We can see time along the x-axis and the percentage of job postings that match the keyword. The graph shows almost linear growth from 2012 to 2015 with a hockey-stick like increase in 2016.

We can also compare the job ads for python and R.

Blue shows “Python machine learning” and orange shows “R machine learning”.

We see a more pronounced story compared to Google search volume. The percentage of job ads available to indeed.com shows that demand for Python machine learning skills has been dominating R machine learning skills since at least 2012 with the gap only widening in recent years.

KDNuggets Survey Results: More People Using Python for Machine Learning

We can get some insight into the tools used by machine learning practitioners by reviewing the results for the KDnuggets Software Poll Results.

Here’s a quote from the 2016 results:

R remains the leading tool, with 49% share, but Python grows faster and almost catches up to R.

— Gregory Piatetsky

The poll tracks the tools used by machine learning and data science professionals, where a participant can select more than one tool (which is the norm I would expect)

Here is the growth of Python for machine learning over the last 4 years:

2016 45.8% 2015 30.3% 2014 19.5% 2013 13.3% 1 2 3 4 2016 45.8% 2015 30.3% 2014 19.5% 2013 13.3%

Below is a plot of this growth.

We can see a near linear growth trend where Python s used by just under 50% of profesionals in 2016.

It is important to note that the number of participants in the poll has also grown from many hundreds to thousands in recent years and participants are self-selected.

What is interesting is that scikit-learn also appears separately on the poll, accounting for 17.2%.

For more information see: KDnuggets 2016 Software Poll Results.

O’Reilly Survey Results: More People Using Python for Machine Learning

O’Reilly performs an annual Data Science Salary Survey.

They collect a lot of data from professional data scientists and machine learning practitioners and present the results in very nice reports. For example, here is the 2016 Data Science Salary Survey report [View the PDF Report].

The survey tracks tool usage of practitioners, and as with the KDNuggets data.

Quoting from the key findings from the 2016 report, we can see that Python plays an important role in data science salary.

Python and Spark are among the tools that contribute most to salary.

— Page 1, 2016 Data Science Salary Survey report.

Reviewing the survey results, we can see a similar growth trend in use of the use of the Python ecosystem for machine learning over the last 4 years.

2016 54% 2015 51% 2014 42% (interpreted from "
244;244;machinelearningmastery.com;http://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/;2016-08-30;Feature Importance and Feature Selection With XGBoost in Python;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37

# use feature importance for feature selection from numpy import loadtxt from numpy import sort from xgboost import XGBClassifier from sklearn . model_selection import train_test_split from sklearn . metrics import accuracy_score from sklearn . feature_selection import SelectFromModel # load data dataset = loadtxt ( 'pima-indians-diabetes.csv' , delimiter = "","" ) # split data into X and y X = dataset [ : , 0 : 8 ] Y = dataset [ : , 8 ] # split data into train and test sets X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.33 , random_state = 7 ) # fit model on all training data model = XGBClassifier ( ) model . fit ( X_train , y_train ) # make predictions for test data and evaluate y_pred = model . predict ( X_test ) predictions = [ round ( value ) for value in y_pred ] accuracy = accuracy_score ( y_test , predictions ) print ( ""Accuracy: %.2f%%"" % ( accuracy * 100.0 ) ) # Fit model using each importance as a threshold thresholds = sort ( model . feature_importances_ ) for thresh in thresholds : # select features using threshold selection = SelectFromModel ( model , threshold = thresh , prefit = True ) select_X_train = selection . transform ( X_train ) # train model selection_model = XGBClassifier ( ) selection_model . fit ( select_X_train , y_train ) # eval model select_X_test = selection . transform ( X_test ) y_pred = selection_model . predict ( select_X_test ) predictions = [ round ( value ) for value in y_pred ] accuracy = accuracy_score ( y_test , predictions ) print ( ""Thresh=%.3f, n=%d, Accuracy: %.2f%%"" % ( thresh , select_X_train . shape [ 1 ] , accuracy* 100.0 ) )"
245;245;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/;2019-05-19;How to Develop a Deep CNN for Multi-Label Classification of Photos;"Tweet Share Share

Last Updated on January 10, 2020

The Planet dataset has become a standard computer vision benchmark that involves multi-label classification or tagging the contents satellite photos of Amazon tropical rainforest.

The dataset was the basis of a data science competition on the Kaggle website and was effectively solved. Nevertheless, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch.

This includes how to develop a robust test harness for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.

In this tutorial, you will discover how to develop a convolutional neural network to classify satellite photos of the Amazon tropical rainforest.

After completing this tutorial, you will know:

How to load and prepare satellite photos of the Amazon tropical rainforest for modeling.

How to develop a convolutional neural network for photo classification from scratch and improve model performance.

How to develop a final model and use it to make ad hoc predictions on new data.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Update Sept/2019 : Provided more guidance in the explanation for downloading the dataset.

: Provided more guidance in the explanation for downloading the dataset. Update Oct/2019: Updated for Keras 2.3.0 and TensorFlow 2.0.0.

Tutorial Overview

This tutorial is divided into seven parts; they are:

Introduction to the Planet Dataset How to Prepare Data for Modeling Model Evaluation Measure How to Evaluate a Baseline Model How to Improve Model Performance How to use Transfer Learning How to Finalize the Model and Make Predictions

Introduction to the Planet Dataset

The “Planet: Understanding the Amazon from Space” competition was held on Kaggle in 2017.

The competition involved classifying small squares of satellite images taken from space of the Amazon rainforest in Brazil in terms of 17 classes, such as “agriculture“, “clear“, and “water“. Given the name of the competition, the dataset is often referred to simply as the “Planet dataset“.

The color images were provided in both TIFF and JPEG format with the size 256×256 pixels. A total of 40,779 images were provided in the training dataset and 40,669 images were provided in the test set for which predictions were required.

The problem is an example of a multi-label image classification task, where one or more class labels must be predicted for each label. This is different from multi-class classification, where each image is assigned one from among many classes.

The multiple class labels were provided for each image in the training dataset with an accompanying file that mapped the image filename to the string class labels.

The competition was run for approximately four months (April to July in 2017) and a total of 938 teams participated, generating much discussion around the use of data preparation, data augmentation, and the use of convolutional neural networks.

The competition was won by a competitor named “bestfitting” with a public leaderboard F-beta score of 0.93398 on 66% of the test dataset and a private leaderboard F-beta score of 0.93317 on 34% of the test dataset. His approach was described in the post “Planet: Understanding the Amazon from Space, 1st Place Winner’s Interview” and involved a pipeline and ensemble of a large number of models, mostly convolutional neural networks with transfer learning.

It was a challenging competition, although the dataset remains freely available (if you have a Kaggle account), and provides a good benchmark problem for practicing image classification with convolutional neural networks for aerial and satellite datasets.

As such, it is routine to achieve an F-beta score of greater than 80 with a manually designed convolutional neural network and an F-beta score 89+ using transfer learning on this task.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

How to Prepare Data for Modeling

The first step is to download the dataset.

In order to download the data files, you must have a Kaggle account. If you do not have a Kaggle account, you can create one here: Kaggle Homepage.

The dataset can be downloaded from the Planet Data page. This page lists all of the files provided for the competition, although we do not need to download all of the files.

Before you can download the dataset, you must click the “Join Competition” button. You may need to agree to the competition rules, then the dataset will be available for download.

To download a "
246;246;machinelearningmastery.com;http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/;2014-04-15;A Gentle Introduction to Scikit-Learn;"# Sample Decision Tree Classifier

from sklearn import datasets

from sklearn import metrics

from sklearn . tree import DecisionTreeClassifier

# load the iris datasets

dataset = datasets . load_iris ( )

# fit a CART model to the data

model = DecisionTreeClassifier ( )

model . fit ( dataset . data , dataset . target )

print ( model )

# make predictions

expected = dataset . target

predicted = model . predict ( dataset . data )

# summarize the fit of the model

print ( metrics . classification_report ( expected , predicted ) )"
247;247;towardsdatascience.com;https://towardsdatascience.com/serial-dependence-in-binary-sequences-409c5e8f54d0?source=collection_home---4------2-----------------------;2020-04-17;Serial dependence in binary sequences;"In this blog post, I am going to investigate the serial (aka temporal) dependence phenomenon in random binary sequences.

Random binary sequences are sequences of zeros and ones generated by random processes. Primary outputs generated by most random number generators are binary. Binary sequences often encode the occurrence of random events:

Extreme returns on financial markets.

Failures of machines, servers,…

Value-at-Risk exceedance indicators in financial market risk models.

In many of those situations, it is necessary to ensure that the tracked events occur independently of each other. For example, the occurrence of an incident in a production system should not make the system more incident prone. For that, we need to take a close look at the dependence structure of the binary sequence under consideration.

Outline

In this blog post, I am going to develop and test a scoring method for quantifying the dependence strength in random binary sequences. The Meixner Dependency Score is easy to implement and is based on the orthogonal polynomials associated with Geometric distribution.

After reading through this blog post, you will know:

How the problem of serial dependency measurement can be formulated in statistical terms. How to derive the Meixner polynomials from the Geometric distribution. How to calculate the Meixner Dependency Score for a waiting times sequence to quantify the dependence strength. How to test dependency scoring methods using simple Monte-Carlo experiments involving Markov Chains with a known dependency structure.

Statistical formulation of the problem

Given a sequence of random variables X = (X[0], X[1], …, X[n]) taking values in the set {0, 1} and a probability p∈(0,1), I would like to investigate the serial dependencies between the elements of X. This investigation should be based on a sample x drawn from X, i.e. a finite sequence of zeros and ones (x[0], x[1],..., x[n]). This is a very difficult and deep problem, and in this blog post I am going to focus on collecting evidence to support or reject the following two basic hypotheses:

The random variables X[i] have the distribution Ber(p) (Bernoulli distribution with the success probability p). The random variables X[i] are independent.

If we assume that the elements of x are independent samples from a fixed Bernoulli random variable X[0], then we can estimate the probability p by calculating the average of x[0], x[1], …, x[n]. This is because the expectation of X[0] is 𝔼 X[0] = p.

To set an appropriate context for the serial dependence detection problem, let me remark that random binary sequences x are often associated with discreet observations of a system with two states. We say that “an event” occurred at time i, if x[0] = 1.

How can we check whether our events occur independently of each other and gather evidence that there is no serial dependence between the event times?

We often tend to see serial dependence in situations where there is none. Typical examples are the “winning or lucky streaks” experienced by gamblers in casinos.

To rigorously investigate the question of serial dependence in the sequence X based on the observation x, we can look at the distribution of waiting times between the events. For example the sequence

[0, 1, 1, 0, 0, 1, 0, 0]

yields the following sequence of waiting times

[1, 3].

Note that the waiting time calculation discards the initial and trailing zeros in the event sequence x.

To formally define the waiting time sequence y = (y[1],…,y[m]) based on x, consider the sequence of indices I=(i[1],i[2],…), such that for each i∈ I we have x[i] =1. We set

For an i.i.d. sequence of Bernoulli random variables, the sequence of waiting times consists of i.i.d. random variables with the geometric distribution. Let’s have a closer look at its properties.

The Geometric Distribution

The probability mass function (PMF) of the geometric distribution with parameter p is given by

The PMF of the geometric distribution with the parameter p=0.1 looks as follows.

The probability mass function of the geometric distribution with the parameter p=0.1

One way of test whether a waiting times sequence follows the geometric distribution is to look at the orthogonal polynomials generated by that distribution.

A family of orthogonal polynomials is intimately tied to every probability distribution μ on ℝ. For any such distribution, we can define a scalar product between (square-integrable) real-valued functions f and g as

where Y is a random variable with distribution μ. For a geometric distribution, the above scalar product takes the form

We say that the functions f and g are orthogonal (with respect to μ) if and only if ⟨f,g⟩ = 0.

Finally, a sequence of polynomials (q[i]) with i≥ 0 is called orthogonal, if and only if, ⟨q[k], q[i]⟩ = 0 for all k ≠ i, and each q[i] has degree i. As a consequence, we have 𝔼 q[i](Y) = 0 for i>0. This is the tool I am going to use to check whether a given Y follows a geometri"
248;248;machinelearningmastery.com;https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/;2018-12-23;How to Create a Bagging Ensemble of Deep Learning Models in Keras;"# bagging mlp ensemble on blobs dataset

from sklearn . datasets import make_blobs

from sklearn . utils import resample

from sklearn . metrics import accuracy_score

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Dense

from matplotlib import pyplot

from numpy import mean

from numpy import std

import numpy

from numpy import array

from numpy import argmax

# evaluate a single mlp model

def evaluate_model ( trainX , trainy , testX , testy ) :

# encode targets

trainy_enc = to_categorical ( trainy )

testy_enc = to_categorical ( testy )

# define model

model = Sequential ( )

model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' ) )

model . add ( Dense ( 3 , activation = 'softmax' ) )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( trainX , trainy_enc , epochs = 50 , verbose = 0 )

# evaluate the model

_ , test_acc = model . evaluate ( testX , testy_enc , verbose = 0 )

return model , test_acc

# make an ensemble prediction for multi-class classification

def ensemble_predictions ( members , testX ) :

# make predictions

yhats = [ model . predict ( testX ) for model in members ]

yhats = array ( yhats )

# sum across ensemble members

summed = numpy . sum ( yhats , axis = 0 )

# argmax across classes

result = argmax ( summed , axis = 1 )

return result

# evaluate a specific number of members in an ensemble

def evaluate_n_members ( members , n_members , testX , testy ) :

# select a subset of members

subset = members [ : n_members ]

# make prediction

yhat = ensemble_predictions ( subset , testX )

# calculate accuracy

return accuracy_score ( testy , yhat )

# generate 2d classification dataset

dataX , datay = make_blobs ( n_samples = 55000 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 )

X , newX = dataX [ : 5000 , : ] , dataX [ 5000 : , : ]

y , newy = datay [ : 5000 ] , datay [ 5000 : ]

# multiple train-test splits

n_splits = 10

scores , members = list ( ) , list ( )

for _ in range ( n_splits ) :

# select indexes

ix = [ i for i in range ( len ( X ) ) ]

train_ix = resample ( ix , replace = True , n_samples = 4500 )

test_ix = [ x for x in ix if x not in train_ix ]

# select data

trainX , trainy = X [ train_ix ] , y [ train_ix ]

testX , testy = X [ test_ix ] , y [ test_ix ]

# evaluate model

model , test_acc = evaluate_model ( trainX , trainy , testX , testy )

print ( '>%.3f' % test_acc )

scores . append ( test_acc )

members . append ( model )

# summarize expected performance

print ( 'Estimated Accuracy %.3f (%.3f)' % ( mean ( scores ) , std ( scores ) ) )

# evaluate different numbers of ensembles on hold out set

single_scores , ensemble_scores = list ( ) , list ( )

for i in range ( 1 , n_splits + 1 ) :

ensemble_score = evaluate_n_members ( members , i , newX , newy )

newy_enc = to_categorical ( newy )

_ , single_score = members [ i - 1 ] . evaluate ( newX , newy_enc , verbose = 0 )

print ( '> %d: single=%.3f, ensemble=%.3f' % ( i , single_score , ensemble_score ) )

ensemble_scores . append ( ensemble_score )

single_scores . append ( single_score )

# plot score vs number of ensemble members

print ( 'Accuracy %.3f (%.3f)' % ( mean ( single_scores ) , std ( single_scores ) ) )

x_axis = [ i for i in range ( 1 , n_splits + 1 ) ]

pyplot . plot ( x_axis , single_scores , marker = 'o' , linestyle = 'None' )

pyplot . plot ( x_axis , ensemble_scores , marker = 'o' )"
249;249;machinelearningmastery.com;http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/;2014-09-25;Discover Feature Engineering, How to Engineer Features and How to Get Good at It;"Tweet Share Share

Last Updated on December 27, 2019

Feature engineering is an informal topic, but one that is absolutely known and agreed to be key to success in applied machine learning.

In creating this guide I went wide and deep and synthesized all of the material I could.

You will discover what feature engineering is, what problem it solves, why it matters, how to engineer features, who is doing it well and where you can go to learn more and get good at it.

If you read one article on feature engineering, I want it to be this one.

feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books, but it is absolutely vital to ML success. […] Much of the success of machine learning is actually success in engineering features that a learner can understand.

— Scott Locklin, in “Neglected machine learning ideas”

Problem that Feature Engineering Solves

When your goal is to get the best possible results from a predictive model, you need to get the most from what you have.

This includes getting the best results from the algorithms you are using. It also involves getting the most out of the data for your algorithms to work with.

How do you get the most out of your data for predictive modeling?

This is the problem that the process and practice of feature engineering solves.

Actually the success of all Machine Learning algorithms depends on how you present the data.

— Mohammad Pezeshki, answer to “What are some general tips on feature selection and engineering that every data scientist should know?”

Importance of Feature Engineering

The features in your data will directly influence the predictive models you use and the results you can achieve.

You can say that: the better the features that you prepare and choose, the better the results you will achieve. It is true, but it also misleading.

The results you achieve are a factor of the model you choose, the data you have available and the features you prepared. Even your framing of the problem and objective measures you’re using to estimate accuracy play a part. Your results are dependent on many inter-dependent properties.

You need great features that describe the structures inherent in your data.

Better features means flexibility.

You can choose “the wrong models” (less than optimal) and still get good results. Most models can pick up on good structure in data. The flexibility of good features will allow you to use less complex models that are faster to run, easier to understand and easier to maintain. This is very desirable.

Better features means simpler models.

With well engineered features, you can choose “the wrong parameters” (less than optimal) and still get good results, for much the same reasons. You do not need to work as hard to pick the right models and the most optimized parameters.

With good features, you are closer to the underlying problem and a representation of all the data you have available and could use to best characterize that underlying problem.

Better features means better results.

The algorithms we used are very standard for Kagglers. […] We spent most of our efforts in feature engineering.

— Xavier Conort, on “Q&A with Xavier Conort” on winning the Flight Quest challenge on Kaggle

What is Feature Engineering?

Here is how I define feature engineering:

Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.

You can see the dependencies in this definition:

The performance measures you’ve chosen (RMSE? AUC?)

The framing of the problem (classification? regression?)

The predictive models you’re using (SVM?)

The raw data you have selected and prepared (samples? formatting? cleaning?)

feature engineering is manually designing what the input x’s should be

— Tomasz Malisiewicz, answer to “What is feature engineering?”

Feature Engineering is a Representation Problem

Machine learning algorithms learn a solution to a problem from sample data.

In this context, feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?

It’s deep. Doing well in machine learning, even in artificial intelligence in general comes back to representation problems. It’s hard stuff, perhaps unknowable (or at best intractable) to know the best representation to use, a priori.

you have to turn your inputs into things the algorithm can understand

— Shayne Miel, answer to “What is the intuitive explanation of feature engineering in machine learning?”

Feature Engineering is an Art

It is an art like engineering is an art, like programming is an art, like medicine is an art.

There are well defined procedures that are methodical, provable and understood.

The data is a variable and is different every time. You get good at deciding which procedures to use and when, by practice. By empirical appren"
250;250;news.mit.edu;http://news.mit.edu/2020/data-feminism-catherine-dignazio-0309;;The elephant in the server room;"Suppose you would like to know mortality rates for women during childbirth, by country, around the world. Where would you look? One option is the WomanStats Project, the website of an academic research effort investigating the links between the security and activities of nation-states, and the security of the women who live in them.

The project, founded in 2001, meets a need by patching together data from around the world. Many countries are indifferent to collecting statistics about women’s lives. But even where countries try harder to gather data, there are clear challenges to arriving at useful numbers — whether it comes to women’s physical security, property rights, and government participation, among many other issues.

For instance: In some countries, violations of women’s rights may be reported more regularly than in other places. That means a more responsive legal system may create the appearance of greater problems, when it provides relatively more support for women. The WomanStats Project notes many such complications.

Thus the WomanStats Project offers some answers — for example, Australia, Canada, and much of Western Europe have low childbirth mortality rates — while also showing what the challenges are to taking numbers at face value. This, according to MIT professor Catherine D’Ignazio, makes the site unusual, and valuable.

“The data never speak for themselves,” says D’Ignazio, referring to the general problem of finding reliable numbers about women’s lives. “There are always humans and institutions speaking for the data, and different people have their own agendas. The data are never innocent.”

Now D’Ignazio, an assistant professor in MIT’s Department of Urban Studies and Planning, has taken a deeper look at this issue in a new book, co-authored with Lauren Klein, an associate professor of English and quantitative theory and methods at Emory University. In the book, “Data Feminism,” published this month by the MIT Press, the authors use the lens of intersectional feminism to scrutinize how data science reflects the social structures it emerges from.

“Intersectional feminism examines unequal power,” write D’Ignazio and Klein, in the book’s introduction. “And in our contemporary world, data is power too. Because the power of data is wielded unjustly, it must be challenged and changed.”

The 4 percent problem

To see a clear case of power relations generating biased data, D’Ignazio and Klein note, consider research led by MIT’s own Joy Buolamwini, who as a graduate student in a class studying facial-recognition programs, observed that the software in question could not “see” her face. Buolamwini found that for the facial-recognition system in question, the software was based on a set of faces which were 78 percent male and 84 percent white; only 4 percent were female and dark-skinned, like herself.

Subsequent media coverage of Buolamwini’s work, D’Ignazio and Klein write, contained “a hint of shock.” But the results were probably less surprising to those who are not white males, they think.

“If the past is racist, oppressive, sexist, and biased, and that’s your training data, that is what you are tuning for,” D’Ignazio says.

Or consider another example, from tech giant Amazon, which tested an automated system that used AI to sort through promising CVs sent in by job applicants. One problem: Because a high percentage of company employees were men, the algorithm favored men’s names, other things being equal.

“They thought this would help [the] process, but of course what it does is train the AI [system] to be biased toward women, because they themselves have not hired that many women,” D’Ignazio observes.

To Amazon’s credit, it did recognize the problem. Moreover, D’Ignazio notes, this kind of issue is a problem that can be addressed. “Some of the technologies can be reformed with a more participatory process, or better training data. … If we agree that’s a good goal, one path forward is to adjust your training set and include more people of color, more women.”

“Who’s on the team? Who had the idea? Who’s benefiting?”

Still, the question of who participates in data science is, as the authors write, “the elephant in the server room.” As of 2011, only 26 percent of all undergraduates receiving computer science degrees in the U.S. were women. That is not only a low figure, but actually a decline from past levels: In 1985, 37 percent of computer science graduates were women, the highest mark on record.

As a result of the lack of diversity in the field, D’Ignazio and Klein believe, many data projects are radically limited in their ability to see all facets of the complex social situations they purport to measure.

“We want to try to tune people in to these kinds of power relationships and why they matter deeply,” D’Ignazio says. “Who’s on the team? Who had the idea? Who’s benefiting from the project? Who’s potentially harmed by the project?”

In all, D’Ignazio and Klein outline seven princ"
251;251;machinelearningmastery.com;http://machinelearningmastery.com/applied-machine-learning-weka-mini-course/;2016-08-14;Weka Machine Learning Mini-Course;"Tweet Share Share

Last Updated on August 22, 2019

Become A Machine Learning Practitioner in 14-Days

Machine learning is a fascinating study, but how do you actually use it on your own problems?

You may be confused as to how best prepare your data for machine learning, which algorithms to use or how to choose one model over another.

In this post you will discover a 14-part crash course into applied machine learning using the Weka platform without a single mathematical equation or line of programming code.

After completing this mini course:

You will know how to work through a dataset end-to-end and deliver a set of predictions or a high-performance model.

You will know your way around the Weka machine learning workbench including how to explore algorithms and design controlled experiments.

You will know how to create multiple views of your problem, evaluate multiple algorithms and use statistics to choose the best performing model for your own predictive modeling problems.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

(Tip: You might want to print or bookmark this page so that you can refer back to it later)

Who Is This Mini-Course For?

Before we get started, let’s make sure you are in the right place. The list below provides some general guidelines as to who this course was designed for.

Don’t panic if you don’t match these points exactly, you might just need to brush up in one area or another to keep up.

You are a developer that knows a little machine learning.

This means you know about some of the basics of machine learning like cross validation, some algorithms and the bias-variance trade-off. It does not mean that you are a machine learning PhD, just that you know the landmarks or know where to look them up.

This mini-course is not a textbook on machine learning.

It will take you from a developer that knows a little machine learning to a developer who can use the Weka platform to work through a dataset from beginning to end and deliver a set of predictions or a high performance model.

Mini-Course Overview (what to expect)

This mini-course is divided into 14 parts.

Each lesson was designed to take you about 30 minutes. You might finish some much sooner and for others you may choose to go deeper and spend more time.

You can complete each part as quickly or as slowly as you like. A comfortable schedule may be to complete one lesson per day over a two week period. Highly recommended.

The topics you will cover over the next 14 lessons are as follows:

Lesson 01 : Download and Install Weka.

: Download and Install Weka. Lesson 02 : Load Standard Machine Learning Datasets.

: Load Standard Machine Learning Datasets. Lesson 03 : Descriptive Stats and Visualization.

: Descriptive Stats and Visualization. Lesson 04 : Rescale Your Data.

: Rescale Your Data. Lesson 05 : Perform Feature Selection on Your Data.

: Perform Feature Selection on Your Data. Lesson 06 : Machine Learning Algorithms in Weka.

: Machine Learning Algorithms in Weka. Lesson 07 : Estimate Model Performance.

: Estimate Model Performance. Lesson 08 : Baseline Performance On Your Data.

: Baseline Performance On Your Data. Lesson 09 : Classification Algorithms.

: Classification Algorithms. Lesson 10 : Regression Algorithms.

: Regression Algorithms. Lesson 11 : Ensemble Algorithms.

: Ensemble Algorithms. Lesson 12 : Compare the Performance of Algorithms.

: Compare the Performance of Algorithms. Lesson 13 : Tune Algorithm Parameters.

: Tune Algorithm Parameters. Lesson 14: Save Your Model.

This is going to be a lot of fun.

You’re going to have to do some work though, a little reading, a little tinkering in Weka. You want to get started in applied machine learning right?

(Tip: All of the answers these lessons can be found on this blog, use the search feature)

Any questions at all, please post in the comments below.

Share your results in the comments.

Hang in there, don’t give up!

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Lesson 01: Download and Install Weka

The first thing to do is install the Weka software on your workstation.

Weka is free open source software. It is written in Java and can run on any platform that supports Java, including:

Windows.

Mac OS X.

Linux.

You can download Weka as standalone software or as a version bundled with Java.

If you do not already have Java installed on your system, I recommend downloading and installing a version bundled with Java.

Your task for this lesson is to visit the Weka download page, download and install Weka on your workstation.

Lesson 02: Load Standard Machine Learning Datasets

Now that you have Weka installed, y"
252;252;machinelearningmastery.com;https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/;2019-01-29;How to Choose Loss Functions When Training Deep Learning Neural Networks;"# mlp for the blobs multi-class classification problem with kl divergence loss

from sklearn . datasets import make_blobs

from keras . layers import Dense

from keras . models import Sequential

from keras . optimizers import SGD

from keras . utils import to_categorical

from matplotlib import pyplot

# generate 2d classification dataset

X , y = make_blobs ( n_samples = 1000 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 )

# one hot encode output variable

y = to_categorical ( y )

# split into train and test

n_train = 500

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# define model

model = Sequential ( )

model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' , kernel_initializer = 'he_uniform' ) )

model . add ( Dense ( 3 , activation = 'softmax' ) )

# compile model

opt = SGD ( lr = 0.01 , momentum = 0.9 )

model . compile ( loss = 'kullback_leibler_divergence' , optimizer = opt , metrics = [ 'accuracy' ] )

# fit model

history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 100 , verbose = 0 )

# evaluate the model

_ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 )

_ , test_acc = model . evaluate ( testX , testy , verbose = 0 )

print ( 'Train: %.3f, Test: %.3f' % ( train_acc , test_acc ) )

# plot loss during training

pyplot . subplot ( 211 )

pyplot . title ( 'Loss' )

pyplot . plot ( history . history [ 'loss' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_loss' ] , label = 'test' )

pyplot . legend ( )

# plot accuracy during training

pyplot . subplot ( 212 )

pyplot . title ( 'Accuracy' )

pyplot . plot ( history . history [ 'accuracy' ] , label = 'train' )

pyplot . plot ( history . history [ 'val_accuracy' ] , label = 'test' )

pyplot . legend ( )"
253;253;machinelearningmastery.com;https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/;2019-07-14;How to Implement Wasserstein Loss for Generative Adversarial Networks;"Tweet Share Share

The Wasserstein Generative Adversarial Network, or Wasserstein GAN, is an extension to the generative adversarial network that both improves the stability when training the model and provides a loss function that correlates with the quality of generated images.

It is an important extension to the GAN model and requires a conceptual shift away from a discriminator that predicts the probability of a generated image being “real” and toward the idea of a critic model that scores the “realness” of a given image.

This conceptual shift is motivated mathematically using the earth mover distance, or Wasserstein distance, to train the GAN that measures the distance between the data distribution observed in the training dataset and the distribution observed in the generated examples.

In this post, you will discover how to implement Wasserstein loss for Generative Adversarial Networks.

After reading this post, you will know:

The conceptual shift in the WGAN from discriminator predicting a probability to a critic predicting a score.

The implementation details for the WGAN as minor changes to the standard deep convolutional GAN.

The intuition behind the Wasserstein loss function and how implement it from scratch.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into five parts; they are:

GAN Stability and the Discriminator What Is a Wasserstein GAN? Implementation Details of the Wasserstein GAN How to Implement Wasserstein Loss Common Point of Confusion With Expected Labels

GAN Stability and the Discriminator

Generative Adversarial Networks, or GANs, are challenging to train.

The discriminator model must classify a given input image as real (from the dataset) or fake (generated), and the generator model must generate new and plausible images.

The reason GANs are difficult to train is that the architecture involves the simultaneous training of a generator and a discriminator model in a zero-sum game. Stable training requires finding and maintaining an equilibrium between the capabilities of the two models.

The discriminator model is a neural network that learns a binary classification problem, using a sigmoid activation function in the output layer, and is fit using a binary cross entropy loss function. As such, the model predicts a probability that a given input is real (or fake as 1 minus the predicted) as a value between 0 and 1.

The loss function has the effect of penalizing the model proportionally to how far the predicted probability distribution differs from the expected probability distribution for a given image. This provides the basis for the error that is back propagated through the discriminator and the generator in order to perform better on the next batch.

The WGAN relaxes the role of the discriminator when training a GAN and proposes the alternative of a critic.

What Is a Wasserstein GAN?

The Wasserstein GAN, or WGAN for short, was introduced by Martin Arjovsky, et al. in their 2017 paper titled “Wasserstein GAN.”

It is an extension of the GAN that seeks an alternate way of training the generator model to better approximate the distribution of data observed in a given training dataset.

Instead of using a discriminator to classify or predict the probability of generated images as being real or fake, the WGAN changes or replaces the discriminator model with a critic that scores the realness or fakeness of a given image.

This change is motivated by a mathematical argument that training the generator should seek a minimization of the distance between the distribution of the data observed in the training dataset and the distribution observed in generated examples. The argument contrasts different distribution distance measures, such as Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and the Earth-Mover (EM) distance, referred to as Wasserstein distance.

The most fundamental difference between such distances is their impact on the convergence of sequences of probability distributions.

— Wasserstein GAN, 2017.

They demonstrate that a critic neural network can be trained to approximate the Wasserstein distance, and, in turn, used to effectively train a generator model.

… we define a form of GAN called Wasserstein-GAN that minimizes a reasonable and efficient approximation of the EM distance, and we theoretically show that the corresponding optimization problem is sound.

— Wasserstein GAN, 2017.

Importantly, the Wasserstein distance has the properties that it is continuous and differentiable and continues to provide a linear gradient, even after the critic is well trained.

The fact that the EM distance is continuous and differentiable a.e. means that we can (and should) train the critic till optimality. […] the more we train the critic, the more reliable gradient of the Wasserstein w"
254;254;towardsdatascience.com;https://towardsdatascience.com/can-we-use-machine-learning-to-forecast-oil-prices-during-the-2020-collapse-4873f03336e9?source=collection_home---4------3-----------------------;2020-04-19;Can We Use Machine Learning To Forecast Oil Prices During The 2020 Collapse?;"A Sensitive Commodity

Photo by Science in HD on Unsplash

Fundamentals of Oil Pricing

Oil is a commodity notorious for being able to go in the complete opposite direction after a single market event.

This is because the fundamentals of oil prices are rarely based on real-time data, instead, it is driven by externalities, making our attempt to forecast it all the more challenging.

2020: A Year Of Ups & Downs

COVID-19

In 2020, COVID-19’s repercussions acted as a reminder of how unpredictable and sensitive oil prices are relative to external shocks.

Early 2020

At the beginning of the year, oil prices were soaring because of the OPEC-led supply cuts, U.S. transactions on multiple major oil exporters, and escalating tensions in Libya.

Mid-2020

However, all of that took a major turn when the health of the global economy was put into speculation after COVID-19, and to make matters worse, industry experts believe it is now “virtually impossible” to confidently forecast the price of oil.

What is more confusing is that presidents have been preaching the virtues of cheap oil for decades, including President Trump himself just a month ago.

Donald Trump Tweet Praising Low Oil Prices — Mar 9, 2020

Donald Trump Praising High Oil Prices — Apr 2, 2020

However, Trump is now doing whatever it takes to push the prices back up, including posting a tweet that caused oil prices to temporarily soar up by 25% — the biggest one-day gain in recorded history.

OPEC Deal — April 9th, 2020

The historic OPEC deal to cut production by 10% has only worked to stem the damage that is still being done to the market. Oil and gas producers are still cutting their dividends and capital spending in efforts to protect their balance sheets in the face of escalating financial losses.

Why Should We Care About Oil Prices?

The reason we are creating this model is because of how linked the health of the economy is to oil prices, whenever there is a slight deviation from the norm in oil prices, the economy is impacted drastically as evident by the parallel movements on Google Trends.

(Closely Intertwined Economy & Oil Prices — Google Trends, 2020)

Time Series Analysis

Time series analysis is an insightful way to look at how a certain commodity changes over time, however, we need to go a step further and create a forecasting model using machine learning’s ARIMA.

What is ARIMA?

An autoregressive integrated moving average model is a form of regression analysis that predicts future moves by examining the difference between the values in the series as opposed to actual values.

It is the perfect time to implement this algorithm as we don’t expect any more majorly historic deals anytime soon given the recency of the OPEC deal.

Forecast Period

Timeframe 1: 20th April 2020–1st October 2020

(COVID-19 Statistics — Google News Apr 18, 2020)

The first timeframe we are forecasting is from the 20th April 2020–1st October 2020, making up almost half of the year.

The rationale behind this is because we have not reached the global peak of COVID-19 yet, giving us a reasonable level of assurance that a fully free COVID-19 market can’t exist within merely 5 and a half months as based on previous pandemic’s timelines, making this our more accurate forecast.

Timeframe 2: 20th April 2020–1st January 2025

This timeframe will act as our prediction for the estimated recovery time until oil prices can go back to their high $50s before 2020's crash.

Photo by Erik Mclean on Unsplash

Dataset

Our dataset is sourced from the U.S. Energy Information Administration and contains 37 years’ worth of daily historic Brent Oil prices from the 17th of May 1987 — 17th of April 2020, meaning it includes a week of oil price movements after the recent OPEC deal.

Training

After preprocessing the data, we found out that training the data from 2000 onwards demonstrates a higher level of accuracy.

Training Dataset — Oil Prices [2000–2020]

Statistical Fine-Tuning of Model

Differencing in ARIMA

The entire point of differencing is to make the time series stationary, and the difference is measured between today and yesterday until we reach a point where the statistical properties are constant over time.

Testing If We Have a Stationary Time Series

We run an Augmented Dickey-Fuller, and if the p-value > 0.005, which was true in our case at 0.297299, we go ahead with differencing.

Visual Representation of Non-Stationary Time Series — Oil Prices [2019/05–2020/05]

After running three tests, we are reassured that the decision of selecting an order of differencing value of 1 is the most appropriate.

Thereafter, we identify if the model requires AR terms by inspecting the Partial Autocorrelation (PACF) plot, which shows the correlation between the series and its lag.

PACF: 1st and 2nd Diffferencing Autocorrelation

Then, we find the order of the Moving Average term q, which is the error of the lagged forecast, by looking at the ACF to see how many MA terms are required "
255;255;news.mit.edu;http://news.mit.edu/2020/mit-press-candlewick-press-collaborate-new-book-imprints-children-teens-0417;;The MIT Press and Candlewick Press to collaborate on new imprints for children, teens;"The MIT Press and Candlewick Press have announced an innovative publishing project that will pair the expertise, reach, and creativity of both organizations in a wholly new endeavor — the first joint project of its kind by a university press and a children’s publisher.

Two new imprints, MIT Kids Press and MITeen Press, to be led creatively and brought to market by Candlewick, will publish engaging and ambitious books for children and young adult readers under new MIT branding. Covering topics ranging from planetary science to the internet and the environment, the list will be reviewed and approved by an MIT-based advisory board comprising members of the MIT Press and eminent faculty, who will also propose acquisitions, identify writers, and help check all titles for scientific validity and factual accuracy. All manufacturing and commercial details will be handled by Candlewick, with some marketing and publicity support from the MIT Press. The books will be published in the UK and Australia by Candlewick’s sister companies, Walker Books UK and Walker Books Australia, and will be sold by the Walker/Candlewick International Sales team in all languages, formats, and territories throughout the world.

The imprints’ first lists, currently slated for autumn 2021, will include books for readers across a span of ages. These will include:

• “Ada and the Galaxies,” a picture book by Alan Lightman, a professor of the practice of the humanities at MIT who is perhaps best known for his internationally best-selling novel “Einstein’s Dreams,” and coauthor Olga Pastuchiv, illustrated by Susanna Chapman;

• “Hanmoji,” a guide to learning Chinese through fun emoji mashups, by Jennifer 8. Lee, Jason Li, and An Xiao Mina; and

• “MIT App Inventor,” a practical guide to app coding for middle-grade readers, by Hal Abelson, Class of 1922 Professor of Computer Science and Engineering in the Department of Electrical Engineering and Computer Science at MIT and a founding director of Creative Commons and of the Free Software Foundation.

“We at the MIT Press have long made it our mission to reach a broad audience of thinkers and doers with the most innovative work in a range of fields,” says Amy Brand, director of the MIT Press. “Consistent with our mission, we have also had some success in recent years reaching younger audiences and we’re truly thrilled to be partnering with Candlewick to take this opportunity to the next level by launching dedicated imprints for young kids and teens under the MIT name.”

Karen Lotz, president and publisher of Candlewick Press and group managing director of the Walker Books Group, noted that the unprecedented collaboration marked a new frontier in publishing and offered a unique opportunity to open young readers’ minds.

“This is a dream come true,” Lotz said. “What started out as a conversation among friends grew and grew. When we started brainstorming in earnest with Amy Brand and Bill Smith of the MIT Press and their great team, as well as extraordinary individuals from the broader MIT community who are committed to innovative K-12 learning, such as Angela Belcher, MIT’s James Mason Crafts Professor of Biological Engineering and Materials Science and Engineering, we realized there was so much in common between us.”

“First and foremost, we shared an ambition to fill the truly international need for new kinds of nonfiction and fiction on important STEAM [science, technology, engineering, arts, and mathematics] topics. We also have a passion for finding new and better ways to connect readers with our books. Through these conversations, it became apparent to us at Candlewick how lucky we would be to secure a broad partnership together with the MIT Press. It has taken a little time to work through the details, as uncharted journeys often do, but we are thrilled with the model and how our publishing plans have coalesced. We have been so gratified to collaborate with MIT Press on creating the new lists, with titles we hope will expand the imaginations and aspirations of the next generations of budding thinkers, designers, scientists, leaders, and inventors,” Lotz says.

“The collaboration of MIT and Candlewick is a no-brainer. In this joint venture, which I am proud to be a part of, we bring together a leading institution of science and a leading children’s book publisher to help inspire awe and understanding of the natural world in our young people,” says Alan Lightman, coauthor of “Ada and the Galaxies.”

“In these uncertain times, the internet and language are more vital than ever to build bridges across cultures and borders. English and Chinese may be lingua francas for billions of people, but emoji is the universal language of this moment,” “Hanmoji” coauthor Jason Li wrote. “To that end, we’re thrilled to be partnering with MITeen Press to bring together the worlds of emoji enthusiasts, Chinese language learners, and anyone interested in the fascinating evolution of the written word.”"
256;256;machinelearningmastery.com;https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/;2017-10-19;How to Develop a Deep Learning Bag-of-Words Model for Sentiment Analysis (Text Classification);"from numpy import array

from string import punctuation

from os import listdir

from collections import Counter

from nltk . corpus import stopwords

from keras . preprocessing . text import Tokenizer

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Dropout

from pandas import DataFrame

from matplotlib import pyplot

# load doc into memory

def load_doc ( filename ) :

# open the file as read only

file = open ( filename , 'r' )

# read all text

text = file . read ( )

# close the file

file . close ( )

return text

# turn a doc into clean tokens

def clean_doc ( doc ) :

# split into tokens by white space

tokens = doc . split ( )

# remove punctuation from each token

table = str . maketrans ( '' , '' , punctuation )

tokens = [ w . translate ( table ) for w in tokens ]

# remove remaining tokens that are not alphabetic

tokens = [ word for word in tokens if word . isalpha ( ) ]

# filter out stop words

stop_words = set ( stopwords . words ( 'english' ) )

tokens = [ w for w in tokens if not w in stop_words ]

# filter out short tokens

tokens = [ word for word in tokens if len ( word ) > 1 ]

return tokens

# load doc, clean and return line of tokens

def doc_to_line ( filename , vocab ) :

# load the doc

doc = load_doc ( filename )

# clean doc

tokens = clean_doc ( doc )

# filter by vocab

tokens = [ w for w in tokens if w in vocab ]

return ' ' . join ( tokens )

# load all docs in a directory

def process_docs ( directory , vocab , is_trian ) :

lines = list ( )

# walk through all files in the folder

for filename in listdir ( directory ) :

# skip any reviews in the test set

if is_trian and filename . startswith ( 'cv9' ) :

continue

if not is_trian and not filename . startswith ( 'cv9' ) :

continue

# create the full path of the file to open

path = directory + '/' + filename

# load and clean the doc

line = doc_to_line ( path , vocab )

# add to list

lines . append ( line )

return lines

# evaluate a neural network model

def evaluate_mode ( Xtrain , ytrain , Xtest , ytest ) :

scores = list ( )

n_repeats = 30

n_words = Xtest . shape [ 1 ]

for i in range ( n_repeats ) :

# define network

model = Sequential ( )

model . add ( Dense ( 50 , input_shape = ( n_words , ) , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile network

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit network

model . fit ( Xtrain , ytrain , epochs = 50 , verbose = 2 )

# evaluate

loss , acc = model . evaluate ( Xtest , ytest , verbose = 0 )

scores . append ( acc )

print ( '%d accuracy: %s' % ( ( i + 1 ) , acc ) )

return scores

# prepare bag of words encoding of docs

def prepare_data ( train_docs , test_docs , mode ) :

# create the tokenizer

tokenizer = Tokenizer ( )

# fit the tokenizer on the documents

tokenizer . fit_on_texts ( train_docs )

# encode training data set

Xtrain = tokenizer . texts_to_matrix ( train_docs , mode = mode )

# encode training data set

Xtest = tokenizer . texts_to_matrix ( test_docs , mode = mode )

return Xtrain , Xtest

# load the vocabulary

vocab_filename = 'vocab.txt'

vocab = load_doc ( vocab_filename )

vocab = vocab . split ( )

vocab = set ( vocab )

# load all training reviews

positive_lines = process_docs ( 'txt_sentoken/pos' , vocab , True )

negative_lines = process_docs ( 'txt_sentoken/neg' , vocab , True )

train_docs = negative_lines + positive_lines

# load all test reviews

positive_lines = process_docs ( 'txt_sentoken/pos' , vocab , False )

negative_lines = process_docs ( 'txt_sentoken/neg' , vocab , False )

test_docs = negative_lines + positive_lines

# prepare labels

ytrain = array ( [ 0 for _ in range ( 900 ) ] + [ 1 for _ in range ( 900 ) ] )

ytest = array ( [ 0 for _ in range ( 100 ) ] + [ 1 for _ in range ( 100 ) ] )

modes = [ 'binary' , 'count' , 'tfidf' , 'freq' ]

results = DataFrame ( )

for mode in modes :

# prepare data for mode

Xtrain , Xtest = prepare_data ( train_docs , test_docs , mode )

# evaluate model on data for mode

results [ mode ] = evaluate_mode ( Xtrain , ytrain , Xtest , ytest )

# summarize results

print ( results . describe ( ) )

# plot results

results . boxplot ( )"
257;257;news.mit.edu;http://news.mit.edu/2019/computing-and-ai-humanistic-perspectives-0924;;Computing and artificial intelligence: Humanistic perspectives from MIT;"The MIT Stephen A. Schwarzman College of Computing (SCC) will reorient the Institute to bring the power of computing and artificial intelligence to all fields at MIT, and to allow the future of computing and AI to be shaped by all MIT disciplines.

To support ongoing planning for the new college, Dean Melissa Nobles invited faculty from all 14 of MIT’s humanistic disciplines in the School of Humanities, Arts, and Social Sciences to respond to two questions:

1) What domain knowledge, perspectives, and methods from your field should be integrated into the new MIT Schwarzman College of Computing, and why?

2) What are some of the meaningful opportunities that advanced computing makes possible in your field?

As Nobles says in her foreword to the series, “Together, the following responses to these two questions offer something of a guidebook to the myriad, productive ways that technical, humanistic, and scientific fields can join forces at MIT, and elsewhere, to further human and planetary well-being.”

The following excerpts highlight faculty responses, with links to full commentaries. The excerpts are sequenced by fields in the following order: the humanities, arts, and social sciences.

Foreword by Melissa Nobles, professor of political science and the Kenan Sahin Dean of the MIT School of Humanities, Arts, and Social Sciences

“The advent of artificial intelligence presents our species with an historic opportunity — disguised as an existential challenge: Can we stay human in the age of AI? In fact, can we grow in humanity, can we shape a more humane, more just, and sustainable world? With a sense of promise and urgency, we are embarked at MIT on an accelerated effort to more fully integrate the technical and humanistic forms of discovery in our curriculum and research, and in our habits of mind and action.” Read more >>

Comparative Media Studies: William Uricchio, professor of comparative media studies

“Given our research and practice focus, the CMS perspective can be key for understanding the implications of computation for knowledge and representation, as well as computation’s relationship to the critical process of how knowledge works in culture — the way it is formed, shared, and validated.”

Recommended action: “Bring media and computer scholars together to explore issues that require both areas of expertise: text-generating algorithms (that force us to ask what it means to be human); the nature of computational gatekeepers (that compels us to reflect on implicit cultural priorities); and personalized filters and texts (that require us to consider the shape of our own biases).” Read more >>

Global Languages: Emma J. Teng, the T.T. and Wei Fong Chao Professor of Asian Civilizations

“Language and culture learning are gateways to international experiences and an important means to develop cross-cultural understanding and sensitivity. Such understanding is essential to addressing the social and ethical implications of the expanding array of technology affecting everyday life across the globe.”

Recommended action: “We aim to create a 21st-century language center to provide a convening space for cross-cultural communication, collaboration, action research, and global classrooms. We also plan to keep the intimate size and human experience of MIT’s language classes, which only increase in value as technology saturates the world.” Read more >>

History: Jeffrey Ravel, professor of history and head of MIT History

“Emerging innovations in computational methods will continue to improve our access to the past and the tools through which we interpret evidence. But the field of history will continue to be served by older methods of scholarship as well; critical thinking by human beings is fundamental to our endeavors in the humanities.”

Recommended action: “Call on the nuanced debates in which historians engage about causality to provide a useful frame of reference for considering the issues that will inevitably emerge from new computing technologies. This methodology of the history field is a powerful way to help imagine our way out of today’s existential threats.” Read more >>

Linguistics: Faculty of MIT Linguistics

“Perhaps the most obvious opportunities for computational and linguistics research concern the interrelation between specific hypotheses about the formal properties of language and their computational implementation in the form of systems that learn, parse, and produce human language.”

Recommended action: “Critically, transformative new tools have come from researchers at institutions where linguists work side-by-side with computational researchers who are able to translate back and forth between computational properties of linguistic grammars and of other systems.” Read more >>

Literature: Shankar Raman, with Mary C. Fuller, professors of literature

“In the age of AI, we could invent new tools for reading. Making the expert reading skills we teach MIT students even partially available to "
258;258;machinelearningmastery.com;https://machinelearningmastery.com/taxonomy-of-time-series-forecasting-problems/;2018-08-07;Taxonomy of Time Series Forecasting Problems;"Tweet Share Share

Last Updated on August 5, 2019

When you are presented with a new time series forecasting problem, there are many things to consider.

The choice that you make directly impacts each step of the project from the design of a test harness to evaluate forecast models to the fundamental difficulty of the forecast problem that you are working on.

It is possible to very quickly narrow down the options by working through a series of questions about your time series forecasting problem. By considering a few themes and questions within each theme, you narrow down the type of problem, test harness, and even choice of algorithms for your project.

In this post, you will discover a framework that you can use to quickly understand and frame your time series forecasting problem.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Framework Overview

Time series forecasting involves developing and using a predictive model on data where there is an ordered relationship between observations.

You can learn more about what time series forecasting is in this post:

Before you get started on your project, you can answer a few questions and greatly improve your understanding of the structure of your forecast problem, the structure of the model requires, and how to evaluate it.

The framework presented in this post is divided into seven parts; they are:

Inputs vs. Outputs Endogenous vs. Exogenous Unstructured vs. Structured Regression vs. Classification Univariate vs. Multivariate Single-step vs. Multi-step Static vs. Dynamic Contiguous vs. Discontiguous

I recommend working through this framework before starting any time series forecasting project.

Your answers may not be crisp on the first time through and the questions may require to you study the data, the domain, and talk to experts and stakeholders.

Update your answers as you learn more as it will help to keep you on track, avoid distractions, and develop the actual model that you need for your project.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Inputs vs. Outputs

Generally, a prediction problem involves using past observations to predict or forecast one or more possible future observations.

The goal is to guess about what might happen in the future.

When you are required to make a forecast, it is critical to think about the data that you will have available to make the forecast and what you will be guessing about the future.

We can summarize this as what are the inputs and outputs of the model when making a single forecast.

Inputs : Historical data provided to the model in order to make a single forecast.

: Historical data provided to the model in order to make a single forecast. Outputs: Prediction or forecast for a future time step beyond the data provided as input.

The input data is not the data used to train the model. We are not at that point yet. It is the data used to make one forecast, for example the last seven days of sales data to forecast the next one day of sales data.

Defining the inputs and outputs of the model forces you to think about what exactly is or may be required to make a forecast.

You may not be able to be specific when it comes to input data. For example, you may not know whether one or multiple prior time steps are required to make a forecast. But you will be able to identify the variables that could be used to make a forecast.

What are the inputs and outputs for a forecast?

2. Endogenous vs. Exogenous

The input data can be further subdivided in order to better understand its relationship to the output variable.

An input variable is endogenous if it is affected by other variables in the system and the output variable depends on it.

In a time series, the observations for an input variable depend upon one another. For example, the observation at time t is dependent upon the observation at t-1; t-1 may depend on t-2, and so on.

An input variable is an exogenous variable if it is independent of other variables in the system and the output variable depends upon it.

Put simply, endogenous variables are influenced by other variables in the system (including themselves) whereas as exogenous variables are not and are considered as outside the system.

Endogenous : Input variables that are influenced by other variables in the system and on which the output variable depends.

: Input variables that are influenced by other variables in the system and on which the output variable depends. Exogenous: Input variables that are not influenced by other variables in the system and on which the output variable depends.

Typically, a time series forecasting problem has endogenous variables (e.g. the output is a functio"
259;259;machinelearningmastery.com;https://machinelearningmastery.com/best-practices-document-classification-deep-learning/;2017-10-22;Best Practices for Text Classification with Deep Learning;"Tweet Share Share

Last Updated on August 7, 2019

Text classification describes a general class of problems such as predicting the sentiment of tweets and movie reviews, as well as classifying email as spam or not.

Deep learning methods are proving very good at text classification, achieving state-of-the-art results on a suite of standard academic benchmark problems.

In this post, you will discover some best practices to consider when developing deep learning models for text classification.

After reading this post, you will know:

The general combination of deep learning methods to consider when starting your text classification problems.

The first architecture to try with specific advice on how to configure hyperparameters.

That deeper networks may be the future of the field in terms of flexibility and capability.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into 5 parts; they are:

Word Embeddings + CNN = Text Classification Use a Single Layer CNN Architecture Dial in CNN Hyperparameters Consider Character-Level CNNs Consider Deeper CNNs for Classification

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Word Embeddings + CNN = Text Classification

The modus operandi for text classification involves the use of a word embedding for representing words and a Convolutional Neural Network (CNN) for learning how to discriminate documents on classification problems.

Yoav Goldberg, in his primer on deep learning for natural language processing, comments that neural networks in general offer better performance than classical linear classifiers, especially when used with pre-trained word embeddings.

The non-linearity of the network, as well as the ability to easily integrate pre-trained word embeddings, often lead to superior classification accuracy.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

He also comments that convolutional neural networks are effective at document classification, namely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in a way that is invariant to their position within the input sequences.

Networks with convolutional and pooling layers are useful for classification tasks in which we expect to find strong local clues regarding class membership, but these clues can appear in different places in the input. […] We would like to learn that certain sequences of words are good indicators of the topic, and do not necessarily care where they appear in the document. Convolutional and pooling layers allow the model to learn to find such local indicators, regardless of their position.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

The architecture is therefore comprised of three key pieces:

Word Embedding: A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation. Convolutional Model: A feature extraction model that learns to extract salient features from documents represented using a word embedding. Fully Connected Model: The interpretation of extracted features in terms of a predictive output.

Yoav Goldberg highlights the CNNs role as a feature extractor model in his book:

… the CNN is in essence a feature-extracting architecture. It does not constitute a standalone, useful network on its own, but rather is meant to be integrated into a larger network, and to be trained to work in tandem with it in order to produce an end result. The CNNs layer’s responsibility is to extract meaningful sub-structures that are useful for the overall prediction task at hand.

— Page 152, Neural Network Methods for Natural Language Processing, 2017.

The tying together of these three elements is demonstrated in perhaps one of the most widely cited examples of the combination, described in the next section.

2. Use a Single Layer CNN Architecture

You can get good results for document classification with a single layer CNN, perhaps with differently sized kernels across the filters to allow grouping of word representations at different scales.

Yoon Kim in his study of the use of pre-trained word vectors for classification tasks with Convolutional Neural Networks found that using pre-trained static word vectors does very well. He suggests that pre-trained word embeddings that were trained on very large text corpora, such as the freely available word2vec vectors trained on 100 billion tokens from Google news may offer good universal features for use in natural language processing.

Despite little tuning of hyperparameters, a simple CNN with one layer of convolution performs remarkably"
260;260;news.mit.edu;http://news.mit.edu/2020/3-questions-charles-stewart-pandemic-impact-2020-elections-0415;2020-03-19;3 Questions: Charles Stewart on the Covid-19 pandemic’s impact on the 2020 elections;"American voters are facing the unprecedented prospect of electing a new president amid a global pandemic. Already, the Covid-19 crisis has led some states to cancel in-person voting in favor of voting by mail, while other states have delayed primaries or held them with physical distancing guidelines that have forced voters to wait in long lines beyond the physical confines of their polling places. The Democratic National Convention, at which the Democratic Party will formally select its challenger and running mate to face incumbent President Donald Trump, has been delayed a month and is now scheduled for late August — though it may end up taking place online.



Charles Stewart III is the Kenan Sahin Distinguished Professor of Political Science at MIT and the founder of the MIT Election Data and Science Lab. SHASS Communications spoke with him recently about the broader impacts of the pandemic on the U.S. elections, in particular the decisions that need to be made, quickly, to increase the extent of voting-by-mail and other safe voting methods for the general election in November.

Q: Given the need for social distancing and the uncertain timeline of the pandemic, what are the greatest risks you see to the forthcoming presidential election? What steps can be taken now to ensure that a fair and representative election takes place in November?

A: The most obvious risk is that fear of infection, indeed, fear of death, will reduce turnout in the November election. The consequences of reduced turnout would be catastrophic for American democracy. Not only would it deny a voice to millions of Americans, it would throw the legitimacy of the outcome into question.

There are other risks to be navigated that are related to the turnout issue, but are also distinct conceptually. The first of these is logistical chaos. Because of the Covid-19 crisis, it is clear that more voting needs to be done by mail than has ever been done in the U.S. This is a view that I entirely support.

Yet this is easier said than done. In 2016, only 20 percent of voters cast their ballot by mail. The recent growth in vote-by-mail has been concentrated among a small number of western states, so that in the east, the percentage is much lower — around 10 percent. If we are to get the percentage of voters overall voting by mail to the 50-60 percent level, this will require states, like Massachusetts, that have previously only had 5 percent mail-vote rates to ramp it up to 50 percent.

That’s a big lift. As I point out in my recent LawFare blog post, voting by mail at a large scale requires serious attention to a number of processes and logistical challenges. The states that currently cast all their votes by mail — Colorado, Oregon, and Washington — have taken decades to get where they are. Can the rest of the states get even halfway to where these three other states are in the confines of six months? If they can’t — if the mail ballots end up going to the wrong places, or they end up being rejected in large numbers because of signature-matching problems — then the post-election period will put Florida 2000 to shame. That’s the worst-case scenario. I’m more optimistic than to believe this will be where we end up. But that’s the risk we face.

The second risk is lack of legitimacy because of how the election ends up being run. This points us in two directions. On the one hand, states could inadequately implement expanded vote-by-mail programs. This will lead to contentious lawsuits and voters believing that their votes didn’t count, and that the winner was chosen through an arbitrary process. On the other hand, the further states push voting by mail, the further residents who distrust mail voting — mainly because of the fraud opportunities — the more people will doubt the legitimacy of the outcome because they believe the election was stolen because so many mail ballots were just flowing around, willy-nilly.

For reasons I don’t entirely understand, voting by mail has been associated with the Democratic Party. Thus, if the Democrat wins in 2020, a lot of Republicans will believe that happened because Democrats were able to steal the election through the expansion of mail balloting.

So, what can be done? First, despite the risks, mail balloting should be expanded. If decisions are made now to take that road, almost all states can spend the next six months getting the logistical ducks in a row to make this happen successfully. And, speaking practically, the seven states that are the most closely divided politically — the battleground states — seem well-positioned to prepare for an onslaught of mail ballots. Yet, as mentioned in the LawFare blog post, expanding mail balloting will be a big lift for most states. As I said, you can’t just flip a switch.

Second, states should spend time planning how to make in-person voting safe. For a variety of reasons, I’m predicting that millions of voters will still vote in person. Obviously, with personal dista"
261;261;machinelearningmastery.com;https://machinelearningmastery.com/create-lists-of-machine-learning-algorithms/;2014-11-02;Take Control By Creating Targeted Lists of Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

Any book on machine learning will list and describe dozens of machine learning algorithms.

Once you start using tools and libraries you will discover dozens more. This can really wear you down, if you think you need to know about every possible algorithm out there.

A simple trick to tackle this feeling and take some control back is to make lists of machine learning algorithms.

This ridiculously simple tactic can give you a lot of power. You can use it to give you a list of methods to try when tackling a whole new class of problem. It can also give you a list of ideas when you get stuck on a dataset or your favorite method does not give you good results.

In this post you will discover the benefits of creating lists of machine learning algorithms, how to do it, how to do it well and why you should start creating your first list of algorithms today.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Dealing with So Many Algorithms

There are hundreds of machine learning algorithms.

I see this leading to two problems:

1. Overwhelm

The fact that there are so many algorithms to choose from and try on a given machine learning problem causes some people to freeze up and do nothing.

The fact is, you don’t need to get the best result, you only need a result – a beachhead on the problem – and you can get there by spot checking a few algorithms.

2. Favorites

Because there are so many algorithms, some people select one or two favorite algorithms and only use them. This limits the results they can achieve and the problems that they can address.

Favorites are dangerous. Some algorithms are more powerful than others, but that power comes at a cost of complexity and parsimony. They are tools, leave your emotional attachment at the door.

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

Take Control of the Algorithms

You need focus, a starting point to address the problem of dealing with so many machine learning algorithms.

This involves finding the edges and pushing back the fog on what is out there and what you can use when. This will give you a sense of control over the algorithms and help you to wield them rather than make you feel overwhelmed.

The great thing is, you don’t need to become an expert in each algorithm to make progress. You don’t need to know very much about each algorithm at all.

Collecting simple information such as algorithm names and the general problems to which they are applicable can help you quickly and confidently build familiarization and confidence with the extent of machine learning algorithms available.

How to Build and Maintain a List of Algorithms

The answer is to build your own personal list of machine learning algorithms.

I’m a list maker, and this method really lights up my brain.

Open a text file, word document or spreadsheet and start listing down the names of algorithms. It’s that simple. You can also list the general class to which the algorithm belongs and the general types of problems that it can address.

Define your own categories. This list is a tool to help you understand and navigate the machine learning tools at your disposal. Customize the list to include the algorithm details that you care about.

Examples of Algorithm Lists To Create

Below are 10 examples of machine learning algorithm lists that you could create.

Regression algorithms

SVM algorithms

Data projection algorithms

Deep learning algorithms

Time series forecasting algorithms

Rating system algorithms

Recommender system algorithms

Feature selection algorithms

Class imbalance algorithms

Decision tree algorithms

Tips for Great Algorithm Lists

Creating a list of algorithms is relatively easy. The hard part is knowing why you want the list. The “why” will help you define the type of list you want to create and the algorithm properties you want to describe in your list.

Start with the current project you are working on or your current interests. For example, if you are working on a time series or image classification problem, list all the algorithms that you could apply to that problem. If you are deeply interested in Support Vector Machines, list all the variations of SVM that you can find.

Don’t try to create the perfect list in one sitting. Create it and keep adding to it over days and weeks. It is a useful resource that you can turn back to again and again and add to as your knowledge and experience grows.

In summary, 5 tips for creating great algorithm lists are:

Start with why you want the list and use that to define the type of list to create.

Only capture the algorithm properties you actually need, keep it as simple"
262;262;news.mit.edu;http://news.mit.edu/2020/mit-companies-covid-19-0326;;MIT-affiliated companies take on Covid-19;"As the world grapples with the public health crises and myriad disruptions brought on by the Covid-19 pandemic, many efforts to address its impact are underway.

Several of those initiatives are being led by companies that were founded by MIT alumni, professors, students, and researchers.

These companies’ efforts are as wide ranging and complex as the challenges brought on by Covid-19. They leverage expertise in biological engineering, mobile technology, data analytics, community engagement, and other fields MIT has long focused on.

The companies, a few of whom are featured here, are also at very different stages of deployment, but they are all driven by a desire to use science, engineering, and entrepreneurship to solve the world’s most pressing problems.

Moderna Therapeutics

On Jan. 11, Chinese authorities shared the genetic sequence of Covid-19. Just two days later, members of a research team from Moderna Therapeutics, in collaboration with the National Institutes of Health, finalized the design of a vaccine they hope will prevent infection from the disease.

Moderna was founded by Institute Professor Robert Langer, who is also a faculty member at the Institute for Medical Engineering and Sciences (IMES), investor Noubar Afeyan PhD ’87, and researchers from Harvard Medical School in 2010. The company develops treatments that leverage specialized transporter molecules in cells known as messenger RNAs. Messenger RNAs bring instructions from genes to the cellular machinery that makes proteins. By creating specially modified mRNA, Moderna believes it can develop therapies to treat and prevent a number of diseases in humans.

Following its design of a potential Covid-19 vaccine, the company quickly moved to manufacture the mRNA vaccine for clinical trials. On March 16, just 65 days after Covid-19 was sequenced, Moderna began human trials, according to the company.

The first stage of the trials is expected to last six weeks and will focus on the safety of the vaccine as well as the immune response it provokes in participants. The company has said that while a commercially available vaccine is not likely to be available for at least 12-18 months, it is possible that under emergency use, a vaccine could be available to some people sooner.

Alnylam Pharmaceuticals

On March 5, Alnylam Pharmaceuticals announced that its partnership with Vir Biotechnology, which focuses on treating infectious diseases, would extend to developing therapeutics for coronavirus infections, including Covid-19.

Alnylam was founded in 2002 by Institute Professor Phil Sharp, who is also a faculty member at the Institute for Medical Engineering and Sciences (IMES), Professor David Bartel, former MIT professor Paul Schimmel, MIT postdocs Tom Tuschl and Phil Zamore, and investors.

The company is already approved to treat patients with certain rare genetic diseases using its patented RNA interference technology. RNA interference, or RNAi, is a method of stopping the expression of specific genes through the manipulation of existing regulatory processes in the human body.

“[RNAi] technology is now strongly validated in a variety of ways and the promise of it is really remarkable,” says Sharp, who currently sits on Alnylam’s scientific advisory board with Bartel and Schimmel. “It’s the creation of a whole new therapeutic modality that I think we’ll be using 100 years from now.”

Under the terms of the extended collaboration, the companies will use Alnylam’s recent advances in delivering its RNAi technology to the lungs, in addition to Vir’s infectious disease capabilities, to identify and advance drug candidates.

Sharp says that even if the collaboration doesn’t lead to a treatment for the current Covid-19 outbreak, it holds tremendous potential for helping victims of infectious diseases down the line.

Dimagi

Dimagi, which provides a platform for creating mobile apps that can be used offline by cell phones of all types, recently began freely offering its mobile tool to organizations responding to the Covid-19 outbreak around the world.

The company’s platform is currently being used by hundreds of thousands of front-line health care workers globally. By enabling people with no coding experience to create mobile apps that work in environments with no cellular service, the company has transformed health care treatment for millions of people in low- and middle-income countries.

The company has already seen governments adopt its platform for Covid-19 response, including the Ogun state government of Nigeria, and it is also exploring use cases with officials from the U.S. Centers for Disease Control and Prevention in California.

The company was formed in 2002 when Jonathan Jackson’03 SM ’05 met co-founder Vikram Kumar, who was then a graduate research assistant in MIT’s Media Lab and on his way to earning his MD in the MIT-Harvard Division of Health Sciences and Technology.

Since then, Dimagi’s solutions have been used for a variety of l"
263;263;news.mit.edu;http://news.mit.edu/2020/to-self-drive-in-snow-look-under-road-0226;;To self-drive in the snow, look under the road;"Car companies have been feverishly working to improve the technologies behind self-driving cars. But so far even the most high-tech vehicles still fail when it comes to safely navigating in rain and snow.

This is because these weather conditions wreak havoc on the most common approaches for sensing, which usually involve either lidar sensors or cameras. In the snow, for example, cameras can no longer recognize lane markings and traffic signs, while the lasers of lidar sensors malfunction when there’s, say, stuff flying down from the sky.

MIT researchers have recently been wondering whether an entirely different approach might work. Specifically, what if we instead looked under the road?

A team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) has developed a new system that uses an existing technology called ground-penetrating radar (GPR) to send electromagnetic pulses underground that measure the area’s specific combination of soil, rocks, and roots. Specifically, the CSAIL team used a particular form of GPR instrumentation developed at MIT Lincoln Laboratory called localizing ground-penetrating radar, or LGPR. The mapping process creates a unique fingerprint of sorts that the car can later use to localize itself when it returns to that particular plot of land.

“If you or I grabbed a shovel and dug it into the ground, all we’re going to see is a bunch of dirt,” says CSAIL PhD student Teddy Ort, lead author on a new paper about the project that will be published in the IEEE Robotics and Automation Letters journal later this month. “But LGPR can quantify the specific elements there and compare that to the map it’s already created, so that it knows exactly where it is, without needing cameras or lasers.”

In tests, the team found that in snowy conditions the navigation system’s average margin of error was on the order of only about an inch compared to clear weather. The researchers were surprised to find that it had a bit more trouble with rainy conditions, but was still only off by an average of 5.5 inches. (This is because rain leads to more water soaking into the ground, leading to a larger disparity between the original mapped LGPR reading and the current condition of the soil.)

The researchers said the system’s robustness was further validated by the fact that, over a period of six months of testing, they never had to unexpectedly step in to take the wheel.

“Our work demonstrates that this approach is actually a practical way to help self-driving cars navigate poor weather without actually having to be able to ‘see’ in the traditional sense using laser scanners or cameras,” says MIT Professor Daniela Rus, director of CSAIL and senior author on the new paper, which will also be presented in May at the International Conference on Robotics and Automation in Paris.

While the team has only tested the system at low speeds on a closed country road, Ort said that existing work from Lincoln Laboratory suggests that the system could easily be extended to highways and other high-speed areas.

This is the first time that developers of self-driving systems have employed ground-penetrating radar, which has previously been used in fields like construction planning, landmine detection, and even lunar exploration. The approach wouldn’t be able to work completely on its own, since it can’t detect things above ground. But its ability to localize in bad weather means that it would couple nicely with lidar and vision approaches.

“Before releasing autonomous vehicles on public streets, localization and navigation have to be totally reliable at all times,” says Roland Siegwart, a professor of autonomous systems at ETH Zurich who was not involved in the project. “The CSAIL team’s innovative and novel concept has the potential to push autonomous vehicles much closer to real-world deployment.”

One major benefit of mapping out an area with LGPR is that underground maps tend to hold up better over time than maps created using vision or lidar, since features of an above-ground map are much more likely to change. LGPR maps also take up only about 80 percent of the space used by traditional 2D sensor maps that many companies use for their cars.

While the system represents an important advance, Ort notes that it’s far from road-ready. Future work will need to focus on designing mapping techniques that allow LGPR datasets to be stitched together to be able to deal with multi-lane roads and intersections. In addition, the current hardware is bulky and 6 feet wide, so major design advances need to be made before it’s small and light enough to fit into commercial vehicles.

Ort and Rus co-wrote the paper with CSAIL postdoc Igor Gilitschenski. The project was supported, in part, by MIT Lincoln Laboratory."
264;264;news.mit.edu;http://news.mit.edu/2020/sugar-factories-colonial-indonesia-olken-dell-0206;;The complex effects of colonial rule in Indonesia;"The areas of Indonesia where Dutch colonial rulers built a huge sugar-producing industry in the 1800s remain more economically productive today than other parts of the country, according to a study co-authored by an MIT economist.

The research, focused on the Indonesian island of Java, introduces new data into the study of the economic effects of colonialism. The finding shows that around villages where the Dutch built sugar-processing factories from the 1830 through the 1870s, there is today greater economic activity, more extensive manufacturing, and even more schools, along with higher local education levels.

“The places where the Dutch established [sugar factories] persisted as manufacturing centers,” says Benjamin Olken, a professor of economics at MIT and co-author of a paper detailing the results, which appears in the January issue of the Review of Economic Studies.

The historical link between this “Dutch Cultivation System” and economic activity today has likely been transmitted “through a couple of forces,” Olken suggests. One of them, he says, is the building of “complementary infrastructure” such as railroads and roads, which remain in place in contemporary Indonesia.

The other mechanism, Olken says, is that “industries grew up around the sugar [industry], and those industries persisted. And once you have this manufacturing environment, that can lead to other changes: More infrastructure and more schools have persisted in these areas as well.”

To be sure, Olken says, the empirical conclusions of the study do not represent validation of Dutch colonial rule, which lasted from the early 1600s until 1949 and significantly restricted the rights and self-constructed political institutions of Indonesians. Dutch rule had long-lasting effects in many areas of civic life, and the Dutch Cultivation System used forced labor, for one thing.

“This paper is not trying to argue that the [Dutch] colonial enterprise was a net good for the people of the time,” Olken emphasizes. “I want to be very clear on that. That’s not what we’re saying.”

Instead, the study was designed to evaluate the empirical effects of the Dutch Cultivation System, and the outcome of the research was not necessarily what Olken would have anticipated.

“The results are striking,” Olken says. “They just jump out at you.”

The paper, “The Development Effects of the Extractive Colonial Economy: The Dutch Cultivation System in Java,” is co-authored by Olken and Melissa Dell PhD ’12, a professor of economics at Harvard University.

On the ground

Historically in Java, the most populous of Indonesia’s many islands, the main crop had been rice. Starting in the 1830s, the Dutch instituted a sugar-growing system in some areas, building 94 sugar-processing factories, as well as roads and railroads to transport materials and products.

Generally the Dutch would export high-quality sugar from Indonesia while keeping lower-quality sugar in the country. Overall, the system became massive; at one point in the mid-19th century, sugar production in Java accounted for one-third of the Dutch government’s revenues and 4 percent of Dutch GDP. By one estimate, a quarter of the population was involved in the industry.

In developing their research, Olken and Dell used 19th century data from government archives in the Netherlands, as well as modern data from Indonesia. The Dutch built the processing plants next to rivers in places with enough flat land to sustain extensive sugar crops; to conduct the study, the researchers looked at economic activity near sugar-processing factories and compared it with economic activity in similar areas that lacked factories.

“In the 1850s, the Dutch spent four years on the ground collecting detailed information for the over 10,000 villages that contributed land and labor to the Cultivation System,” Dell notes. The researchers digitized those records and, as she states, “painstakingly merged them” with economic and demograhic records from the same locations today

As the results show, places close to factories are 25-30 percentage points less agricultural in economic composition than those away from factories, and they have more manufacturing, by 6-7 percentage points. They also have 9 percent more employment in retail.

Areas within 1 kilometer of a sugar factory have a railroad density twice that of similar places 5 to 20 kilometers from factories; by 1980, they were also 45 percent more likely to have electricity and 4 percent more likely to have a high school. They also have local populations with a full year more of education, on average, than areas not situated near old sugar factories.

The study shows there is also about 10 to 15 percent more public-land use in villages that were part of the Dutch Cultivation System, a data point that holds steady in both 1980 and 2003.

“The key thing that underlies this paper, in multiple respects, is the linking of the historical data and the modern data,” Olken says. The researc"
265;265;news.mit.edu;http://news.mit.edu/2018/using-data-science-improve-public-policy-hackathon-0423;;Using data science to improve public policy;"100 researchers and students from MIT and six other universities gathered on campus this April for the first weekend-long MIT Policy Hackathon. This interdisciplinary event teamed data science, engineering, and policy students to explore solutions to real societal challenges submitted by sponsor organizations.

The hackathon, subtitled “Data to Decisions,” was organized and run by students from MIT’s Institute for Data, Systems, and Society (IDSS). Participants used datasets provided by nonprofit, education, and government institutions to pitch solutions to complex challenges in cybersecurity, health, energy and climate, transportation, and the future of work. A panel of judges evaluated the pitches and read final policy proposals.

“It’s a different type of hackathon in that it is focused on public policy outcomes,” says Amy Umaretiya, a student organizer with IDSS’s Master’s program in Technology and Policy (TPP). “We have these concrete challenges put forth by organizations that have data analytics needs for social good that aren’t being met.”

“We wanted to create a hackathon where interdisciplinary teams tackle complex societal problems,” explains Marco Miotti, a doctoral student at IDSS. “The challenges were structured so you can’t solve them without both data and policy expertise.”

Data and diaper need

The winning team, called NappyTime, worked on the health challenge with sponsors from Yale’s Mental health Outreach for MotherS (MOMS) Partnership and the National Diaper Bank Network (NDBN). The problem: lack of a sufficient supply of diapers to keep babies and toddlers clean, dry, and healthy. Diaper need affects one in three low- and middle-income families in the U.S., and can have a significant impact on the physical, mental, and economic well-being of both children and parents.

“The crux of this proposal was the need for more data on diaper need, and quantifying the benefits of addressing it,” says Lawrence Baker, a TPP student on the winning team. “We wanted to propose policies that would be inexpensive for Connecticut, easy to implement, and would allow the collection of more data.”

Lori Wallace, a postdoctoral associate with MOMS, mentored the challenge. She was joined by Lynn Comer of NDBN, who was also a judge. Both remarked on the applicability and ingenuity of the winning proposal, which is under a nondisclosure agreement. “By focusing on cost-effective means to embed the provision of diapers into our childcare system, the hackathon participants presented actionable solutions to this real life issue,” they said.

Hackathon judge Frank Field, a senior research engineer with IDSS who is associate director of TPP, was also impressed with the team’s proposal and presentation, both during their pitch and the follow-up Q&A session. Says Field: “Their proposal went right at the problem. It was pitched in the form of a staged implementation, which cleverly skirts some of the classic impediments to policy innovations by targeting some populations immediately while centering on issues whose resolution could inform and refine a wider deployment.""

Real data, real problems

Two local challenges were sponsored by organizations within the Massachusetts government. The first, in transportation, looked for solutions to congestion on local highways and train lines. Transportation proposals considered the impact of autonomous vehicles and suggested ideas ranging from shielded bike paths to establishing a car-free area within Kendall Square.

Meanwhile, the energy and climate challenge asked teams to help Boston meet its goal of carbon neutrality by 2050. Climate hackers worked with green building data that had previously been locked up in PDF format. Hackathon organizers partnered with WattzOn, a software company working in utility data, to extract the information into a usable dataset.

“It was great that the problem was drawn from the real world, that we had real data, and that our mentors had working knowledge of relevant policies. It really showed us how to work through a policy problem with various administrative constraints, interfering regulations, and incomplete data,” says energy and climate hacker Zhen Dai, a PhD student researching the science and policy of solar geoengineering at Harvard University.

The cybersecurity challenge, sponsored by Boston University Law School’s Technology and Cyberlaw Clinic, called upon students to examine the multitude of legislative and policy approaches that U.S. states use to respond to cybersecurity breaches such as those experienced by Equifax and Yahoo. Teams were asked to consolidate and compare these approaches across states, and then propose ways to identify and implement some of the more effective policies.

“It’s important to have people who can think at the intersection of computer science and policy,” says Nathaniel Fruchter, a hackathon organizer and TPP student. “The breach dataset provided to the hackers has been largely unexplored by academia or"
266;266;machinelearningmastery.com;http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/;2016-06-16;Display Deep Learning Model Training History in Keras;"# Visualize training history

from keras . models import Sequential

from keras . layers import Dense

import matplotlib . pyplot as plt

import numpy

# load pima indians dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# create model

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# Compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# Fit the model

history = model . fit ( X , Y , validation_split = 0.33 , epochs = 150 , batch_size = 10 , verbose = 0 )

# list all data in history

print ( history . history . keys ( ) )

# summarize history for accuracy

plt . plot ( history . history [ 'accuracy' ] )

plt . plot ( history . history [ 'val_accuracy' ] )

plt . title ( 'model accuracy' )

plt . ylabel ( 'accuracy' )

plt . xlabel ( 'epoch' )

plt . legend ( [ 'train' , 'test' ] , loc = 'upper left' )

plt . show ( )

# summarize history for loss

plt . plot ( history . history [ 'loss' ] )

plt . plot ( history . history [ 'val_loss' ] )

plt . title ( 'model loss' )

plt . ylabel ( 'loss' )

plt . xlabel ( 'epoch' )

plt . legend ( [ 'train' , 'test' ] , loc = 'upper left' )"
267;267;towardsdatascience.com;https://towardsdatascience.com/redacting-sensitive-information-from-doctors-patient-notes-51773a9c494b?source=collection_home---4------2-----------------------;2020-04-19;Redacting sensitive information from doctors’ patient notes;"Redacting sensitive information from doctors’ patient notes

COVID-19 public dataset on GCP from cases in Italy (part 2)

This article is the second one of a series about the release of a public dataset made of doctors medical notes about patients affected by COVID19. You will find learnings about Google Cloud DLP API that was used for the redaction of sensitive information from medical notes. If you haven’t seen it already, you can find the first article here.

To reiterate my commitment to the community, I will keep the public database up-to-date with the latest cases published by the Italian Society of Medical and Interventional Radiology (ISMIR). And if, as a side effect, you can learn a thing or two about GCP, then this series will exceed my expectations 💪 🙏 . The code used in this pipeline is available in my Github repo.

By the way, I’m very proud to see that the community is thinking of ways to leverage this data. An avenue raised by Jérôme MASSOT is to leverage this dataset and conduct a:"
268;268;machinelearningmastery.com;https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/;2018-12-30;How to Develop a Stacking Ensemble for Deep Learning Neural Networks in Python With Keras;"# stacked generalization with neural net meta model on blobs dataset

from sklearn . datasets import make_blobs

from sklearn . metrics import accuracy_score

from keras . models import load_model

from keras . utils import to_categorical

from keras . utils import plot_model

from keras . models import Model

from keras . layers import Input

from keras . layers import Dense

from keras . layers . merge import concatenate

from numpy import argmax

# load models from file

def load_all_models ( n_models ) :

all_models = list ( )

for i in range ( n_models ) :

# define filename for this ensemble

filename = 'models/model_' + str ( i + 1 ) + '.h5'

# load model from file

model = load_model ( filename )

# add to list of members

all_models . append ( model )

print ( '>loaded %s' % filename )

return all_models

# define stacked model from multiple member input models

def define_stacked_model ( members ) :

# update all layers in all models to not be trainable

for i in range ( len ( members ) ) :

model = members [ i ]

for layer in model . layers :

# make not trainable

layer . trainable = False

# rename to avoid 'unique layer name' issue

layer . name = 'ensemble_' + str ( i + 1 ) + '_' + layer . name

# define multi-headed input

ensemble_visible = [ model . input for model in members ]

# concatenate merge output from each model

ensemble_outputs = [ model . output for model in members ]

merge = concatenate ( ensemble_outputs )

hidden = Dense ( 10 , activation = 'relu' ) ( merge )

output = Dense ( 3 , activation = 'softmax' ) ( hidden )

model = Model ( inputs = ensemble_visible , outputs = output )

# plot graph of ensemble

plot_model ( model , show_shapes = True , to_file = 'model_graph.png' )

# compile

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

return model

# fit a stacked model

def fit_stacked_model ( model , inputX , inputy ) :

# prepare input data

X = [ inputX for _ in range ( len ( model . input ) ) ]

# encode output data

inputy_enc = to_categorical ( inputy )

# fit model

model . fit ( X , inputy_enc , epochs = 300 , verbose = 0 )

# make a prediction with a stacked model

def predict_stacked_model ( model , inputX ) :

# prepare input data

X = [ inputX for _ in range ( len ( model . input ) ) ]

# make prediction

return model . predict ( X , verbose = 0 )

# generate 2d classification dataset

X , y = make_blobs ( n_samples = 1100 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 )

# split into train and test

n_train = 100

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

print ( trainX . shape , testX . shape )

# load all models

n_members = 5

members = load_all_models ( n_members )

print ( 'Loaded %d models' % len ( members ) )

# define ensemble model

stacked_model = define_stacked_model ( members )

# fit stacked model on test dataset

fit_stacked_model ( stacked_model , testX , testy )

# make predictions and evaluate

yhat = predict_stacked_model ( stacked_model , testX )

yhat = argmax ( yhat , axis = 1 )

acc = accuracy_score ( testy , yhat )"
269;269;news.mit.edu;http://news.mit.edu/2020/how-crystals-form-surfaces-paes-0402;;Technique reveals how crystals form on surfaces;"The process of crystallization, in which atoms or molecules line up in orderly arrays like soldiers in formation, is the basis for many of the materials that define modern life, including the silicon in microchips and solar cells. But while many useful applications for crystals involve their growth on solid surfaces (rather than in solution), there has been a dearth of good tools for studying this type of growth.

Now, a team of researchers at MIT and Draper has found a way to reproduce the growth of crystals on surfaces, but at a larger scale that makes the process much easier to study and analyze. The new approach is described in a paper in the journal Nature Materials, by Robert Macfarlane and Leonardo Zomberg at MIT, and Diana Lewis PhD ’19 and David Carter at Draper.

Rather than assembling these crystals from actual atoms, the key to making the process easy to observe and quantify was the use of “programmable atom equivalents,” or PAEs, Macfarlane explains. This works because the ways atoms line up into crystal lattices is entirely a matter of geometry and doesn’t rely on the specific chemical or electronic properties of its constituents.

The team used spherical nanoparticles of gold, coated with specially selected single strands of genetically engineered DNA, giving the particles roughly the appearance of Koosh balls. Single DNA strands have the inherent property of attaching themselves tightly to the corresponding reciprocal strands, to form the classic double helix, so this configuration provides a surefire way of getting the particles to align themselves in precisely the desired way.

“If I put a very dense brush of DNA on the particle, it’s going to make as many bonds with as many nearest neighbors as it can,” Macfarlane says. “And if you design everything appropriately and process it correctly, they will form ordered crystal structures.” While that process has been known for some years, this work is the first to apply that principle to study the growth of crystals on surfaces.

“Understanding how crystals grow upward from a surface is incredibly important for a lot of different fields,” he says. The semiconductor industry, for example, is based on the growth of large single-crystal or multi-crystalline materials that must be controlled with great precision, yet the details of the process are difficult to study. That’s why the use of oversized analogs such as the PAEs can be of such benefit.

The PAEs, he says, “crystallize in exactly the same pathways that molecules and atoms do. And so they are a very nice proxy system for understanding how crystallization occurs.” With this system, the properties of the DNA dictate how the particles assemble and the 3D configuration they end up in.

They designed the system such that the crystals nucleate and grow starting from a surface and “by tailoring the interactions both between particles, and between the particles and the DNA-coated surface, we can dictate the size, the shape, the orientation and the degree of anisotropy (directionality) in the crystal,” Macfarlane says.

“By understanding the process this is going through to actually form these crystals, we can potentially use that to understand crystallization processes in general,” he adds.

He explains that not only are the resulting crystal structures about 100 times larger than the actual atomic ones, but their formation processes are also much slower. The combination makes the process much easier to analyze in detail. Earlier methods of characterizing such crystalline structures only showed their final states, thus missing complexities in the formation process.

“I could change the DNA sequence. I can change the number of DNA strands in the particle. I can change the size of the particle and I can tweak each of these individual handles independently,” Macfarlane says. “So if I wanted to be able to say, OK, I hypothesize that this particular structure might be favored under these conditions if I tuned the energetics in such a way, that’s a much easier system to study with the PAEs than it would be with atoms themselves.”

The system is very effective, he says, but DNA strands modified in a manner that allows for attachment to nanoparticles can be quite expensive. As a next step, the Macfarlane lab has also developed polymer-based building blocks that show promise in replicating these same crystallization processes and materials, but can be made inexpensively at a multigram scale.

The work was funded by the Air Force Office of Scientific Research and supported by a Draper fellowship and the National Science Foundation and used facilities of the Materials Technology Laboratory at MIT."
270;270;machinelearningmastery.com;http://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/;2016-02-04;Tune Machine Learning Algorithms in R (random forest case study);"customRF < - list ( type = ""Classification"" , library = ""randomForest"" , loop = NULL )

customRF $ parameters < - data . frame ( parameter = c ( ""mtry"" , ""ntree"" ) , class = rep ( ""numeric"" , 2 ) , label = c ( ""mtry"" , ""ntree"" ) )

customRF $ grid < - function ( x , y , len = NULL , search = ""grid"" ) { }

customRF $ fit < - function ( x , y , wts , param , lev , last , weights , classProbs , . . . ) {

randomForest ( x , y , mtry = param $ mtry , ntree = param $ ntree , . . . )

}

customRF $ predict < - function ( modelFit , newdata , preProc = NULL , submodels = NULL )

predict ( modelFit , newdata )

customRF $ prob < - function ( modelFit , newdata , preProc = NULL , submodels = NULL )

predict ( modelFit , newdata , type = ""prob"" )

customRF $ sort < - function ( x ) x [ order ( x [ , 1 ] ) , ]"
271;271;machinelearningmastery.com;https://machinelearningmastery.com/cost-sensitive-decision-trees-for-imbalanced-classification/;2020-01-28;Cost-Sensitive Decision Trees for Imbalanced Classification;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28

# grid search class weights with decision tree for imbalance classification from numpy import mean from sklearn . datasets import make_classification from sklearn . model_selection import GridSearchCV from sklearn . model_selection import RepeatedStratifiedKFold from sklearn . tree import DecisionTreeClassifier # generate dataset X , y = make_classification ( n_samples = 10000 , n_features = 2 , n_redundant = 0 , n_clusters_per_class = 1 , weights = [ 0.99 ] , flip_y = 0 , random_state = 3 ) # define model model = DecisionTreeClassifier ( ) # define grid balance = [ { 0 : 100 , 1 : 1 } , { 0 : 10 , 1 : 1 } , { 0 : 1 , 1 : 1 } , { 0 : 1 , 1 : 10 } , { 0 : 1 , 1 : 100 } ] param_grid = dict ( class_weight = balance ) # define evaluation procedure cv = RepeatedStratifiedKFold ( n_splits = 10 , n_repeats = 3 , random_state = 1 ) # define grid search grid = GridSearchCV ( estimator = model , param_grid = param_grid , n_jobs = - 1 , cv = cv , scoring = 'roc_auc' ) # execute the grid search grid_result = grid . fit ( X , y ) # report the best configuration print ( ""Best: %f using %s"" % ( grid_result . best_score_ , grid_result . best_params_ ) ) # report all configurations means = grid_result . cv_results_ [ 'mean_test_score' ] stds = grid_result . cv_results_ [ 'std_test_score' ] params = grid_result . cv_results_ [ 'params' ] for mean , stdev , param in zip ( means , stds , params ) : print ( ""%f (%f) with: %r"" % ( mean , stdev , param ) )"
272;272;machinelearningmastery.com;https://machinelearningmastery.com/statistical-methods-in-an-applied-machine-learning-project/;2018-06-24;10 Examples of How to Use Statistical Methods in a Machine Learning Project;"Tweet Share Share

Last Updated on August 8, 2019

Statistics and machine learning are two very closely related fields.

In fact, the line between the two can be very fuzzy at times. Nevertheless, there are methods that clearly belong to the field of statistics that are not only useful, but invaluable when working on a machine learning project.

It would be fair to say that statistical methods are required to effectively work through a machine learning predictive modeling project.

In this post, you will discover specific examples of statistical methods that are useful and required at key steps in a predictive modeling problem.

After completing this post, you will know:

Exploratory data analysis, data summarization, and data visualizations can be used to help frame your predictive modeling problem and better understand the data.

That statistical methods can be used to clean and prepare data ready for modeling.

That statistical hypothesis tests and estimation statistics can aid in model selection and in presenting the skill and predictions from final models.

Discover statistical hypothesis testing, resampling methods, estimation statistics and nonparametric methods in my new book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we are going to look at 10 examples of where statistical methods are used in an applied machine learning project.

This will demonstrate that a working knowledge of statistics is essential for successfully working through a predictive modeling problem.

Problem Framing Data Understanding Data Cleaning Data Selection Data Preparation Model Evaluation Model Configuration Model Selection Model Presentation Model Predictions

Need help with Statistics for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Problem Framing

Perhaps the point of biggest leverage in a predictive modeling problem is the framing of the problem.

This is the selection of the type of problem, e.g. regression or classification, and perhaps the structure and types of the inputs and outputs for the problem.

The framing of the problem is not always obvious. For newcomers to a domain, it may require significant exploration of the observations in the domain.

For domain experts that may be stuck seeing the issues from a conventional perspective, they too may benefit from considering the data from multiple perspectives.

Statistical methods that can aid in the exploration of the data during the framing of a problem include:

Exploratory Data Analysis . Summarization and visualization in order to explore ad hoc views of the data.

. Summarization and visualization in order to explore ad hoc views of the data. Data Mining. Automatic discovery of structured relationships and patterns in the data.

2. Data Understanding

Data understanding means having an intimate grasp of both the distributions of variables and the relationships between variables.

Some of this knowledge may come from domain expertise, or require domain expertise in order to interpret. Nevertheless, both experts and novices to a field of study will benefit from actually handeling real observations form the domain.

Two large branches of statistical methods are used to aid in understanding data; they are:

Summary Statistics . Methods used to summarize the distribution and relationships between variables using statistical quantities.

. Methods used to summarize the distribution and relationships between variables using statistical quantities. Data Visualization. Methods used to summarize the distribution and relationships between variables using visualizations such as charts, plots, and graphs.

3. Data Cleaning

Observations from a domain are often not pristine.

Although the data is digital, it may be subjected to processes that can damage the fidelity of the data, and in turn any downstream processes or models that make use of the data.

Some examples include:

Data corruption.

Data errors.

Data loss.

The process of identifying and repairing issues with the data is called data cleaning

Statistical methods are used for data cleaning; for example:

Outlier detection . Methods for identifying observations that are far from the expected value in a distribution.

. Methods for identifying observations that are far from the expected value in a distribution. Imputation. Methods for repairing or filling in corrupt or missing values in observations.

4. Data Selection

Not all observations or all variables may be relevant when modeling.

The process of reducing the scope of data to those elements that are most useful for making predictions is called data selection.

Two types of statistical methods that are used for data selection include:

Data Sample . Methods to systematically create smaller representative samples from larger datasets.

. Methods to systematically cre"
273;273;machinelearningmastery.com;https://machinelearningmastery.com/long-short-term-memory-recurrent-neural-networks-mini-course/;2017-08-15;Mini-Course on Long Short-Term Memory Recurrent Neural Networks with Keras;"Tweet Share Share

Last Updated on August 14, 2019

Long Short-Term Memory (LSTM) recurrent neural networks are one of the most interesting types of deep learning at the moment.

They have been used to demonstrate world-class results in complex problem domains such as language translation, automatic image captioning, and text generation.

LSTMs are different to multilayer Perceptrons and convolutional neural networks in that they are designed specifically for sequence prediction problems.

In this mini-course, you will discover how you can quickly bring LSTM models to your own sequence forecasting problems.

After completing this mini-course, you will know:

What LSTMs are, how they are trained, and how to prepare data for training LSTM models.

How to develop a suite of LSTM models including stacked, bidirectional, and encoder-decoder models.

How you can get the most out of your models with hyperparameter optimization, updating, and finalizing models.

Discover how to develop LSTMs such as stacked, bidirectional, CNN-LSTM, Encoder-Decoder seq2seq and more in my new book, with 14 step-by-step tutorials and full code.

Let’s get started.

Note: This is a big guide; you may want to bookmark it.

Who Is This Mini-Course For?

Before we get started, let’s make sure you are in the right place.

This course is for developers that know some applied machine learning and need to get good at LSTMs fast.

Maybe you want or need to start using LSTMs on your project. This guide was written to help you do that quickly and efficiently.

You know your way around Python.

You know your way around SciPy.

You know how to install software on your workstation.

You know how to wrangle your own data.

You know how to work through a predictive modeling problem with machine learning.

You may know a little bit of deep learning.

You may know a little bit of Keras.

You know how to set up your workstation to use Keras and scikit-learn; if not, you can learn how to here:

This guide was written in the top-down and results-first machine learning style that you’re used to. It will teach you how to get results, but it is not a panacea.

You will develop useful skills by working through this guide.

After completing this course, you will:

Know how LSTMs work.

Know how to prepare data for LSTMs.

Know how to apply a suite of types of LSTMs.

Know how to tune LSTMs to a problem.

Know how to save an LSTM model and use it to make predictions.

Next, let’s review the lessons.

Need help with LSTMs for Sequence Prediction? Take my free 7-day email course and discover 6 different LSTM architectures (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Mini-Course Overview

This mini-course is broken down into 14 lessons.

You could complete one lesson per day (recommended) or complete all of the lessons in one day (hardcore!).

It really depends on the time you have available and your level of enthusiasm.

Below are 14 lessons that will get you started and productive with LSTMs in Python. The lessons are divided into three main themes: foundations, models, and advanced.

Foundations

The focus of these lessons are the things that you need to know before using LSTMs.

Lesson 01 : What are LSTMs?

: What are LSTMs? Lesson 02 : How LSTMs are trained

: How LSTMs are trained Lesson 03 : How to prepare data for LSTMs

: How to prepare data for LSTMs Lesson 04: How to develop LSTMs in Keras

Models

Lesson 05 : How to develop Vanilla LSTMs

: How to develop Vanilla LSTMs Lesson 06 : How to develop Stacked LSTMs

: How to develop Stacked LSTMs Lesson 07 : How to develop CNN LSTMs

: How to develop CNN LSTMs Lesson 08 : How to develop Encoder-Decoder LSTMs

: How to develop Encoder-Decoder LSTMs Lesson 09 : How to develop Bi-directional LSTMs

: How to develop Bi-directional LSTMs Lesson 10 : How to develop LSTMs with Attention

: How to develop LSTMs with Attention Lesson 11: How to develop Generative LSTMs

Advanced

Lesson 12 : How to tune LSTM hyperparameters

: How to tune LSTM hyperparameters Lesson 13 : How to update LSTM models

: How to update LSTM models Lesson 14: How to make predictions with LSTMs

Each lesson could take you 60 seconds or up to 60 minutes. Take your time and complete the lessons at your own pace. Ask questions, and even post results in the comments below.

The lessons expect you to go off and find out how to do things. I will give you hints, but part of the point of each lesson is to force you to learn where to go to look for help (hint, I have all of the answers on this blog; use the search).

I do provide more help in the early lessons because I want you to build up some confidence and inertia.

Hang in there; don’t give up!

Foundations

The lessons in this section are designed to give you an understanding of how LSTMs work and how to implement LSTM models using the Keras library.

Lesson 1: What are LSTMs?

Goal

The goal of this lesson is to understa"
274;274;machinelearningmastery.com;http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/;2016-08-16;A Gentle Introduction to XGBoost for Applied Machine Learning;"Tweet Share Share

Last Updated on August 21, 2019

XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.

XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.

In this post you will discover XGBoost and get a gentle introduction to what is, where it came from and how you can learn more.

After reading this post you will know:

What XGBoost is and the goals of the project.

Why XGBoost must be apart of your machine learning toolkit.

Where you can learn more to start using XGBoost on your next machine learning project.

Discover how to configure, fit, tune and evaluation gradient boosting models with XGBoost in my new book, with 15 step-by-step tutorial lessons, and full python code.

Let’s get started.

Need help with XGBoost in Python? Take my free 7-day email course and discover configuration, tuning and more (with sample code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

What is XGBoost?

XGBoost stands for eXtreme Gradient Boosting.

The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.

— Tianqi Chen, in answer to the question “What is the difference between the R gbm (gradient boosting machine) and xgboost (extreme gradient boosting)?” on Quora

It is an implementation of gradient boosting machines created by Tianqi Chen, now with contributions from many developers. It belongs to a broader collection of tools under the umbrella of the Distributed Machine Learning Community or DMLC who are also the creators of the popular mxnet deep learning library.

Tianqi Chen provides a brief and interesting back story on the creation of XGBoost in the post Story and Lessons Behind the Evolution of XGBoost.

XGBoost is a software library that you can download and install on your machine, then access from a variety of interfaces. Specifically, XGBoost supports the following main interfaces:

Command Line Interface (CLI).

C++ (the language in which the library is written).

Python interface as well as a model in scikit-learn.

R interface as well as a model in the caret package.

Julia.

Java and JVM languages like Scala and platforms like Hadoop.

XGBoost Features

The library is laser focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features.

Model Features

The implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:

Gradient Boosting algorithm also called gradient boosting machine including the learning rate.

algorithm also called gradient boosting machine including the learning rate. Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.

with sub-sampling at the row, column and column per split levels. Regularized Gradient Boosting with both L1 and L2 regularization.

System Features

The library provides a system for use in a range of computing environments, not least:

Parallelization of tree construction using all of your CPU cores during training.

of tree construction using all of your CPU cores during training. Distributed Computing for training very large models using a cluster of machines.

for training very large models using a cluster of machines. Out-of-Core Computing for very large datasets that don’t fit into memory.

for very large datasets that don’t fit into memory. Cache Optimization of data structures and algorithm to make best use of hardware.

Algorithm Features

The implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:

Sparse Aware implementation with automatic handling of missing data values.

implementation with automatic handling of missing data values. Block Structure to support the parallelization of tree construction.

to support the parallelization of tree construction. Continued Training so that you can further boost an already fitted model on new data.

XGBoost is free open source software available for use under the permissive Apache-2 license.

Why Use XGBoost?

The two reasons to use XGBoost are also the two goals of the project:

Execution Speed. Model Performance.

1. XGBoost Execution Speed

Generally, XGBoost is fast. Really fast when compared to other implementations of gradient boosting.

Szilard Pafka performed some objective benchmarks comparing the performance of XGBoost to other implementations of gradient boosting and bagged decision trees. He wrote up his results in May 2015 in the blog post titled “Benchmarking"
275;275;machinelearningmastery.com;https://machinelearningmastery.com/how-to-caption-photos-with-deep-learning/;2017-11-12;How to Automatically Generate Textual Descriptions for Photographs with Deep Learning;"Tweet Share Share

Last Updated on August 7, 2019

Captioning an image involves generating a human readable textual description given an image, such as a photograph.

It is an easy problem for a human, but very challenging for a machine as it involves both understanding the content of an image and how to translate this understanding into natural language.

Recently, deep learning methods have displaced classical methods and are achieving state-of-the-art results for the problem of automatically generating descriptions, called “captions,” for images.

In this post, you will discover how deep neural network models can be used to automatically generate descriptions for images, such as photographs.

After completing this post, you will know:

About the challenge of generating textual descriptions for images and the need to combine breakthroughs from computer vision and natural language processing.

About the elements that comprise a neural feature captioning model, namely the feature extractor and language model.

How the elements of the model can be arranged into an Encoder-Decoder, possibly with the use of an attention mechanism.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

This post is divided into 3 parts; they are:

Describing an Image with Text Neural Captioning Model Encoder-Decoder Architecture

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

Describing an Image with Text

Describing an image is the problem of generating a human-readable textual description of an image, such as a photograph of an object or scene.

The problem is sometimes called “automatic image annotation” or “image tagging.”

It is an easy problem for a human, but very challenging for a machine.

A quick glance at an image is sufficient for a human to point out and describe an immense amount of details about the visual scene. However, this remarkable ability has proven to be an elusive task for our visual recognition models

— Deep Visual-Semantic Alignments for Generating Image Descriptions, 2015.

A solution requires both that the content of the image be understood and translated to meaning in the terms of words, and that the words must string together to be comprehensible. It combines both computer vision and natural language processing and marks a true challenging problem in broader artificial intelligence.

Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.

— Show and Tell: A Neural Image Caption Generator, 2015.

Further, the problems can range in difficulty; let’s look at three different variations on the problem with examples.

1. Classify Image

Assign an image a class label from one of hundreds or thousands of known classes.

2. Describe Image

Generate a textual description of the contents image.

3. Annotate Image

Generate textual descriptions for specific regions on the image.

The general problem can also be extended to describe images over time in video.

In this post, we will focus our attention on describing images, which we will describe as ‘image captioning.’

Neural Captioning Model

Neural network models have come to dominate the field of automatic caption generation; this is primarily because the methods are demonstrating state-of-the-art results.

The two dominant methods prior to end-to-end neural network models for generating image captions were template-based methods and nearest-neighbor-based methods and modifying existing captions.

Prior to the use of neural networks for generating captions, two main approaches were dominant. The first involved generating caption templates which were filled in based on the results of object detections and attribute discovery. The second approach was based on first retrieving similar captioned images from a large database then modifying these retrieved captions to fit the query. […] Both of these approaches have since fallen out of favour to the now dominant neural network methods.

— Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, 2015.

Neural network models for captioning involve two main elements:

Feature Extraction. Language Model.

Feature Extraction Model

The feature extraction model is a neural network that given an image is able to extract the salient features, often in the form of a fixed-length vector.

The extracted features are an internal representation of the image, not something directly intelligible.

A deep convolutional neural network, or CNN, is used as the feature extraction submodel. This network can be trained directly on the images in the image captioning dataset.

Alternately, a pre-trained"
276;276;machinelearningmastery.com;https://machinelearningmastery.com/how-to-load-visualize-and-explore-a-complex-multivariate-multistep-time-series-forecasting-dataset/;2018-10-11;How to Load, Visualize, and Explore a Multivariate Multistep Time Series Dataset;"Tweet Share Share

Last Updated on August 5, 2019

Real-world time series forecasting is challenging for a whole host of reasons not limited to problem features such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites.

The EMC Data Science Global Hackathon dataset, or the ‘Air Quality Prediction‘ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days.

In this tutorial, you will discover and explore the Air Quality Prediction dataset that represents a challenging multivariate, multi-site, and multi-step time series forecasting problem.

After completing this tutorial, you will know:

How to load and explore the chunk-structure of the dataset.

How to explore and visualize the input and target variables for the dataset.

How to use the new understanding to outline a suite of methods for framing the problem, preparing the data, and modeling the dataset.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Update Apr/2019: Fixed bug in the calculation of the total missing values (thanks zhangzhe).

Tutorial Overview

This tutorial is divided into seven parts; they are:

Problem Description Load Dataset Chunk Data Structure Input Variables Target Variables A Wrinkle With Target Variables Thoughts on Modeling

Problem Description

The EMC Data Science Global Hackathon dataset, or the ‘Air Quality Prediction‘ dataset for short, describes weather conditions at multiple sites and requires a prediction of air quality measurements over the subsequent three days.

Specifically, weather observations such as temperature, pressure, wind speed, and wind direction are provided hourly for eight days for multiple sites. The objective is to predict air quality measurements for the next three days at multiple sites. The forecast lead times are not contiguous; instead, specific lead times must be forecast over the 72 hour forecast period; they are:

+1, +2, +3, +4, +5, +10, +17, +24, +48, +72 1 +1, +2, +3, +4, +5, +10, +17, +24, +48, +72

Further, the dataset is divided into disjoint but contiguous chunks of data, with eight days of data followed by three days that require a forecast.

Not all observations are available at all sites or chunks and not all output variables are available at all sites and chunks. There are large portions of missing data that must be addressed.

The dataset was used as the basis for a short duration machine learning competition (or hackathon) on the Kaggle website in 2012.

Submissions for the competition were evaluated against the true observations that were withheld from participants and scored using Mean Absolute Error (MAE). Submissions required the value of -1,000,000 to be specified in those cases where a forecast was not possible due to missing data. In fact, a template of where to insert missing values was provided and required to be adopted for all submissions (what a pain).

A winning entrant achieved a MAE of 0.21058 on the withheld test set (private leaderboard) using random forest on lagged observations. A writeup of this solution is available in the post:

In this tutorial, we will explore this dataset in order to better understand the nature of the forecast problem and suggest approaches for how it may be modeled.

Load Dataset

The first step is to download the dataset and load it into memory.

The dataset can be downloaded for free from the Kaggle website. You may have to create an account and log in, in order to be able to download the dataset.

Download the entire dataset, e.g. “Download All” to your workstation and unzip the archive in your current working directory with the folder named ‘AirQualityPrediction‘

You should have five files in the AirQualityPrediction/ folder; they are:

SiteLocations.csv

SiteLocations_with_more_sites.csv

SubmissionZerosExceptNAs.csv

TrainingData.csv

sample_code.r

Our focus will be the ‘TrainingData.csv‘ that contains the training dataset, specifically data in chunks where each chunk is eight contiguous days of observations and target variables.

The test dataset (remaining three days of each chunk) is not available for this dataset at the time of writing.

Open the ‘TrainingData.csv‘ file and review the contents. The unzipped data file is relatively small (21 megabytes) and will easily fit into RAM.

Reviewing the contents of the file, we can see that the data file contains a header row.

We can also see that missing data is marked with the ‘NA‘ value, which Pandas will automatically convert to NumPy.NaN.

We can see that the ‘weekday‘ column contains the day as a string, whereas all other data is numeric.

Below are the first few lines of the data file for reference.

""rowID"",""chunkID"",""position_"
277;277;news.mit.edu;http://news.mit.edu/2020/inactive-pill-ingredients-could-raise-dose-your-medication-0317;;“Inactive” pill ingredients could raise the dose of your medication;"The average medication contains a mix of eight “inactive” ingredients added to pills to make them taste better, last longer, and stabilize the active ingredients within. Some of those additives are now getting a closer look for their ability to cause allergic reactions in some patients. But now, in a new twist, MIT researchers have discovered that two other inactive ingredients may actually boost medication strength to the benefit of some patients.

In a study published March 17 in Cell Reports, researchers report that vitamin A palmitate, a common supplement, and gum resin, a popular glazing agent for pills and chewing gum — could make hundreds of drugs more effective, from blood-clotting agents and anti-cancer drugs to over-the-counter pain relievers. They also outline a method for using machine learning to find other inactive ingredients with untapped therapeutic value.

“Anything you ingest has a potential effect, but tracing that effect to the molecular level can be a Herculean effort,” says the study’s senior author Giovanni Traverso, an assistant professor in the Department of Mechanical Engineering and a gastroenterologist at Brigham and Women’s Hospital. “Machine learning gives you a way to narrow down the search space.”

The researchers chose to focus their search on two proteins in the body known for their outsized role in drug delivery: the transporter protein P-glycoprotein (P-gp) and the metabolic protein UDP-Glucuronosyltranferase-2B7 (UGT2B7). One or both are involved in modulating the effects of 20 percent of the nearly 1,900 drugs approved by the U.S. Food and Drug Administration (FDA).

The researchers wanted to know if any of the FDA’s 800 approved food and drug additives would skew the functioning of either protein. Screening all 800 compounds by hand would be tedious and expensive. So, instead, they built a computer platform to do the work for them, adapting a method used by pharmaceutical companies to rule out drug-on-drug interactions.

They fed the system the chemical structures of the FDA’s 800 inactive ingredients, as well as millions of drugs and other compounds known to interfere with enzyme functioning. They then asked the platform to predict which food and drug additives would be most likely to disrupt P-gp and UGT2B7 and alter a drug’s potency by letting more into the body, in the case of P-gp, or slowing its exit, as in UGT2B7.

Machine learning allowed the researchers to quickly make comparisons between millions of drugs and inactive ingredients to identify the additives most likely to have an effect. Two top candidates emerged: vitamin A palmitate, as a predicted inhibitor of P-gp, and abietic acid, an ingredient in gum resin (basically, tree sap), as a predicted inhibitor of UGT2B7.

The researchers next moved to physically test the computer’s predictions in the lab. In one experiment, they gave mice vitamin A-fortified water followed by a normal dose of the blood-clotter, warfarin. With a simple blood test, they confirmed the mice had absorbed 30 percent more medication, a strong indication that vitamin A had improved the uptake of warfarin.

In a second experiment, they treated a small slice of pig liver with a substance that loses its ability to fluoresce as UGT2B7 digests it. When abietic acid was added, the substance continued to fluoresce. Drug developers use the test to confirm that a drug acts as an enzyme inhibitor, and here, researchers confirmed that abietic acid had, in fact, targeted UGT2B7 as predicted. Though no actual drug was tested, the results suggest that if gum resin were taken with a common pain reliever like ibuprofen, it could increase its strength, Traverso says, much as vitamin A had for warfarin in mice.

Machine learning methods are increasingly helping to identify and design new drugs. In a recent discovery, MIT researchers used a deep learning algorithm to find an entirely new antibiotic in the Drug Repurposing Hub, a database of compounds approved, or under review, for human use. Hiding in plain sight as a proposed diabetes treatment, the compound was identified because the algorithm had no preconceived ideas of what a bacteria-killing agent should look like.

Much like the Drug Repurposing Hub, the FDA’s inactive ingredient list is a big draw for drug developers. The ingredients are already on the market, even if they have yet to be approved for a new use, says the study’s lead author, Daniel Reker, a Swiss National Science Foundation postdoc at MIT’s Koch Institute for Integrative Cancer Research. If a promising biological association is uncovered, the discovery can be moved quickly to clinical trials. It can take years, by contrast, to test the safety of new molecules synthesized or discovered in the lab.

“While further tests are necessary to understand how strong these effects are in humans, our algorithms drew new conclusions that could have immediate impact,” says Reker. “Drug discovery is such a long and costly process, we’re excite"
278;278;machinelearningmastery.com;https://machinelearningmastery.com/implement-machine-learning-algorithm-performance-metrics-scratch-python/;2016-10-18;How To Implement Machine Learning Metrics From Scratch in Python;"# Example of Calculating and Displaying a Pretty Confusion Matrix

# calculate a confusion matrix

def confusion_matrix ( actual , predicted ) :

unique = set ( actual )

matrix = [ list ( ) for x in range ( len ( unique ) ) ]

for i in range ( len ( unique ) ) :

matrix [ i ] = [ 0 for x in range ( len ( unique ) ) ]

lookup = dict ( )

for i , value in enumerate ( unique ) :

lookup [ value ] = i

for i in range ( len ( actual ) ) :

x = lookup [ actual [ i ] ]

y = lookup [ predicted [ i ] ]

matrix [ y ] [ x ] += 1

return unique , matrix

# pretty print a confusion matrix

def print_confusion_matrix ( unique , matrix ) :

print ( '(A)' + ' ' . join ( str ( x ) for x in unique ) )

print ( '(P)---' )

for i , x in enumerate ( unique ) :

print ( ""%s| %s"" % ( x , ' ' . join ( str ( x ) for x in matrix [ i ] ) ) )

# Test confusion matrix with integers

actual = [ 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ]

predicted = [ 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 ]

unique , matrix = confusion_matrix ( actual , predicted )"
279;279;news.mit.edu;http://news.mit.edu/2017/maryann-gong-named-2017-ncaa-woman-year-top-30-honoree-0907;;Maryann Gong named 2017 NCAA Woman of the Year Top 30 honoree;"Former MIT All-American cross country/track standout Maryann Gong, from Livermore, California, has been named as a Top 30 honoree for the 2017 NCAA Woman of the Year Award. Gong is just the third student-athlete in MIT history to be honored as a Top 30 selection, joining Margaret Guo '16 (swimming and diving) and Lisa K. Arel '92 (gymnastics). Guo captured the 2016 Woman of the Year award and became the first MIT student-athlete to earn the honor.

This year, the NCAA received a program-record 543 school nominees, which were then trimmed to 145 female student-athletes that were nominated by conferences and an independent selection committee. Gong was one of 53 Division III student-athletes to advance to that stage, and she is now among the final 10 from Division III.

“It’s kind of hard to believe because there are so many people, so to be one of the top 30 is really an honor and I’m really grateful about it,” Gong says. “It’s definitely a great way to end my undergraduate career at MIT.”

Named as the CoSIDA Division III National Academic All-America of the Year for a second straight season, Gong is a 15-time All-American and one of the most decorated female student-athletes in MIT history. In 2016-17, she was a three-time indoor track and field All-American as she anchored the distance medley relay team that finished as the national runner-up. Posting a perfect 5.0 GPA as an undergraduate at MIT, she is currently pursuing her master’s degree at MIT in engineering with a concentration in artificial intelligence. A former NCAA champion in the 3,000 meters, Gong was the recipient of the 2017 NCAA Outdoor Track and Field Elite 90 award.

“The Top 30 honorees are remarkable representatives of the thousands of women competing in college sports each year,” says Sarah Hebberd, chair of the Woman of the Year selection committee and director of compliance at Georgia. “They have seized every opportunity available to them on the field of play, in the classroom and in the community, and we are proud to recognize them for their outstanding achievements.”

The selection committee will name nine finalists, with three from each division, in late September. From those nine finalists, the NCAA Committee on Women’s Athletics will select the 2017 Woman of the Year. That ceremony will take place on Oct. 23 at a ceremony in Indianapolis.

For the latest on MIT Athletics, follow the Engineers via social media on Twitter, Facebook, Instagram and YouTube."
280;280;machinelearningmastery.com;https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/;2019-03-24;How to Manually Scale Image Pixel Data for Deep Learning;"# example of per-channel centering (subtract mean)

from numpy import asarray

from PIL import Image

# load image

image = Image . open ( 'sydney_bridge.jpg' )

pixels = asarray ( image )

# convert from integers to floats

pixels = pixels . astype ( 'float32' )

# calculate per-channel means and standard deviations

means = pixels . mean ( axis = ( 0 , 1 ) , dtype = 'float64' )

print ( 'Means: %s' % means )

print ( 'Mins: %s, Maxs: %s' % ( pixels . min ( axis = ( 0 , 1 ) ) , pixels . max ( axis = ( 0 , 1 ) ) ) )

# per-channel centering of pixels

pixels -= means

# confirm it had the desired effect

means = pixels . mean ( axis = ( 0 , 1 ) , dtype = 'float64' )

print ( 'Means: %s' % means )"
281;281;machinelearningmastery.com;http://machinelearningmastery.com/a-data-driven-approach-to-machine-learning/;2014-09-28;A Data-Driven Approach to Choosing Machine Learning Algorithms;"Tweet Share Share

Last Updated on April 4, 2018

If You Knew Which Algorithm or Algorithm Configuration To Use,

You Would Not Need To Use Machine Learning

There is no best machine learning algorithm or algorithm parameters.

I want to cure you of this type of silver bullet mindset.

I see these questions a lot, even daily:

Which is the best machine learning algorithm?

What is the mapping between machine learning algorithms and problems?

What are the best parameters for a machine learning algorithm?

There is a pattern to these questions.

You generally do not and cannot know the answers to these questions beforehand. You must discover it through empirical study.

There are some broad brush heuristics to answer these questions, but even these can trip you up if you are looking to get the most from an algorithm or a problem.

In this post, I want to encourage you to break free of this mindset and take hold of a data-driven approach that is going to change they way you approach machine learning.

Best Machine Learning Algorithm

Some algorithms have more “power” than others. They are non-parametric or highly flexible and adaptive, or highly self-tuning or all of the above.

Typically this power comes at a cost of difficulty to implement, the need for very large datasets, limited scalability, or a large number of coefficients that may result in over-fitting.

With bigger datasets, there has been a renewed interest in simpler methods that scale and perform well.

Which is the best algorithm, the algorithm that you should always try and spend the most time learning?

I could throw out some names, but the smartest answer is “none” and “all“.

No Best Machine Learning Algorithm

You cannot know a priori which algorithm will be best suited for your problem.

Read the above line again. Meditate on it.

Meditate on it.

You can apply your favorite algorithm.

You can apply the algorithm recommended in a book or paper.

You can apply the algorithm that is winning the most Kaggle competitions right now.

You can apply the algorithm that works best with your test rig, infrastructure, database, or whatever.

These are biases.

They are short-cuts in thinking that save time. Some may, in fact, be very useful short-cuts, but which ones?

By definition, biases will limit the solutions that you can achieve, the accuracy you can achieve, and ultimately the impact that you can have.

Mapping of Algorithms to Problems

There are general classes of problems, say supervised problems like classification and regression and unsupervised problems like manifold learning and clustering.

There are more specific instances of these problems in sub-fields of machine learning like Computer Vision, Natural Language Processing and Speech Processing. We can also go the other way, more abstract and consider all of these problems as instances of function approximation and function optimization.

You can map algorithms to classes of problems, for example, there are algorithms that can handle supervised regression problems and supervised classification problems, and both types of problems.

You can also construct catalogs of algorithms, and that may be useful to inspire you as to which algorithms to try.

You can race algorithms on a problem and report the results. Sometimes this is called a bake-off and is popular in some conference proceedings for presenting new algorithms.

Limited Transferability of Algorithm Results

Generally, racing algorithms is anti-intellectual. It is rarely scientifically rigorous (apples to apples).

A key problem with racing algorithms is that you cannot easily transfer the findings from one problem to another. If you believe this statement is true, then reading about algorithm races in papers and blogs does not inform you about which algorithm to try on your problem.

If algorithm A kills algorithm B on Problem X, what does that tell you about algorithm A and B on problem Y? You have to work to relate problems X and Y. Do they have the same or similar properties (attributes, attribute distributions, functional form) that are exploited by the algorithms under study? That’s some hard work.

We do not have fine-grained understandings of when one machine learning algorithm works better than another.

Best Algorithm Parameters

Machine learning algorithms are parametrized so that you can tailor their behavior and results to your problem.

The problem is that “how” to do the tailoring is rarely (if ever) explained. Often, it is poorly understood, even by the algorithm developers themselves.

Generally, machine learning algorithms with stochastic elements are complex systems and must be studied as such. The first order – what effect does the parameter have on the complex system may be described. If so, you may have available some heuristics on how to configure the algorithm as a system.

It is the second order, what effect will it have on your results which is not known. Sometimes you can talk in generalities about t"
282;282;machinelearningmastery.com;https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/;2018-06-19;Statistical Significance Tests for Comparing Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 8, 2019

Comparing machine learning methods and selecting a final model is a common operation in applied machine learning.

Models are commonly evaluated using resampling methods like k-fold cross-validation from which mean skill scores are calculated and compared directly. Although simple, this approach can be misleading as it is hard to know whether the difference between mean skill scores is real or the result of a statistical fluke.

Statistical significance tests are designed to address this problem and quantify the likelihood of the samples of skill scores being observed given the assumption that they were drawn from the same distribution. If this assumption, or null hypothesis, is rejected, it suggests that the difference in skill scores is statistically significant.

Although not foolproof, statistical hypothesis testing can improve both your confidence in the interpretation and the presentation of results during model selection.

In this tutorial, you will discover the importance and the challenge of selecting a statistical hypothesis test for comparing machine learning models.

After completing this tutorial, you will know:

Statistical hypothesis tests can aid in comparing machine learning models and choosing a final model.

The naive application of statistical hypothesis tests can lead to misleading results.

Correct use of statistical tests is challenging, and there is some consensus for using the McNemar’s test or 5×2 cross-validation with a modified paired Student t-test.

Discover statistical hypothesis testing, resampling methods, estimation statistics and nonparametric methods in my new book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Update Oct/2018: Added link to an example of using McNemar’s test.

Tutorial Overview

This tutorial is divided into 5 parts; they are:

The Problem of Model Selection Statistical Hypothesis Tests Problem of Choosing a Hypothesis Test Summary of Some Findings Recommendations

Need help with Statistics for Machine Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

The Problem of Model Selection

A big part of applied machine learning is model selection.

We can describe this in its simplest form:

Given the evaluation of two machine learning methods on a dataset, which model do you choose?

You choose the model with the best skill.

That is, the model whose estimated skill when making predictions on unseen data is best. This might be maximum accuracy or minimum error in the case of classification and regression problems respectively.

The challenge with selecting the model with the best skill is determining how much can you trust the estimated skill of each model. More generally:

Is the difference in skill between two machine learning models real, or due to a statistical chance?

We can use statistical hypothesis testing to address this question.

Statistical Hypothesis Tests

Generally, a statistical hypothesis test for comparing samples quantifies how likely it is to observe two data samples given the assumption that the samples have the same distribution.

The assumption of a statistical test is called the null hypothesis and we can calculate statistical measures and interpret them in order to decide whether or not to accept or reject the null hypothesis.

In the case of selecting models based on their estimated skill, we are interested to know whether there is a real or statistically significant difference between the two models.

If the result of the test suggests that there is insufficient evidence to reject the null hypothesis, then any observed difference in model skill is likely due to statistical chance.

If the result of the test suggests that there is sufficient evidence to reject the null hypothesis, then any observed difference in model skill is likely due to a difference in the models.

The results of the test are probabilistic, meaning, it is possible to correctly interpret the result and for the result to be wrong with a type I or type II error. Briefly, a false positive or false negative finding.

Comparing machine learning models via statistical significance tests imposes some expectations that in turn will impact the types of statistical tests that can be used; for example:

Skill Estimate . A specific measure of model skill must be chosen. This could be classification accuracy (a proportion) or mean absolute error (summary statistic) which will limit the type of tests that can be used.

. A specific measure of model skill must be chosen. This could be classification accuracy (a proportion) or mean absolute error (summary statistic) which will limit the type of tests that can be used. Repeated Estimates . A sample of skill scores is required in order to calculate statistics. The repeated training and testing of a given model on the same or differ"
283;283;machinelearningmastery.com;https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/;2020-01-23;Tour of Data Sampling Methods for Imbalanced Classification;"Tweet Share Share

Machine learning techniques often fail or give misleadingly optimistic performance on classification datasets with an imbalanced class distribution.

The reason is that many machine learning algorithms are designed to operate on classification data with an equal number of observations for each class. When this is not the case, algorithms can learn that very few examples are not important and can be ignored in order to achieve good performance.

Data sampling provides a collection of techniques that transform a training dataset in order to balance or better balance the class distribution. Once balanced, standard machine learning algorithms can be trained directly on the transformed dataset without any modification. This allows the challenge of imbalanced classification, even with severely imbalanced class distributions, to be addressed with a data preparation method.

There are many different types of data sampling methods that can be used, and there is no single best method to use on all classification problems and with all classification models. Like choosing a predictive model, careful experimentation is required to discover what works best for your project.

In this tutorial, you will discover a suite of data sampling techniques that can be used to balance an imbalanced classification dataset.

After completing this tutorial, you will know:

The challenge of machine learning with imbalanced classification datasets.

The balancing of skewed class distributions using data sampling techniques.

Tour of data sampling methods for oversampling, undersampling, and combinations of methods.

Discover SMOTE, one-class classification, cost-sensitive learning, threshold moving, and much more in my new book, with 30 step-by-step tutorials and full Python source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into three parts; they are:

Problem of an Imbalanced Class Distribution Balance the Class Distribution With Data Sampling Tour of Popular Data Sampling Methods Oversampling Techniques Undersampling Techniques Combinations of Techniques

Problem of an Imbalanced Class Distribution

Imbalanced classification involves a dataset where the class distribution is not equal.

This means that the number of examples that belong to each class in the training dataset varies, often widely. It is not uncommon to have a severe skew in the class distribution, such as 1:10, 1:1000 or even 1:1000 ratio of examples in the minority class to those in the majority class.

… we define imbalanced learning as the learning process for data representation and information extraction with severe data distribution skews to develop effective decision boundaries to support the decision-making process.

— Page 1, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.

Although often described in terms of two-class classification problems, class imbalance also affects those datasets with more than two classes that may have multiple minority classes or multiple majority classes.

A chief problem with imbalanced classification datasets is that standard machine learning algorithms do not perform well on them. Many machine learning algorithms rely upon the class distribution in the training dataset to gauge the likelihood of observing examples in each class when the model will be used to make predictions.

As such, many machine learning algorithms, like decision trees, k-nearest neighbors, and neural networks, will therefore learn that the minority class is not as important as the majority class and put more attention and perform better on the majority class.

The hitch with imbalanced datasets is that standard classification learning algorithms are often biased towards the majority classes (known as “negative”) and therefore there is a higher misclassification rate in the minority class instances (called the “positive” class).

— Page 79, Learning from Imbalanced Data Sets, 2018.

This is a problem because the minority class is exactly the class that we care most about in imbalanced classification problems.

The reason for this is because the majority class often reflects a normal case, whereas the minority class represents a positive case for a diagnostic, fault, fraud, or other types of exceptional circumstance.

Want to Get Started With Imbalance Classification? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Balance the Class Distribution With Data Sampling

The most popular solution to an imbalanced classification problem is to change the composition of the training dataset.

Techniques designed to change the class distribution in the training dataset are generally referred to as sampling methods or resampling methods as we are sampling an existing data sample.

Sampling methods seem to be the dominate type of approach in the community as they tackle imbalanced learning in"
284;284;machinelearningmastery.com;https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/;2018-11-20;How to Use Weight Decay to Reduce Overfitting of Neural Network in Keras;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34

# grid search regularization values for moons dataset from sklearn . datasets import make_moons from keras . layers import Dense from keras . models import Sequential from keras . regularizers import l2 from matplotlib import pyplot # generate 2d classification dataset X , y = make_moons ( n_samples = 100 , noise = 0.2 , random_state = 1 ) # split into train and test n_train = 30 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] # grid search values values = [ 1e - 1 , 1e - 2 , 1e - 3 , 1e - 4 , 1e - 5 , 1e - 6 ] all_train , all_test = list ( ) , list ( ) for param in values : # define model model = Sequential ( ) model . add ( Dense ( 500 , input_dim = 2 , activation = 'relu' , kernel_regularizer = l2 ( param ) ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) # fit model model . fit ( trainX , trainy , epochs = 4000 , verbose = 0 ) # evaluate the model _ , train_acc = model . evaluate ( trainX , trainy , verbose = 0 ) _ , test_acc = model . evaluate ( testX , testy , verbose = 0 ) print ( 'Param: %f, Train: %.3f, Test: %.3f' % ( param , train_acc , test_acc ) ) all_train . append ( train_acc ) all_test . append ( test_acc ) # plot train and test means pyplot . semilogx ( values , all_train , label = 'train' , marker = 'o' ) pyplot . semilogx ( values , all_test , label = 'test' , marker = 'o' ) pyplot . legend ( ) pyplot . show ( )"
285;285;news.mit.edu;http://news.mit.edu/2020/researchers-discover-new-way-control-infrared-light-0130;;Researchers discover a new way to control infrared light;"In the 1950s, the field of electronics began to change when the transistor replaced vacuum tubes in computers. The change, which entailed replacing large and slow components with small and fast ones, was a catalyst for the enduring trend of miniaturization in computer design. No such revolution has yet hit the field of infrared optics, which remains reliant on bulky moving parts that preclude building small systems.

However, a team of researchers at MIT Lincoln Laboratory, together with Professor Juejun Hu and graduate students from MIT's Department of Materials Science and Engineering, is devising a way to control infrared light by using phase-change materials instead of moving parts. These materials have the ability to change their optical properties when energy is added to them.

“There are multiple possible ways where this material can enable new photonic devices that impact people’s lives,” says Hu. “For example, it can be useful for energy-efficient optical switches, which can improve network speed and reduce power consumption of internet data centers. It can enable reconfigurable meta-optical devices, such as compact, flat infrared zoom lenses without mechanical moving parts. It can also lead to new computing systems, which can make machine learning faster and more power-efficient compared to current solutions.”

A fundamental property of phase-change materials is that they can change how fast light travels through them (the refractive index). “There are already ways to modulate light using a refractive index change, but phase-change materials can change almost 1,000 times better,” says Jeffrey Chou, a team member formerly in the laboratory's Advanced Materials and Microsystems Group.

The team successfully controlled infrared light in multiple systems by using a new class of phase-change material containing the elements germanium, antimony, selenium, and tellurium, collectively known as GSST. This work is discussed in a paper published in Nature Communications.

A phase-change material's magic occurs in the chemical bonds that tie its atoms together. In one phase state, the material is crystalline, with its atoms arranged in an organized pattern. This state can be changed by applying a short, high-temperature spike of thermal energy to the material, causing the bonds in the crystal to break down and then reform in a more random, or amorphous, pattern. To change the material back to the crystalline state, a long- and medium-temperature pulse of thermal energy is applied.

“This changing of the chemical bonds allows for different optical properties to emerge, similar to the differences between coal (amorphous) and diamond (crystalline),” says Christopher Roberts, another Lincoln Laboratory member of the research team. “While both materials are mostly carbon, they have vastly different optical properties.”

Currently, phase-change materials are used for industry applications, such as Blu-ray technology and rewritable DVDs, because their properties are useful for storing and erasing a large amount of information. But so far, no one has used them in infrared optics because they tend to be transparent in one state and opaque in the other. (Think of the diamond, which light can pass through, and coal, which light cannot penetrate.) If light cannot pass through one of the states, then that light cannot be adequately controlled for a range of uses; instead, a system would only be able to work like an on/off switch, allowing light to either pass through the material or not pass through at all.

However, the research team found that that by adding the element selenium to the original material (called GST), the material's absorption of infrared light in the crystalline phase decreased dramatically — in essence, changing it from an opaque coal-like material to a more transparent diamond-like one. What's more, the large difference in the refractive index of the two states affects the propagation of light through them.

“This change in refractive index, without introducing optical loss, allows for the design of devices that control infrared light without the need for mechanical parts,” Roberts says.

As an example, imagine a laser beam that is pointing in one direction and needs to be changed to another. In current systems, a large mechanical gimbal would physically move a lens to steer the beam to another position. A thin-film lens made of GSST would be able change positions by electrically reprogramming the phase-change materials, enabling beam steering with no moving parts.

The team has already tested the material successfully in a moving lens. They have also demonstrated its use in infrared hyperspectral imaging, which is used to analyze images for hidden objects or information, and in a fast optical shutter that was able to close in nanoseconds.

The potential uses for GSST are vast, and an ultimate goal for the team is to design reconfigurable optical chips, lenses, and filters, which currently must be re"
286;286;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/;2017-11-09;How to Develop a Word-Level Neural Language Model and Use it to Generate Text;"Tweet Share Share

Last Updated on August 7, 2019

A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence.

Neural network models are a preferred method for developing statistical language models because they can use a distributed representation where different words with similar meanings have similar representation and because they can use a large context of recently observed words when making predictions.

In this tutorial, you will discover how to develop a statistical language model using deep learning in Python.

After completing this tutorial, you will know:

How to prepare text for developing a word-based language model.

How to design and fit a neural language model with a learned embedding and an LSTM hidden layer.

How to use the learned language model to generate new text with similar statistical properties as the source text.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Update Apr/2018: Fixed mismatch between 100 input words in description of the model and 50 in the actual model.

Tutorial Overview

This tutorial is divided into 4 parts; they are:

The Republic by Plato Data Preparation Train Language Model Use Language Model

The Republic by Plato

The Republic is the classical Greek philosopher Plato’s most famous work.

It is structured as a dialog (e.g. conversation) on the topic of order and justice within a city state

The entire text is available for free in the public domain. It is available on the Project Gutenberg website in a number of formats.

You can download the ASCII text version of the entire book (or books) here:

Download the book text and place it in your current working directly with the filename ‘republic.txt‘

Open the file in a text editor and delete the front and back matter. This includes details about the book at the beginning, a long analysis, and license information at the end.

The text should begin with:

BOOK I. I went down yesterday to the Piraeus with Glaucon the son of Ariston,

…

And end with

…

And it shall be well with us both in this life and in the pilgrimage of a thousand years which we have been describing.

Save the cleaned version as ‘republic_clean.txt’ in your current working directory. The file should be about 15,802 lines of text.

Now we can develop a language model from this text.

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

Data Preparation

We will start by preparing the data for modeling.

The first step is to look at the data.

Review the Text

Open the text in an editor and just look at the text data.

For example, here is the first piece of dialog:

BOOK I. I went down yesterday to the Piraeus with Glaucon the son of Ariston,

that I might offer up my prayers to the goddess (Bendis, the Thracian

Artemis.); and also because I wanted to see in what manner they would

celebrate the festival, which was a new thing. I was delighted with the

procession of the inhabitants; but that of the Thracians was equally,

if not more, beautiful. When we had finished our prayers and viewed the

spectacle, we turned in the direction of the city; and at that instant

Polemarchus the son of Cephalus chanced to catch sight of us from a

distance as we were starting on our way home, and told his servant to

run and bid us wait for him. The servant took hold of me by the cloak

behind, and said: Polemarchus desires you to wait. I turned round, and asked him where his master was. There he is, said the youth, coming after you, if you will only wait. Certainly we will, said Glaucon; and in a few minutes Polemarchus

appeared, and with him Adeimantus, Glaucon’s brother, Niceratus the son

of Nicias, and several others who had been at the procession. Polemarchus said to me: I perceive, Socrates, that you and your

companion are already on your way to the city. You are not far wrong, I said. …

What do you see that we will need to handle in preparing the data?

Here’s what I see from a quick look:

Book/Chapter headings (e.g. “BOOK I.”).

British English spelling (e.g. “honoured”)

Lots of punctuation (e.g. “–“, “;–“, “?–“, and more)

Strange names (e.g. “Polemarchus”).

Some long monologues that go on for hundreds of lines.

Some quoted dialog (e.g. ‘…’)

These observations, and more, suggest at ways that we may wish to prepare the text data.

The specific way we prepare the data really depends on how we intend to model it, which in turn depends on how we intend to use it.

Language Model Design

In this tutorial, we will develop a model of the text that we can then use to generate new sequences of text.

The language model will be statistical and will predict the probability of ea"
287;287;news.mit.edu;http://news.mit.edu/2019/four-newmac-championships-successful-fall-season-mit-athletics-0114;;MIT athletes earn four NEWMAC championships;"The fall season was another successful one for MIT Athletics as the Engineers achieved both athletically and academically.

MIT teams claimed four New England Women’s and Men’s Athletic Conference (NEWMAC) Championships, eight programs were nationally-ranked, and four teams represented MIT at NCAA Championship events programs. Six student-athletes were named All-Americans, 52 earned All-Conference honors, and 11 Engineers were recognized with major awards from the NEWMAC.

Academically, MIT amassed 96 NEWMAC Academic All-Conference selections and seven Google Cloud Academic All-America Team members as MIT is now one of just two schools nationally to have over 300 all-time Google Cloud Academic All-America honorees.

At the conclusion of the fall, MIT was ranked No. 12 nationally in the Learfield Directors’ Cup standings out of 449 NCAA Division III institutions. The Engineers generated 218.5 points, which was based on each team’s finish at NCAA Championship events.

Men’s Cross Country finished 16th at the NCAA Championship and captured the program’s 21st NEWMAC Championship, maintaining its status as the only team to win the title in conference history. Head coach Halston Taylor was named NEWMAC Coach of the Year for the fifth year in a row and 16th time during his career. Sophomore Billy Woltz earned NEWMAC Athlete of the Year accolades, first-year Andrew Mah collected NEWMAC Rookie of the Year plaudits, and senior Josh Rosenkranz was selected to NEWMAC All-Sportsmanship Team. Seven student-athletes earned spots on the NEWMAC All-Conference team while Mah, Rosenkranz, Woltz, and junior Josh Derrick qualified for the U.S. Track and Field and Cross Country Coaches Association (USTFCCCA) New England All-Region Team.

Women’s Cross Country captured third place at the NCAA Championship and won the program’s 12th straight NEWMAC Championship. Senior Leandra Zimmermann earned USTFCCCA All-America honors while head coach Halston Taylor was voted the USTFCCCA New England Region Coach of the Year for the fifth time. First-year Einat Gavish was named the NEWMAC Rookie of the Year and junior Marissa McPhillips was selected for the NEWMAC All-Sportsmanship Team. Eight student-athletes represented MIT on the NEWMAC All-Conference Team, including USTFCCCA New England All-Region honorees Gavish, Zimmermann, junior Katie Bacher, and sophomores Katie Collins and Jenna Melanson.

Field Hockey advanced to the NEWMAC Championship for the third year in a row but fell to Smith College, 2-1, in overtime. Junior Devon Goetz was tabbed for National Field Hockey Coaches Association (NFHCA) All-America Third Team accolades and was joined by sophomores Megan Flynn and Amanda Garofalo on the NFHCA New England East All-Region Team. The trio also picked up NEWMAC All-Conference awards while sophomore Jennah Haque represented the Engineers on the NEWMAC All-Sportsmanship Team.

MIT Football was crowned NEWMAC Champions, marking the program’s second conference title, and qualified for the NCAA Tournament for the second time in Institute history. In addition to receiving 13 NEWMAC All-Conference Team selections, first-year head coach Brian Bubna was named the NEWMAC Coach of the Year, senior Udgam Goyal was voted the NEWMAC Offensive Athlete of the Year, and senior Riley Quinn earned a spot on the NEWMAC All-Sportsmanship Team. Junior Ben Bennington was selected to the New England Football Writers Division II/III All-New England Team while Quinn was the recipient of the organization’s Jerry Nason Award for Senior Achievement, which recognizes a student-athlete succeeding in football against all odds. For the second year in a row, Goyal was named the Google Cloud Academic All-America Team Member of the Year for Division III Football. He was also joined by junior AJ Iversen on the Google Cloud Academic All-America First Team.

Men’s Soccer fell to Wheaton College, 3-1, in penalty kicks after a scoreless double-overtime quarterfinal game in the NEWMAC Championship Tournament. Junior Jeremy Cowham, and seniors Thad Daguilh and Wesley Woo earned NEWMAC All-Conference accolades while senior David Wu was selected to the NEWMAC All-Sportsmanship Team. Woo represented the Engineers on the United Soccer Coaches All-Region Second Team and was voted to the Google Cloud Academic All-America Third Team.

Women’s Soccer claimed its second straight and fifth overall NEWMAC Championship after defeating Springfield College, 2-1, in overtime. The Engineers’ season ended with a 1-0 loss at Amherst College in the second round of the NCAA Tournament. In addition to receiving seven NEWMAC All-Conference Team selections, head coach Martin Desmarais was named the NEWMAC Coach of the Year for the fifth time in six seasons, junior Sophia Struckman was voted the NEWMAC Offensive Athlete of the Year, senior Hailey Nichols repeated as the NEWMAC Defensive Athlete of the Year, and senior Allie Hrabchak earned a spot on the NEWMAC All-Sportsmanship Team. Struckman was t"
288;288;news.mit.edu;http://news.mit.edu/2020/neuroscience-memory-cells-interpret-new-0406;;Neuroscientists find memory cells that help us interpret new situations;"Imagine you are meeting a friend for dinner at a new restaurant. You may try dishes you haven’t had before, and your surroundings will be completely new to you. However, your brain knows that you have had similar experiences — perusing a menu, ordering appetizers, and splurging on dessert are all things that you have probably done when dining out.

MIT neuroscientists have now identified populations of cells that encode each of these distinctive segments of an overall experience. These chunks of memory, stored in the hippocampus, are activated whenever a similar type of experience takes place, and are distinct from the neural code that stores detailed memories of a specific location.

The researchers believe that this kind of “event code,” which they discovered in a study of mice, may help the brain interpret novel situations and learn new information by using the same cells to represent similar experiences.

“When you encounter something new, there are some really new and notable stimuli, but you already know quite a bit about that particular experience, because it’s a similar kind of experience to what you have already had before,” says Susumu Tonegawa, a professor of biology and neuroscience at the RIKEN-MIT Laboratory of Neural Circuit Genetics at MIT’s Picower Institute for Learning and Memory.

Tonegawa is the senior author of the study, which appears today in Nature Neuroscience. Chen Sun, an MIT graduate student, is the lead author of the paper. New York University graduate student Wannan Yang and Picower Institute technical associate Jared Martin are also authors of the paper.

Encoding abstraction

It is well-established that certain cells in the brain’s hippocampus are specialized to store memories of specific locations. Research in mice has shown that within the hippocampus, neurons called place cells fire when the animals are in a specific location, or even if they are dreaming about that location.

In the new study, the MIT team wanted to investigate whether the hippocampus also stores representations of more abstract elements of a memory. That is, instead of firing whenever you enter a particular restaurant, such cells might encode “dessert,” no matter where you’re eating it.

To test this hypothesis, the researchers measured activity in neurons of the CA1 region of the mouse hippocampus as the mice repeatedly ran a four-lap maze. At the end of every fourth lap, the mice were given a reward. As expected, the researchers found place cells that lit up when the mice reached certain points along the track. However, the researchers also found sets of cells that were active during one of the four laps, but not the others. About 30 percent of the neurons in CA1 appeared to be involved in creating this “event code.”

“This gave us the initial inkling that besides a code for space, cells in the hippocampus also care about this discrete chunk of experience called lap 1, or this discrete chunk of experience called lap 2, or lap 3, or lap 4,” Sun says.

To further explore this idea, the researchers trained mice to run a square maze on day 1 and then a circular maze on day 2, in which they also received a reward after every fourth lap. They found that the place cells changed their activity, reflecting the new environment. However, the same sets of lap-specific cells were activated during each of the four laps, regardless of the shape of the track. The lap-encoding cells’ activity also remained consistent when laps were randomly shortened or lengthened.

“Even in the new spatial locations, cells still maintain their coding for the lap number, suggesting that cells that were coding for a square lap 1 have now been transferred to code for a circular lap 1,” Sun says.

The researchers also showed that if they used optogenetics to inhibit sensory input from a part of the brain called the medial entorhinal cortex (MEC), lap-encoding did not occur. They are now investigating what kind of input the MEC region provides to help the hippocampus create memories consisting of chunks of an experience.

Two distinct codes

These findings suggest that, indeed, every time you eat dinner, similar memory cells are activated, no matter where or what you’re eating. The researchers theorize that the hippocampus contains “two mutually and independently manipulatable codes,” Sun says. One encodes continuous changes in location, time, and sensory input, while the other organizes an overall experience into smaller chunks that fit into known categories such as appetizer and dessert.

“We believe that both types of hippocampal codes are useful, and both are important,” Tonegawa says. “If we want to remember all the details of what happened in a specific experience, moment-to-moment changes that occurred, then the continuous monitoring is effective. But on the other hand, when we have a longer experience, if you put it into chunks, and remember the abstract order of the abstract chunks, that’s more effective than monitoring this lon"
289;289;machinelearningmastery.com;https://machinelearningmastery.com/how-to-load-and-explore-household-electricity-usage-data/;2018-09-27;How to Load and Explore Household Electricity Usage Data;"# load and clean-up data

from numpy import nan

from pandas import read_csv

# load all data

dataset = read_csv ( 'household_power_consumption.txt' , sep = ';' , header = 0 , low_memory = False , infer_datetime_format = True , parse_dates = { 'datetime' : [ 0 , 1 ] } , index_col = [ 'datetime' ] )

# summarize

print ( dataset . shape )

print ( dataset . head ( ) )

# mark all missing values

dataset . replace ( '?' , nan , inplace = True )

# add a column for for the remainder of sub metering

values = dataset . values . astype ( 'float32' )

dataset [ 'sub_metering_4' ] = ( values [ : , 0 ] * 1000 / 60 ) - ( values [ : , 4 ] + values [ : , 5 ] + values [ : , 6 ] )

# save updated dataset

dataset . to_csv ( 'household_power_consumption.csv' )

# load the new dataset and summarize

dataset = read_csv ( 'household_power_consumption.csv' , header = 0 , infer_datetime_format = True , parse_dates = [ 'datetime' ] , index_col = [ 'datetime' ] )"
290;290;machinelearningmastery.com;https://machinelearningmastery.com/how-to-perform-object-detection-in-photographs-with-mask-r-cnn-in-keras/;2019-05-23;How to Use Mask R-CNN in Keras for Object Detection in Photographs;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44

# example of inference with a pre-trained coco model from keras . preprocessing . image import load_img from keras . preprocessing . image import img_to_array from mrcnn . visualize import display_instances from mrcnn . config import Config from mrcnn . model import MaskRCNN # define 81 classes that the coco model knowns about class_names = [ 'BG' , 'person' , 'bicycle' , 'car' , 'motorcycle' , 'airplane' , 'bus' , 'train' , 'truck' , 'boat' , 'traffic light' , 'fire hydrant' , 'stop sign' , 'parking meter' , 'bench' , 'bird' , 'cat' , 'dog' , 'horse' , 'sheep' , 'cow' , 'elephant' , 'bear' , 'zebra' , 'giraffe' , 'backpack' , 'umbrella' , 'handbag' , 'tie' , 'suitcase' , 'frisbee' , 'skis' , 'snowboard' , 'sports ball' , 'kite' , 'baseball bat' , 'baseball glove' , 'skateboard' , 'surfboard' , 'tennis racket' , 'bottle' , 'wine glass' , 'cup' , 'fork' , 'knife' , 'spoon' , 'bowl' , 'banana' , 'apple' , 'sandwich' , 'orange' , 'broccoli' , 'carrot' , 'hot dog' , 'pizza' , 'donut' , 'cake' , 'chair' , 'couch' , 'potted plant' , 'bed' , 'dining table' , 'toilet' , 'tv' , 'laptop' , 'mouse' , 'remote' , 'keyboard' , 'cell phone' , 'microwave' , 'oven' , 'toaster' , 'sink' , 'refrigerator' , 'book' , 'clock' , 'vase' , 'scissors' , 'teddy bear' , 'hair drier' , 'toothbrush' ] # define the test configuration class TestConfig ( Config ) : NAME = ""test"" GPU_COUNT = 1 IMAGES_PER_GPU = 1 NUM_CLASSES = 1 + 80 # define the model rcnn = MaskRCNN ( mode = 'inference' , model_dir = './' , config = TestConfig ( ) ) # load coco model weights rcnn . load_weights ( 'mask_rcnn_coco.h5' , by_name = True ) # load photograph img = load_img ( 'elephant.jpg' ) img = img_to_array ( img ) # make prediction results = rcnn . detect ( [ img ] , verbose = 0 ) # get dictionary for first prediction r = results [ 0 ] # show photo with bounding boxes, masks, class labels and scores display_instances ( img , r [ 'rois' ] , r [ 'masks' ] , r [ 'class_ids' ] , class_names , r [ 'scores' ] )"
291;291;machinelearningmastery.com;https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/;2018-04-26;How to Calculate Correlation Between Variables in Python;"# generate related variables

from numpy import mean

from numpy import std

from numpy . random import randn

from numpy . random import seed

from matplotlib import pyplot

# seed random number generator

seed ( 1 )

# prepare data

data1 = 20 * randn ( 1000 ) + 100

data2 = data1 + ( 10 * randn ( 1000 ) + 50 )

# summarize

print ( 'data1: mean=%.3f stdv=%.3f' % ( mean ( data1 ) , std ( data1 ) ) )

print ( 'data2: mean=%.3f stdv=%.3f' % ( mean ( data2 ) , std ( data2 ) ) )

# plot

pyplot . scatter ( data1 , data2 )"
292;292;news.mit.edu;http://news.mit.edu/2020/event-horizon-telescope-observations-black-hole-powered-jet-0414;;Event Horizon Telescope observes a black hole-powered jet;"One year ago, the Event Horizon Telescope (EHT) Collaboration published the first image of a black hole in the nearby radio galaxy M87. The collaboration has now extracted additional new information from the EHT data on the distant quasar 3C 279, allowing them to image in the finest detail ever a relativistic jet that is believed to originate from the vicinity of a supermassive black hole. New analyses, led by Jae-Young Kim from the Max Planck Institute for Radio Astronomy in Bonn, Germany, enabled the collaboration to trace the jet back to its launch point, close to where violently variable radiation from across the electromagnetic spectrum arises. The results are published in the April 2020 issue of Astronomy & Astrophysics.

The EHT collaboration is continuing to extract information from the groundbreaking data collected in its April 2017 global campaign. One target of the observations was a galaxy 5 billion light years away in the constellation Virgo that scientists classify as a quasar because an ultra-luminous source of energy at its center shines and flickers as gas falls into a giant black hole. This target, labeled 3C 279, contains a black hole about 1 billion times more massive than our sun. Twin fire hose-like jets of plasma erupt from the black hole and disk system at velocities close to the speed of light, a consequence of the enormous forces unleashed as matter descends into the black hole’s immense gravity. To capture the new image, the EHT uses a technique called very long baseline interferometry (VLBI), which synchronizes and links radio dishes around the world. By combining this network to form one huge virtual Earth-size telescope, the EHT is able to resolve objects as small as 20 micro-arcseconds on the sky — the equivalent of someone on Earth identifying an orange on the moon. Data recorded at all the EHT sites around the world are transported to special supercomputers at MIT Haystack Observatory and MPIfR in Bonn, Germany, where they are combined. The combined dataset is then carefully calibrated and analyzed by a team of experts, which then enables EHT scientists to produce images with the finest detail possible from the surface of the Earth. For 3C 279, the unprecedented resolution of the EHT reveals fine features of the jet that have never been seen before. In particular, the newly analyzed data show that the normally straight jet has an unexpected twisted shape at its base. Jae-Young Kim, of the Max Planck Institute for Radio Astronomy and lead author of the paper, is enthusiastic and at the same time puzzled: “We knew that every time you open a new window to the universe you can find something new. Here, where we expected to find the region where the jet forms by going to the sharpest image possible, we find a kind of perpendicular structure. This is like finding a very different shape by opening the smallest matryoshka doll.” Colin Lonsdale, director of MIT Haystack Observatory and vice chair of the EHT directing board, explains: “This array was developed specifically for the purpose of imaging the shadows of black holes, but as so often happens in science, improved capabilities lead to unexpected discoveries. This surprising result for 3C 279 is a good example, providing new information on the process of jet formation that challenges current understanding.” “The results are very surprising,” says Kazunori Akiyama, a Jansky Fellow of the National Radio Astronomy Observatory at MIT Haystack Observatory. Akiyama developed imaging techniques for the EHT to create the first images of the black hole in M87; these algorithms were also used to create the images of quasar 3C 279. “When we observed the quasar for four days within one week, we assumed that we would not see these dynamical changes because the source is so far away (100 times further from Earth than M87). But the EHT observations were so sharp that for the first time we could see tiny changes in motions of the jets within this time frame.” Opportunities to conduct EHT observing campaigns occur once a year in early springtime, but the March-April 2020 campaign had to be canceled in response to the Covid-19 global outbreak. In announcing the cancellation, Michael Hecht, MIT Haystack Observatory astronomer and EHT deputy project director, concluded that: “We will now devote our full concentration to completion of scientific publications from the 2017 data and dive into the analysis of data obtained with the enhanced EHT array in 2018. We are looking forward to observations with the EHT array expanded to 11 observatories in the spring of 2021.” The individual telescopes involved in the EHT collaboration are: the Atacama Large Millimetre Telescope, the Atacama Pathfinder EXplorer, the Greenland Telescope (since 2018), the IRAM 30-meter Telescope, the IRAM NOEMA Observatory (expected 2021), the Kitt Peak Telescope (expected 2021), the James Clerk Maxwell Telescope, the Large Millimeter Telescope, the Submillimeter Array, the Subm"
293;293;machinelearningmastery.com;https://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates/;2019-01-01;Impact of Dataset Size on Deep Learning Model Skill And Performance Estimates;"# study of test set size for an mlp on the circles problem

from sklearn . datasets import make_circles

from keras . layers import Dense

from keras . models import Sequential

from numpy import mean

from matplotlib import pyplot

# create dataset

def create_dataset ( n_test , n_train = 1000 , noise = 0.1 ) :

# generate samples

n_samples = n_train + n _ test

X , y = make_circles ( n_samples = n_samples , noise = noise , random_state = 1 )

# split into train and test, first n for test

trainX , testX = X [ : n_train , : ] , X [ n_train : , : ]

trainy , testy = y [ : n_train ] , y [ n_train : ]

# return samples

return trainX , trainy , testX , testy

# fit an mlp model

def fit_model ( trainX , trainy ) :

# define model

model = Sequential ( )

model . add ( Dense ( 25 , input_dim = 2 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# fit model

model . fit ( trainX , trainy , epochs = 500 , verbose = 0 )

return model

# evaluate a test set of a given size on the fit models

def evaluate_test_set_size ( models , n_test ) :

# create dataset

_ , _ , testX , testy = create_dataset ( n_test )

scores = list ( )

for model in models :

# evaluate the model

_ , test_acc = model . evaluate ( testX , testy , verbose = 0 )

scores . append ( test_acc )

return scores

# create fixed training dataset

trainX , trainy , _ , _ = create_dataset ( 10 )

# fit one model for each repeat

n_repeats = 10

models = [ fit_model ( trainX , trainy ) for _ in range ( n_repeats ) ]

print ( 'Fit %d models' % n_repeats )

# define test set sizes to evaluate

sizes = [ 100 , 1000 , 5000 , 10000 ]

score_sets , means = list ( ) , list ( )

for n_test in sizes :

# evaluate a test set of a given size on the models

scores = evaluate_test_set_size ( models , n_test )

score_sets . append ( scores )

# summarize score for size

mean_score = mean ( scores )

means . append ( mean_score )

print ( 'Test Size=%d, Test Accuracy %.3f' % ( n_test , mean_score* 100 ) )

# summarize relationship of test size to test accuracy

pyplot . plot ( sizes , means , marker = 'o' )

pyplot . show ( )

# plot distributions of test size to test accuracy

pyplot . boxplot ( score_sets , labels = sizes )"
294;294;news.mit.edu;http://news.mit.edu/2020/graduate-engineering-business-programs-rank-us-news-2021-0317;;MIT graduate engineering, business programs ranked highly by U.S. News for 2021;"MIT’s graduate program in engineering has again earned a No. 1 spot in U.S. News and World Report’s annual rankings, a place it has held since 1990, when the magazine first ranked such programs.

The MIT Sloan School of Management also placed highly, occupying the No. 5 spot for the best graduate business programs.

Among individual engineering disciplines, MIT placed first in six areas: aerospace/aeronautical/astronautical engineering (tied with Caltech), chemical engineering, computer engineering, electrical/electronic/communications engineering (tied with Stanford University and the University of California at Berkeley), materials engineering, and mechanical engineering. It placed second in nuclear engineering.

In the rankings of individual MBA specialties, MIT placed first in four areas: business analytics, information systems, production/operations, and project management. It placed second in supply chain/logistics.

U.S. News does not issue annual rankings for all doctoral programs but revisits many every few years. In 2018, MIT ranked in the top five for 24 of the 37 science disciplines evaluated.

The magazine bases its rankings of graduate schools of engineering and business on two types of data: reputational surveys of deans and other academic officials, and statistical indicators that measure the quality of a school’s faculty, research, and students. The magazine’s less-frequent rankings of programs in the sciences, social sciences, and humanities are based solely on reputational surveys."
295;295;machinelearningmastery.com;https://machinelearningmastery.com/promise-deep-learning-natural-language-processing/;2017-09-24;Promise of Deep Learning for Natural Language Processing;"Tweet Share Share

Last Updated on August 7, 2019

The promise of deep learning in the field of natural language processing is the better performance by models that may require more data but less linguistic expertise to train and operate.

There is a lot of hype and large claims around deep learning methods, but beyond the hype, deep learning methods are achieving state-of-the-art results on challenging problems. Notably in natural language processing.

In this post, you will discover the specific promises that deep learning methods have for tackling natural language processing problems.

After reading this post, you will know:

The promises of deep learning for natural language processing.

What practitioners and research scientists have to say about the promise of deep learning in NLP.

Key deep learning methods and applications for natural language processing.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Promise of Deep Learning

Deep learning methods are popular, primarily because they are delivering on their promise.

That is not to say that there is no hype around the technology, but that the hype is based on very real results that are being demonstrated across a suite of very challenging artificial intelligence problems from computer vision and natural language processing.

Some of the first large demonstrations of the power of deep learning were in natural language processing, specifically speech recognition. More recently in machine translation.

In this post, we will look at five specific promises of deep learning methods in the field of natural language processing. Promises highlighted recently by researchers and practitioners in the field, people who may be more tempered than the average reported in what the promises may be.

In summary, they are:

The Promise of Drop-in Replacement Models. That is, deep learning methods can be dropped into existing natural language systems as replacement models that can achieve commensurate or better performance. The Promise of New NLP Models. That is, deep learning methods offer the opportunity of new modeling approaches to challenging natural language problems like sequence-to-sequence prediction. The Promise of Feature Learning. That is, that deep learning methods can learn the features from natural language required by the model, rather than requiring that the features be specified and extracted by an expert. The Promise of Continued Improvement. That is, that the performance of deep learning in natural language processing is based on real results and that the improvements appear to be continuing and perhaps speeding up. The Promise of End-to-End Models. That is, that large end-to-end deep learning models can be fit on natural language problems offering a more general and better-performing approach.

We will now take a closer look at each.

There are other promises of deep learning for natural language processing; these were just the 5 that I chose to highlight.

What do you think the promise of deep learning is for natural language processing?

Let me know in the comments below.

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

1. Promise of Drop-in Replacement Models

The first promise for deep learning in natural language processing is the ability to replace existing linear models with better performing models capable of learning and exploiting nonlinear relationships.

Yoav Goldberg, in his primer on neural networks for NLP researchers, highlights both that deep learning methods are achieving impressive results.

More recently, neural network models started to be applied also to textual natural language signals, again with very promising results.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

He goes on to highlight that the methods are easy to use and can sometimes be used to wholesale replace existing linear methods.

Recently, the field has seen some success in switching from such linear models over sparse inputs to non-linear neural-network models over dense inputs. While most of the neural network techniques are easy to apply, sometimes as almost drop-in replacements of the old linear classifiers, there is in many cases a strong barrier of entry.

— A Primer on Neural Network Models for Natural Language Processing, 2015.

2. Promise of New NLP Models

Another promise is that deep learning methods facilitate developing entirely new models.

One strong example is the use of recurrent neural networks that are able learn and condition output over very long sequences. The approach is sufficiently different in that they allow the practitioner to break free of traditional modeling assumptions and in turn achieve state-of-"
296;296;news.mit.edu;http://news.mit.edu/2019/how-new-kilogram-measures-up-0514;;How the new kilogram measures up;"CAMBRIDGE, Mass. – It promises to be a weighty moment: On Monday, May 20 (World Metrology Day) the definition for the kilogram, the base unit of mass, will change. You might not notice it when weighing fruits and vegetables at the grocery store, but the new definition could have positive implications for areas where extremely precise measurements are required, such as with the use of nanodevices, when concocting the correct dose of compounds used in a medicine, or when calculating the weight of a newly discovered subatomic particle.

On Monday, May 20 at 4:00 p.m., Professor Wolfgang Ketterle, the 2001 Nobel laureate in physics, will present a special lecture exploring the new standards of measurement and why this is such a historic shift for scientists around the world. Ketterle will delve into how, for the first time ever, the definition for the kilogram — as well as the definitions for the base units of charge, temperature, and mole — will shift from being based on physical objects that can change over time, to fundamental constants that scientists around the world will be able to reproduce.

Reporters are invited to attend to learn more about how this seemingly insignificant alteration to the underpinnings of our measurement system could not only help reduce uncertainty when calculating values using the international standards of measurement, but also help democratize access to the system of weights and measures so that it is no longer tied to a specific country or place.

WHAT:

World Metrology Day Special Lecture: The New Kilogram

Presented by Professor Wolfgang Ketterle, the 2001 Nobel Laureate in Physics

WHERE:

MIT’s Huntington Hall

Building 10, Room 250 (map: http://whereis.mit.edu/?go=10)

WHEN:

Monday, May 20, 2019

4:00 P.M. – 5:00 P.M.

Media RSVP:

Reporters interested in attending should email Abby Abazorius at abbya@mit.edu or expertrequests@mit.edu to RSVP and for more information."
297;297;machinelearningmastery.com;http://machinelearningmastery.com/save-load-keras-deep-learning-models/;2019-05-12;How to Save and Load Your Keras Deep Learning Model;"# MLP for Pima Indians Dataset Serialize to JSON and HDF5

from keras . models import Sequential

from keras . layers import Dense

from keras . models import model_from_json

import numpy

import os

# fix random seed for reproducibility

numpy . random . seed ( 7 )

# load pima indians dataset

dataset = numpy . loadtxt ( ""pima-indians-diabetes.csv"" , delimiter = "","" )

# split into input (X) and output (Y) variables

X = dataset [ : , 0 : 8 ]

Y = dataset [ : , 8 ]

# create model

model = Sequential ( )

model . add ( Dense ( 12 , input_dim = 8 , activation = 'relu' ) )

model . add ( Dense ( 8 , activation = 'relu' ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# Compile model

model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

# Fit the model

model . fit ( X , Y , epochs = 150 , batch_size = 10 , verbose = 0 )

# evaluate the model

scores = model . evaluate ( X , Y , verbose = 0 )

print ( ""%s: %.2f%%"" % ( model . metrics_names [ 1 ] , scores [ 1 ] * 100 ) )

# serialize model to JSON

model_json = model . to_json ( )

with open ( ""model.json"" , ""w"" ) as json_file :

json_file . write ( model_json )

# serialize weights to HDF5

model . save_weights ( ""model.h5"" )

print ( ""Saved model to disk"" )

# later...

# load json and create model

json_file = open ( 'model.json' , 'r' )

loaded_model_json = json_file . read ( )

json_file . close ( )

loaded_model = model_from_json ( loaded_model_json )

# load weights into new model

loaded_model . load_weights ( ""model.h5"" )

print ( ""Loaded model from disk"" )

# evaluate loaded model on test data

loaded_model . compile ( loss = 'binary_crossentropy' , optimizer = 'rmsprop' , metrics = [ 'accuracy' ] )

score = loaded_model . evaluate ( X , Y , verbose = 0 )"
298;298;machinelearningmastery.com;https://machinelearningmastery.com/applications-of-deep-learning-for-computer-vision/;2019-03-12;9 Applications of Deep Learning for Computer Vision;"Tweet Share Share

Last Updated on July 5, 2019

The field of computer vision is shifting from statistical methods to deep learning neural network methods.

There are still many challenging problems to solve in computer vision. Nevertheless, deep learning methods are achieving state-of-the-art results on some specific problems.

It is not just the performance of deep learning models on benchmark problems that is most interesting; it is the fact that a single model can learn meaning from images and perform vision tasks, obviating the need for a pipeline of specialized and hand-crafted methods.

In this post, you will discover nine interesting computer vision tasks where deep learning methods are achieving some headway.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Overview

In this post, we will look at the following computer vision problems where deep learning has been used:

Image Classification Image Classification With Localization Object Detection Object Segmentation Image Style Transfer Image Colorization Image Reconstruction Image Super-Resolution Image Synthesis Other Problems

Note, when it comes to the image classification (recognition) tasks, the naming convention from the ILSVRC has been adopted. Although the tasks focus on images, they can be generalized to the frames of video.

I have tried to focus on the types of end-user problems that you may be interested in, as opposed to more academic sub-problems where deep learning does well.

Each example provides a description of the problem, an example, and references to papers that demonstrate the methods and results.

Do you have a favorite computer vision application for deep learning that is not listed?

Let me know in the comments below.

Image Classification

Image classification involves assigning a label to an entire image or photograph.

This problem is also referred to as “object classification” and perhaps more generally as “image recognition,” although this latter task may apply to a much broader set of tasks related to classifying the content of images.

Some examples of image classification include:

Labeling an x-ray as cancer or not (binary classification).

Classifying a handwritten digit (multiclass classification).

Assigning a name to a photograph of a face (multiclass classification).

A popular example of image classification used as a benchmark problem is the MNIST dataset.

A popular real-world version of classifying photos of digits is The Street View House Numbers (SVHN) dataset.

For state-of-the-art results and relevant papers on these and other image classification tasks, see:

There are many image classification tasks that involve photographs of objects. Two popular examples include the CIFAR-10 and CIFAR-100 datasets that have photographs to be classified into 10 and 100 classes respectively.

The Large Scale Visual Recognition Challenge (ILSVRC) is an annual competition in which teams compete for the best performance on a range of computer vision tasks on data drawn from the ImageNet database. Many important advancements in image classification have come from papers published on or about tasks from this challenge, most notably early papers on the image classification task. For example:

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Image Classification With Localization

Image classification with localization involves assigning a class label to an image and showing the location of the object in the image by a bounding box (drawing a box around the object).

This is a more challenging version of image classification.

Some examples of image classification with localization include:

Labeling an x-ray as cancer or not and drawing a box around the cancerous region.

Classifying photographs of animals and drawing a box around the animal in each scene.

A classical dataset for image classification with localization is the PASCAL Visual Object Classes datasets, or PASCAL VOC for short (e.g. VOC 2012). These are datasets used in computer vision challenges over many years.

The task may involve adding bounding boxes around multiple examples of the same object in the image. As such, this task may sometimes be referred to as “object detection.”

The ILSVRC2016 Dataset for image classification with localization is a popular dataset comprised of 150,000 photographs with 1,000 categories of objects.

Some examples of papers on image classification with localization include:

Object Detection

Object detection is the task of image classification with localization, although an image may contain multiple objects that require localization and classification.

This is a more challenging task than simple image "
299;299;machinelearningmastery.com;https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/;2016-11-08;How To Implement The Decision Tree Algorithm From Scratch In Python;"# CART on the Bank Note dataset

from random import seed

from random import randrange

from csv import reader

# Load a CSV file

def load_csv ( filename ) :

file = open ( filename , ""rt"" )

lines = reader ( file )

dataset = list ( lines )

return dataset

# Convert string column to float

def str_column_to_float ( dataset , column ) :

for row in dataset :

row [ column ] = float ( row [ column ] . strip ( ) )

# Split a dataset into k folds

def cross_validation_split ( dataset , n_folds ) :

dataset_split = list ( )

dataset_copy = list ( dataset )

fold_size = int ( len ( dataset ) / n_folds )

for i in range ( n_folds ) :

fold = list ( )

while len ( fold ) < fold_size :

index = randrange ( len ( dataset_copy ) )

fold . append ( dataset_copy . pop ( index ) )

dataset_split . append ( fold )

return dataset_split

# Calculate accuracy percentage

def accuracy_metric ( actual , predicted ) :

correct = 0

for i in range ( len ( actual ) ) :

if actual [ i ] == predicted [ i ] :

correct += 1

return correct / float ( len ( actual ) ) * 100.0

# Evaluate an algorithm using a cross validation split

def evaluate_algorithm ( dataset , algorithm , n_folds , * args ) :

folds = cross_validation_split ( dataset , n_folds )

scores = list ( )

for fold in folds :

train_set = list ( folds )

train_set . remove ( fold )

train_set = sum ( train_set , [ ] )

test_set = list ( )

for row in fold :

row_copy = list ( row )

test_set . append ( row_copy )

row_copy [ - 1 ] = None

predicted = algorithm ( train_set , test_set , * args )

actual = [ row [ - 1 ] for row in fold ]

accuracy = accuracy_metric ( actual , predicted )

scores . append ( accuracy )

return scores

# Split a dataset based on an attribute and an attribute value

def test_split ( index , value , dataset ) :

left , right = list ( ) , list ( )

for row in dataset :

if row [ index ] < value :

left . append ( row )

else :

right . append ( row )

return left , right

# Calculate the Gini index for a split dataset

def gini_index ( groups , classes ) :

# count all samples at split point

n_instances = float ( sum ( [ len ( group ) for group in groups ] ) )

# sum weighted Gini index for each group

gini = 0.0

for group in groups :

size = float ( len ( group ) )

# avoid divide by zero

if size == 0 :

continue

score = 0.0

# score the group based on the score for each class

for class_val in classes :

p = [ row [ - 1 ] for row in group ] . count ( class_val ) / size

score += p * p

# weight the group score by its relative size

gini += ( 1.0 - score ) * ( size / n_instances )

return gini

# Select the best split point for a dataset

def get_split ( dataset ) :

class_values = list ( set ( row [ - 1 ] for row in dataset ) )

b_index , b_value , b_score , b_groups = 999 , 999 , 999 , None

for index in range ( len ( dataset [ 0 ] ) - 1 ) :

for row in dataset :

groups = test_split ( index , row [ index ] , dataset )

gini = gini_index ( groups , class_values )

if gini < b_score :

b_index , b_value , b_score , b_groups = index , row [ index ] , gini , groups

return { 'index' : b_index , 'value' : b_value , 'groups' : b_groups }

# Create a terminal node value

def to_terminal ( group ) :

outcomes = [ row [ - 1 ] for row in group ]

return max ( set ( outcomes ) , key = outcomes . count )

# Create child splits for a node or make terminal

def split ( node , max_depth , min_size , depth ) :

left , right = node [ 'groups' ]

del ( node [ 'groups' ] )

# check for a no split

if not left or not right :

node [ 'left' ] = node [ 'right' ] = to_terminal ( left + right )

return

# check for max depth

if depth >= max_depth :

node [ 'left' ] , node [ 'right' ] = to_terminal ( left ) , to_terminal ( right )

return

# process left child

if len ( left ) <= min_size :

node [ 'left' ] = to_terminal ( left )

else :

node [ 'left' ] = get_split ( left )

split ( node [ 'left' ] , max_depth , min_size , depth + 1 )

# process right child

if len ( right ) <= min_size :

node [ 'right' ] = to_terminal ( right )

else :

node [ 'right' ] = get_split ( right )

split ( node [ 'right' ] , max_depth , min_size , depth + 1 )

# Build a decision tree

def build_tree ( train , max_depth , min_size ) :

root = get_split ( train )

split ( root , max_depth , min_size , 1 )

return root

# Make a prediction with a decision tree

def predict ( node , row ) :

if row [ node [ 'index' ] ] < node [ 'value' ] :

if isinstance ( node [ 'left' ] , dict ) :

return predict ( node [ 'left' ] , row )

else :

return node [ 'left' ]

else :

if isinstance ( node [ 'right' ] , dict ) :

return predict ( node [ 'right' ] , row )

else :

return node [ 'right' ]

# Classification and Regression Tree Algorithm

def decision_tree ( train , test , max_depth , min_size ) :

tree = build_tree ( train , max_depth , min_size )

predictions = list ( )

for row in test :

prediction = predict ( tree , row )

predictions . append ( prediction )

return ( predictions )

#"
300;300;news.mit.edu;http://news.mit.edu/2020/3-questions-how-marine-life-can-recover-by-2050-0403;2020-03-19;3 Questions: Greg Britten on how marine life can recover by 2050;"As the largest ecosystem on the planet, the ocean provides incredible resources and benefits to humanity — including contributing 2.5 percent of global GDP and 1.5 percent of global employment, as well as regulating our climate, providing clean energy, and producing much of the oxygen we breathe. But exploitation and human pressures — like pollution, overfishing, and climate change — have stressed its life-support systems, depleting biodiversity, reducing habitats, and undermining ocean productivity.

Study and public awareness of the of these problems, as well as the beauty of these ecosystems, has led to conservation efforts beginning in the 1980s. By that time, however, significant damage had been done and some losses were permanent. Years of increased management and international policy since then have made measurable gains. At the same time, growing human populations are leaning harder on ocean resources. Understanding the critical need to rebuild these habitats and species populations has reached the level of the United Nations, which instated the Sustainable Development Goal 14 to “conserve and sustainably use the oceans, seas and marine resources for sustainable development.” The effort sets benchmarks and indicators of environmental successes in the area but threats, both local and international, persist and in some cases are worsening.

In a new Nature Review paper, Greg Britten, a postdoc in the MIT Department of Earth, Atmospheric and Planetary Sciences, and his colleagues examine different aspects of marine life and argue that aggressive interventions could lead to recovery of marine life by 2050. Here, he elucidates some of the findings from this work, which was supported, in part, by the Simons Collaboration on Computational Biogeochemical Modeling of Marine Ecosystems/CBIOMES.

Q: What is the current state of the world’s marine life and what recovery efforts have been attempted in the past?

A: While marine populations have been exploited throughout all of human history, the rate and magnitude of exploitation expanded exponentially between the 1950s and 1990s, largely due to the advent of industrial-scale fishing technology and large-scale habitat destruction via development of coastal areas. By the year 2000, it was estimated that the oceans’ “big fish” (tunas, large sharks, and billfish) were depleted by 90 percent relative to pre-exploitation levels. Further, approximately 60 percent of the world’s fisheries were considered “collapsed,” meaning that catches were at, or below, 10 percent of their historical maximum. At the same time, habitat destruction reached unprecedented levels — particularly in coastal areas.

These findings caused a tremendous response when revealed to the public that led to widespread calls for conservation intervention. Since then, marine exploitation has been significantly curtailed in much of the developed world, to a point where levels of exploitation are widely considered “sustainable”. Major global policy initiatives, like the Convention on the Trade of Endangered Species (CITES) and improvements to the Clean Water Act, also significantly reduced conservation threats like pollution, as well as the implementation of the International Convention for the Prevention of Pollution from Ships.

But this does not mean that populations immediately rebounded — indeed, they did not. It can take many years and decades for populations to fully rebuild to previous levels after the rate of exploitation has been reduced, and the impact of historical pollution and habitat destruction can linger for decades or longer. Furthermore, rates of exploitation and habitat destruction in the rest of the developing world have not been reduced as quickly, or remain unknown, while agreements to limit pollution and habitat destruction are generally also much weaker in developing countries.

Q: Tell us about your assessment of various interventions and potential future outcomes. What efforts have been successful so far, and where is there room for improvement?

A: We used a very large synthesis of available data to calculate historical and future trajectories of depleted marine populations under various levels of exploitation globally. We also documented the rates of recovery of habitats and ecosystems after pollution reductions and remediations were implemented.

We found that conservation and pollution reduction efforts, along with global environmental policy initiatives, have had a strong net positive influence on the recovery of marine populations, habitats, and ecosystems. We documented many cases of coral reef and mangrove recovery after local pollution remediation efforts. These occurred on a similar time scale as fish stocks, ranging from one to two decades for saltmarshes, to 30 years to a century for deep-sea corals and sponges that grow more slowly and are facing climate change, trawling, and oil spills. Globally, our research showed that the number of species listed as endangere"
301;301;machinelearningmastery.com;https://machinelearningmastery.com/deep-learning-for-time-series-forecasting/;;Deep Learning for Time Series Forecasting;"Deep Learning for Time Series Forecasting

Predict the Future with MLPs , CNNs and LSTMs in Python

$37 USD Deep learning methods offer a lot of promise for time series forecasting, such as the automatic learning of temporal dependence and the automatic handling of temporal structures like trends and seasonality. In this new Ebook written in the friendly Machine Learning Mastery style that you’re used to, skip the math and jump straight to getting results. With clear explanations, standard Python libraries, and step-by-step tutorial lessons you’ll discover how to develop deep learning models for your own time series forecasting projects. About this Ebook: Read on all devices : PDF format Ebook, no DRM.

: PDF format Ebook, no DRM. Tons of tutorials : 5 parts, 25 step-by-step lessons, 575 pages.

: 5 parts, 25 step-by-step lessons, 575 pages. Real-world projects : 2 large end-to-end tutorial projects.

: 2 large end-to-end tutorial projects. Many datasets : Univariate, multivariate, multi-step, and more.

: Univariate, multivariate, multi-step, and more. Working code: 131 Python (.py) code files included. Clear, Complete End-to-End Examples. Convinced?

Click to jump straight to the packages.

The book is very clear, well written and easy to apply. Moreover, Jason is always reactive to questions, helpful, and ready to give advices and help target the part that treats the ML problem you are trying to solve or to give advice. I definitely recommend the it. Saad Mouti Postdoctoral Fellow

Excellent book covering / comparing both deep learning and classical methods for time series. The code is clear and easily transferable to my own work. Claude Pelletier Machine Learning Lead

…why deep learning?

The Promise of Deep Learning for Time Series Forecasting

Traditionally, time series forecasting has been dominated by linear methods because they are well understood and effective on many simpler forecasting problems.

Deep learning neural networks are able to automatically learn arbitrary complex mappings from inputs to outputs and support multiple inputs and outputs.

Multilayer Perceptrons (MLPs)

Generally, neural networks like Multilayer Perceptrons or MLPs provide capabilities that are offered by few algorithms, such as:

Robust to Noise . Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values.

. Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values. Nonlinear . Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships.

. Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships. Multivariate Inputs . An arbitrary number of input features can be specified, providing direct support for multivariate forecasting.

. An arbitrary number of input features can be specified, providing direct support for multivariate forecasting. Multi-step Forecasts. An arbitrary number of output values can be specified, providing

direct support for multi-step and even multivariate forecasting.

For these capabilities alone, feedforward neural networks may be useful for time series forecasting.

Convolutional Neural Networks (CNNs)

Convolutional Neural Networks or CNNs are a type of neural network that was designed to efficiently handle image data.

The ability of CNNs to learn and automatically extract features from raw input data can be applied to time series forecasting problems. A sequence of observations can be treated like a one-dimensional image that a CNN model can read and distill into the most salient elements.

Feature Learning. Automatic identification, extraction and distillation of salient features from raw input data that pertain directly to the prediction problem that is being modeled.

CNNs get the benefits of Multilayer Perceptrons for time series forecasting, namely support for multivariate input, multivariate output and learning arbitrary but complex functional relationships, but do not require that the model learn directly from lag observations. Instead, the model can learn a representation from a large input sequence that is most relevant for the prediction problem.

Long Short-Term Memory Networks (LSTMs)

Recurrent neural networks like the Long Short-Term Memory network or LSTM add the explicit handling of order between observations when learning a mapping function from inputs to outputs, not offered by MLPs or CNNs. They are a type of neural network that adds native support for input data comprised of sequences of observations.

Native Support for Sequences. Recurrent neural networks directly add support for input sequence data.

This capability of LSTMs has been used to great effect in complex natural language processing problems such as neural machine translation whe"
302;302;machinelearningmastery.com;https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/;2019-04-02;How to Normalize, Center, and Standardize Image Pixels in Keras;"# example of using ImageDataGenerator to normalize images

from keras . datasets import mnist

from keras . utils import to_categorical

from keras . models import Sequential

from keras . layers import Conv2D

from keras . layers import MaxPooling2D

from keras . layers import Dense

from keras . layers import Flatten

from keras . preprocessing . image import ImageDataGenerator

# load dataset

( trainX , trainY ) , ( testX , testY ) = mnist . load_data ( )

# reshape dataset to have a single channel

width , height , channels = trainX . shape [ 1 ] , trainX . shape [ 2 ] , 1

trainX = trainX . reshape ( ( trainX . shape [ 0 ] , width , height , channels ) )

testX = testX . reshape ( ( testX . shape [ 0 ] , width , height , channels ) )

# one hot encode target values

trainY = to_categorical ( trainY )

testY = to_categorical ( testY )

# confirm scale of pixels

print ( 'Train min=%.3f, max=%.3f' % ( trainX . min ( ) , trainX . max ( ) ) )

print ( 'Test min=%.3f, max=%.3f' % ( testX . min ( ) , testX . max ( ) ) )

# create generator (1.0/255.0 = 0.003921568627451)

datagen = ImageDataGenerator ( rescale = 1.0 / 255.0 )

# prepare an iterators to scale images

train_iterator = datagen . flow ( trainX , trainY , batch_size = 64 )

test_iterator = datagen . flow ( testX , testY , batch_size = 64 )

print ( 'Batches train=%d, test=%d' % ( len ( train_iterator ) , len ( test_iterator ) ) )

# confirm the scaling works

batchX , batchy = train_iterator . next ( )

print ( 'Batch shape=%s, min=%.3f, max=%.3f' % ( batchX . shape , batchX . min ( ) , batchX . max ( ) ) )

# define model

model = Sequential ( )

model . add ( Conv2D ( 32 , ( 3 , 3 ) , activation = 'relu' , input_shape = ( width , height , channels ) ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Conv2D ( 64 , ( 3 , 3 ) , activation = 'relu' ) )

model . add ( MaxPooling2D ( ( 2 , 2 ) ) )

model . add ( Flatten ( ) )

model . add ( Dense ( 64 , activation = 'relu' ) )

model . add ( Dense ( 10 , activation = 'softmax' ) )

# compile model

model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ] )

# fit model with generator

model . fit_generator ( train_iterator , steps_per_epoch = len ( train_iterator ) , epochs = 5 )

# evaluate model

_ , acc = model . evaluate_generator ( test_iterator , steps = len ( test_iterator ) , verbose = 0 )"
303;303;news.mit.edu;http://news.mit.edu/2020/david-rand-0412;;Playing a new tune;"What’s it like being a professor at the MIT Sloan School of Management? David Rand has an answer you might not expect.

“Being an academic is like being in a punk-rock band,” says Rand, the Erwin H. Schell Professor in MIT Sloan’s Marketing Group.

Oh? How so?

“The short version is, in both cases, you start by trying to come up with a new idea that nobody’s used before,” explains Rand, who studies human behavior, cooperation, and social networks. “In academia, it’s a good research idea, and in music it’s a cool riff or melody. Then you take that kernel and spend a lot of time developing it into this cohesive whole that you try to make a perfect as possible.”

Sounds reasonable. What happens next?

“Once that’s done, you have to capture it in a way you can share with other people, which is either writing the paper, or recording the song,” continues Rand, who also has a joint appointment with MIT's Department of Brain and Cognitive Sciences. “That’s always the most painful part. It’s always tempting to just start new projects or songs, rather than putting in the work to finish the recording you’ve already created, but you have to do it.”

Rand spent years playing guitar and bass in punk-rock bands when he was younger, but has generated hits of a different kind more recently, as a professor who writes innovative academic papers on social phenomena such as cooperation and the spread of misinformation on social media. Much of Rand’s work explores what happens when people’s behavior is guided by intuitive thinking or a more deliberative mode of cognition. With that framework in mind, he seeks to understand what decisions people will make in social settings, like whether to pay costs to help others, what news to believe and share online, and whom to vote for.

Rand is now running the academic version of a recording studio, if you will. He is the director of MIT’s Human Cooperation Laboratory and co-director of MIT’s Applied Cooperation Team — settings where he acts like a record producer, collaborating with other scholars to help them pursue their own research ideas.

For his distinctive body of research, Rand joined MIT with tenure in 2018, feeling that his work was “very MIT, very Sloan” in its emphases on networks and real-world impact.

“I visited, and once I saw what people are doing, I said, ‘This is great,’” Rand recalls. “There are so many connections between my interests and what people are working on in the marketing group, in other groups at Sloan, and across MIT generally.”

Holidays in the sun

Rand grew up in Ithaca, New York, where his father was an applied math professor at Cornell University. As an undergraduate at Cornell himself, Rand majored in computational biology. That helped him become interested in evolutionary biology — including questions about how cooperation and altruism fit in a framework of evolutionary competition.

But academia wasn’t the only thing that got Rand interested in cooperation — so did playing in punk bands, which for him included traveling down from Cornell to Florida and other places on winter break and in the summers.

“I had grown up with the basic understanding that people were by nature selfish, although my parents would say, ‘You didn’t get that from us,’” Rand reflects. “I guess it was growing up in the 1980s.”

Still, he continues, while touring with his band, “we had so many experiences of total strangers being nice to us and helping us. The parents of fans, every show, would be like, ‘Here’s some random band from New York, you can sleep on the floor, we’ll make you breakfast.’ Our van broke down and a mechanic helped us for free because he felt bad for us. It transformed my idea of human nature. At least, under the right circumstances, people can be very prosocial.”

A couple years after college, Rand was accepted into Harvard University’s PhD program in systems biology, although without, he says, a firm grasp of what he wanted to study. However, taking a class on evolutionary game theory, Rand recounts, “I fell in love with the prisoner’s dilemma,” the classic problem in which two prisoners can collectively benefit the most by cooperating, but individually benefit by pursuing their own self-interests.

Rand starting doing experiments about the prisoner’s dilemma, motivated by a simple question concerning such situations — “What do people actually do?” — and has never really stopped. Since 2008, he has co-authored over 100 peer-reviewed papers about cooperation, altruism, and the spread of ideas and behavior in networks.

Cambridge calling

In 2009, Rand earned his PhD from Harvard, and, after some postdoctoral fellowships, joined the Yale University faculty in 2013. He earned tenure from Yale in 2018. The same year, he received his job offer from MIT Sloan and joined the Institute, attracted in part by the opportunities for interdisciplinary research.

“I think there are tons of opportunities for real innovation that come from combining approaches from dif"
304;304;machinelearningmastery.com;http://machinelearningmastery.com/load-csv-machine-learning-data-weka/;2016-06-22;How To Load CSV Machine Learning Data in Weka;"Tweet Share Share

Last Updated on August 22, 2019

You must be able to load your data before you can start modeling it.

In this post you will discover how you can load your CSV dataset in Weka. After reading this post, you will know:

About the ARFF file format and how it is the default way to represent data in Weka.

How to load a CSV file in the Weka Explorer and save it in ARFF format.

How to load a CSV file in the ArffViewer tool and save it in ARFF format.

This tutorial assumes that you already have Weka installed.

Discover how to prepare data, fit models, and evaluate their predictions, all without writing a line of code in my new book, with 18 step-by-step tutorials and 3 projects with Weka.

Let’s get started.

How to Talk About Data in Weka

Machine learning algorithms are primarily designed to work with arrays of numbers.

This is called tabular or structured data because it is how data looks in a spreadsheet, comprised of rows and columns.

Weka has a specific computer science centric vocabulary when describing data:

Instance : A row of data is called an instance, as in an instance or observation from the problem domain.

: A row of data is called an instance, as in an instance or observation from the problem domain. Attribute: A column of data is called a feature or attribute, as in feature of the observation.

Each attribute can have a different type, for example:

Real for numeric values like 1.2.

for numeric values like 1.2. Integer for numeric values without a fractional part like 5.

for numeric values without a fractional part like 5. Nominal for categorical data like “dog” and “cat”.

for categorical data like “dog” and “cat”. String for lists of words, like this sentence.

On classification problems, the output variable must be nominal. For regression problems, the output variable must be real.

Need more help with Weka for Machine Learning? Take my free 14-day email course and discover how to use the platform step-by-step. Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Data in Weka

Weka prefers to load data in the ARFF format.

ARFF is an acronym that stands for Attribute-Relation File Format. It is an extension of the CSV file format where a header is used that provides metadata about the data types in the columns.

For example, the first few lines of the classic iris flowers dataset in CSV format looks as follows:

5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 1 2 3 4 5 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa

The same file in ARFF format looks as follows:

@RELATION iris @ATTRIBUTE sepallength REAL @ATTRIBUTE sepalwidth REAL @ATTRIBUTE petallength REAL @ATTRIBUTE petalwidth REAL @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @RELATION iris @ATTRIBUTE sepallength REAL @ATTRIBUTE sepalwidth REAL @ATTRIBUTE petallength REAL @ATTRIBUTE petalwidth REAL @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa

You can see that directives start with the at symbol (@) and that there is one for the name of the dataset (e.g. @RELATION iris), there is a directive to define the name and datatype of each attribute (e.g. @ATTRIBUTE sepallength REAL) and there is a directive to indicate the start of the raw data (e.g. @DATA).

Lines in an ARFF file that start with a percentage symbol (%) indicate a comment.

Values in the raw data section that have a question mark symbol (?) indicate an unknown or missing value. The format supports numeric and categorical values as in the iris example above, but also supports dates and string values.

Depending on your installation of Weka, you may or may not have some default datasets in your Weka installation directory under the data/ subdirectory. These default datasets distributed with Weka are in the ARFF format and have the .arff file extension.

Load CSV Files in the ARFF-Viewer

Your data is not likely to be in ARFF format.

In fact, it is much more likely to be in Comma Separated Value (CSV) format. This is a simple format where data is laid out in a table of rows and columns and a comma is used to separate the values on a row. Quotes may also be used to surround values, especially if the data contains strings of text with spaces.

The CSV format is easily exported from Microsoft Excel, so once you can get your data into Excel, you can easily convert it to CSV format.

Weka provides a handy tool to load CSV files and save them in ARFF. You only need to do this on"
305;305;news.mit.edu;http://news.mit.edu/2016/scene-at-mit-puppy-love-1026;;Scene at MIT: Puppy love;"Members of the MIT Puppy Lab brought their puppy love to the portico of Building 10 with unseasonably warm temperatures near 80 degrees Fahrenheit on Wednesday, Oct. 19.

Puppy Lab founder Stephanie Ku ’14, a graduate student in the Harvard-MIT Health Sciences and Technology program, says the dogs were an instant success when Puppy Lab launched in May: “The first day, we were in the Lobby 10 Green Room, as I call it, and the temperature rose by like 5 to 10 degrees because there were so many people,” Ku says. “That’s not ideal in one way because no one wants to be in a crowded environment — you can barely touch the dogs — but at the same time, it’s an awesome thing because it shows us that there is a high demand and people were really interested.”

When Ku got her own dog, Wingnut, last year, she began to think about how students miss their dogs at home and find to be very helpful the MIT Libraries' Furry First Fridays, as well as therapy dogs brought in around finals. “MIT’s a hard place, and we all know that, but we all love it as well, and we just want to make sure that we support the students who are here, and the staff and faculty, because they also have stress,” Ku says.

“I thought it might be nice to have dogs more consistently available and to try to reach out to the dog community that’s already at MIT instead of relying on external therapy dog organizations,” she explains.

Around the same time, the MIT MindHandHeart Initiative was seeking grant proposals for campus wellness activities, and Ku’s plan for the Puppy Lab was supported with a $1,000 award. “MindHandHeart has also provided moral support and any kind of support that I really need if I run into a problem with logistics or with publicity,” Ku says.

Ku worked with MIT librarian Ellen Finnie, who is an instructor with the therapy group Dog B.O.N.E.S., to get eight new dogs certified as therapy dogs for the spring launch. “She’s been really vital in that process,” Ku says. “So right now we’re kind of in parallel programs: The Furry First Fridays program still runs on the first Friday of the month, and Puppy Lab does its own weekly programs on Wednesdays.”

Based on a survey she conducted, Ku says, “It seems like Puppy Lab is something a lot of people really wanted and really appreciate. I’ve gotten overwhelmingly positive feedback overall.” Next, she’d like to make the Puppy Lab an official student group.

Submitted by: Denis Paiste/Materials Processing Center | Photo by: Denis Paiste.

Have a creative photo of campus life you'd like to share? Submit it to Scene at MIT."
306;306;news.mit.edu;http://news.mit.edu/2020/mit-energy-economics-class-inspires-students-pursue-clean-energy-careers-0413;;Energy economics class inspires students to pursue clean energy careers;"Jing Li, an assistant professor of applied economics in the MIT Sloan School of Management, stands at the front of the classroom and encourages her undergraduate students to dig deeper. “Why was this a good idea?” she prompts. “How did people come up with these numbers?”

It’s the second-to-last day of class, and the students in 15.0201/14.43 (Economics of Energy, Innovation, and Sustainability) are discussing their teams’ results and the logic behind the decisions they made in the Electricity Strategy Game — a main feature of this elective.

“[With] so much magic,” a student quips in response to Li’s question, to a chorus of laughter.

The real magic, they all know, is in Li’s approach to teaching: She holds her students accountable for their conclusions and throws them head-first into challenging problems to help them confidently engage with the complexities of energy economics.

“She didn’t baby us with tiny data sets. She gave us the real deal,” says Wilbur Li, a senior computer science major and mechanical engineering minor (no relation to Jing Li). He initially took the class to round out his fall semester schedule, unsure if he would keep it due to a rigorous class load. However, just a couple of weeks into the semester, he was sold on the class.

“It’s one of those classes at MIT that isn’t really a requirement for anyone, but it’s a class that only draws people who are genuinely interested in the subject area,” he says. “That made for really good discussions. You could tell that people were interested beyond an academic sense.”

15.0201/14.43, a part of MITEI’s interdisciplinary Energy Studies minor, is a relatively new course. The class, which is also offered as graduate-level course 15.020, made its debut in the spring 2019 semester and was developed to expand the energy economics offerings at MIT. Part of the motivation for creating 15.0201/14.43 stemmed from the fact that Professor Christopher Knittel’s course, 15.037/15.038 (Energy Economics and Policy), is consistently in high demand, without enough supply to accommodate interested students.

“Professor Knittel and I have positioned our two courses so that someone who wants to get a taste of energy economics could take either one and come away with a good mental map of the field, but also that someone who is very serious about a future career in energy would find it useful to take both,” says Li.

Li’s class focuses on innovation and employs environmental economics principles and business cases to explore the development and adoption of new technology, and business strategies related to sustainability.

“The class has been particularly attractive to students who are interested in the energy landscape, such as how energy markets impact and relate to local environmental issues and how to provide energy to parts of the globe that currently lack access to affordable or reliable energy,” she says. “It has also appealed to students interested in applied microeconomics.”

In addition to crunching large data sets and bringing in guest speakers, such as Paul Joskow, the Elizabeth and James Killian Professor of Economics Emeritus and chair of MIT’s Department of Economics, a major element of the class — and a runaway favorite of many of the students — is the Electricity Strategy Game. The game was created by professors Severin Borenstein and James Bushnell for the University of California at Berkley’s Haas School of Business.

The game is designed to replicate the world of deregulated wholesale electricity markets. Players are divided into firms and utilize electricity generation portfolios, based on actual portfolios of the largest generation firms in the California market, to compete in a sequence of daily electricity spot markets, in which commodities are traded for immediate delivery. Each portfolio contains differing generation technologies (thermal, nuclear, and hydro), with varying operating costs. Spot market conditions vary from hour to hour and day to day. Players must develop strategies to deploy their assets over a sequence of spot markets while accounting for the cost structure of their portfolio, varying levels of hourly electricity demand, and strategies of other players. The game is conducted in six rounds, with the second half of the game taking into account carbon permits. Winners are determined by the financial performance of their firm and an evaluation of the logic of the firm’s actions, which the teams describe in a series of memos to Li.

“I loved the Electricity Strategy Game! It was really fun to have to figure out how to predict demand and then how to price supply accordingly,” says Anupama Phatak, a junior mechanical engineering major and economics minor. “The bid for portfolios was also a really cool process. I put a lot of time and effort into understanding the game and developing a strategy, so it made the process all the more rewarding when my team won.”

Wilbur Li echoed Phatak’s enthusiasm. “My favorite part of the gam"
307;307;news.mit.edu;http://news.mit.edu/2020/pandemic-health-response-economic-recovery-0401;;The data speak: Stronger pandemic response yields better economic recovery;"The research described in this article has been published as a working paper but has not yet been peer-reviewed by experts in the field.

With much of the U.S. in shutdown mode to limit the spread of the Covid-19 disease, a debate has sprung up about when the country might “reopen” commerce, to limit economic fallout from the pandemic. But as a new study co-authored by an MIT economist shows, taking care of public health first is precisely what generates a stronger economic rebound later.

The study, using data from the flu pandemic that swept the U.S. in 1918-1919, finds cities that acted more emphatically to limit social and civic interactions had more economic growth following the period of restrictions.

Indeed, cities that implemented social-distancing and other public health interventions just 10 days earlier than their counterparts saw a 5 percent relative increase in manufacturing employment after the pandemic ended, through 1923. Similarly, an extra 50 days of social distancing was worth a 6.5 percent increase in manufacturing employment, in a given city.

“We find no evidence that cities that acted more aggressively in public health terms performed worse in economic terms,” says Emil Verner, an assistant professor in the MIT Sloan School of Management and co-author of a new paper detailing the findings. “If anything, the cities that acted more aggressively performed better.”

With that in mind, he observes, the idea of a “trade-off” between public health and economic activity does not hold up to scrutiny; places that are harder hit by a pandemic are unlikely to rebuild their economic capacities as quickly, compared to areas that are more intact.

“It casts doubt on the idea there is a trade-off between addressing the impact of the virus, on the one hand, and economic activity, on the other hand, because the pandemic itself is so destructive for the economy,” Verner says.

The study, “Pandemics Depress the Economy, Public Health Interventions Do Not: Evidence from the 1918 Flu,” was posted to the Social Science Research Network as a working paper on March 26. In addition to Verner, the co-authors are Sergio Correia, an economist with the U.S. Federal Reserve, and Stephen Luck, an economist with the Federal Reserve Bank of New York.

Evaluating economic consequences

To conduct the research, the three scholars examined mortality statistics from the U.S. Centers for Disease Control (CDC), historical economic data from the U.S. Census Bureau, and banking statistics compiled by finance economist Mark D. Flood, using the “Annual Reports of the Comptroller of Currency,” a government publication.

As Verner notes, the researchers were motivated to investigate the 1918-1919 flu pandemic to see what lessons from it might be applicable to the current crisis.

“The genesis of the study is that we’re interested in what the expected economic impacts of today’s coronavirus are going to be, and what is the right way to think about the economic consequences of the public health and social distancing interventions we’re seeing all around the world,” Verner says.

Scholars have known that the varying use of “nonpharmaceutical interventions,” or social-distancing measures, correlated to varying health outcomes across cities in 1918 and 1919. When that pandemic hit, U.S. cities that shut down schools earlier, such as St. Louis, fared better against the flu than places implementing shutdowns later, such as Philadelphia. The current study extends that framework to economic activity.

Quite a bit like today, social distancing measures back then included school and theater closures, bans on public gatherings, and restricted business activity.

“The nonpharmaceutical interventions that were implemented in 1918 interestingly resemble many of the policies that are being used today to reduce the spread of Covid-19,” Verner says.

Overall, the study indicates, the economic impact of the pandemic was severe. Using state-level data, the researchers find an 18 percent drop in manufacturing output through 1923, well after the last wave of the flu hit in 1919.

Looking at the effect across 43 cities, however, the researchers found significantly different economic outcomes, linked to different social distancing policies. The best-performing cities included Oakland, California; Omaha, Nebraska; Portland, Oregon; and Seattle, which all enforced above average duration and intensity of social distancing in 1918. Cities that instituted significantly fewer than days of social distancing in 1918, and saw manufacturing struggle afterward, include Philadelphia; St. Paul, Minnesota; and Lowell, Massachusetts.

“What we find is that areas that were more severely affected in the 1918 flu pandemic see a sharp and persistent decline in a number of measures of economic activity, including manufacturing employment, manufacturing output, bank loans, and the stock of consumer durables,” Verner says.

Banking issues

As far as banking goes, the study"
308;308;news.mit.edu;http://news.mit.edu/2020/mirrored-chip-could-enable-handheld-dark-field-microscopes-0224;;Mirrored chip could enable handheld dark-field microscopes;"Do a Google search for dark-field images, and you’ll discover a beautifully detailed world of microscopic organisms set in bright contrast to their midnight-black backdrops. Dark-field microscopy can reveal intricate details of translucent cells and aquatic organisms, as well as faceted diamonds and other precious stones that would otherwise appear very faint or even invisible under a typical bright-field microscope.

Scientists generate dark-field images by fitting standard microscopes with often costly components to illumate the sample stage with a hollow, highly angled cone of light. When a translucent sample is placed under a dark-field microscope, the cone of light scatters off the sample’s features to create an image of the sample on the microscope’s camera, in bright contrast to the dark background.

Now, engineers at MIT have developed a small, mirrored chip that helps to produce dark-field images, without dedicated expensive components. The chip is slightly larger than a postage stamp and as thin as a credit card. When placed on a microscope’s stage, the chip emits a hollow cone of light that can be used to generate detailed dark-field images of algae, bacteria, and similarly translucent tiny objects.

Credit: Cecile Chazot

The new optical chip can be added to standard microscopes as an affordable, downsized alternative to conventional dark-field components. The chip may also be fitted into hand-held microscopes to produce images of microorganisms in the field.

“Imagine you’re a marine biologist,” says Cecile Chazot, a graduate student in MIT’s Department of Materials Science and Engineering. “You normally have to bring a big bucket of water into the lab to analyze. If the sample is bad, you have to go back out to collect more samples. If you have a hand-held, dark-field microscope, you can check a drop in your bucket while you’re out at sea, to see if you can go home or if you need a new bucket.”

Chazot is the lead author of a paper detailing the team’s new design, published today in the journal Nature Photonics. Her co-authors are Sara Nagelberg, Igor Coropceanu, Kurt Broderick, Yunjo Kim, Moungi Bawendi, Peter So, and Mathias Kolle of MIT, along with Christopher Rowlands at Imperial College London and Maik Scherer of Papierfabrik Louisenthal GmbH in Germany.

Forever fluorescent

In an ongoing effort, members of Kolle’s lab are designing materials and devices that exhibit long-lasting “structural colors” that do not rely on dyes or pigmentation. Instead, they employ nano- and microscale structures that reflect and scatter light much like tiny prisms or soap bubbles. They can therefore appear to change colors depending on how their structures are arranged or manipulated.

Structural color can be seen in the iridescent wings of beetles and butterflies, the feathers of birds, as well as fish scales and some flower petals. Inspired by examples of structural color in nature, Kolle has been investigating various ways to manipulate light from a microscopic, structural perspective.

As part of this effort, he and Chazot designed a small, three-layered chip that they originally intended to use as a miniature laser. The middle layer functions as the chip’s light source, made from a polymer infused with quantum dots — tiny nanoparticles that emit light when excited with fluorescent light. Chazot likens this layer to a glowstick bracelet, where the reaction of two chemicals creates the light; except here no chemical reaction is needed — just a bit of blue light will make the quantum dots shine in bright orange and red colors.

“In glowsticks, eventually these chemicals stop emitting light,” Chazot says. “But quantum dots are stable. If you were to make a bracelet with quantum dots, they would be fluorescent for a very long time.”

Over this light-generating layer, the researchers placed a Bragg mirror — a structure made from alternating nanoscale layers of transparent materials, with distinctly different refractive indices, meaning the degrees to which the layers reflect incoming light.

The Bragg mirror, Kolle says, acts as a sort of “gatekeeper” for the photons that are emitted by the quantum dots. The arrangement and thicknesses of the mirror’s layers is such that it lets photons escape up and out of the chip, but only if the light arrives at the mirror at high angles. Light arriving at lower angles is bounced back down into the chip.

The researchers added a third feature below the light-generating layer to recycle the photons initially rejected by the Bragg mirror. This third layer is molded out of solid, transparent epoxy coated with a reflective gold film and resembles a miniature egg crate, pocked with small wells, each measuring about 4 microns in diameter.

Chazot lined this surface with a thin layer of highly reflective gold — an optical arrangement that acts to catch any light that reflects back down from the Bragg mirror, and ping-pong that light back up, likely at a new angle that the mirr"
309;309;machinelearningmastery.com;https://machinelearningmastery.com/how-to-study-machine-learning-algorithms/;2014-10-21;How to Study Machine Learning Algorithms;"Tweet Share Share

Last Updated on August 12, 2019

Algorithms make up a big part of machine learning.

You select and apply machine learning algorithms to build a model from your data, select features, combine the predictions from multiple models and even evaluate the capabilities of a given model.

In this post you will review 5 different approaches that you can use to study machine learning algorithms.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

1. List Machine Learning Algorithms

There are a lot of machine learning algorithms and it can feel overwhelming.

Even defining what a machine learning algorithm is, can be tricky.

A great place to start out is to make your own lists of algorithms. Start a text file, word document or spreadsheet and list algorithm names. Also list the general category or categories to which each algorithm belongs.

This simple tactic can help you build familiarity with the different types and classes of algorithms available. Later as you get more experienced, lists like this can prompt you and give you ideas of different methods to spot check on your problem.

Some examples of algorithm lists to get you started include:

Get your FREE Algorithms Mind Map

I've created a handy mind map of 60+ algorithms organized by type.

Download it, print it and use it.

Download For Free

Also get exclusive access to the machine learning algorithms email mini-course.

2. Apply Machine Learning Algorithms

Machine Learning algorithm do not exist in isolation, they are best understood when applied to a dataset.

Apply algorithms to problems to understand them. Practice applied machine learning. It sounds simple, but you will be amazed at the number of people paralyzed to make this small step from theory into action.

This may mean working on a problem that matters to you, a competition dataset or a classical machine learning dataset.

Use a machine learning platform like Weka, R or scikit-learn to get access to many machine learning algorithms.

Start to build up an intuition for different types of algorithms, such as decision trees and support vector machines. Think about their required preconditions and the effects the parameters have on results.

Build up confidence in applying different algorithms. You should be spot checking algorithms on your problems.

3. Describe Machine Learning Algorithms

The next step in understanding a machine learning algorithm is to explore what is already understood about the algorithm.

This could be done before you apply the algorithm, but I think it is valuable to have a working intuition of the algorithm in action as context before diving into the algorithm description.

You can research an algorithm. This includes locating and reading the primary sources where the algorithm was first described as well as authoritative interpretations of the algorithm in textbooks and review papers.

Conference papers, competition results and even forms and Q&A websites can help you better understand the best practices and usage heuristics for an algorithm.

As you are researching an algorithm, build up a description. I like to use a well-defined algorithm description template.

You can continue to add to this template you discover more about an algorithm. You can add references, list the pseudocode for the algorithm and list best practices and usage heuristics.

This is a valuable techniques and you can build up your own mini-encyclopedia of algorithm descriptions for your own reference (for example, see Clever Algorithms for 45 algorithm recipes).

For more information on the template that I use, check out the post “How to Learn a Machine Learning Algorithm“.

4. Implement Machine Learning Algorithms

Implementing machine learning algorithms is an excellent way to get a concrete understanding of how an algorithm works.

There are many micro-decisions that have to be made when implementing an algorithm. Some of these decision points are exposed with algorithm configuration parameters, but many are not.

By implementing an algorithm yourself you will get a feeling for just how to customize the algorithm and choose what to expose and what decision points to fix in place.

Implementing algorithms from scratch will help you understand the mathematical descriptions and extensions of an algorithm. This may sound counter-intuitive. The mathematical descriptions are idealized and often provide a snap-shot description of a given processes within an algorithm. Once you translate them into code, the implications of those descriptions may be a lot more obvious.

You can leverage tutorials and open source implementations of algorithms to help you get through those difficult parts.

Note that a “my first implementation” of an algorithm will be less scalable and more fragile than a production grade implementation you may find in a machine learning tool or"
310;310;machinelearningmastery.com;https://machinelearningmastery.com/why-learn-probability-for-machine-learning/;2019-09-10;5 Reasons to Learn Probability for Machine Learning;"Tweet Share Share

Last Updated on November 8, 2019

Probability is a field of mathematics that quantifies uncertainty.

It is undeniably a pillar of the field of machine learning, and many recommend it as a prerequisite subject to study prior to getting started. This is misleading advice, as probability makes more sense to a practitioner once they have the context of the applied machine learning process in which to interpret it.

In this post, you will discover why machine learning practitioners should study probabilities to improve their skills and capabilities.

After reading this post, you will know:

Not everyone should learn probability; it depends where you are in your journey of learning machine learning.

Many algorithms are designed using the tools and techniques from probability, such as Naive Bayes and Probabilistic Graphical Models.

The maximum likelihood framework that underlies the training of many machine learning algorithms comes from the field of probability.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Overview

This tutorial is divided into seven parts; they are:

Reasons to NOT Learn Probability Class Membership Requires Predicting a Probability Some Algorithms Are Designed Using Probability Models Are Trained Using a Probabilistic Framework Models Can Be Tuned With a Probabilistic Framework Probabilistic Measures Are Used to Evaluate Model Skill One More Reason

Reasons to NOT Learn Probability

Before we go through the reasons that you should learn probability, let’s start off by taking a small look at the reason why you should not.

I think you should not study probability if you are just getting started with applied machine learning.

It’s not required . Having an appreciation for the abstract theory that underlies some machine learning algorithms is not required in order to use machine learning as a tool to solve problems.

. Having an appreciation for the abstract theory that underlies some machine learning algorithms is not required in order to use machine learning as a tool to solve problems. It’s slow . Taking months to years to study an entire related field before starting machine learning will delay you achieving your goals of being able to work through predictive modeling problems.

. Taking months to years to study an entire related field before starting machine learning will delay you achieving your goals of being able to work through predictive modeling problems. It’s a huge field. Not all of probability is relevant to theoretical machine learning, let alone applied machine learning.

I recommend a breadth-first approach to getting started in applied machine learning.

I call this the results-first approach. It is where you start by learning and practicing the steps for working through a predictive modeling problem end-to-end (e.g. how to get results) with a tool (such as scikit-learn and Pandas in Python).

This process then provides the skeleton and context for progressively deepening your knowledge, such as how algorithms work and, eventually, the math that underlies them.

After you know how to work through a predictive modeling problem, let’s look at why you should deepen your understanding of probability.

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

1. Class Membership Requires Predicting a Probability

Classification predictive modeling problems are those where an example is assigned a given label.

An example that you may be familiar with is the iris flowers dataset where we have four measurements of a flower and the goal is to assign one of three different known species of iris flower to the observation.

We can model the problem as directly assigning a class label to each observation.

Input : Measurements of a flower.

: Measurements of a flower. Output: One iris species.

A more common approach is to frame the problem as a probabilistic class membership, where the probability of an observation belonging to each known class is predicted.

Input : Measurements of a flower.

: Measurements of a flower. Output: Probability of membership to each iris species.

Framing the problem as a prediction of class membership simplifies the modeling problem and makes it easier for a model to learn. It allows the model to capture ambiguity in the data, which allows a process downstream, such as the user to interpret the probabilities in the context of the domain.

The probabilities can be transformed into a crisp class label by choosing the class with the largest probability. The probabilities can also be scaled or transformed using a probability calibration process.

This choice of a class membership framing of the problem interpretation of the prediction"
311;311;news.mit.edu;http://news.mit.edu/2020/ten-mit-awarded-2020-paul-and-daisy-soros-fellowships-new-americans-0414;;Ten from MIT awarded 2020 Paul and Daisy Soros Fellowships for New Americans;"Ten MIT students and alumni are among the 30 recipients of this year’s Paul and Daisy Soros Fellowships for New Americans. The MIT-affiliated winners are Pelkins Mbacham Ajanoh, Sanath Devalapurkar, Mohamed Ismail, Connie Liu, Mark Aurel Nagy, Jin Park, Pooja Reddy, Riana Shah, Anthony Tabet, and Jason Ku Wang. They were selected from a pool of over 2,200 applicants.

As Soros winner Pooja Reddy notes, “I could not have asked for a better environment to grow than MIT. I was always surrounded by other immigrants and children of immigrants in all my classes. … With a real emphasis on education, my professors always showed so much compassion, which made me feel seen as a person. This drove me to persevere and learn more.”

The P.D. Soros Fellowship provides up to $90,000 in funding for graduate studies. Interested students should contact Kim Benard, assistant dean of distinguished fellowships. The deadline for this year’s application is Oct. 29, 2020.

Pelkins Mbacham Ajanoh ’18

Pelkins Mbacham Ajanoh graduated from MIT in 2018 with a BS in mechanical engineering. The Soros fellowship will fund his graduate studies at Harvard University where he will earn dual MBA and MS in engineering sciences degrees.

Ajanoh was born and raised in Limbe, Cameroon. He lost his father at age 13, and his mother subsequently immigrated to the US to financially support her children. After graduating high school and receiving the top score on Cameroon’s national exam, Ajanoh joined his mother in Texas, earned an associate’s degree at a community college, and enrolled at University of Texas at Arlington. After learning of MIT’s need-blind admissions policy, he applied to MIT and was accepted as a transfer student.

At MIT, Ajanoh became interested in the topic of creating economic opportunity in vulnerable communities through entrepreneurship, which led him to found CassVita, an agribusiness that converts cassava into shelf-stable flour. CassVita empowers over 300 farmers in Cameroon and its products are sold in over 30 supermarkets locally and internationally. In recognition for his work at MIT, Ajanoh was awarded the Albert G. Hill Prize and the Suzanne Berger Award for Future Global Leaders.

Sanath Devalapurkar

Sanath Devalapurkar will graduate from MIT in May 2020 with a BS in mathematics and a minor in physics. His Soros award will support his doctoral studies in mathematics at Harvard University.

Devalapurkar was born in Adoni, India, and lived in several different countries and U.S. states while growing up. After graduating high school in Los Angeles, he matriculated at MIT at age 16. Shortly after arriving at MIT, Devalapurkar began sitting in on graduate-level courses in mathematics, which fueled his passion and curiosity for the field. He is particularly interested in algebraic topology and algebraic geometry, subfields of math, and quantum field theory in physics.

Devalapurkar credits his interests to his parents’ unwavering support, his mentors in high school, and to Professor Haynes Miller and postdoc Jeremy Hahn in the MIT Department of Mathematics. During his time at MIT, Devalapurkar has worked on projects at the Emory Research Experiences for Undergraduates (REU), MIT’s Summer Program for Undergraduate Research, and the University of Chicago REU, all of which have helped reinforce his enthusiasm for math.

Mohamed Ismail

A PhD student in building technology in the MIT Department of Architecture, Mohamed Ismail is researching the application of structural optimization to the alleviation of housing insecurity in the Global South. Born to Sudanese parents who immigrated to the U.S. for educational opportunities, Ismail moved with his family to the Philippines when he was eight.

In the Philippines, Ismail witnessed how environmental issues are in fact human rights issues, which led him to environmental activism. He returned to the U.S. for college to learn how the built environment could improve societal well-being rather than harm it. Ismail received his bachelor’s degree in civil and structural engineering at Duke University before receiving his Master of Architecture at the University of Virginia. After graduating, he became a faculty lecturer at the UVA School of Architecture, teaching parametric structural design and digital workflows to architecture students.

At MIT, Ismail was an MIT Tata Center fellow, working with the Digital Structures research group to design low-cost, low-carbon structural components for housing in developing economies. Following his PhD, Ismail hopes to enrich the design profession with new methods that integrate structural performance into the architectural design process.

Connie Liu ‘16

An engineer turned educator turned nonprofit founder, Connie Liu graduated from MIT in 2016 with a BS in mechanical engineering. She was born in San Diego, California, the youngest of three children, to parents who had emigrated from China.

Growing up, Liu had a strong interest in science and"
312;312;news.mit.edu;http://news.mit.edu/2020/mit-opens-sean-collier-care-center-for-patients-with-covid-19-0415;;MIT opens the Sean Collier Care Center for patients with Covid-19;"Throughout the course of the Covid-19 emergency, MIT has worked in partnership with its neighbors in the City of Cambridge, whenever possible, to provide medical supplies, equipment, and services to the larger community. In this spirit of collaboration, MIT has opened the Sean Collier Care Center, a fully licensed 75-bed facility for patients with Covid-19.

Located on the MIT campus and named for fallen MIT Police Officer Sean Collier, who was killed in the line of duty on April 18, 2013, the center will provide care for members of the MIT community and individuals from the broader Cambridge community. The center will be funded by MIT and staffed by MIT Medical. It is designed to alleviate the anticipated hospital bed shortage in the Commonwealth of Massachusetts as cases of Covid-19 approach peak levels in the coming weeks. It will focus exclusively on patients who would benefit from “eyes on” clinical care but are at very low risk for requiring ventilators or other intensive care.

“We are proud to help our neighbors in Cambridge by creating the Sean Collier Care Center,” says MIT Medical Director Cecilia Stuopis. “With this facility, we hope to do our part to ease some of the strain that our fellow health care facilities are feeling at this time.”

Eligible patients from the Cambridge community will be referred to the new center by clinicians at Mount Auburn Hospital and other local ambulatory care centers. Patients must transfer directly from one of these partner organizations to come to the new facility.

“Creating this center on such short notice was an incredible team effort, bringing together partners from emergency management, student life, athletics, finance, facilities, space planning, and many others,” says Stuopis. “It has been a deeply rewarding experience and is the epitome of what ‘One MIT’ truly means.”"
313;313;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-multilayer-perceptron-models-for-time-series-forecasting/;2018-11-08;How to Develop Multilayer Perceptron Models for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Multilayer Perceptrons, or MLPs for short, can be applied to time series forecasting.

A challenge with using MLPs for time series forecasting is in the preparation of the data. Specifically, lag observations must be flattened into feature vectors.

In this tutorial, you will discover how to develop a suite of MLP models for a range of standard time series forecasting problems.

The objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem.

In this tutorial, you will discover how to develop a suite of Multilayer Perceptron models for a range of standard time series forecasting problems.

After completing this tutorial, you will know:

How to develop MLP models for univariate time series forecasting.

How to develop MLP models for multivariate time series forecasting.

How to develop MLP models for multi-step time series forecasting.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into four parts; they are:

Univariate MLP Models Multivariate MLP Models Multi-Step MLP Models Multivariate Multi-Step MLP Models

Univariate MLP Models

Multilayer Perceptrons, or MLPs for short, can be used to model univariate time series forecasting problems.

Univariate time series are a dataset comprised of a single series of observations with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence.

This section is divided into two parts; they are:

Data Preparation MLP Model

Data Preparation

Before a univariate series can be modeled, it must be prepared.

The MLP model will learn a function that maps a sequence of past observations as input to an output observation. As such, the sequence of observations must be transformed into multiple examples from which the model can learn.

Consider a given univariate sequence:

[10, 20, 30, 40, 50, 60, 70, 80, 90] 1 [10, 20, 30, 40, 50, 60, 70, 80, 90]

We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned.

X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ... 1 2 3 4 5 X, y 10, 20, 30 40 20, 30, 40 50 30, 40, 50 60 ...

The split_sequence() function below implements this behavior and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.

# split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of the pattern seq_x , seq_y = sequence [ i : end_ix ] , sequence [ end_ix ] X . append ( seq_x ) y . append ( seq_y ) return array ( X ) , array ( y )

We can demonstrate this function on our small contrived dataset above.

The complete example is listed below.

# univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence(sequence, n_steps): X, y = list(), list() for i in range(len(sequence)): # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len(sequence)-1: break # gather input and output parts of the pattern seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] X.append(seq_x) y.append(seq_y) return array(X), array(y) # define input sequence raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90] # choose a number of time steps n_steps = 3 # split into samples X, y = split_sequence(raw_seq, n_steps) # summarize the data for i in range(len(X)): print(X[i], y[i]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # univariate data preparation from numpy import array # split a univariate sequence into samples def split_sequence ( sequence , n_steps ) : X , y = list ( ) , list ( ) for i in range ( len ( sequence ) ) : # find the end of this pattern end_ix = i + n_steps # check if we are beyond the sequence if end_ix > len ( sequence ) - 1 : break # gather input and output parts of "
314;314;machinelearningmastery.com;http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/;2013-12-22;How to Define Your Machine Learning Problem;"Tweet Share Share

Last Updated on June 7, 2016

The first step in any project is defining your problem. You can use the most powerful and shiniest algorithms available, but the results will be meaningless if you are solving the wrong problem.

In this post you will learn the process for thinking deeply about your problem before you get started. This is unarguably the most important aspect of applying machine learning.

Problem Definition Framework

I use a simple framework when defining a new problem to address with machine learning. The framework helps me to quickly understand the elements and motivation for the problem and whether machine learning is suitable or not.

The framework involves answering three questions to varying degrees of thoroughness:

Step 1 : What is the problem?

: What is the problem? Step 2 : Why does the problem need to be solved?

: Why does the problem need to be solved? Step 3: How would I solve the problem?

Step 1: What is the Problem

The first step is defining the problem. I use a number of tactics to collect this information.

Informal description

Describe the problem as though you were describing it to a friend or colleague. This can provide a great starting point for highlighting areas that you might need to fill. It also provides the basis for a one sentence description you can use to share your understanding of the problem.

For example: I need a program that will tell me which tweets will get retweets.

Formalism

In a previous blog post defining machine learning you learned about Tom Mitchell’s machine learning formalism. Here it is again to refresh your memory.

A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

Use this formalism to define the T, P, and E for your problem.

For example:

Task (T): Classify a tweet that has not been published as going to get retweets or not.

(T): Classify a tweet that has not been published as going to get retweets or not. Experience (E): A corpus of tweets for an account where some have retweets and some do not.

(E): A corpus of tweets for an account where some have retweets and some do not. Performance (P): Classification accuracy, the number of tweets predicted correctly out of all tweets considered as a percentage.

Assumptions

Create a list of assumptions about the problem and it’s phrasing. These may be rules of thumb and domain specific information that you think will get you to a viable solution faster.

It can be useful to highlight questions that can be tested against real data because breakthroughs and innovation occur when assumptions and best practice are demonstrated to be wrong in the face of real data. It can also be useful to highlight areas of the problem specification that may need to be challenged, relaxed or tightened.

For example:

The specific words used in the tweet matter to the model.

The specific user that retweets does not matter to the model.

The number of retweets may matter to the model.

Older tweets are less predictive than more recent tweets.

Similar problems

What other problems have you seen or can you think of that are like the problem you are trying to solve? Other problems can inform the problem you are trying to solve by highlighting limitations in your phrasing of the problem such as time dimensions and conceptual drift (where the concept being modeled changes over time). Other problems can also point to algorithms and data transformations that could be adopted to spot check performance.

For example: A related problem would be email spam discrimination that uses text messages as input data and needs binary classification decision.

Step 2: Why does the the problem need to be solved?

The second step is to think deeply about why you want or need the problem solved.

Motivation

Consider your motivation for solving the problem. What need will be fulfilled when the problem is solved?

For example, you may be solving the problem as a learning exercise. This is useful to clarify as you can decide that you don’t want to use the most suitable method to solve the problem, but instead you want to explore methods that you are not familiar with in order to learn new skills.

Alternatively, you may need to solve the problem as part of a duty at work, ultimately to keep your job.

Solution Benefits

Consider the benefits of having the problem solved. What capabilities does it enable?

It is important to be clear on the benefits of the problem being solved to ensure that you capitalize on them. These benefits can be used to sell the project to colleagues and management to get buy in and additional time or budget resources.

If it benefits you personally, then be clear on what those benefits are and how you will know when you have got them. For example, if it’s a tool or utility, then what will you be able to do with that utility that you can’t do now "
315;315;machinelearningmastery.com;https://machinelearningmastery.com/tour-of-generative-adversarial-network-models/;2019-07-09;A Tour of Generative Adversarial Network Models;"Tweet Share Share

Last Updated on July 12, 2019

Generative Adversarial Networks, or GANs, are deep learning architecture generative models that have seen wide success.

There are thousands of papers on GANs and many hundreds of named-GANs, that is, models with a defined name that often includes “GAN“, such as DCGAN, as opposed to a minor extension to the method. Given the vast size of the GAN literature and number of models, it can be, at the very least, confusing and frustrating as to know what GAN models to focus on.

In this post, you will discover the Generative Adversarial Network models that you need to know to establish a useful and productive foundation in the field.

After reading this post, you will know:

The foundation GAN models that provide the basis for the field of study.

The extension GAN models that build upon what works and lead the way for more advanced models.

The advanced GAN models that push the limits of the architecture and achieve impressive results.

Discover how to develop DCGANs, conditional GANs, Pix2Pix, CycleGANs, and more with Keras in my new GANs book, with 29 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Foundation Generative Adversarial Network (GAN) Deep Convolutional Generative Adversarial Network (DCGAN)

Extensions Conditional Generative Adversarial Network (cGAN) Information Maximizing Generative Adversarial Network (InfoGAN) Auxiliary Classifier Generative Adversarial Network (AC-GAN) Stacked Generative Adversarial Network (StackGAN) Context Encoders Pix2Pix

Advanced Wasserstein Generative Adversarial Network (WGAN) Cycle-Consistent Generative Adversarial Network (CycleGAN) Progressive Growing Generative Adversarial Network (Progressive GAN) Style-Based Generative Adversarial Network (StyleGAN) Big Generative Adversarial Network (BigGAN)



Foundation Generative Adversarial Networks

This section summarizes the foundational GAN models from which most, if not all, other GANs build upon.

Generative Adversarial Network (GAN)

The Generative Adversarial Network architecture and first empirical demonstration of the approach was described in the 2014 paper by Ian Goodfellow, et al. titled “Generative Adversarial Networks.”

The paper describes the architecture succinctly involving a generator model that takes as input points from a latent space and generates an image, and a discriminator model that classifies images as either real (from the dataset) or fake (output by the generator).

We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake.

— Generative Adversarial Networks, 2014.

The models are comprised of fully connected layers (MLPs) with ReLU activations in the generator and maxout activations in the discriminator and was applied to standard image datasets such as MNIST and CIFAR-10.

We trained adversarial nets as a range of datasets including MNIST, the Toronto Face Database (TFD), and CIFAR-10. The generator nets used a mixture of rectifier linear activations and sigmoid activations, while the discriminator net used maxout activations. Dropout was applied in training the discriminator net.

— Generative Adversarial Networks, 2014.

Deep Convolutional Generative Adversarial Network (DCGAN)

The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model.

We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.

— Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, 2015.

The DCGAN is important because it suggested the constraints on the model required to effectively develop high-quality generator models in practice. This architecture, in turn, provided the basis for the rapid development of a large number of GAN extensions and applications.

We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings.

— Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, 2015.

Want to Develop GANs from Scratch? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Generative Adversarial"
316;316;machinelearningmastery.com;https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/;2018-12-16;How to Avoid Overfitting in Deep Learning Neural Networks;"Tweet Share Share

Last Updated on August 6, 2019

Training a deep neural network that can generalize well to new data is a challenging problem.

A model with too little capacity cannot learn the problem, whereas a model with too much capacity can learn it too well and overfit the training dataset. Both cases result in a model that does not generalize well.

A modern approach to reducing generalization error is to use a larger model that may be required to use regularization during training that keeps the weights of the model small. These techniques not only reduce overfitting, but they can also lead to faster optimization of the model and better overall performance.

In this post, you will discover the problem of overfitting when training neural networks and how it can be addressed with regularization methods.

After reading this post, you will know:

Underfitting can easily be addressed by increasing the capacity of the network, but overfitting requires the use of specialized techniques.

Regularization methods like weight decay provide an easy way to control overfitting for large neural network models.

A modern recommendation for regularization is to use early stopping with dropout and a weight constraint.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into four parts; they are:

The Problem of Model Generalization and Overfitting Reduce Overfitting by Constraining Model Complexity Methods for Regularization Regularization Recommendations

The Problem of Model Generalization and Overfitting

The objective of a neural network is to have a final model that performs well both on the data that we used to train it (e.g. the training dataset) and the new data on which the model will be used to make predictions.

The central challenge in machine learning is that we must perform well on new, previously unseen inputs — not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.

— Page 110, Deep Learning, 2016.

We require that the model learn from known examples and generalize from those known examples to new examples in the future. We use methods like a train/test split or k-fold cross-validation only to estimate the ability of the model to generalize to new data.

Learning and also generalizing to new cases is hard.

Too little learning and the model will perform poorly on the training dataset and on new data. The model will underfit the problem. Too much learning and the model will perform well on the training dataset and poorly on new data, the model will overfit the problem. In both cases, the model has not generalized.

Underfit Model . A model that fails to sufficiently learn the problem and performs poorly on a training dataset and does not perform well on a holdout sample.

. A model that fails to sufficiently learn the problem and performs poorly on a training dataset and does not perform well on a holdout sample. Overfit Model . A model that learns the training dataset too well, performing well on the training dataset but does not perform well on a hold out sample.

. A model that learns the training dataset too well, performing well on the training dataset but does not perform well on a hold out sample. Good Fit Model. A model that suitably learns the training dataset and generalizes well to the old out dataset.

A model fit can be considered in the context of the bias-variance trade-off.

An underfit model has high bias and low variance. Regardless of the specific samples in the training data, it cannot learn the problem. An overfit model has low bias and high variance. The model learns the training data too well and performance varies widely with new unseen examples or even statistical noise added to examples in the training dataset.

In order to generalize well, a system needs to be sufficiently powerful to approximate the target function. If it is too simple to fit even the training data then generalization to new data is also likely to be poor. […] An overly complex system, however, may be able to approximate the data in many different ways that give similar errors and is unlikely to choose the one that will generalize best …

— Page 241, Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999.

We can address underfitting by increasing the capacity of the model. Capacity refers to the ability of a model to fit a variety of functions; more capacity, means that a model can fit more types of functions for mapping inputs to outputs. Increasing the capacity of a model is easily achieved by changing the structure of the model, such as adding more layers and/or more nodes to layers.

Because an underfit model is so easily addressed, it is more common to have an overfit model.

An overfit model is easily diagnosed by mon"
317;317;machinelearningmastery.com;https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/;2018-01-02;How to Configure an Encoder-Decoder Model for Neural Machine Translation;"Tweet Share Share

Last Updated on August 7, 2019

The encoder-decoder architecture for recurrent neural networks is achieving state-of-the-art results on standard machine translation benchmarks and is being used in the heart of industrial translation services.

The model is simple, but given the large amount of data required to train it, tuning the myriad of design decisions in the model in order get top performance on your problem can be practically intractable. Thankfully, research scientists have used Google-scale hardware to do this work for us and provide a set of heuristics for how to configure the encoder-decoder model for neural machine translation and for sequence prediction generally.

In this post, you will discover the details of how to best configure an encoder-decoder recurrent neural network for neural machine translation and other natural language processing tasks.

After reading this post, you will know:

The Google study that investigated each model design decision in the encoder-decoder model to isolate their effects.

The results and recommendations for design decisions like word embeddings, encoder and decoder depth, and attention mechanisms.

A set of base model design decisions that can be used as a starting point on your own sequence-to-sequence projects.

Discover how to develop deep learning models for text classification, translation, photo captioning and more in my new book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Encoder-Decoder Model for Neural Machine Translation

The Encoder-Decoder architecture for recurrent neural networks is displacing classical phrase-based statistical machine translation systems for state-of-the-art results.

As evidence, by their 2016 paper “Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,” Google now uses the approach in their core of their Google Translate service.

A problem with this architecture is that the models are large, in turn requiring very large datasets on which to train. This has the effect of model training taking days or weeks and requiring computational resources that are generally very expensive. As such, little work has been done on the impact of different design choices on the model and their impact on model skill.

This problem is addressed explicitly by Denny Britz, et al. in their 2017 paper “Massive Exploration of Neural Machine Translation Architectures.” In the paper, they design a baseline model for a standard English-to-German translation task and enumerate a suite of different model design choices and describe their impact on the skill of the model. They claim that the complete set of experiments consumed more than 250,000 GPU compute hours, which is impressive, to say the least.

We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures.

In this post, we will look at some of the findings from this paper that we can use to tune our own neural machine translation models, as well as sequence-to-sequence models in general.

For more background on the Encoder-Decoder architecture and the attention mechanism, see the posts:

Need help with Deep Learning for Text Data? Take my free 7-day email crash course now (with code). Click to sign-up and also get a free PDF Ebook version of the course. Start Your FREE Crash-Course Now

Baseline Model

We can start-off by describing the baseline model used as the starting point for all experiments.

A baseline model configuration was chosen such that the model would perform reasonably well on the translation task.

Embedding: 512-dimensions

RNN Cell: Gated Recurrent Unit or GRU

Encoder: Bidirectional

Encoder Depth: 2-layers (1 layer in each direction)

Decoder Depth: 2-layers

Attention: Bahdanau-style

Optimizer: Adam

Dropout: 20% on input

Each experiment started with the baseline model and varied one element in an attempt to isolate the impact of the design decision on the model skill, in this case, BLEU scores.

Embedding Size

A word-embedding is used to represent words input to the encoder.

This is a distributed representation where each word is mapped to a fixed-sized vector of continuous values. The benefit of this approach is that different words with similar meaning will have a similar representation.

This distributed representation is often learned while fitting the model on the training data. The embedding size defines the length of the vectors used to represent words. It is generally believed that a larger dimensionality will result in a more expressive representation, and in turn, better skill.

Interestingly, the results show that the largest size tested did achieve the best results, but the benefit of increasing the size was minor overall.

[results show] t"
318;318;machinelearningmastery.com;https://machinelearningmastery.com/types-of-classification-in-machine-learning/;2020-04-07;4 Types of Classification Tasks in Machine Learning;"# example of binary classification task

from numpy import where

from collections import Counter

from sklearn . datasets import make_blobs

from matplotlib import pyplot

# define dataset

X , y = make_blobs ( n_samples = 1000 , centers = 2 , random_state = 1 )

# summarize dataset shape

print ( X . shape , y . shape )

# summarize observations by class label

counter = Counter ( y )

print ( counter )

# summarize first few examples

for i in range ( 10 ) :

print ( X [ i ] , y [ i ] )

# plot the dataset and color the by class label

for label , _ in counter . items ( ) :

row_ix = where ( y == label ) [ 0 ]

pyplot . scatter ( X [ row_ix , 0 ] , X [ row_ix , 1 ] , label = str ( label ) )

pyplot . legend ( )"
319;319;machinelearningmastery.com;https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/;2019-04-11;How to Configure Image Data Augmentation in Keras;"Tweet Share Share

Last Updated on July 5, 2019

Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.

Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.

The Keras deep learning neural network library provides the capability to fit models using image data augmentation via the ImageDataGenerator class.

In this tutorial, you will discover how to use image data augmentation when training deep learning neural networks.

After completing this tutorial, you will know:

Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.

Image data augmentation is supported in the Keras deep learning library via the ImageDataGenerator class.

How to use shift, flip, brightness, and zoom image data augmentation.

Discover how to build models for photo classification, object detection, face recognition, and more in my new computer vision book, with 30 step-by-step tutorials and full source code.

Let’s get started.

Update May/2019 : Fixed data type for pixel values when plotting.

: Fixed data type for pixel values when plotting. Update Jun/2019: Fixed small typo in API example (thanks Georgios).

Tutorial Overview

This tutorial is divided into eight parts; they are:

Image Data Augmentation Sample Image Image Augmentation With ImageDataGenerator Horizontal and Vertical Shift Augmentation Horizontal and Vertical Flip Augmentation Random Rotation Augmentation Random Brightness Augmentation Random Zoom Augmentation

Image Data Augmentation

The performance of deep learning neural networks often improves with the amount of data available.

Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.

Image data augmentation is perhaps the most well-known type of data augmentation and involves creating transformed versions of images in the training dataset that belong to the same class as the original image.

Transforms include a range of operations from the field of image manipulation, such as shifts, flips, zooms, and much more.

The intent is to expand the training dataset with new, plausible examples. This means, variations of the training set images that are likely to be seen by the model. For example, a horizontal flip of a picture of a cat may make sense, because the photo could have been taken from the left or right. A vertical flip of the photo of a cat does not make sense and would probably not be appropriate given that the model is very unlikely to see a photo of an upside down cat.

As such, it is clear that the choice of the specific data augmentation techniques used for a training dataset must be chosen carefully and within the context of the training dataset and knowledge of the problem domain. In addition, it can be useful to experiment with data augmentation methods in isolation and in concert to see if they result in a measurable improvement to model performance, perhaps with a small prototype dataset, model, and training run.

Modern deep learning algorithms, such as the convolutional neural network, or CNN, can learn features that are invariant to their location in the image. Nevertheless, augmentation can further aid in this transform invariant approach to learning and can aid the model in learning features that are also invariant to transforms such as left-to-right to top-to-bottom ordering, light levels in photographs, and more.

Image data augmentation is typically only applied to the training dataset, and not to the validation or test dataset. This is different from data preparation such as image resizing and pixel scaling; they must be performed consistently across all datasets that interact with the model.

Want Results with Deep Learning for Computer Vision? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Sample Image

We need a sample image to demonstrate standard data augmentation techniques.

In this tutorial, we will use a photograph of a bird titled “Feathered Friend” by AndYaDontStop, released under a permissive license.

Download the image and save it in your current working directory with the filename ‘bird.jpg‘.

Image Augmentation With ImageDataGenerator

The Keras deep learning library provides the ability to use data augmentation automatically when training a model.

This is achieved by using the ImageDataGenerator class.

First, the class may be instantiated and the configuration for the types of dat"
320;320;machinelearningmastery.com;https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/;2019-09-19;Discrete Probability Distributions for Machine Learning;"Tweet Share Share

Last Updated on February 10, 2020

The probability for a discrete random variable can be summarized with a discrete probability distribution.

Discrete probability distributions are used in machine learning, most notably in the modeling of binary and multi-class classification problems, but also in evaluating the performance for binary classification models, such as the calculation of confidence intervals, and in the modeling of the distribution of words in text for natural language processing.

Knowledge of discrete probability distributions is also required in the choice of activation functions in the output layer of deep learning neural networks for classification tasks and selecting an appropriate loss function.

Discrete probability distributions play an important role in applied machine learning and there are a few distributions that a practitioner must know about.

In this tutorial, you will discover discrete probability distributions used in machine learning.

After completing this tutorial, you will know:

The probability of outcomes for discrete random variables can be summarized using discrete probability distributions.

A single binary outcome has a Bernoulli distribution, and a sequence of binary outcomes has a Binomial distribution.

A single categorical outcome has a Multinoulli distribution, and a sequence of categorical outcomes has a Multinomial distribution.

Discover bayes opimization, naive bayes, maximum likelihood, distributions, cross entropy, and much more in my new book, with 28 step-by-step tutorials and full Python source code.

Let’s get started.

Tutorial Overview

This tutorial is divided into five parts; they are:

Discrete Probability Distributions Bernoulli Distribution Binomial Distribution Multinoulli Distribution Multinomial Distribution

Discrete Probability Distributions

A random variable is the quantity produced by a random process.

A discrete random variable is a random variable that can have one of a finite set of specific outcomes. The two types of discrete random variables most commonly used in machine learning are binary and categorical.

Binary Random Variable : x in {0, 1}

: x in {0, 1} Categorical Random Variable: x in {1, 2, …, K}.

A binary random variable is a discrete random variable where the finite set of outcomes is in {0, 1}. A categorical random variable is a discrete random variable where the finite set of outcomes is in {1, 2, …, K}, where K is the total number of unique outcomes.

Each outcome or event for a discrete random variable has a probability.

The relationship between the events for a discrete random variable and their probabilities is called the discrete probability distribution and is summarized by a probability mass function, or PMF for short.

For outcomes that can be ordered, the probability of an event equal to or less than a given value is defined by the cumulative distribution function, or CDF for short. The inverse of the CDF is called the percentage-point function and will give the discrete outcome that is less than or equal to a probability.

PMF : Probability Mass Function, returns the probability of a given outcome.

: Probability Mass Function, returns the probability of a given outcome. CDF : Cumulative Distribution Function, returns the probability of a value less than or equal to a given outcome.

: Cumulative Distribution Function, returns the probability of a value less than or equal to a given outcome. PPF: Percent-Point Function, returns a discrete value that is less than or equal to the given probability.

There are many common discrete probability distributions.

The most common are the Bernoulli and Multinoulli distributions for binary and categorical discrete random variables respectively, and the Binomial and Multinomial distributions that generalize each to multiple independent trials.

Binary Random Variable : Bernoulli Distribution

: Bernoulli Distribution Sequence of a Binary Random Variable : Binomial Distribution

: Binomial Distribution Categorical Random Variable : Multinoulli Distribution

: Multinoulli Distribution Sequence of a Categorical Random Variable: Multinomial Distribution

In the following sections, we will take a closer look at each of these distributions in turn.

There are additional discrete probability distributions that you may want to explore, including the Poisson Distribution and the Discrete Uniform Distribution.

Want to Learn Probability for Machine Learning Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Bernoulli Distribution

The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome as either a 0 or 1.

x in {0, 1}

A “Bernoulli trial” is an experiment or case where the outcome follows a Bernoulli distribution. The distribution and the trial are named after the Swiss mathema"
321;321;machinelearningmastery.com;http://machinelearningmastery.com/evaluate-machine-learning-algorithms-with-r/;2016-01-31;How to Evaluate Machine Learning Algorithms with R;"# Linear Discriminant Analysis

set . seed ( seed )

fit . lda < - train ( diabetes ~ . , data = dataset , method = ""lda"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control )

# Logistic Regression

set . seed ( seed )

fit . glm < - train ( diabetes ~ . , data = dataset , method = ""glm"" , metric = metric , trControl = control )

# GLMNET

set . seed ( seed )

fit . glmnet < - train ( diabetes ~ . , data = dataset , method = ""glmnet"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control )

# SVM Radial

set . seed ( seed )

fit . svmRadial < - train ( diabetes ~ . , data = dataset , method = ""svmRadial"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control , fit = FALSE )

# kNN

set . seed ( seed )

fit . knn < - train ( diabetes ~ . , data = dataset , method = ""knn"" , metric = metric , preProc = c ( ""center"" , ""scale"" ) , trControl = control )

# Naive Bayes

set . seed ( seed )

fit . nb < - train ( diabetes ~ . , data = dataset , method = ""nb"" , metric = metric , trControl = control )

# CART

set . seed ( seed )

fit . cart < - train ( diabetes ~ . , data = dataset , method = ""rpart"" , metric = metric , trControl = control )

# C5.0

set . seed ( seed )

fit . c50 < - train ( diabetes ~ . , data = dataset , method = ""C5.0"" , metric = metric , trControl = control )

# Bagged CART

set . seed ( seed )

fit . treebag < - train ( diabetes ~ . , data = dataset , method = ""treebag"" , metric = metric , trControl = control )

# Random Forest

set . seed ( seed )

fit . rf < - train ( diabetes ~ . , data = dataset , method = ""rf"" , metric = metric , trControl = control )

# Stochastic Gradient Boosting (Generalized Boosted Modeling)

set . seed ( seed )"
322;322;news.mit.edu;http://news.mit.edu/2020/can-financial-disclosure-climate-risk-accelerate-climate-action-0416;;Can financial disclosure of climate risk accelerate climate action?;"The Covid-19 pandemic could be a dry run for future impacts of climate change, with challenging and unprecedented situations requiring rapid and aggressive responses worldwide. A proactive approach to climate change aimed at minimizing such impacts will inevitably involve significant cuts in greenhouse gas (GHG) emissions and investment in more resilient infrastructure. Although current global mitigation and adaptation efforts are proceeding slowly, one emerging strategy could serve as an accelerant: the financial disclosure of climate risk by companies. Such disclosure, if practiced more widely and consistently, could lower the risks of climate change by redirecting investments away from GHG-emitting activities and pinpointing infrastructure that needs to be made more resilient.

Toward that end, the MIT Joint Program on the Science and Policy of Global Change engaged dozens of decision-makers in the financial sector and industry in a two-hour panel discussion on climate-related financial risk. Held as a Zoom meeting on March 26 and facilitated by joint program Co-Director Ronald Prinn, the discussion featured six finance and economics experts from the Bank of England, the Bank of Canada, HSBC, BP, and MIT. Panelists described how their organization has been affected by climate-related financial risk and the steps it’s taking to address it, how climate and economic scenarios could be useful in better understanding climate-related financial risks, and potential research that an institution like MIT could pursue to advance the state of knowledge in this area.

Organizational impacts and responses

Physical risks — potential losses due to more frequent and severe weather driven by climate change — and transition risks — potential losses due to a rapid transition to a low-carbon economy — pose significant economic threats to financial institutions and industrial firms. Those represented on the panel have taken notice and are mounting systemic responses.

Theresa Löber, head of the Bank of England’s Climate Hub, noted that the bank has taken a lead role in ensuring that financial firms develop an enhanced approach to managing the financial risks of climate change. Each institution under its control is required to appoint a senior representative who’s accountable to the bank, incorporate physical and transition risk into its existing risk management framework, perform scenario analyses, and properly disclose climate risks. The largest firms must also undergo a climate stress test.

A climate focus is also prominent at the Bank of Canada, as part of its mandate to promote economic stability. The bank participates in a network of central banks focused on greening the financial system through an exchange of ideas on how best to assess climate-related risk, and conducts its own studies of different climate and economic scenarios.

“Generally, what we’re finding is that there’s a tradeoff between physical and transition risks depending on the pathway you look at,” said Craig Johnston, senior economist at the Bank of Canada. “If we do nothing [to reduce emissions], we see very limited transition risks, but the highest level of physical risks. On the other side of things, a rapid transformation toward a low-carbon economy has the highest transition risks, but it does mitigate physical risks to some degree.”

Guided by the actions of central banks and evolving market forces, private banks and firms in other sectors are taking climate-related financial disclosure seriously.

Alan Smith, global head of risk strategy at HSBC, observed that every kind of risk the financial institution faces is affected by climate change, so that the issue now informs all of the company’s activities.

Spencer Dale, group chief economist at BP, observed that as a major oil and gas company facing a global energy transition to low-carbon sources, BP sees the issue of financial disclosure of climate risk as having less to do with the firm’s financial risks and more to do with its core purpose and structure. A key consideration is how products sold by BP can be consistent with a companywide goal of achieving net-zero emissions by 2050. While carbon offsets, carbon capture technology, and tree planting could be part of the solution, the company’s main challenge will be to shift the business to zero-carbon products.

Best practices for using scenarios in climate-related financial risk assessment

All six panelists saw an important role for scenarios — projections of how the climate and economy are likely to evolve under different climate policies and rates of energy technology market penetration — in enabling financial institutions and businesses to assess climate-related financial risk. There was general agreement that scenarios should not be seen as predictions, but instead as a range of plausible potential outcomes with varying levels of uncertainty.

Recognizing the inherent uncertainty of any single scenario, Dale noted that BP has surveyed t"
323;323;machinelearningmastery.com;https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/;2019-07-11;How to Code the GAN Training Algorithm and Loss Functions;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

# gan training algorithm def train_gan ( generator , discriminator , dataset , latent_dim , n_epochs , n_batch ) : # calculate the number of batches per epoch batches_per_epoch = int ( len ( dataset ) / n_batch ) # calculate the number of training iterations n_steps = batches_per_epoch * n_epochs # gan training algorithm for i in range ( n_steps ) : # generate points in the latent space z = randn ( latent_dim * n_batch ) # reshape into a batch of inputs for the network z = z . reshape ( n_batch , latent_dim ) # generate fake images fake = generator . predict ( z ) # select a batch of random real images ix = randint ( 0 , len ( dataset ) , n_batch ) # retrieve real images real = dataset [ ix ] # update weights of the discriminator model # ... # generate points in the latent space z = randn ( latent_dim * n_batch ) # reshape into a batch of inputs for the network z = z . reshape ( n_batch , latent_dim ) # generate fake images fake = generator . predict ( z ) # classify as real or fake result = discriminator . predict ( fake ) # update weights of the generator model # ..."
324;324;news.mit.edu;http://news.mit.edu/2020/computational-thinking-class-enables-students-engage-covid-19-response-0407;;Computational thinking class enables students to engage in Covid-19 response;"When an introductory computational science class, which is open to the general public, was repurposed to study the Covid-19 pandemic this spring, the instructors saw student registration rise from 20 students to nearly 300.

Introduction to Computational Thinking (6.S083/18.S190), which applies data science, artificial intelligence, and mathematical models using the Julia programming language developed at MIT, was introduced in the fall as a pilot half-semester class. It was launched as part of the MIT Stephen A. Schwarzman College of Computing’s computational thinking program and spearheaded by Department of Mathematics Professor Alan Edelman and Visiting Professor David P. Sanders. They very quickly were able to fast-track the curriculum to focus on applications to Covid-19 responses; students were equally fast in jumping on board.

“Everyone at MIT wants to contribute,” says Edelman. “While we at the Julia Lab are doing research in building tools for scientists, Dave and I thought it would be valuable to teach the students about some of the fundamentals related to computation for drug development, disease models, and such.”

The course is offered through MIT’s Department of Electronic Engineering and Computer Science and the Department of Mathematics. “This course opens a trove of opportunities to use computation to better understand and contain the Covid-19 pandemic,” says MIT Computer Science and Artificial Intelligence Laboratory Director Daniela Rus.

The fall version of the class had a maximum enrollment of 20 students, but the spring class has ballooned to nearly 300 students in one weekend, almost all from MIT. “We’ve had a tremendous response,” Edelman says. “This definitely stressed the MIT sign-up systems in ways that I could not have imagined.”

Sophomore Shinjini Ghosh, majoring in computer science and linguistics, says she was initially drawn to the class to learn Julia, “but also to develop the skills to do further computational modeling and conduct research on the spread and possible control of Covid-19.”

""There's been a lot of misinformation about the epidemiology and statistical modeling of the coronavirus,” adds sophomore Raj Movva, a computer science and biology major. “I think this class will help clarify some details, and give us a taste of how one might actually make predictions about the course of a pandemic.""

Edelman says that he has always dreamed of an interdisciplinary modern class that would combine the machine learning and AI of a “data-driven” world, the modern software and systems possibilities that Julia allows, and the physical models, differential equations, and scientific machine learning of the “physical world.”

He calls this class “a natural outgrowth of Julia Lab's research, and that of the general cooperative open-source Julia community.” For years, this online community collaborates to create tools to speed up the drug approval process, aid in scientific machine learning and differential equations, and predict infectious disease transmission. “The lectures are open to the world, following the great MIT tradition of MIT open courses,” says Edelman.

So when MIT turned to virtual learning to de-densify campus, the transition to an online, remotely taught version of the class was not too difficult for Edelman and Sanders.

""Even though we have run open remote learning courses before, it's never the same as being able to see the live audience in front of you,” says Edelman. “However, MIT students ask such great questions in the Zoom chat, so that it remains as intellectually invigorating as ever.""

Sanders, a Marcos Moshinsky research fellow currently on leave as a professor at the National University of Mexico, is working on techniques for accelerating global optimization. Involved with the Julia Lab since 2014, Sanders has worked with Edelman on various teaching, research, and outreach projects related to Julia, and his YouTube tutorials have reached over 100,000 views. “His videos have often been referred to as the best way to learn the Julia language,” says Edelman.

Edelman will also be enlisting some help from Philip, his family’s Corgi who until recently had been a frequent wanderer of MIT’s halls and classrooms. “Philip is a well-known Julia expert whose image has been classified many times by Julia’s AI Systems,” says Edelman. “Students are always happy when Philip participates in the online classes.”"
325;325;machinelearningmastery.com;https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/;2018-10-23;How to Grid Search SARIMA Hyperparameters for Time Series Forecasting;"# grid search sarima hyperparameters for monthly car sales dataset

from math import sqrt

from multiprocessing import cpu_count

from joblib import Parallel

from joblib import delayed

from warnings import catch_warnings

from warnings import filterwarnings

from statsmodels . tsa . statespace . sarimax import SARIMAX

from sklearn . metrics import mean_squared_error

from pandas import read_csv

# one-step sarima forecast

def sarima_forecast ( history , config ) :

order , sorder , trend = config

# define model

model = SARIMAX ( history , order = order , seasonal_order = sorder , trend = trend , enforce_stationarity = False , enforce_invertibility = False )

# fit model

model_fit = model . fit ( disp = False )

# make one step forecast

yhat = model_fit . predict ( len ( history ) , len ( history ) )

return yhat [ 0 ]

# root mean squared error or rmse

def measure_rmse ( actual , predicted ) :

return sqrt ( mean_squared_error ( actual , predicted ) )

# split a univariate dataset into train/test sets

def train_test_split ( data , n_test ) :

return data [ : - n_test ] , data [ - n_test : ]

# walk-forward validation for univariate data

def walk_forward_validation ( data , n_test , cfg ) :

predictions = list ( )

# split dataset

train , test = train_test_split ( data , n_test )

# seed history with training dataset

history = [ x for x in train ]

# step over each time-step in the test set

for i in range ( len ( test ) ) :

# fit model and make forecast for history

yhat = sarima_forecast ( history , cfg )

# store forecast in list of predictions

predictions . append ( yhat )

# add actual observation to history for the next loop

history . append ( test [ i ] )

# estimate prediction error

error = measure_rmse ( test , predictions )

return error

# score a model, return None on failure

def score_model ( data , n_test , cfg , debug = False ) :

result = None

# convert config to a key

key = str ( cfg )

# show all warnings and fail on exception if debugging

if debug :

result = walk_forward_validation ( data , n_test , cfg )

else :

# one failure during model validation suggests an unstable config

try :

# never show warnings when grid searching, too noisy

with catch_warnings ( ) :

filterwarnings ( ""ignore"" )

result = walk_forward_validation ( data , n_test , cfg )

except :

error = None

# check for an interesting result

if result is not None :

print ( ' > Model[%s] %.3f' % ( key , result ) )

return ( key , result )

# grid search configs

def grid_search ( data , cfg_list , n_test , parallel = True ) :

scores = None

if parallel :

# execute configs in parallel

executor = Parallel ( n_jobs = cpu_count ( ) , backend = 'multiprocessing' )

tasks = ( delayed ( score_model ) ( data , n_test , cfg ) for cfg in cfg_list )

scores = executor ( tasks )

else :

scores = [ score_model ( data , n_test , cfg ) for cfg in cfg_list ]

# remove empty results

scores = [ r for r in scores if r [ 1 ] != None ]

# sort configs by error, asc

scores . sort ( key = lambda tup : tup [ 1 ] )

return scores

# create a set of sarima configs to try

def sarima_configs ( seasonal = [ 0 ] ) :

models = list ( )

# define config lists

p_params = [ 0 , 1 , 2 ]

d_params = [ 0 , 1 ]

q_params = [ 0 , 1 , 2 ]

t_params = [ 'n' , 'c' , 't' , 'ct' ]

P_params = [ 0 , 1 , 2 ]

D_params = [ 0 , 1 ]

Q_params = [ 0 , 1 , 2 ]

m_params = seasonal

# create config instances

for p in p_params :

for d in d_params :

for q in q_params :

for t in t_params :

for P in P_params :

for D in D_params :

for Q in Q_params :

for m in m_params :

cfg = [ ( p , d , q ) , ( P , D , Q , m ) , t ]

models . append ( cfg )

return models

if __name__ == '__main__' :

# load dataset

series = read_csv ( 'monthly-car-sales.csv' , header = 0 , index_col = 0 )

data = series . values

print ( data . shape )

# data split

n_test = 12

# model configs

cfg_list = sarima_configs ( seasonal = [ 0 , 6 , 12 ] )

# grid search

scores = grid_search ( data , cfg_list , n_test )

print ( 'done' )

# list top 3 configs

for cfg , error in scores [ : 3 ] :"
326;326;news.mit.edu;http://news.mit.edu/2019/mit-sounding-explores-far-reaching-musical-frontiers-0927;;MIT Sounding 2019-20 explores far-reaching musical frontiers;"Now in its eighth year, 2019-20 MIT Sounding presents another season of wide-ranging musical offerings that have found a vibrant home at MIT.

“The program feeds the hunger of a diverse audience for music at MIT,” says Evan Ziporyn, faculty director of the MIT Center for Art, Science and Technology (CAST) and curator of the series. “We try to give students a sense of exploration, while also developing a larger-scale dialogue with local audiences.”

The eclectic journey continues with Boston premieres of music from New York, Czechia, and Nepal, as well as returning artists who have wowed local audiences and who continue to push new musical boundaries. Add a septet of turntable artists, a multimedia score by Tod Machover, and a virtual reality-enhanced, dataset-driven “space opera” by artist Matthew Ritchie, and you have an abundant season of MIT Sounding.

Glenn Branca: New York’s enfant terrible

The year started with a bang with “Branca Lives: The Glenn Branca Ensemble/Ambient Orchestra,"" an all-too-rare performance of music by the proto-punk legend, who passed away in 2018.

“Branca’s symphonies for multiple guitars — sometimes up to 100 at a time — were Brutalism in musical form,” says Ziporyn. “He embraced the energy of noise, distortion, and feedback, but in a carefully organized way, activating overtones and microtones to create amazing, almost hallucinogenic textures. He was thinking orchestrally, building out from the sound of the electric guitar rather than from classical instruments. Then he began to write for acoustic orchestra and found ways to get the same effects.”

“Branca Lives"" presents the composer’s eponymous guitar ensemble, led by his longtime concertmaster and collaborator, Reg Bloor. Their set will include Branca’s “The Light (for David),” a tribute to David Bowie. Ziporyn and the Ambient Orchestra will open the concert with Boston premieres of two of Branca’s rarely performed orchestral works — “Symphony No. 14 (2,000,000,000 Light Years from Home)” and “Freeform.”

“It’s brilliant and surprising music that deserves to be known,” adds Ziporyn.

Lochan Rijal shares music of Nepal

Despite an ever-shrinking global culture, many musical traditions remain overlooked, including the music of Nepal. “काँचो आवाज (Raw Sounds),” a program that celebrates Nepal’s unique musical heritage, seeks to address that oversight.

“काँचो आवाज (Raw Sounds)” features Lochan Rijal, the award-winning Nepali multi-instrumentalist singer and songwriter, performing new and traditional compositions based on his own musical narrative of everyday life in Nepal. The head of Kathmandu University’s Department of Music, Rijal will play the sarangi, a traditional short-necked fiddle, and the Gandharva lute arbaja, recently discovered in Rijal’s research in Nepal.

During his residency, Rijal will discuss a temple restoration project and Nepal’s musical traditions in a public lecture.

Iva Bittová with MITSO

Legendary Czech vocalist/violinist Iva Bittová is a familiar force of nature at MIT, having performed with the improvisational trio EVIYAN, and collaborated with the Festival Jazz Ensemble and Pilobolus Dance for MIT One World.

Bittová returns this October as composer to launch the MIT Symphony Orchestra’s (MITSO) 2019-20 season in “The Heart is a Bell.” The concert pairs two pieces by 20th century Czech female composers: Bittová’s “Zvon” and Vítězslava Kapralova’s “Suita Rustica.” Composed 75 years apart, both works draw on Czech and Slovak folk culture, seen through a modern lens.

At once personal and avant-garde, “Zvon” features Bittova’s voice, jazz combo, elements of world music and cabaret, and improvisation by members of the orchestra. “We’re widening the orchestral landscape,” says Ziporyn, who steps in as acting MITSO director this academic year.

Additional projects and performances

What happens when seven DJs gather, challenged to make music together rather than as solo acts? Audiences will find out this January, in “the wave function collapses.” The unique program features harbanger (pronounced “harbinger”), a turntable septet with visiting artists Harry Allen and DJ Rob Swift, known for their work with Public Enemy and The Source magazine. “The wave function collapses” is the culmination of a two-week workshop facilitated by Eran Egozy, professor of the practice in music technology at MIT and co-founder and CTO of Harmonix Music Systems. The 2020 Independent Activities Period (IAP) offering includes two courses: a history of DJ culture by hip hop activist and “Media Assassin” Harry Allen, and hands-on DJ instruction by DJ Rob Swift.

Virtuoso violinist Johnny Gandelsman performed Johann Sebastian Bach’s ""Sonatas and Partitas"" as part of MIT Sounding’s 2015 season. The adventurous soloist returns this spring to perform “Bach’s Cello Suites” on the violin — which can be challenging, given the two instruments’ very different voicings. But this isn’t reinvention for its own sake, says Ziporyn. It’s sim"
327;327;news.mit.edu;http://news.mit.edu/2020/proteins-cytokine-storms-covid-19-0416;;Proteins may halt the severe cytokine storms seen in Covid-19 patients;"One of the defining features of Covid-19 is the excessive immune response that can occur in severe cases. This burst of immune overreaction, also called a cytokine storm, damages the lungs and can be fatal.

A team of MIT researchers has developed specialized proteins, similar in structure to antibodies, that they believe could soak up these excess cytokines.

“The idea is that they can be injected into the body and bind to the excessive cytokines as generated by the cytokine storm, removing the excessive cytokines and alleviating the symptoms from the infection,” says Rui Qing, an MIT research scientist who is one of the senior authors of the study.

The researchers have reported their initial findings in the journal Quarterly Review of Biophysics (QRB) Discovery, and they now hope to begin testing their proteins in human cells and in animal models of cytokine release and coronavirus infection.

Shuguang Zhang, a principal research scientist in the MIT Media Lab’s Laboratory of Molecular Architecture, is also a senior author of the paper. Shilei Hao, a visiting scientist at MIT, is the lead author of the study, and David Jin, CEO and president of Avalon GloboCare, is also an author.

A molecular sponge

The researchers’ work on blocking cytokine storms grew out of a project that Zhang began 10 years ago to develop modified versions of membrane-embedded proteins. These proteins are usually difficult to study because once they are extracted from the cell membrane, they only maintain their structure if they are suspended in special types of detergents.

After working on the problem for several years, Zhang and Qing developed a method for modifying the hydrophobic regions of these proteins, making them soluble in water and much easier to study. Their method, called the QTY code, calls for replacing some hydrophobic amino acids with hydrophilic amino acids that have similar structures. Leucine is converted to glutamine, isoleucine and valine are converted to threonine, and phenylalanine is converted to tyrosine.

Following the development of the QTY code, Jin approached Zhang’s lab with the idea of designing water-soluble versions of proteins known as cytokine receptors. These receptors are found on the surface of immune cells, where they bind to cytokines — signaling proteins that stimulate inflammation and other immune responses.

Jin believed that proteins that mimic these cytokine receptors could help combat cytokine storms, which can be produced by viral or bacterial infections, including HIV and hepatitis. They can also occur as a side effect of cancer immunotherapy.

In April 2019, Zhang’s team set out to design proteins that could sop up these excess cytokines like a sponge. To do that, they used the QTY code to make water-soluble versions of cytokine receptors. When proteins are soluble in water, they can travel efficiently through the human bloodstream, while the original, hydrophobic versions of the proteins would likely stick to cells that they encountered.

The researchers also attached an antibody segment called the Fc region to their water-soluble receptor proteins. This region helps to further stabilize the proteins in the bloodstream, and makes them less likely to be attacked by the immune system.

The researchers designed proteins that mimic six different cytokine receptors, which can bind to cytokines such as interferon and interleukin, as well as a class of cytokines called chemokines. In laboratory tests of the proteins’ binding strength, the researchers found that their modified proteins were able to bind to cytokines with similar strength as naturally occurring cytokine receptors.

“The cytokine receptors that we designed will soak up the majority of the excessive cytokines that are released during the cytokine storm,” Jin says.

Driven by curiosity

In March, when evidence began to suggest that the SARS-CoV-2 virus was inducing cytokine storms in some patients, the researchers realized that the receptor proteins they had designed might be able to help. They decided to quickly publish the results they have generated so far, and they are now planning to do additional tests in human cells and in animal models of Covid-19 infection.

The potential usefulness of this approach underscores the importance of “curiosity-driven research,” Zhang says.

“As it turns out, our research initiated in April 2019 is directly relevant to the treatment of Covid-19 infected patients,” he says. “Curiosity-driven, or even proactive research often leads to preparedness, which is key to preventing future disasters.”

The researchers have filed for patents on the proteins that they designed, as well as on their overall approach to creating water-soluble cytokine receptors. They hope to license the technology quickly and to collaborate with pharmaceutical and biotech companies who can help to move it toward clinical trials.

“Obviously this approach will need further animal studies, and potentially human clinical"
328;328;machinelearningmastery.com;http://machinelearningmastery.com/grid-search-arima-hyperparameters-with-python/;2017-01-17;How to Grid Search ARIMA Model Hyperparameters with Python;"import warnings

from pandas import read_csv

from pandas import datetime

from statsmodels . tsa . arima_model import ARIMA

from sklearn . metrics import mean_squared_error

# evaluate an ARIMA model for a given order (p,d,q)

def evaluate_arima_model ( X , arima_order ) :

# prepare training dataset

train_size = int ( len ( X ) * 0.66 )

train , test = X [ 0 : train_size ] , X [ train_size : ]

history = [ x for x in train ]

# make predictions

predictions = list ( )

for t in range ( len ( test ) ) :

model = ARIMA ( history , order = arima_order )

model_fit = model . fit ( disp = 0 )

yhat = model_fit . forecast ( ) [ 0 ]

predictions . append ( yhat )

history . append ( test [ t ] )

# calculate out of sample error

error = mean_squared_error ( test , predictions )

return error

# evaluate combinations of p, d and q values for an ARIMA model

def evaluate_models ( dataset , p_values , d_values , q_values ) :

dataset = dataset . astype ( 'float32' )

best_score , best_cfg = float ( ""inf"" ) , None

for p in p_values :

for d in d_values :

for q in q_values :

order = ( p , d , q )

try :

mse = evaluate_arima_model ( dataset , order )

if mse < best_score :

best_score , best_cfg = mse , order

print ( 'ARIMA%s MSE=%.3f' % ( order , mse ) )

except :

continue

print ( 'Best ARIMA%s MSE=%.3f' % ( best_cfg , best_score ) )

# load dataset

def parser ( x ) :

return datetime . strptime ( '190' + x , '%Y-%m' )

series = read_csv ( 'shampoo-sales.csv' , header = 0 , parse_dates = [ 0 ] , index_col = 0 , squeeze = True , date_parser = parser )

# evaluate parameters

p_values = [ 0 , 1 , 2 , 4 , 6 , 8 , 10 ]

d_values = range ( 0 , 3 )

q_values = range ( 0 , 3 )

warnings . filterwarnings ( ""ignore"" )"
329;329;towardsdatascience.com;https://towardsdatascience.com/what-5-years-of-a-relationships-messages-look-like-45921155e3f2?source=collection_home---4------0-----------------------;2020-04-16;What 5 Years of a Relationship’s Messages Look Like;"Of course, modern messaging is more than just the words you exchange (there’s a reason the Oxford Dictionary word-of-the-year in 2015 was an emoji). Surprisingly, the initial scrape of the Telegram HTML actually preserved the original emoji.

Native Python does not cope with these characters, however there’s a neat little package (imaginatively called ‘emoji’) that allows you to identify, count, and decode emojis in strings.

If we investigate the ‘non-text’ messages, we can see that emojis are an especially important communication method for my girlfriend. I, on the other hand, add value by sharing photos and links (generally from Reddit), and stickers (a feature that Telegram was relatively early to introduce).

We can attempt to see how volumes of these non-text messages have changed over time using the same rolling average trick. However, beyond an early 2018 spike in Stickers (when we worked out that you could download custom sets — truly, a gamechaging moment), there aren’t many discernable patterns.

Instead, let’s build a graph showing the accumilation of these non-text messages by type. To do this, we can employ NumPy’s cumsum() method, which will take a cumulative sum of a series. Supposing we have a dataframe whose index is a date range, and each column describes the number of messages of each type sent on that day:

#Create a list of message types

required_cols = ['Emoji','Photo','Sticker','Link','Voice message','Animation'] #Create a new dataframe of just these types

df_types_cum = df_types[required_cols] #Iterate through the columns and replace

#each with the cumulative sum version

for i in required_cols:

df_types_cum[i] = np.cumsum(df_types_cum[i]) #Use Pandas' in-built plot method to show it

df_types_cum.plot.area(figsize=(15,6), lw=0)

This produces the following chart.

Again, we can see the introduction and uptake of stickers (the amber segment), as well as an acceleration in the use of emojis from 2018 onwards."
330;330;news.mit.edu;http://news.mit.edu/2020/three-mit-students-named-2020-goldwater-scholars-0409;;Katie Collins, Vaishnavi Phadnis, and Vaibhavi Shah named 2020-21 Goldwater Scholars;"MIT students Katie Collins, Vaishnavi Phadnis, and Vaibhavi Shah have been selected to receive a Barry Goldwater Scholarship for the 2020-21 academic year. Over 5,000 college students from across the United States were nominated for the scholarships, from which only 396 recipients were selected based on academic merit.

The Goldwater scholarships have been conferred since 1989 by the Barry Goldwater Scholarship and Excellence in Education Foundation. These scholarships have supported undergraduates who go on to become leading scientists, engineers, and mathematicians in their respective fields. All of the 2020-21 Goldwater Scholars intend to obtain a doctorate in their area of research, including the three MIT recipients.

Katie Collins, a third-year majoring in brain and cognitive sciences with minors in computer science and biomedical engineering, got involved with research in high school, when she worked on computational models of metabolic networks and synthetic gene networks in the lab of Department of Electrical Engineering and Computer Science Professor Timothy Lu at MIT. It was this project that led her to realize how challenging it is to model and analyze complex biological networks. She also learned that machine learning can provide a path for exploring these networks and understanding human diseases. This realization has coursed a scientific path for Collins that is equally steeped in computer science and human biology.

Over the past few years, Collins has become increasingly interested in the human brain, particularly what machine learning can learn from human common-sense reasoning and the way brains process sparse, noisy data. “I aim to develop novel computational algorithms to analyze complex, high-dimensional data in biomedicine, as well as advance modelling paradigms to improve our understanding of human cognition,” explains Collins. In his letter of recommendation, Professor Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences and one of Collins’ mentors, wrote, “It is very difficult to imagine a better candidate for the Goldwater fellowship.” Collins plans to pursue a PhD studying machine learning or computational neuroscience and to one day run her own lab. “I hope to become a professor, leading a research program at the interface of computer science and cognitive neuroscience.”

Vaishnavi Phadnis, a second-year majoring in computer science and molecular biology, sees molecular and cellular biology as the bridge between chemistry and life, and she’s been enthralled with understanding that bridge since 7th grade, when she learned about the chemical basis of the cell. Phadnis spent two years working in a cancer research lab while still in high school, an experience which convinced her that research was not just her passion but also her future. “In my first week at MIT, I approached Professor Robert Weinberg, and I’ve been grateful to do research in his lab ever since,” she says.

“Vaishnavi’s exuberance makes her a joy to have in the lab,” wrote Weinberg, who is the Daniel Ludwig Professor in the Department of Biology. Phadnis is investigating ferroptosis, a recently discovered, iron-dependent form of cell death that may be relevant in neurodegeneration and also a potential strategy for targeting highly aggressive cancer cells. “She is a phenomenon who has vastly exceeded our expectations of the powers of someone her age,” Weinberg says. Phadnis is thankful to Weinberg and all the scientific mentors, both past and present, that have inspired her along her research path. Deciphering the mechanisms behind fundamental cellular processes and exploring their application in human diseases is something Phadnis plans to continue doing in her future as a physician-scientist after pursuing an MD/PhD. “I hope to devote most of my time to leading my own research group, while also practicing medicine,” she says.

Vaibhavi Shah, a third-year studying biological engineering with a minor in science, technology and society, spent a lot of time in high school theorizing ways to tackle major shortcomings in medicine and science with the help of technology. “When I came to college, I was able to bring some of these ideas to fruition,” she says, working with both the Big Data in Radiology Group at the University of California at San Francisco and the lab of Professor Mriganka Sur, the Newton Professor of Neuroscience in the Department of Brain and Cognitive Sciences.

Shah is particularly interested in integrating innovative research findings with traditional clinical practices. According to her, technology, like computer vision algorithms, can be adopted to diagnose diseases such as Alzheimer’s, allowing patients to start appropriate treatments earlier. “This is often harder to do at smaller, rural institutions that may not always have a specialist present,” says Shah, and algorithms can help fill that gap. One of aims of Shah’s research is to improve the efficienc"
331;331;machinelearningmastery.com;https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/;2019-01-24;Understand the Impact of Learning Rate on Neural Network Performance;"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76

# study of patience for the learning rate drop schedule on the blobs problem from sklearn . datasets import make_blobs from keras . layers import Dense from keras . models import Sequential from keras . optimizers import SGD from keras . utils import to_categorical from keras . callbacks import Callback from keras . callbacks import ReduceLROnPlateau from keras import backend from matplotlib import pyplot # monitor the learning rate class LearningRateMonitor ( Callback ) : # start of training def on_train_begin ( self , logs = { } ) : self . lrates = list ( ) # end of each training epoch def on_epoch_end ( self , epoch , logs = { } ) : # get and store the learning rate optimizer = self . model . optimizer lrate = float ( backend . get_value ( self . model . optimizer . lr ) ) self . lrates . append ( lrate ) # prepare train and test dataset def prepare_data ( ) : # generate 2d classification dataset X , y = make_blobs ( n_samples = 1000 , centers = 3 , n_features = 2 , cluster_std = 2 , random_state = 2 ) # one hot encode output variable y = to_categorical ( y ) # split into train and test n_train = 500 trainX , testX = X [ : n_train , : ] , X [ n_train : , : ] trainy , testy = y [ : n_train ] , y [ n_train : ] return trainX , trainy , testX , testy # fit a model and plot learning curve def fit_model ( trainX , trainy , testX , testy , patience ) : # define model model = Sequential ( ) model . add ( Dense ( 50 , input_dim = 2 , activation = 'relu' , kernel_initializer = 'he_uniform' ) ) model . add ( Dense ( 3 , activation = 'softmax' ) ) # compile model opt = SGD ( lr = 0.01 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] ) # fit model rlrp = ReduceLROnPlateau ( monitor = 'val_loss' , factor = 0.1 , patience = patience , min_delta = 1E - 7 ) lrm = LearningRateMonitor ( ) history = model . fit ( trainX , trainy , validation_data = ( testX , testy ) , epochs = 200 , verbose = 0 , callbacks = [ rlrp , lrm ] ) return lrm . lrates , history . history [ 'loss' ] , history . history [ 'accuracy' ] # create line plots for a series def line_plots ( patiences , series ) : for i in range ( len ( patiences ) ) : pyplot . subplot ( 220 + ( i + 1 ) ) pyplot . plot ( series [ i ] ) pyplot . title ( 'patience=' + str ( patiences [ i ] ) , pad = - 80 ) pyplot . show ( ) # prepare dataset trainX , trainy , testX , testy = prepare_data ( ) # create learning curves for different patiences patiences = [ 2 , 5 , 10 , 15 ] lr_list , loss_list , acc_list , = list ( ) , list ( ) , list ( ) for i in range ( len ( patiences ) ) : # fit model and plot learning curves for a patience lr , loss , acc = fit_model ( trainX , trainy , testX , testy , patiences [ i ] ) lr_list . append ( lr ) loss_list . append ( loss ) acc_list . append ( acc ) # plot learning rates line_plots ( patiences , lr_list ) # plot loss line_plots ( patiences , loss_list ) # plot accuracy line_plots ( patiences , acc_list )"
332;332;machinelearningmastery.com;http://machinelearningmastery.com/feature-selection-with-the-caret-r-package/;2014-09-21;Feature Selection with the Caret R Package;"# ensure results are repeatable

set . seed ( 7 )

# load the library

library ( mlbench )

library ( caret )

# load the dataset

data ( PimaIndiansDiabetes )

# prepare training scheme

control < - trainControl ( method = ""repeatedcv"" , number = 10 , repeats = 3 )

# train the model

model < - train ( diabetes ~ . , data = PimaIndiansDiabetes , method = ""lvq"" , preProcess = ""scale"" , trControl = control )

# estimate variable importance

importance < - varImp ( model , scale = FALSE )

# summarize importance

print ( importance )

# plot importance"
333;333;machinelearningmastery.com;https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/;2019-02-26;How to use Learning Curves to Diagnose Machine Learning Model Performance;"Tweet Share Share

Last Updated on August 6, 2019

A learning curve is a plot of model learning performance over experience or time.

Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally. The model can be evaluated on the training dataset and on a hold out validation dataset after each update during training and plots of the measured performance can created to show learning curves.

Reviewing learning curves of models during training can be used to diagnose problems with learning, such as an underfit or overfit model, as well as whether the training and validation datasets are suitably representative.

In this post, you will discover learning curves and how they can be used to diagnose the learning and generalization behavior of machine learning models, with example plots showing common learning problems.

After reading this post, you will know:

Learning curves are plots that show changes in learning performance over time in terms of experience.

Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.

Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain.

Discover how to train faster, reduce overfitting, and make better predictions with deep learning models in my new book, with 26 step-by-step tutorials and full source code.

Let’s get started.

Overview

This tutorial is divided into three parts; they are:

Learning Curves Diagnosing Model Behavior Diagnosing Unrepresentative Datasets

Learning Curves in Machine Learning

Generally, a learning curve is a plot that shows time or experience on the x-axis and learning or improvement on the y-axis.

Learning curves (LCs) are deemed effective tools for monitoring the performance of workers exposed to a new task. LCs provide a mathematical representation of the learning process that takes place as task repetition occurs.

— Learning curve models and applications: Literature review and research directions, 2011.

For example, if you were learning a musical instrument, your skill on the instrument could be evaluated and assigned a numerical score each week for one year. A plot of the scores over the 52 weeks is a learning curve and would show how your learning of the instrument has changed over time.

Learning Curve: Line plot of learning (y-axis) over experience (x-axis).

Learning curves are widely used in machine learning for algorithms that learn (optimize their internal parameters) incrementally over time, such as deep learning neural networks.

The metric used to evaluate learning could be maximizing, meaning that better scores (larger numbers) indicate more learning. An example would be classification accuracy.

It is more common to use a score that is minimizing, such as loss or error whereby better scores (smaller numbers) indicate more learning and a value of 0.0 indicates that the training dataset was learned perfectly and no mistakes were made.

During the training of a machine learning model, the current state of the model at each step of the training algorithm can be evaluated. It can be evaluated on the training dataset to give an idea of how well the model is “learning.” It can also be evaluated on a hold-out validation dataset that is not part of the training dataset. Evaluation on the validation dataset gives an idea of how well the model is “generalizing.”

Train Learning Curve : Learning curve calculated from the training dataset that gives an idea of how well the model is learning.

: Learning curve calculated from the training dataset that gives an idea of how well the model is learning. Validation Learning Curve: Learning curve calculated from a hold-out validation dataset that gives an idea of how well the model is generalizing.

It is common to create dual learning curves for a machine learning model during training on both the training and validation datasets.

In some cases, it is also common to create learning curves for multiple metrics, such as in the case of classification predictive modeling problems, where the model may be optimized according to cross-entropy loss and model performance is evaluated using classification accuracy. In this case, two plots are created, one for the learning curves of each metric, and each plot can show two learning curves, one for each of the train and validation datasets.

Optimization Learning Curves : Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.

: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss. Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy.

Now that we are familiar with the use of learning curves in machine learning, le"
334;334;machinelearningmastery.com;https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/;2019-06-30;How to Develop a GAN to Generate CIFAR10 Small Color Photographs;"# example of a dcgan on cifar10

from numpy import expand_dims

from numpy import zeros

from numpy import ones

from numpy import vstack

from numpy . random import randn

from numpy . random import randint

from keras . datasets . cifar10 import load_data

from keras . optimizers import Adam

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import Reshape

from keras . layers import Flatten

from keras . layers import Conv2D

from keras . layers import Conv2DTranspose

from keras . layers import LeakyReLU

from keras . layers import Dropout

from matplotlib import pyplot

# define the standalone discriminator model

def define_discriminator ( in_shape = ( 32 , 32 , 3 ) ) :

model = Sequential ( )

# normal

model . add ( Conv2D ( 64 , ( 3 , 3 ) , padding = 'same' , input_shape = in_shape ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample

model . add ( Conv2D ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample

model . add ( Conv2D ( 128 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# downsample

model . add ( Conv2D ( 256 , ( 3 , 3 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# classifier

model . add ( Flatten ( ) )

model . add ( Dropout ( 0.4 ) )

model . add ( Dense ( 1 , activation = 'sigmoid' ) )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt , metrics = [ 'accuracy' ] )

return model

# define the standalone generator model

def define_generator ( latent_dim ) :

model = Sequential ( )

# foundation for 4x4 image

n_nodes = 256 * 4 * 4

model . add ( Dense ( n_nodes , input_dim = latent_dim ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

model . add ( Reshape ( ( 4 , 4 , 256 ) ) )

# upsample to 8x8

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# upsample to 16x16

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# upsample to 32x32

model . add ( Conv2DTranspose ( 128 , ( 4 , 4 ) , strides = ( 2 , 2 ) , padding = 'same' ) )

model . add ( LeakyReLU ( alpha = 0.2 ) )

# output layer

model . add ( Conv2D ( 3 , ( 3 , 3 ) , activation = 'tanh' , padding = 'same' ) )

return model

# define the combined generator and discriminator model, for updating the generator

def define_gan ( g_model , d_model ) :

# make weights in the discriminator not trainable

d_model . trainable = False

# connect them

model = Sequential ( )

# add generator

model . add ( g_model )

# add the discriminator

model . add ( d_model )

# compile model

opt = Adam ( lr = 0.0002 , beta_1 = 0.5 )

model . compile ( loss = 'binary_crossentropy' , optimizer = opt )

return model

# load and prepare cifar10 training images

def load_real_samples ( ) :

# load cifar10 dataset

( trainX , _ ) , ( _ , _ ) = load_data ( )

# convert from unsigned ints to floats

X = trainX . astype ( 'float32' )

# scale from [0,255] to [-1,1]

X = ( X - 127.5 ) / 127.5

return X

# select real samples

def generate_real_samples ( dataset , n_samples ) :

# choose random instances

ix = randint ( 0 , dataset . shape [ 0 ] , n_samples )

# retrieve selected images

X = dataset [ ix ]

# generate 'real' class labels (1)

y = ones ( ( n_samples , 1 ) )

return X , y

# generate points in latent space as input for the generator

def generate_latent_points ( latent_dim , n_samples ) :

# generate points in the latent space

x_input = randn ( latent_dim * n_samples )

# reshape into a batch of inputs for the network

x_input = x_input . reshape ( n_samples , latent_dim )

return x_input

# use the generator to generate n fake examples, with class labels

def generate_fake_samples ( g_model , latent_dim , n_samples ) :

# generate points in latent space

x_input = generate_latent_points ( latent_dim , n_samples )

# predict outputs

X = g_model . predict ( x_input )

# create 'fake' class labels (0)

y = zeros ( ( n_samples , 1 ) )

return X , y

# create and save a plot of generated images

def save_plot ( examples , epoch , n = 7 ) :

# scale from [-1,1] to [0,1]

examples = ( examples + 1 ) / 2.0

# plot images

for i in range ( n * n ) :

# define subplot

pyplot . subplot ( n , n , 1 + i )

# turn off axis

pyplot . axis ( 'off' )

# plot raw pixel data

pyplot . imshow ( examples [ i ] )

# save plot to file

filename = 'generated_plot_e%03d.png' % ( epoch + 1 )

pyplot . savefig ( filename )

pyplot . close ( )

# evaluate the discriminator, plot generated images, save generator model

def summarize_performance ( epoch , g_model , d_model , dataset , latent_dim , n_samples = 150 ) :

# prepare real samples

X_real , y_real = generate_real_samples ( dataset , n_samples )

# eva"
335;335;machinelearningmastery.com;http://machinelearningmastery.com/introduction-python-deep-learning-library-keras/;2016-05-09;Introduction to Python Deep Learning with Keras;"Tweet Share Share

Last Updated on September 13, 2019

Two of the top numerical platforms in Python that provide the basis for Deep Learning research and development are Theano and TensorFlow.

Both are very powerful libraries, but both can be difficult to use directly for creating deep learning models.

In this post, you will discover the Keras Python library that provides a clean and convenient way to create a range of deep learning models on top of Theano or TensorFlow.

Discover how to develop deep learning models for a range of predictive modeling problems with just a few lines of code in my new book, with 18 step-by-step tutorials and 9 projects.

Let’s get started.

Update Oct/2016 : Updated for Keras 1.1.0, Theano 0.8.2 and TensorFlow 0.10.0.

: Updated for Keras 1.1.0, Theano 0.8.2 and TensorFlow 0.10.0. Update Sep/2019: Updated for Keras 2.2.5 API.

What is Keras?

Keras is a minimalist Python library for deep learning that can run on top of Theano or TensorFlow.

It was developed to make implementing deep learning models as fast and easy as possible for research and development.

It runs on Python 2.7 or 3.5 and can seamlessly execute on GPUs and CPUs given the underlying frameworks. It is released under the permissive MIT license.

Keras was developed and maintained by François Chollet, a Google engineer using four guiding principles:

Modularity : A model can be understood as a sequence or a graph alone. All the concerns of a deep learning model are discrete components that can be combined in arbitrary ways.

: A model can be understood as a sequence or a graph alone. All the concerns of a deep learning model are discrete components that can be combined in arbitrary ways. Minimalism : The library provides just enough to achieve an outcome, no frills and maximizing readability.

: The library provides just enough to achieve an outcome, no frills and maximizing readability. Extensibility : New components are intentionally easy to add and use within the framework, intended for researchers to trial and explore new ideas.

: New components are intentionally easy to add and use within the framework, intended for researchers to trial and explore new ideas. Python: No separate model files with custom file formats. Everything is native Python.

How to Install Keras

Keras is relatively straightforward to install if you already have a working Python and SciPy environment.

You must also have an installation of Theano or TensorFlow on your system already.

You can see installation instructions for both platforms here:

Keras can be installed easily using PyPI, as follows:

sudo pip install keras 1 sudo pip install keras

At the time of writing, the most recent version of Keras is version 2.2.5. You can check your version of Keras on the command line using the following snippet:

You can check your version of Keras on the command line using the following snippet:

python -c ""import keras; print keras.__version__"" 1 python -c ""import keras; print keras.__version__""

Running the above script you will see:

2.2.5 1 2.2.5

You can upgrade your installation of Keras using the same method:

sudo pip install --upgrade keras 1 sudo pip install --upgrade keras

Theano and TensorFlow Backends for Keras

Assuming you have both Theano and TensorFlow installed, you can configure the backend used by Keras.

The easiest way is by adding or editing the Keras configuration file in your home directory:

~/.keras/keras.json 1 ~/.keras/keras.json

Which has the format:

{ ""image_data_format"": ""channels_last"", ""backend"": ""tensorflow"", ""epsilon"": 1e-07, ""floatx"": ""float32"" } 1 2 3 4 5 6 { ""image_data_format"": ""channels_last"", ""backend"": ""tensorflow"", ""epsilon"": 1e-07, ""floatx"": ""float32"" }

In this configuration file you can change the “backend” property from “tensorflow” (the default) to “theano“. Keras will then use the configuration the next time it is run.

You can confirm the backend used by Keras using the following snippet on the command line:

python -c ""from keras import backend; print(backend.backend())"" 1 python -c ""from keras import backend; print(backend.backend())""

Running this with default configuration you will see:

Using TensorFlow backend. tensorflow 1 2 Using TensorFlow backend. tensorflow

You can also specify the backend to use by Keras on the command line by specifying the KERAS_BACKEND environment variable, as follows:

KERAS_BACKEND=theano python -c ""from keras import backend; print(backend.backend())"" 1 KERAS_BACKEND=theano python -c ""from keras import backend; print(backend.backend())""

Running this example prints:

Using Theano backend. theano 1 2 Using Theano backend. theano

Need help with Deep Learning in Python? Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with code). Click to sign-up now and also get a free PDF Ebook version of the course. Start Your FREE Mini-Course Now!

Build Deep Learning Models with Keras

The focus of Keras is the idea of a model.

The main type of"
336;336;news.mit.edu;http://news.mit.edu/2020/bridging-gap-between-human-and-machine-vision-0211;;Bridging the gap between human and machine vision;"Suppose you look briefly from a few feet away at a person you have never met before. Step back a few paces and look again. Will you be able to recognize her face? “Yes, of course,” you probably are thinking. If this is true, it would mean that our visual system, having seen a single image of an object such as a specific face, recognizes it robustly despite changes to the object’s position and scale, for example. On the other hand, we know that state-of-the-art classifiers, such as vanilla deep networks, will fail this simple test.

In order to recognize a specific face under a range of transformations, neural networks need to be trained with many examples of the face under the different conditions. In other words, they can achieve invariance through memorization, but cannot do it if only one image is available. Thus, understanding how human vision can pull off this remarkable feat is relevant for engineers aiming to improve their existing classifiers. It also is important for neuroscientists modeling the primate visual system with deep networks. In particular, it is possible that the invariance with one-shot learning exhibited by biological vision requires a rather different computational strategy than that of deep networks.

A new paper by MIT PhD candidate in electrical engineering and computer science Yena Han and colleagues in Nature Scientific Reports entitled “Scale and translation-invariance for novel objects in human vision” discusses how they study this phenomenon more carefully to create novel biologically inspired networks.

""Humans can learn from very few examples, unlike deep networks. This is a huge difference with vast implications for engineering of vision systems and for understanding how human vision really works,"" states co-author Tomaso Poggio — director of the Center for Brains, Minds and Machines (CBMM) and the Eugene McDermott Professor of Brain and Cognitive Sciences at MIT. ""A key reason for this difference is the relative invariance of the primate visual system to scale, shift, and other transformations. Strangely, this has been mostly neglected in the AI community, in part because the psychophysical data were so far less than clear-cut. Han's work has now established solid measurements of basic invariances of human vision.”

To differentiate invariance rising from intrinsic computation with that from experience and memorization, the new study measured the range of invariance in one-shot learning. A one-shot learning task was performed by presenting Korean letter stimuli to human subjects who were unfamiliar with the language. These letters were initially presented a single time under one specific condition and tested at different scales or positions than the original condition. The first experimental result is that — just as you guessed — humans showed significant scale-invariant recognition after only a single exposure to these novel objects. The second result is that the range of position-invariance is limited, depending on the size and placement of objects.

Next, Han and her colleagues performed a comparable experiment in deep neural networks designed to reproduce this human performance. The results suggest that to explain invariant recognition of objects by humans, neural network models should explicitly incorporate built-in scale-invariance. In addition, limited position-invariance of human vision is better replicated in the network by having the model neurons’ receptive fields increase as they are further from the center of the visual field. This architecture is different from commonly used neural network models, where an image is processed under uniform resolution with the same shared filters.

“Our work provides a new understanding of the brain representation of objects under different viewpoints. It also has implications for AI, as the results provide new insights into what is a good architectural design for deep neural networks,” remarks Han, CBMM researcher and lead author of the study.

Han and Poggio were joined by Gemma Roig and Gad Geiger in the work."
337;337;news.mit.edu;http://news.mit.edu/2020/artificial-intelligence-identifies-new-antibiotic-0220;;Artificial intelligence yields new antibiotic;"Using a machine-learning algorithm, MIT researchers have identified a powerful new antibiotic compound. In laboratory tests, the drug killed many of the world’s most problematic disease-causing bacteria, including some strains that are resistant to all known antibiotics. It also cleared infections in two different mouse models.

The computer model, which can screen more than a hundred million chemical compounds in a matter of days, is designed to pick out potential antibiotics that kill bacteria using different mechanisms than those of existing drugs.

“We wanted to develop a platform that would allow us to harness the power of artificial intelligence to usher in a new age of antibiotic drug discovery,” says James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering. “Our approach revealed this amazing molecule which is arguably one of the more powerful antibiotics that has been discovered.”

In their new study, the researchers also identified several other promising antibiotic candidates, which they plan to test further. They believe the model could also be used to design new drugs, based on what it has learned about chemical structures that enable drugs to kill bacteria.

“The machine learning model can explore, in silico, large chemical spaces that can be prohibitively expensive for traditional experimental approaches,” says Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL).

Barzilay and Collins, who are faculty co-leads for MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health (J-Clinic), are the senior authors of the study, which appears today in Cell. The first author of the paper is Jonathan Stokes, a postdoc at MIT and the Broad Institute of MIT and Harvard.

A new pipeline

Over the past few decades, very few new antibiotics have been developed, and most of those newly approved antibiotics are slightly different variants of existing drugs. Current methods for screening new antibiotics are often prohibitively costly, require a significant time investment, and are usually limited to a narrow spectrum of chemical diversity.

“We’re facing a growing crisis around antibiotic resistance, and this situation is being generated by both an increasing number of pathogens becoming resistant to existing antibiotics, and an anemic pipeline in the biotech and pharmaceutical industries for new antibiotics,” Collins says.

To try to find completely novel compounds, he teamed up with Barzilay, Professor Tommi Jaakkola, and their students Kevin Yang, Kyle Swanson, and Wengong Jin, who have previously developed machine-learning computer models that can be trained to analyze the molecular structures of compounds and correlate them with particular traits, such as the ability to kill bacteria.

The idea of using predictive computer models for “in silico” screening is not new, but until now, these models were not sufficiently accurate to transform drug discovery. Previously, molecules were represented as vectors reflecting the presence or absence of certain chemical groups. However, the new neural networks can learn these representations automatically, mapping molecules into continuous vectors which are subsequently used to predict their properties.

In this case, the researchers designed their model to look for chemical features that make molecules effective at killing E. coli. To do so, they trained the model on about 2,500 molecules, including about 1,700 FDA-approved drugs and a set of 800 natural products with diverse structures and a wide range of bioactivities.

Once the model was trained, the researchers tested it on the Broad Institute’s Drug Repurposing Hub, a library of about 6,000 compounds. The model picked out one molecule that was predicted to have strong antibacterial activity and had a chemical structure different from any existing antibiotics. Using a different machine-learning model, the researchers also showed that this molecule would likely have low toxicity to human cells.

This molecule, which the researchers decided to call halicin, after the fictional artificial intelligence system from “2001: A Space Odyssey,” has been previously investigated as possible diabetes drug. The researchers tested it against dozens of bacterial strains isolated from patients and grown in lab dishes, and found that it was able to kill many that are resistant to treatment, including Clostridium difficile, Acinetobacter baumannii, and Mycobacterium tuberculosis. The drug worked against every species that they tested, with the exception of Pseudomonas aeruginosa, a difficult-to-treat lung pathogen.

To test halicin’s effectiveness in living animals, the researchers used it to treat mice infected with A. baumannii, a bacterium that has infected many U.S. soldiers stationed in Iraq a"
338;338;machinelearningmastery.com;http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/;2019-08-11;A Tour of Machine Learning Algorithms;"Tweet Share Share

Last Updated on December 5, 2019

In this post, we will take a tour of the most popular machine learning algorithms.

It is useful to tour the main algorithms in the field to get a feeling of what methods are available.

There are so many algorithms that it can feel overwhelming when algorithm names are thrown around and you are expected to just know what they are and where they fit.

I want to give you two ways to think about and categorize the algorithms you may come across in the field.

The first is a grouping of algorithms by their learning style .

. The second is a grouping of algorithms by their similarity in form or function (like grouping similar animals together).

Both approaches are useful, but we will focus in on the grouping of algorithms by similarity and go on a tour of a variety of different algorithm types.

After reading this post, you will have a much better understanding of the most popular machine learning algorithms for supervised learning and how they are related.

Discover how machine learning algorithms work including kNN, decision trees, naive bayes, SVM, ensembles and much more in my new book, with 22 tutorials and examples in excel.

Let’s get started.

Algorithms Grouped by Learning Style

There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data.

It is popular in machine learning and artificial intelligence textbooks to first consider the learning styles that an algorithm can adopt.

There are only a few main learning styles or learning models that an algorithm can have and we’ll go through them here with a few examples of algorithms and problem types that they suit.

This taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.

Let’s take a look at three different learning styles in machine learning algorithms:

1. Supervised Learning

Input data is called training data and has a known label or result such as spam/not-spam or a stock price at a time.

A model is prepared through a training process in which it is required to make predictions and is corrected when those predictions are wrong. The training process continues until the model achieves a desired level of accuracy on the training data.

Example problems are classification and regression.

Example algorithms include: Logistic Regression and the Back Propagation Neural Network.

2. Unsupervised Learning

Input data is not labeled and does not have a known result.

A model is prepared by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to systematically reduce redundancy, or it may be to organize data by similarity.

Example problems are clustering, dimensionality reduction and association rule learning.

Example algorithms include: the Apriori algorithm and K-Means.

3. Semi-Supervised Learning

Input data is a mixture of labeled and unlabelled examples.

There is a desired prediction problem but the model must learn the structures to organize the data as well as make predictions.

Example problems are classification and regression.

Example algorithms are extensions to other flexible methods that make assumptions about how to model the unlabeled data.

Overview of Machine Learning Algorithms

When crunching data to model business decisions, you are most typically using supervised and unsupervised learning methods.

A hot topic at the moment is semi-supervised learning methods in areas such as image classification where there are large datasets with very few labeled examples.

Algorithms Grouped By Similarity

Algorithms are often grouped by similarity in terms of their function (how they work). For example, tree-based methods, and neural network inspired methods.

I think this is the most useful way to group algorithms and it is the approach we will use here.

This is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories like Learning Vector Quantization that is both a neural network inspired method and an instance-based method. There are also categories that have the same name that describe the problem and the class of algorithm such as Regression and Clustering.

We could handle these cases by listing algorithms twice or by selecting the group that subjectively is the “best” fit. I like this latter approach of not duplicating algorithms to keep things simple.

In this section, we list many of the popular machine learning algorithms grouped the way we think is the most intuitive. The list is not exhaustive in either the groups or the algorithms, but I think it is representative and will be useful to you to get an idea of the lay of the land.
"
339;339;news.mit.edu;http://news.mit.edu/2020/researching-from-home-picower-science-stays-strong-even-at-distance-0407;;Researching from home: Science stays social, even at a distance;"With all but a skeleton crew staying home from each lab to minimize the spread of Covid-19, scores of Picower Institute researchers are immersing themselves in the considerable amount of scientific work that can done away from the bench. With piles of data to analyze; plenty of manuscripts to write; new skills to acquire; and fresh ideas to conceive, share, and refine for the future, neuroscientists have full plates, even when they are away from their, well, plates. They are proving that science can remain social, even if socially distant.

Ever since the mandatory ramp down of on-campus research took hold March 20, for example, teams of researchers in the lab of Troy Littleton, the Menicon Professor of Neuroscience, have sharpened their focus on two data-analysis projects that are every bit as essential to their science as acquiring the data in the lab in the first place. Research scientist Yulia Akbergenova and graduate student Karen Cunningham, for example, are poring over a huge amount of imaging data showing how the strength of connections between neurons, or synapses, mature and how that depends on the molecular components at the site. Another team, comprised of Picower postdoc Suresh Jetti and graduate students Andres Crane and Nicole Aponte-Santiago, is analyzing another large dataset, this time of gene transcription, to learn what distinguishes two subclasses of motor neurons that form synapses of characteristically different strength.

Work is similarly continuing among researchers in the lab of Elly Nedivi, the William R. (1964) and Linda R. Young Professor of Neuroscience. Since heading home, Senior Research Support Associate Kendyll Burnell has been looking at microscope images tracking how inhibitory interneurons innervate the visual cortex of mice throughout their development. By studying the maturation of inhibition, the lab hopes to improve understanding of the role of inhibitory circuitry in the experience-dependent changes, or plasticity, and development of the visual cortex, she says. As she’s worked, her poodle Soma (named for the central body structure of a neuron) has been by her side.

Despite extra time with comforts of home, though, it’s clear that nobody wanted this current mode of socially distant science. For every lab, it’s tremendously disruptive and costly. But labs are finding many ways to make progress nonetheless.

“Although we are certainly hurting because our lab work is at a standstill, the Miller lab is fortunate to have a large library of multiple-electrode neurophysiological data,” says Picower Professor Earl Miller. “The datasets are very rich. As our hypotheses and analytical tools develop, we can keep going back to old data to ask new questions. We are taking advantage of the wet lab downtime to analyze data and write papers. We have three under review and are writing at least three more right now.”

Miller is inviting new collaborations regardless of the physical impediment of social distancing. A recent lab meeting held via the videoconferencing app Zoom included MIT Department of Brain and Cognitive Sciences Associate Professor Ila Fiete and her graduate student, Mikail Khona. The Miller lab has begun studying how neural rhythms move around the cortex and what that means for brain function. Khona presented models of how timing relationships affect those waves. While this kind of an interaction between labs of the Picower Institute and the McGovern Institute for Brain Research would normally have taken place in person in MIT’s Building 46, neither lab let the pandemic get in the way.

Similarly, the lab of Li-Huei Tsai, Picower Professor and director of the Picower Institute, has teamed up with that of Manolis Kellis, professor in the MIT Computer Science and Artificial Intelligence Laboratory. They’re forming several small squads of experimenters and computational experts to launch analyses of gene expression and other data to illuminate the fate of individual cell types like interneurons or microglia in the context of the Alzheimer’s disease-afflicted brain. Other teams are focusing on analyses of questions such as how pathology varies in brain samples carrying different degrees of genetic risk factors. These analyses will prove useful for stages all along the scientific process, Tsai says, from forming new hypotheses to wrapping up papers that are well underway.

Remote collaboration and communication are proving crucial to researchers in other ways, too, proving that online interactions, though distant, can be quite personally fulfilling.

Nicholas DiNapoli, a research engineer in the lab of Associate Professor Kwanghun Chung, is making the best of time away from the bench by learning about the lab’s computational pipeline for processing the enormous amounts of imaging data it generates. He’s also taking advantage of a new program within the lab in which Senior Computer Scientist Lee Kamentsky is teaching Python computer programming principles to anyone in "
340;340;machinelearningmastery.com;https://machinelearningmastery.com/how-to-predict-room-occupancy-based-on-environmental-factors/;2018-08-28;How to Predict Room Occupancy Based on Environmental Factors;"from pandas import read_csv

from matplotlib import pyplot

# load all data

data1 = read_csv ( 'datatest.txt' , header = 0 , index_col = 1 , parse_dates = True , squeeze = True )

data2 = read_csv ( 'datatraining.txt' , header = 0 , index_col = 1 , parse_dates = True , squeeze = True )

data3 = read_csv ( 'datatest2.txt' , header = 0 , index_col = 1 , parse_dates = True , squeeze = True )

# determine the number of features

n_features = data1 . values . shape [ 1 ]

pyplot . figure ( )

for i in range ( 1 , n_features ) :

# specify the subpout

pyplot . subplot ( n_features , 1 , i )

# plot data from each set

pyplot . plot ( data1 . index , data1 . values [ : , i ] )

pyplot . plot ( data2 . index , data2 . values [ : , i ] )

pyplot . plot ( data3 . index , data3 . values [ : , i ] )

# add a readable name to the plot

pyplot . title ( data1 . columns [ i ] , y = 0.5 , loc = 'right' )"
341;341;machinelearningmastery.com;https://machinelearningmastery.com/suitability-long-short-term-memory-networks-time-series-forecasting/;2017-05-25;On the Suitability of Long Short-Term Memory Networks for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Long Short-Term Memory (LSTM) is a type of recurrent neural network that can learn the order dependence between items in a sequence.

LSTMs have the promise of being able to learn the context required to make predictions in time series forecasting problems, rather than having this context pre-specified and fixed.

Given the promise, there is some doubt as to whether LSTMs are appropriate for time series forecasting.

In this post, we will look at the application of LSTMs to time series forecasting by some of the leading developers of the technique.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

LSTM for Time Series Forecasting

We will take a closer look at a paper that seeks to explore the suitability of LSTMs for time series forecasting.

The paper is titled “Applying LSTM to Time Series Predictable through Time-Window Approaches” (get the PDF, Gers, Eck and Schmidhuber, published in 2001.

They start off by commenting that univariate time series forecasting problems are actually simpler than the types of problems traditionally used to demonstrate the capabilities of LSTMs.

Time series benchmark problems found in the literature … are often conceptually simpler than many tasks already solved by LSTM. They often do not require RNNs at all, because all relevant information about the next event is conveyed by a few recent events contained within a small time window.

The paper focuses on the application of LSTMs to two complex time series forecasting problems and contrasting the results of LSTMs to other types of neural networks.

The focus of the study are two classical time series problems:

Mackey-Glass Series

This is a contrived time series calculated from a differential equation.

For more information, see:

Chaotic Laser Data (Set A)

This is a series taken from a from a contest at the Santa Fe Institute.

Set A is defined as:

A clean physics laboratory experiment. 1,000 points of the fluctuations in a far-infrared laser, approximately described by three coupled nonlinear ordinary differential equations.

For more information, see:

Section 2 “The Competition” in The Future of Time Series, 1993.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Autoregression

An autoregression (AR) approach was used to model these problems.

This means that the next time step was taken as a function of some number of past (or lag) observations.

This is a common approach for classical statistical time series forecasting.

The LSTM is exposed to one input at a time with no fixed set of lag variables, as the windowed-multilayer Perceptron (MLP).

For more information on AR for time series, see the post:

Analysis of Results

Some of the more salient comments were in response to the poor results of the LSTMs on the Mackey-Glass Series problem.

First, they comment that increasing the learning capacity of the network did not help:

Increasing the number of memory blocks did not significantly improve the results.

This may have required a further increase in the number of training epochs. It is also possible that a stack of LSTMs may have improved results.

They comment that in order to do well on the Mackey-Glass Series, the LSTM is required to remember recent past observations, whereas the MLP is given this data explicitly.

The results for the AR-LSTM approach are clearly worse than the results for the time window approaches, for example with MLPs. The AR-LSTM network does not have access to the past as part of its input … [for the LSTM to do well] required remembering one or two events from the past, then using this information before over-writing the same memory cells.

They comment that in general, this poses more of a challenge for LSTMs and RNNs than it does for MLPs.

Assuming that any dynamic model needs all inputs from t-tau …, we note that the AR-RNN has to store all inputs from t-tau to t and overwrite them at the adequate time. This requires the implementation of a circular buffer, a structure quite difficult for an RNN to simulate.

Again, I can’t help but think that a much larger hidden layer (more memory units) and a much deeper network (stacked LSTMs) would be better suited to learn multiple past observations.

They later conclude the paper and discuss that based on the results, LSTMs may not be suited to AR type formulations of time series forecasting, at least when the lagged observations are close to the time being forecasted.

This is a fair conclusion given the LSTMs performance compared to MLPs on the tested univariate problems.

A time window based MLP outperformed the LSTM pure-AR approach on certain time series prediction benchmarks solvab"
342;342;machinelearningmastery.com;https://machinelearningmastery.com/findings-comparing-classical-and-machine-learning-methods-for-time-series-forecasting/;2018-10-30;Comparing Classical and Machine Learning Algorithms for Time Series Forecasting;"Tweet Share Share

Last Updated on August 5, 2019

Machine learning and deep learning methods are often reported to be the key solution to all predictive modeling problems.

An important recent study evaluated and compared the performance of many classical and modern machine learning and deep learning methods on a large and diverse set of more than 1,000 univariate time series forecasting problems.

The results of this study suggest that simple classical methods, such as linear methods and exponential smoothing, outperform complex and sophisticated methods, such as decision trees, Multilayer Perceptrons (MLP), and Long Short-Term Memory (LSTM) network models.

These findings highlight the requirement to both evaluate classical methods and use their results as a baseline when evaluating any machine learning and deep learning methods for time series forecasting in order demonstrate that their added complexity is adding skill to the forecast.

In this post, you will discover the important findings of this recent study evaluating and comparing the performance of a classical and modern machine learning methods on a large and diverse set of time series forecasting datasets.

After reading this post, you will know:

Classical methods like ETS and ARIMA out-perform machine learning and deep learning methods for one-step forecasting on univariate datasets.

Classical methods like Theta and ARIMA out-perform machine learning and deep learning methods for multi-step forecasting on univariate datasets.

Machine learning and deep learning methods do not yet deliver on their promise for univariate time series forecasting, and there is much work to do.

Discover how to build models for multivariate and multi-step time series forecasting with LSTMs and more in my new book, with 25 step-by-step tutorials and full source code.

Let’s get started.

Overview

Spyros Makridakis, et al. published a study in 2018 titled “Statistical and Machine Learning forecasting methods: Concerns and ways forward.”

In this post, we will take a close look at the study by Makridakis, et al. that carefully evaluated and compared classical time series forecasting methods to the performance of modern machine learning methods.

This post is divided into seven sections; they are:

Study Motivation Time Series Datasets Time Series Forecasting Methods Data Preparation One-Step Forecasting Results Multi-Step Forecasting Results Outcomes

Study Motivation

The goal of the study was to clearly demonstrate the capability of a suite of different machine learning methods as compared to classical time series forecasting methods on a very large and diverse collection of univariate time series forecasting problems.

The study was a response to the increasing number of papers and claims that machine learning and deep learning methods offer superior results for time series forecasting with little objective evidence.

Literally hundreds of papers propose new ML algorithms, suggesting methodological advances and accuracy improvements. Yet, limited objective evidence is available regarding their relative performance as a standard forecasting tool.

— Statistical and Machine Learning forecasting methods: Concerns and ways forward, 2018.

The authors clearly lay out three issues with the flood of claims; they are:

Their conclusions are based on a few, or even a single time series, raising questions about the statistical significance of the results and their generalization.

The methods are evaluated for short-term forecasting horizons, often one-step-ahead, not considering medium and long-term ones.

No benchmarks are used to compare the accuracy of ML methods versus alternative ones.

As a response, the study includes eight classical methods and 10 machine learning methods evaluated using one-step and multiple-step forecasts across a collection of 1,045 monthly time series.

Although not definitive, the results are intended to be objective and robust.

Need help with Deep Learning for Time Series? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course

Time Series Datasets

The time series datasets used in the study were drawn from the time series datasets used in the M3-Competition.

The M3-Competition was the third in a series of competitions that sought to discover exactly what algorithms perform well in practice on real time series forecasting problems. The results of the competition were published in the 2000 paper titled “The M3-Competition: Results, Conclusions and Implications.”

The datasets used in the competition were drawn from a wide range of industries and had a range of different time intervals, from hourly to annual.

The 3003 series of the M3-Competition were selected on a quota basis to include various types of time series data (micro, industry, macro, etc.) and different time intervals between successive observations (yearly, quarterly, et"
343;343;machinelearningmastery.com;http://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/;2016-07-27;Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras;"# LSTM with Variable Length Input Sequences to One Character Output

import numpy

from keras . models import Sequential

from keras . layers import Dense

from keras . layers import LSTM

from keras . utils import np_utils

from keras . preprocessing . sequence import pad_sequences

from theano . tensor . shared_randomstreams import RandomStreams

# fix random seed for reproducibility

numpy . random . seed ( 7 )

# define the raw dataset

alphabet = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""

# create mapping of characters to integers (0-25) and the reverse

char_to_int = dict ( ( c , i ) for i , c in enumerate ( alphabet ) )

int_to_char = dict ( ( i , c ) for i , c in enumerate ( alphabet ) )

# prepare the dataset of input to output pairs encoded as integers

num_inputs = 1000

max_len = 5

dataX = [ ]

dataY = [ ]

for i in range ( num_inputs ) :

start = numpy . random . randint ( len ( alphabet ) - 2 )

end = numpy . random . randint ( start , min ( start + max_len , len ( alphabet ) - 1 ) )

sequence_in = alphabet [ start : end + 1 ]

sequence_out = alphabet [ end + 1 ]

dataX . append ( [ char_to_int [ char ] for char in sequence_in ] )

dataY . append ( char_to_int [ sequence_out ] )

print ( sequence_in , '->' , sequence_out )

# convert list of lists to array and pad sequences if needed

X = pad_sequences ( dataX , maxlen = max_len , dtype = 'float32' )

# reshape X to be [samples, time steps, features]

X = numpy . reshape ( X , ( X . shape [ 0 ] , max_len , 1 ) )

# normalize

X = X / float ( len ( alphabet ) )

# one hot encode the output variable

y = np_utils . to_categorical ( dataY )

# create and fit the model

batch_size = 1

model = Sequential ( )

model . add ( LSTM ( 32 , input_shape = ( X . shape [ 1 ] , 1 ) ) )

model . add ( Dense ( y . shape [ 1 ] , activation = 'softmax' ) )

model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ] )

model . fit ( X , y , epochs = 500 , batch_size = batch_size , verbose = 2 )

# summarize performance of the model

scores = model . evaluate ( X , y , verbose = 0 )

print ( ""Model Accuracy: %.2f%%"" % ( scores [ 1 ] * 100 ) )

# demonstrate some model predictions

for i in range ( 20 ) :

pattern_index = numpy . random . randint ( len ( dataX ) )

pattern = dataX [ pattern_index ]

x = pad_sequences ( [ pattern ] , maxlen = max_len , dtype = 'float32' )

x = numpy . reshape ( x , ( 1 , max_len , 1 ) )

x = x / float ( len ( alphabet ) )

prediction = model . predict ( x , verbose = 0 )

index = numpy . argmax ( prediction )

result = int_to_char [ index ]

seq_in = [ int_to_char [ value ] for value in pattern ]"
344;344;news.mit.edu;http://news.mit.edu/2020/urine-sensor-test-detect-lung-tumors-0401;;New sensors could offer early detection of lung tumors;"People who are at high risk of developing lung cancer, such as heavy smokers, are routinely screened with computed tomography (CT), which can detect tumors in the lungs. However, this test has an extremely high rate of false positives, as it also picks up benign nodules in the lungs.

Researchers at MIT have now developed a new approach to early diagnosis of lung cancer: a urine test that can detect the presence of proteins linked to the disease. This kind of noninvasive test could reduce the number of false positives and help detect more tumors in the early stages of the disease.

Early detection is very important for lung cancer, as the five-year survival rates are at least six times higher in patients whose tumors are detected before they spread to distant locations in the body.

“If you look at the field of cancer diagnostics and therapeutics, there’s a renewed recognition of the importance of early cancer detection and prevention. We really need new technologies that are going to give us the capability to see cancer when we can intercept it and intervene early,” says Sangeeta Bhatia, who is the John and Dorothy Wilson Professor of Health Sciences and Technology and Electrical Engineering and Computer Science, and a member of MIT’s Koch Institute for Integrative Cancer Research and the Institute for Medical Engineering and Science.

Bhatia and her colleagues found that the new test, which is based on nanoparticles that can be injected or inhaled, could detect tumors as small as 2.8 cubic millimeters in mice.

Bhatia is the senior author of the study, which appears today in Science Translational Medicine. The paper’s lead authors are MIT and Harvard University graduate students Jesse Kirkpatrick and Ava Soleimany, and former MIT graduate student Andrew Warren, who is now an associate at Third Rock Ventures.

Targeting lung tumors

For several years, Bhatia’s lab has been developing nanoparticles that can detect cancer by interacting with enzymes called proteases. These enzymes help tumor cells to escape their original locations by cutting through proteins of the extracellular matrix.

To find those proteins, Bhatia created nanoparticles coated with peptides (short protein fragments) that are targeted by cancer-linked proteases. The particles accumulate at tumor sites, where the peptides are cleaved, releasing biomarkers that can then be detected in a urine sample.

Her lab has previously developed sensors for colon and ovarian cancer, and in their new study, the researchers wanted to apply the technology to lung cancer, which kills about 150,000 people in the United States every year. People who receive a CT screen and get a positive result often undergo a biopsy or other invasive test to search for lung cancer. In some cases, this procedure can cause complications, so a noninvasive follow-up test could be useful to determine which patients actually need a biopsy, Bhatia says.

“The CT scan is a good tool that can see a lot of things,” she says. “The problem with it is that 95 percent of what it finds is not cancer, and right now you have to biopsy too many patients who test positive.”

To customize their sensors for lung cancer, the researchers analyzed a database of cancer-related genes called the Cancer Genome Atlas and identified proteases that are abundant in lung cancer. They created a panel of 14 peptide-coated nanoparticles that could interact with these enzymes.

The researchers then tested the sensors in two different mouse models of cancer, both of which are engineered with genetic mutations that lead them to naturally develop lung tumors. To help prevent background noise that could come from other organs or the bloodstream, the researchers injected the particles directly into the airway.

Using these sensors, the researchers performed their diagnostic test at three time points: 5 weeks, 7.5 weeks, and 10.5 weeks after tumor growth began. To make the diagnoses more accurate, they used machine learning to train an algorithm to distinguish between data from mice that had tumors and mice that did not.

With this approach, the researchers found that they could accurately detect tumors in one of the mouse models as early as 7.5 weeks, when the tumors were only 2.8 cubic millimeters, on average. In the other strain of mice, tumors could be detected at 5 weeks. The sensors’ success rate was also comparable to or better than the success rate of CT scans performed at the same time points.

Reducing false positives

The researchers also found that the sensors have another important ability — they can distinguish between early-stage cancer and noncancerous inflammation of the lungs. Lung inflammation, common in people who smoke, is one of the reasons that CT scans produce so many false positives.

Bhatia envisions that the nanoparticle sensors could be used as a noninvasive diagnostic for people who get a positive result on a screening test, potentially eliminating the need for a biopsy. For use in huma"
345;345;news.mit.edu;http://news.mit.edu/2019/mit-policy-hackathon-connects-data-driven-problem-solvers-0521;;MIT Policy Hackathon connects data-driven problem solvers;"As the size, complexity, and interconnection of societal systems increase, these systems generate huge amounts of data that can lead to new insights. These data create an opportunity for policymakers aiming to address major societal challenges, provided they have the tools to understand the data and use them for better decision-making.

At a unique MIT event convened by MIT’s Technology and Policy Program (TPP), a part of the Institute for Data, Systems, and Society (IDSS), interdisciplinary teams analyzed data sets and created policy proposals to real challenges submitted by academic groups and local government. The student-run MIT Policy Hackathon gathered data analysts, engineers, scientists, domain experts, and policy specialists to look for creative, data-driven solutions addressing major societal issues.

“One of the goals of the hackathon is to show others the power of using technology and policy together to craft solutions to important societal problems,” says Becca Browder, a Policy Hackathon organizer and student in TPP. “I think the event achieved that goal.”

The hackathon teams worked over 48 hours on one of five challenges in the areas of climate, health, artificial intelligence and ethics, urban planning, and the future of work. The hackathon ended in a proposal pitch session to a panel of judges from academia, government, and industry.

In the climate challenge, sponsored by the City of Boston, teams examined precipitation data to help the city prepare for increased flooding due to climate change.

“The city is taking climate change very seriously,” says Charlie Jewell, director of planning and sustainability for the Boston Water and Sewer Commission. After mentoring and judging the climate challenge, Jewell said there was a “good give-and-take” to be had from partnering with local universities. “The organizers and participants all did such an unbelievable job. I got some great ideas from participants for looking at our rainfall data in different ways. They also showed what kind of data they needed and how we could get it.”

Hackathon participant Minghao Qiu, a student at IDSS in the Social and Engineering Systems doctoral program, also found the opportunity to work directly with stakeholders useful. “The interaction with the challenge sponsor helped me think about how to better communicate my research findings with policymakers in the future,” says Qiu, whose team GAMMDRYL also included TPP alumnus Arthur Yip SM ’14. GAMMDRYL won the climate challenge with a proposal recommending the city team up with a citizen science initiative that crowdsources rainfall data.

“I learned that it is often useful to help decision-makers to understand their data better,” Qiu says.

The overall winner of the hackathon was a team called Dream ER, who worked on the health challenge. This challenge, sponsored by Harvard School of Public Health graduate student Ahmed Mahmoud Abdelfattah, asked for ways to optimize emergency rooms by studying patient traffic and outcome data.

“By using creative visualization techniques, they simulated how their policy suggestions can result in an overall improvement in service efficiency,” Abdelfattah says of the winning team’s proposal. “Their proposal was also quite generalizable, meaning that those same methods they used to examine the data and simulate changes can be applied to other hospitals and other care settings.”

For the AI and ethics challenge, sponsored by the Berkman Klein Center for Internet and Society at Harvard University, teams worked to develop a resource, such as a visualization tool, to help nontechnical policy advocates understand different definitions of ""algorithmic fairness"" — especially in the context of criminal justice risk-assessment tools. Participants had access to data shared by journalists who evaluated COMPAS, a widely-used recidivism risk scoring tool.

The urban planning challenge, sponsored by the City of Boston’s Department of Innovation and Technology, tasked participants with assessing the impact of AirBnB on neighborhood economies and Boston’s affordable housing crisis, using the city’s short-term rental data. The future of work challenge, posed by the MIT Initiative on the Digital Economy (IDE), asked for a broad exploration of the potential for machine learning to automate tasks. Using a data set of work activities put together by researchers at MIT and Carnegie Mellon University, this challenge asked for policy proposals that help predict and prepare for the impact of machine learning automation on industries and workers.

This was the third MIT Policy Hackathon: an inaugural hackathon was held in spring 2018, and another was organized for Boston Hubweek in fall 2018. Students hope to make it a fixture of the program. “IDSS and TPP work on how policy and society interact with science and technology, and how we can use data to enhance policy,” Browder says. “These are also main goals of the hackathon, so there is strong strategic alig"
346;346;news.mit.edu;http://news.mit.edu/2020/new-model-quantifies-impact-quarantine-measures-covid-19-spread-0416;;Model quantifies the impact of quarantine measures on Covid-19’s spread;"The research described in this article has been published on a preprint server but has not yet been peer-reviewed by scientific or medical experts.

Every day for the past few weeks, charts and graphs plotting the projected apex of Covid-19 infections have been splashed across newspapers and cable news. Many of these models have been built using data from studies on previous outbreaks like SARS or MERS. Now, a team of engineers at MIT has developed a model that uses data from the Covid-19 pandemic in conjunction with a neural network to determine the efficacy of quarantine measures and better predict the spread of the virus.

“Our model is the first which uses data from the coronavirus itself and integrates two fields: machine learning and standard epidemiology,” explains Raj Dandekar, a PhD candidate studying civil and environmental engineering. Together with George Barbastathis, professor of mechanical engineering, Dandekar has spent the past few months developing the model as part of the final project in class 2.168 (Learning Machines).

Most models used to predict the spread of a disease follow what is known as the SEIR model, which groups people into “susceptible,” “exposed,” “infected,” and “recovered.” Dandekar and Barbastathis enhanced the SEIR model by training a neural network to capture the number of infected individuals who are under quarantine, and therefore no longer spreading the infection to others.

The model finds that in places like South Korea, where there was immediate government intervention in implementing strong quarantine measures, the virus spread plateaued more quickly. In places that were slower to implement government interventions, like Italy and the United States, the “effective reproduction number” of Covid-19 remains greater than one, meaning the virus has continued to spread exponentially.

The machine learning algorithm shows that with the current quarantine measures in place, the plateau for both Italy and the United States will arrive somewhere between April 15-20. This prediction is similar to other projections like that of the Institute for Health Metrics and Evaluation.

“Our model shows that quarantine restrictions are successful in getting the effective reproduction number from larger than one to smaller than one,” says Barbastathis. “That corresponds to the point where we can flatten the curve and start seeing fewer infections.”

Quantifying the impact of quarantine

In early February, as news of the virus’ troubling infection rate started dominating headlines, Barbastathis proposed a project to students in class 2.168. At the end of each semester, students in the class are tasked with developing a physical model for a problem in the real world and developing a machine learning algorithm to address it. He proposed that a team of students work on mapping the spread of what was then simply known as “the coronavirus.”

“Students jumped at the opportunity to work on the coronavirus, immediately wanting to tackle a topical problem in typical MIT fashion,” adds Barbastathis.

One of those students was Dandekar. “The project really interested me because I got to apply this new field of scientific machine learning to a very pressing problem,” he says.

As Covid-19 started to spread across the globe, the scope of the project expanded. What had originally started as a project looking just at spread within Wuhan, China grew to also include the spread in Italy, South Korea, and the United States.

The duo started modeling the spread of the virus in each of these four regions after the 500th case was recorded. That milestone marked a clear delineation in how different governments implemented quarantine orders.

Armed with precise data from each of these countries, the research team took the standard SEIR model and augmented it with a neural network that learns how infected individuals under quarantine impact the rate of infection. They trained the neural network through 500 iterations so it could then teach itself how to predict patterns in the infection spread.

Using this model, the research team was able to draw a direct correlation between quarantine measures and a reduction in the effective reproduction number of the virus.

“The neural network is learning what we are calling the ‘quarantine control strength function,’” explains Dandekar. In South Korea, where strong measures were implemented quickly, the quarantine control strength function has been effective in reducing the number of new infections. In the United States, where quarantine measures have been slowly rolled out since mid-March, it has been more difficult to stop the spread of the virus.

Predicting the “plateau”

As the number of cases in a particular country decreases, the forecasting model transitions from an exponential regime to a linear one. Italy began entering this linear regime in early April, with the U.S. not far behind it.

The machine learning algorithm Dandekar and Barbastathis have developed p"
